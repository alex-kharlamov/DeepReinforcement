{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from __future__ import division\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape\n",
    "from keras.optimizers import sgd\n",
    "import os\n",
    "import random\n",
    "from os.path import isfile\n",
    "from collections import deque\n",
    "\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 3\n",
    "MAX_REPLAY_STATES = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_GAMES_TRAIN = 5\n",
    "JUMP_FPS = 4\n",
    "WEIGHT_FILE = 'weights.h5'\n",
    "\n",
    "\n",
    "replay = []\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "env = gym.make(\"Skiing-v0\")\n",
    "d = False\n",
    "\n",
    "layers = [\n",
    "    #Reshape((1, 250, 160, 3), input_shape=(250, 160, 3)),\n",
    "    Convolution2D(16, 7, 7, border_mode='same', input_shape=(250, 160, 3)),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2,2)),\n",
    "    Convolution2D(32, 5, 5),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(25, activation=\"relu\"),\n",
    "    Dense(output_dim=3, activation=\"relu\")\n",
    "]\n",
    "\n",
    "\n",
    "def t(st):\n",
    "    return st.reshape(1, 250, 160, 3)\n",
    "\n",
    "model = Sequential(layers)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "model.summary\n",
    "\n",
    "prb = 0.2\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "st = env.reset()\n",
    "st = t(st)\n",
    "for number_game in range(10):\n",
    "  new_state = env.reset()\n",
    "  reward_game = 0\n",
    "  done = False\n",
    "  loss = 0\n",
    "  index_train_per_game = 0\n",
    "  print( '[+] Starting Game ' + str(number_game))\n",
    "  while not done:\n",
    "    env.render()\n",
    "    index_train_per_game += 1\n",
    "    if random.random() < epsilon:\n",
    "      action = np.random.randint(NUM_ACTIONS)\n",
    "    else:\n",
    "      q = model.predict(t(new_state))[0]\n",
    "      action = np.argmax(q)\n",
    "        \n",
    "        \n",
    "    old_state = new_state\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    reward_game += reward\n",
    "    replay.append([new_state, reward, action, done, old_state])\n",
    "    \n",
    "    \n",
    "    if len(replay) > MAX_REPLAY_STATES:\n",
    "        replay.pop(np.random.randint(MAX_REPLAY_STATES) + 1)\n",
    "        \n",
    "    if JUMP_FPS != 1 and index_train_per_game % JUMP_FPS == 0:\n",
    "      continue\n",
    "    \n",
    "    \n",
    "    len_mini_batch = min(len(replay), BATCH_SIZE)\n",
    "    \n",
    "    mini_batch = random.sample(replay, len_mini_batch)\n",
    "    \n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    \n",
    "    for index_rep in range(len_mini_batch):\n",
    "      new_rep_state, reward_rep, action_rep, done_rep, old_rep_state = mini_batch[index_rep]\n",
    "      temp = model.predict(t(new_rep_state))\n",
    "      if index_rep % 10 == 0 and index_rep != 0:\n",
    "          print(temp, \"index = \", index_rep)\n",
    "      old_q = model.predict(t(old_rep_state))[0]\n",
    "      new_q = temp[0]\n",
    "      update_target = np.copy(old_q)\n",
    "      if done_rep:\n",
    "        update_target[action_rep] = -1\n",
    "      else:\n",
    "        update_target[action_rep] = reward_rep + (gamma * np.max(new_q))\n",
    "      X_train.append(old_rep_state)\n",
    "      Y_train.append(update_target)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "        \n",
    "        \n",
    "    loss += np.array(model.train_on_batch(X_train, Y_train))\n",
    "\n",
    "    if reward_game > 200:\n",
    "      break\n",
    "  print (\"[+] End Game {} | Reward {} | Epsilon {:.4f} | TrainPerGame {} | Loss {:.4f} \".format(number_game, reward_game, epsilon, index_train_per_game, loss / index_train_per_game * JUMP_FPS))\n",
    "  if epsilon >= 0.1:\n",
    "    epsilon -= (1 / (NUM_GAMES_TRAIN))\n",
    "  if isfile(WEIGHT_FILE):\n",
    "    os.remove(WEIGHT_FILE)\n",
    "  model.save_weights(WEIGHT_FILE)\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doobuchaisa sama \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
