{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-01-18 17:30:53,110] Making new env: Skiing-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Starting Game 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bea23cb53b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward_game\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"[+] End Game {} | Reward {} | Epsilon {:.4f} | TrainPerGame {} | Loss {:.4f} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_train_per_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mindex_train_per_game\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mJUMP_FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNUM_GAMES_TRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from __future__ import division\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape\n",
    "from keras.optimizers import sgd\n",
    "import os\n",
    "import random\n",
    "from os.path import isfile\n",
    "from collections import deque\n",
    "\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 3\n",
    "MAX_REPLAY_STATES = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_GAMES_TRAIN = 5\n",
    "JUMP_FPS = 4\n",
    "WEIGHT_FILE = 'weights.h5'\n",
    "\n",
    "\n",
    "replay = []\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "env = gym.make(\"Skiing-v0\")\n",
    "d = False\n",
    "\n",
    "layers = [\n",
    "    #Reshape((1, 250, 160, 3), input_shape=(250, 160, 3)),\n",
    "    Convolution2D(16, 7, 7, border_mode='same', input_shape=(250, 160, 3)),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2,2)),\n",
    "    Convolution2D(32, 5, 5),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(25, activation=\"relu\"),\n",
    "    Dense(output_dim=3, activation=\"relu\")\n",
    "]\n",
    "\n",
    "\n",
    "def t(st):\n",
    "    return st.reshape(1, 250, 160, 3)\n",
    "\n",
    "model = Sequential(layers)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "model.summary\n",
    "\n",
    "prb = 0.2\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "st = env.reset()\n",
    "st = t(st)\n",
    "for number_game in range(10):\n",
    "  new_state = env.reset()\n",
    "  reward_game = 0\n",
    "  done = False\n",
    "  loss = 0\n",
    "  index_train_per_game = 0\n",
    "  print( '[+] Starting Game ' + str(number_game))\n",
    "  while not done:\n",
    "    env.render()\n",
    "    index_train_per_game += 1\n",
    "    if random.random() < epsilon:\n",
    "      action = np.random.randint(NUM_ACTIONS)\n",
    "    else:\n",
    "      q = model.predict(t(new_state))[0]\n",
    "      action = np.argmax(q)\n",
    "        \n",
    "        \n",
    "    old_state = new_state\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    reward_game += reward\n",
    "    replay.append([new_state, reward, action, done, old_state])\n",
    "    \n",
    "    \n",
    "    if len(replay) > MAX_REPLAY_STATES:\n",
    "        replay.pop(np.random.randint(MAX_REPLAY_STATES) + 1)\n",
    "        \n",
    "    if JUMP_FPS != 1 and index_train_per_game % JUMP_FPS == 0:\n",
    "      continue\n",
    "    \n",
    "    \n",
    "    len_mini_batch = min(len(replay), BATCH_SIZE)\n",
    "    \n",
    "    mini_batch = random.sample(replay, len_mini_batch)\n",
    "    \n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    \n",
    "    for index_rep in range(len_mini_batch):\n",
    "      new_rep_state, reward_rep, action_rep, done_rep, old_rep_state = mini_batch[index_rep]\n",
    "      temp = model.predict(t(new_rep_state))\n",
    "      if index_rep % 10 == 0 and index_rep != 0:\n",
    "          print(temp, \"index = \", index_rep)\n",
    "      old_q = model.predict(t(old_rep_state))[0]\n",
    "      new_q = temp[0]\n",
    "      update_target = np.copy(old_q)\n",
    "      if done_rep:\n",
    "        update_target[action_rep] = -1\n",
    "      else:\n",
    "        update_target[action_rep] = reward_rep + (gamma * np.max(new_q))\n",
    "      X_train.append(old_rep_state)\n",
    "      Y_train.append(update_target)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "        \n",
    "        \n",
    "    loss += np.array(model.train_on_batch(X_train, Y_train))\n",
    "\n",
    "    if reward_game > 200:\n",
    "      break\n",
    "  loss_print = (loss / index_train_per_game * JUMP_FPS).tolist()\n",
    "  print (\"[+] End Game {} | Reward {} | Epsilon {:.4f} | TrainPerGame {} | Loss [{:.4f}, {:.4f}] \".format(number_game, reward_game, epsilon, index_train_per_game, loss_print[0], loss_print[1]))  if epsilon >= 0.1:\n",
    "    epsilon -= (1 / (NUM_GAMES_TRAIN))\n",
    "  if isfile(WEIGHT_FILE):\n",
    "    os.remove(WEIGHT_FILE)\n",
    "  model.save_weights(WEIGHT_FILE)\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] End Game 0 | Reward -16736.0 | Epsilon 1.0000 | TrainPerGame 1747 | Loss [-0.0000, 24.7219] \n"
     ]
    }
   ],
   "source": [
    "print (\"[+] End Game {} | Reward {} | Epsilon {:.4f} | TrainPerGame {} | Loss [{:.4f}, {:.4f}] \".format(number_game, reward_game, epsilon, index_train_per_game, loss_print[0], loss_print[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.700725988484919e-06, 24.721904754638672]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss / index_train_per_game * JUMP_FPS).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
