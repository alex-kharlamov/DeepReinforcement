{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-18 22:38:56,585] Making new env: Skiing-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Starting Game 0\n",
      "[+] End Game 0 | Reward 5 | Epsilon 0.3500 | TrainPerGame 1946 | Loss [3.9502, 0.0704] \n",
      "[+] Starting Game 1\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from __future__ import division\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape, Dropout\n",
    "from keras.optimizers import sgd\n",
    "import os\n",
    "import random\n",
    "from os.path import isfile\n",
    "from collections import deque\n",
    "\n",
    "from gym.core import Wrapper\n",
    "class RewardForPassingGates(Wrapper):\n",
    "    \"\"\"Skiing wrapper that rewards player +1 for passing gates only.\"\"\"\n",
    "    def _reset(self):\n",
    "        \"\"\"On game reset, remember the hash of initial score\"\"\"\n",
    "        s = self.env.reset()\n",
    "        self.prev_score_hash = hash(s[31:38,67:81].tobytes()) #hash of the image chunk with scoreboard\n",
    "        return s\n",
    "    def _step(self,action):\n",
    "        \"\"\"on each step, if score has changed, give +1 reward, else +0\"\"\"\n",
    "        s,_,done,info = self.env.step(action)\n",
    "        new_score_hash = hash(s[31:38,67:81].tobytes()) #hash of the same image chunk\n",
    "\n",
    "        #reward = +1 if we have just crossed the gate, else 0\n",
    "        r = int(new_score_hash != self.prev_score_hash)\n",
    "\n",
    "        #remember new score\n",
    "        self.prev_score_hash = new_score_hash\n",
    "        return s,r,done,info\n",
    "\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 3\n",
    "MAX_REPLAY_STATES = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_GAMES_TRAIN = 5\n",
    "JUMP_FPS = 5\n",
    "WEIGHT_FILE = 'weights.h5'\n",
    "\n",
    "\n",
    "replay = []\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "\n",
    "env = gym.make(\"Skiing-v0\")\n",
    "\n",
    "env = RewardForPassingGates(env)\n",
    "\n",
    "\n",
    "layers = [\n",
    "    #Reshape((1, 250, 160, 3), input_shape=(250, 160, 3)),\n",
    "    Convolution2D(16, 5, 5, border_mode='same', input_shape=(170, 160, 3)),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2,2)),\n",
    "    Convolution2D(32, 5, 5),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(250, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(output_dim=3, activation='softmax')\n",
    "]\n",
    "\n",
    "\n",
    "def t(st):\n",
    "    return st.reshape(1, 170, 160, 3)\n",
    "\n",
    "model = Sequential(layers)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "model.summary\n",
    "\n",
    "NUM_ACTIONS\n",
    "prb = 0.2\n",
    "gamma = 0.5\n",
    "epsilon = 0.35\n",
    "\n",
    "st = env.reset()\n",
    "\n",
    "st = st[:-80]\n",
    "st = t(st)\n",
    "for number_game in range(10):\n",
    "  new_state = env.reset()\n",
    "  new_state = new_state[:-80]\n",
    "  \n",
    "  reward_game = 0\n",
    "  done = False\n",
    "  loss = 0\n",
    "  index_train_per_game = 0\n",
    "  print( '[+] Starting Game ' + str(number_game))\n",
    "  while not done:\n",
    "    env.render()\n",
    "    index_train_per_game += 1\n",
    "    if random.random() < epsilon:\n",
    "      action = np.random.randint(NUM_ACTIONS)\n",
    "    else:\n",
    "\n",
    "      if new_state.shape[0] != 170:\n",
    "            new_state = new_state[:-80]\n",
    "      q = model.predict(t(new_state))[0]\n",
    "      action = np.argmax(q)\n",
    "        \n",
    "        \n",
    "    old_state = new_state\n",
    "    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    reward_game += reward\n",
    "    replay.append([new_state, reward, action, done, old_state])\n",
    "    \n",
    "    \n",
    "    if len(replay) > MAX_REPLAY_STATES:\n",
    "        replay.pop(np.random.randint(MAX_REPLAY_STATES) + 1)\n",
    "        \n",
    "    if JUMP_FPS != 1 and index_train_per_game % JUMP_FPS == 0:\n",
    "      continue\n",
    "    \n",
    "    \n",
    "    len_mini_batch = min(len(replay), BATCH_SIZE)\n",
    "    \n",
    "    mini_batch = random.sample(replay, len_mini_batch)\n",
    "    \n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    \n",
    "    for index_rep in range(len_mini_batch):\n",
    "      new_rep_state, reward_rep, action_rep, done_rep, old_rep_state = mini_batch[index_rep]\n",
    "      temp = model.predict(t(new_rep_state[:-80]))\n",
    "      if index_rep % 10 == 0 and index_rep != 0:\n",
    "          print(temp, \"index = \", index_rep)\n",
    "      \n",
    "      if old_rep_state.shape[0] != 170:\n",
    "            old_rep_state = old_rep_state[:-80]\n",
    "    \n",
    "      old_q = model.predict(t(old_rep_state))[0]\n",
    "      new_q = temp[0]\n",
    "      update_target = np.copy(old_q)\n",
    "      if done_rep:\n",
    "        update_target[action_rep] = -1\n",
    "      else:\n",
    "        update_target[action_rep] = reward_rep + (gamma * np.max(new_q))\n",
    "      X_train.append(old_rep_state)\n",
    "      Y_train.append(update_target)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "   \n",
    "        \n",
    "    loss += np.array(model.train_on_batch(X_train, Y_train))\n",
    "\n",
    "    if reward_game > 200:\n",
    "      break\n",
    "  loss_print = (loss / index_train_per_game * JUMP_FPS).tolist()\n",
    "  print (\"[+] End Game {} | Reward {} | Epsilon {:.4f} | TrainPerGame {} | Loss [{:.4f}, {:.4f}] \".format(number_game, reward_game, epsilon, index_train_per_game, loss_print[0], loss_print[1]))\n",
    "  #if epsilon >= 0.8:\n",
    "  #  epsilon -= (1 / (NUM_GAMES_TRAIN))\n",
    "  if isfile(WEIGHT_FILE):\n",
    "    os.remove(WEIGHT_FILE)\n",
    "  model.save_weights(WEIGHT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.24852782e-08,   9.91017699e-01,   9.01098147e-08], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(t(new_state))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('lunch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
