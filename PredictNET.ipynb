{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from skimage.transform import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('train-1.renju') as f:\n",
    "    content = f.readlines()\n",
    "data = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "' '.join(list(filter(lambda a: 'p' not in a, data[34])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "f = open('renju-mod', 'w')\n",
    "for elem in data:\n",
    "    list(filter(lambda a: 'p' not in a, elem))\n",
    "    f.write(' '.join(list(filter(lambda a: 'p' not in a, elem))) +'\\n')  # python will convert \\n to os.linesep\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_number(letter):\n",
    "    #lst = {'a':0, 'b':1,}\n",
    "\n",
    "    return ord(letter) - ord('a')\n",
    "\n",
    "def get_pos(elem):\n",
    "    w = get_number(elem[0])\n",
    "    h = int(elem[1:]) - 1\n",
    "    return w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data = data[int(0.7 * len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data[: int(0.7 * len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen(data, test=False):\n",
    "    part_size = len(data)//410\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        while(len(neural_games) < part_size):\n",
    "            elem = data[np.random.randint(len(data))]\n",
    "            cur_pos = np.zeros((15,15, 3))\n",
    "            game = elem.split()\n",
    "            for iterator in range(1, len(game) - 1):\n",
    "                elem = game[iterator]\n",
    "                w, h = get_pos(elem)\n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos[w][h][0] = 1 \n",
    "                else:\n",
    "                    cur_pos[w][h][1] = 1 \n",
    "                \n",
    "                for i in range(15):\n",
    "                    for j in range(15):\n",
    "                        if cur_pos[i][j][0] > 0:\n",
    "                            cur_pos[i][j][2] += 1\n",
    "                        \n",
    "                        \n",
    "                cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "                w, h = get_pos(game[iterator + 1])\n",
    "                ans = h * 15 + w\n",
    "                #print(cur_pos)\n",
    "                neural_ans.append(ans)\n",
    "                neural_games.append(cur_pos_to_app)\n",
    "                if test == False:\n",
    "                    neural_ans.append(ans)\n",
    "                    neural_ans.append(ans)\n",
    "                    neural_ans.append(ans)\n",
    "                    neural_games.append(np.rot90(cur_pos_to_app, 1))\n",
    "                    neural_games.append(np.rot90(cur_pos_to_app, 2))\n",
    "                    neural_games.append(np.rot90(cur_pos_to_app, 3))\n",
    "                if (len(neural_games) >= part_size):\n",
    "                    break\n",
    "\n",
    "\n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        neural_games.resize((part_size, 1, 15,15,3))\n",
    "        yield (neural_games, neural_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 15, 15, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 2)         56        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 4)         76        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 6)           222       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 8)           440       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 12)          876       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 16)          1744      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "fczerominus2 (Dense)         (None, 3300)              478500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3300)              13200     \n",
      "_________________________________________________________________\n",
      "fczeromins1 (Dense)          (None, 2200)              7262200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2200)              8800      \n",
      "_________________________________________________________________\n",
      "fczero (Dense)               (None, 1100)              2421100   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1100)              4400      \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 550)               605550    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 550)               2200      \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 225)               123975    \n",
      "=================================================================\n",
      "Total params: 10,923,339.0\n",
      "Trainable params: 10,909,039.0\n",
      "Non-trainable params: 14,300.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model.add(Convolution2D(2, (3,3), input_shape=(15, 15, 4), activation='relu'))\n",
    "model.add(Convolution2D(4, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(6, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(8, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(12, (3,3), activation='relu'))\n",
    "model.add(Convolution2D(16, (3,3), activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "\n",
    "model.add(Dense(3300, activation='relu', name='fczerominus2'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2200, activation='relu', name='fczeromins1'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1100, activation='relu', name='fczero'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(550, activation='relu', name='fc1'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback, TQDMCallback\n",
    "# keras, model definition...\n",
    "model_saver = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.6f}.hdf5\",\n",
    "                                              monitor='val_acc', verbose=0,\n",
    "                                              save_best_only=True, save_weights_only=False,\n",
    "                                              mode='auto')\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                          histogram_freq=0, \n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d54232e1ef499b8b86f53530577cbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf9dea67cfd4383b9e8bff89e3db0d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5457eca5a9478ea1f60eb8382f19c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc8d65df13d49d7bb94ad4f8300a7e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ebfc351f1441cf857f12b14d61df41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b57d2630224717b09d8900af7a1490"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a545fd434a4d1aa1a91cc29cde40f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit_generator(data_gen(data),\n",
    "        steps_per_epoch=500, epochs=350, verbose=0,\n",
    "                   validation_data = data_gen(test_data, test=True), validation_steps=2,\n",
    "                   callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save(\"BigConv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_check_env = load_model(\"ReshapedModelsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model, save_model\n",
    "\n",
    "#save_model(model, \"med_modelv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"ReshapedModel0.46-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_policy = load_model(\"ReshapedModel0.46-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"ReshapedModelsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Renju(object): \n",
    "    def __init__(self, player):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = player\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        \n",
    "        \n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        \n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "                \n",
    "        reward = 0\n",
    "        \n",
    "        def check_limit(i, j):\n",
    "            if i >= 0 and i <= 14 and j >= 0 and j <= 14:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if check_limit(i + 5, j):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i + 1][j][0] == self.cur_pos[i + 2][j][0] == self.cur_pos[i + 3][j][0] == self.cur_pos[i + 4][j][0] == self.cur_pos[i + 5][j][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                            \n",
    "                if check_limit(i, j + 5):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i][j + 1][0] == self.cur_pos[i][j + 2][0] == self.cur_pos[i][j + 3][0] == self.cur_pos[i][j + 4][0] == self.cur_pos[i][j + 5][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                            \n",
    "                if check_limit(i - 5, j):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i - 1][j][0] == self.cur_pos[i - 2][j][0] == self.cur_pos[i - 3][j][0] == self.cur_pos[i - 4][j][0] == self.cur_pos[i - 5][j][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                \n",
    "                \n",
    "                \n",
    "                if check_limit(i, j - 5):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i][j - 1][0] == self.cur_pos[i][j - 2][0] == self.cur_pos[i][j - 3][0] == self.cur_pos[i][j - 4][0] == self.cur_pos[i][j - 5][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                \n",
    "                \n",
    "                \n",
    "                if check_limit(i + 5, j + 5):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i + 1][j + 1][0] == self.cur_pos[i + 2][j + 2][0] == self.cur_pos[i + 3][j + 3][0] == self.cur_pos[i + 4][j + 4][0] == self.cur_pos[i + 5][j + 5][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                            \n",
    "                if check_limit(i - 5, j - 5):\n",
    "                    if self.cur_pos[i][j][0] == self.cur_pos[i - 1][j - 1][0] == self.cur_pos[i - 2][j - 2][0] == self.cur_pos[i - 3][j - 3][0] == self.cur_pos[i - 4][j - 4][0] == self.cur_pos[i - 5][j - 5][0]:\n",
    "                        if self.cur_pos[i][j][0] == 1:\n",
    "                            if self.player == 1:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = -1\n",
    "                \n",
    "        \n",
    "        done = True if reward != 0 else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if done:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def step(self, action):\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        if reward != 0:\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            return self.in_step(np.argmax(model_check_env.predict(np.array([[env.cur_pos]]))))\n",
    "            \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        return self.cur_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RenjuTEST(object): \n",
    "    def __init__(self, player, RL):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = player\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        self.RL = RL\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        \n",
    "        \n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        self.moves_done += 1\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "            \n",
    "            \n",
    "        w = (action % 15) + 5\n",
    "        h = (action // 15) + 5\n",
    "        \n",
    "        self.ext_pos[w][h] = self.cur_player\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "                \n",
    "        reward = 0\n",
    "        \n",
    "        rdiag = [0, 0, 0]\n",
    "        rdiaginv = [0, 0, 0]\n",
    "        ldiag = [0, 0, 0]\n",
    "        ldiaginv = [0, 0, 0]\n",
    "        rrow = [0, 0, 0]\n",
    "        lrow = [0, 0, 0]\n",
    "        rcol = [0, 0, 0]\n",
    "        lcol = [0, 0, 0]\n",
    "        \n",
    "        broken = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            if self.ext_pos[w + i][h + i]:\n",
    "                if not broken[0]:\n",
    "                    rdiag[int(self.ext_pos[w + i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[0] = 1\n",
    "                \n",
    "            if self.ext_pos[w][h + i]:\n",
    "                if not broken[1]:\n",
    "                    rcol[int(self.ext_pos[w][h + i])] += 1\n",
    "                else:\n",
    "                    broken[1] = 1\n",
    "                \n",
    "            if self.ext_pos[w - i][h + i]:\n",
    "                if not broken[2]:\n",
    "                    ldiag[int(self.ext_pos[w - i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[2] = 1\n",
    "                \n",
    "            \n",
    "            if self.ext_pos[w - i][h]:\n",
    "                if not broken[3]:\n",
    "                    lrow[int(self.ext_pos[w - i][h])] += 1\n",
    "                else:\n",
    "                    broken[3] = 1\n",
    "            \n",
    "            if self.ext_pos[w - i][h - i]:\n",
    "                if not broken[4]:\n",
    "                    rdiaginv[int(self.ext_pos[w - i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[4] = 1\n",
    "            \n",
    "            if self.ext_pos[w][h - i]:\n",
    "                if not broken[5]:\n",
    "                    lcol[int(self.ext_pos[w][h - i])] += 1\n",
    "                else:\n",
    "                    broken[5] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h - i]:\n",
    "                if not broken[6]:\n",
    "                    ldiaginv[int(self.ext_pos[w + i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[6] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h]:\n",
    "                if not broken[7]:\n",
    "                    rrow[int(self.ext_pos[w + i][h])] += 1\n",
    "                else:\n",
    "                    broken[7] = 1\n",
    "        \n",
    "        \n",
    "        if rdiag[self.cur_player] + rdiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if ldiag[self.cur_player] + ldiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \n",
    "        if rrow[self.cur_player] + lrow[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if lcol[self.cur_player] + rcol[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \"\"\"\n",
    "        print('rdiag', rdiag)\n",
    "        print('rdiaginv', rdiaginv)\n",
    "        print('ldiag', ldiag)\n",
    "        print('ldiaginv', ldiaginv)\n",
    "        print('rrow', rrow)\n",
    "        print('lrow', lrow)\n",
    "        print('rcol', rcol)\n",
    "        print('lcol', lcol)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        done = True if (self.moves_done == 225 or reward != 0) else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if self.moves_done == 225:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def net_ans(self, model, mode = 'all'):\n",
    "        s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        if mode == 'one':\n",
    "            return np.argmax(s)\n",
    "        else:\n",
    "            #return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "            return np.argsort(s)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" \n",
    "        RL_move = dqn.compute_q_values([env.cur_pos])\n",
    "        RL_move = sorted(range(len(RL_move)), key=lambda k: RL_move[k], reverse=True)\n",
    "        \n",
    "        action = 0\n",
    "        for act in RL_move:\n",
    "            if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                action = act\n",
    "                break\n",
    "\n",
    "        \n",
    "        print(\"RL:\", action)\n",
    "        \n",
    "        net_move = self.net_ans(model)\n",
    "            \n",
    "        action = 0\n",
    "        for act in net_move:\n",
    "            if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                action = act\n",
    "                break\n",
    "       \n",
    "         \"\"\"\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        \n",
    "        if done:\n",
    "            self.reset()\n",
    "        #print(\"Net:\", action)\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                print(self.cur_pos[i][j][1], end=' ')\n",
    "            print(\"\\n\")\n",
    "        \"\"\"\n",
    "            \n",
    "        if reward != 0:\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "            \n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "                #print(\"Net:\", action)\n",
    "            return self.in_step(action)\n",
    "            \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for i in range(15):\n",
    "                for j in range(15):\n",
    "                    flag = 0\n",
    "                    if self.cur_pos[i][j][0] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.cur_pos[i][j][1] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        if mode == 'debug':\n",
    "            for i in range(25):\n",
    "                for j in range(25):\n",
    "                    flag = 0\n",
    "                    if self.ext_pos[i][j] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.ext_pos[i][j] == 2:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        return self.cur_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 153.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tester = RenjuTEST(1, None)\n",
    "\n",
    "win = 0\n",
    "loose = 0\n",
    "all_game = 0\n",
    "\n",
    "\n",
    "for i in tqdm(range(2000)):\n",
    "    cur_pos, reward, done, info = tester.step(1)\n",
    "    if reward == 1:\n",
    "        win += 1\n",
    "    if reward == -1:\n",
    "        loose += 1\n",
    "    if done:\n",
    "        all_game += 1\n",
    "    #tester.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%lprun -f  tester.step tester.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 222 loose: 0 all rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"win:\", win, \"loose:\", loose, \"all rate:\", win / all_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_check_env.predict(np.array([[env.cur_pos]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205,\n",
       " 157,\n",
       " 79,\n",
       " 191,\n",
       " 63,\n",
       " 202,\n",
       " 22,\n",
       " 175,\n",
       " 111,\n",
       " 91,\n",
       " 189,\n",
       " 152,\n",
       " 142,\n",
       " 34,\n",
       " 25,\n",
       " 160,\n",
       " 72,\n",
       " 33,\n",
       " 204,\n",
       " 36,\n",
       " 176,\n",
       " 19,\n",
       " 57,\n",
       " 48,\n",
       " 137,\n",
       " 178,\n",
       " 133,\n",
       " 52,\n",
       " 136,\n",
       " 106,\n",
       " 151,\n",
       " 132,\n",
       " 77,\n",
       " 187,\n",
       " 102,\n",
       " 103,\n",
       " 92,\n",
       " 26,\n",
       " 118,\n",
       " 88,\n",
       " 20,\n",
       " 42,\n",
       " 109,\n",
       " 35,\n",
       " 67,\n",
       " 172,\n",
       " 121,\n",
       " 126,\n",
       " 78,\n",
       " 93,\n",
       " 119,\n",
       " 169,\n",
       " 61,\n",
       " 39,\n",
       " 177,\n",
       " 153,\n",
       " 71,\n",
       " 24,\n",
       " 148,\n",
       " 59,\n",
       " 201,\n",
       " 18,\n",
       " 162,\n",
       " 107,\n",
       " 161,\n",
       " 104,\n",
       " 139,\n",
       " 122,\n",
       " 146,\n",
       " 64,\n",
       " 62,\n",
       " 190,\n",
       " 127,\n",
       " 186,\n",
       " 194,\n",
       " 199,\n",
       " 41,\n",
       " 167,\n",
       " 198,\n",
       " 74,\n",
       " 166,\n",
       " 56,\n",
       " 124,\n",
       " 40,\n",
       " 215,\n",
       " 131,\n",
       " 134,\n",
       " 174,\n",
       " 159,\n",
       " 164,\n",
       " 51,\n",
       " 182,\n",
       " 87,\n",
       " 117,\n",
       " 37,\n",
       " 140,\n",
       " 110,\n",
       " 155,\n",
       " 185,\n",
       " 168,\n",
       " 76,\n",
       " 54,\n",
       " 171,\n",
       " 154,\n",
       " 7,\n",
       " 206,\n",
       " 49,\n",
       " 193,\n",
       " 149,\n",
       " 73,\n",
       " 90,\n",
       " 123,\n",
       " 163,\n",
       " 31,\n",
       " 147,\n",
       " 183,\n",
       " 192,\n",
       " 138,\n",
       " 108,\n",
       " 58,\n",
       " 89,\n",
       " 4,\n",
       " 170,\n",
       " 65,\n",
       " 69,\n",
       " 116,\n",
       " 70,\n",
       " 94,\n",
       " 145,\n",
       " 66,\n",
       " 47,\n",
       " 50,\n",
       " 184,\n",
       " 200,\n",
       " 44,\n",
       " 21,\n",
       " 5,\n",
       " 2,\n",
       " 27,\n",
       " 86,\n",
       " 125,\n",
       " 80,\n",
       " 105,\n",
       " 97,\n",
       " 101,\n",
       " 209,\n",
       " 141,\n",
       " 156,\n",
       " 220,\n",
       " 179,\n",
       " 181,\n",
       " 55,\n",
       " 17,\n",
       " 75,\n",
       " 208,\n",
       " 43,\n",
       " 219,\n",
       " 216,\n",
       " 30,\n",
       " 14,\n",
       " 96,\n",
       " 6,\n",
       " 223,\n",
       " 46,\n",
       " 120,\n",
       " 150,\n",
       " 32,\n",
       " 12,\n",
       " 213,\n",
       " 135,\n",
       " 212,\n",
       " 217,\n",
       " 95,\n",
       " 195,\n",
       " 222,\n",
       " 15,\n",
       " 3,\n",
       " 196,\n",
       " 10,\n",
       " 214,\n",
       " 13,\n",
       " 144,\n",
       " 130,\n",
       " 221,\n",
       " 1,\n",
       " 28,\n",
       " 81,\n",
       " 197,\n",
       " 100,\n",
       " 115,\n",
       " 84,\n",
       " 11,\n",
       " 45,\n",
       " 207,\n",
       " 9,\n",
       " 82,\n",
       " 180,\n",
       " 165,\n",
       " 60,\n",
       " 211,\n",
       " 29,\n",
       " 224,\n",
       " 85,\n",
       " 99,\n",
       " 128,\n",
       " 16,\n",
       " 129,\n",
       " 0,\n",
       " 38,\n",
       " 114,\n",
       " 210,\n",
       " 53,\n",
       " 173,\n",
       " 83,\n",
       " 112,\n",
       " 188,\n",
       " 218,\n",
       " 68,\n",
       " 98,\n",
       " 143,\n",
       " 23,\n",
       " 158,\n",
       " 8,\n",
       " 203,\n",
       " 113]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def net_ans(env):\n",
    "    s = model_check_env.predict(np.array([[env.cur_pos]]))[0]\n",
    "    return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "\n",
    "net_ans(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(model_normal.layers)):\n",
    "    model.layers[i + 1].set_weights(model_normal.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 15, 15, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 8)         224       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 550)               744150    \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 225)               123975    \n",
      "=================================================================\n",
      "Total params: 868,349.0\n",
      "Trainable params: 868,349.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "model_policy = Sequential()\n",
    "\n",
    "model_policy.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model_policy.add(Convolution2D(8, (3,3), input_shape=(15, 15, 4), activation='relu'))\n",
    "\n",
    "model_policy.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "model_policy.add(Dense(550, activation='relu', name='fc1'))\n",
    "\n",
    "model_policy.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model_policy.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   12/5000: episode: 1, duration: 1.496s, episode steps: 12, steps per second: 8, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 121.833 [51.000, 220.000], mean observation: 0.109 [0.000, 24.000], loss: 0.502264, mean_absolute_error: 0.008889, mean_q: 0.023879\n",
      "   24/5000: episode: 2, duration: 0.256s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 115.083 [13.000, 191.000], mean observation: 0.103 [0.000, 24.000], loss: 0.000005, mean_absolute_error: 0.004432, mean_q: 0.009392\n",
      "   31/5000: episode: 3, duration: 0.138s, episode steps: 7, steps per second: 51, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 78.857 [20.000, 141.000], mean observation: 0.046 [0.000, 13.000], loss: 0.000003, mean_absolute_error: 0.004433, mean_q: 0.007376\n",
      "   39/5000: episode: 4, duration: 0.172s, episode steps: 8, steps per second: 46, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 118.750 [25.000, 212.000], mean observation: 0.049 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004430, mean_q: 0.008592\n",
      "   47/5000: episode: 5, duration: 0.167s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 119.000 [27.000, 221.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000002, mean_absolute_error: 0.004432, mean_q: 0.009858\n",
      "   63/5000: episode: 6, duration: 0.359s, episode steps: 16, steps per second: 45, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 122.688 [18.000, 196.000], mean observation: 0.176 [0.000, 32.000], loss: 0.000003, mean_absolute_error: 0.004438, mean_q: 0.123187\n",
      "   70/5000: episode: 7, duration: 0.144s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 112.429 [3.000, 199.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000012, mean_absolute_error: 0.004440, mean_q: 0.015407\n",
      "   84/5000: episode: 8, duration: 0.301s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 104.643 [9.000, 208.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.355905\n",
      "   89/5000: episode: 9, duration: 0.123s, episode steps: 5, steps per second: 41, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 114.400 [33.000, 171.000], mean observation: 0.030 [0.000, 10.000], loss: 0.000002, mean_absolute_error: 0.004435, mean_q: 0.009028\n",
      "   94/5000: episode: 10, duration: 0.122s, episode steps: 5, steps per second: 41, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 83.600 [23.000, 149.000], mean observation: 0.030 [0.000, 10.000], loss: 0.000003, mean_absolute_error: 0.004437, mean_q: 0.011483\n",
      "  101/5000: episode: 11, duration: 0.175s, episode steps: 7, steps per second: 40, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 126.143 [71.000, 192.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004434, mean_q: 0.017552\n",
      "  110/5000: episode: 12, duration: 0.230s, episode steps: 9, steps per second: 39, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 121.778 [0.000, 222.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000019, mean_absolute_error: 0.004441, mean_q: 0.138880\n",
      "  117/5000: episode: 13, duration: 0.155s, episode steps: 7, steps per second: 45, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 138.286 [57.000, 221.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004436, mean_q: 0.061090\n",
      "  137/5000: episode: 14, duration: 0.475s, episode steps: 20, steps per second: 42, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 117.500 [11.000, 221.000], mean observation: 0.259 [0.000, 40.000], loss: 0.003334, mean_absolute_error: 0.004497, mean_q: 0.328834\n",
      "  162/5000: episode: 15, duration: 0.568s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 127.680 [17.000, 208.000], mean observation: 0.339 [0.000, 50.000], loss: 0.018333, mean_absolute_error: 0.004577, mean_q: 0.351211\n",
      "  171/5000: episode: 16, duration: 0.184s, episode steps: 9, steps per second: 49, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 97.778 [27.000, 182.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000010, mean_absolute_error: 0.004441, mean_q: 0.110015\n",
      "  178/5000: episode: 17, duration: 0.145s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 129.000 [46.000, 209.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000003, mean_absolute_error: 0.004434, mean_q: 0.156712\n",
      "  201/5000: episode: 18, duration: 0.492s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 107.217 [7.000, 224.000], mean observation: 0.332 [0.000, 46.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.750133\n",
      "  209/5000: episode: 19, duration: 0.198s, episode steps: 8, steps per second: 40, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 118.125 [20.000, 222.000], mean observation: 0.055 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.262138\n",
      "  217/5000: episode: 20, duration: 0.183s, episode steps: 8, steps per second: 44, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 74.875 [9.000, 201.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.287270\n",
      "  240/5000: episode: 21, duration: 0.536s, episode steps: 23, steps per second: 43, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 118.565 [1.000, 222.000], mean observation: 0.323 [0.000, 46.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.728817\n",
      "  252/5000: episode: 22, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 70.667 [1.000, 223.000], mean observation: 0.109 [0.000, 24.000], loss: 0.002420, mean_absolute_error: 0.004502, mean_q: 0.429510\n",
      "  270/5000: episode: 23, duration: 0.438s, episode steps: 18, steps per second: 41, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 119.611 [3.000, 218.000], mean observation: 0.214 [0.000, 36.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.533932\n",
      "  279/5000: episode: 24, duration: 0.186s, episode steps: 9, steps per second: 48, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 128.222 [3.000, 211.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.164623\n",
      "  304/5000: episode: 25, duration: 0.554s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 108.000 [0.000, 213.000], mean observation: 0.385 [0.000, 50.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.771123\n",
      "  321/5000: episode: 26, duration: 0.433s, episode steps: 17, steps per second: 39, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 127.235 [46.000, 210.000], mean observation: 0.196 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.592215\n",
      "  341/5000: episode: 27, duration: 0.496s, episode steps: 20, steps per second: 40, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 102.800 [20.000, 215.000], mean observation: 0.259 [0.000, 40.000], loss: 0.052108, mean_absolute_error: 0.004672, mean_q: 0.695034\n",
      "  346/5000: episode: 28, duration: 0.117s, episode steps: 5, steps per second: 43, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 133.000 [52.000, 205.000], mean observation: 0.030 [0.000, 10.000], loss: 0.000014, mean_absolute_error: 0.004435, mean_q: 0.025117\n",
      "  364/5000: episode: 29, duration: 0.383s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 115.167 [1.000, 214.000], mean observation: 0.212 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.695104\n",
      "  387/5000: episode: 30, duration: 0.498s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 100.522 [8.000, 213.000], mean observation: 0.329 [0.000, 46.000], loss: 0.003710, mean_absolute_error: 0.004499, mean_q: 0.382149\n",
      "  401/5000: episode: 31, duration: 0.346s, episode steps: 14, steps per second: 41, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 127.571 [37.000, 222.000], mean observation: 0.140 [0.000, 28.000], loss: 0.000016, mean_absolute_error: 0.004442, mean_q: 0.093374\n",
      "  416/5000: episode: 32, duration: 0.311s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 104.333 [4.000, 214.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000015, mean_absolute_error: 0.004445, mean_q: 0.123988\n",
      "  429/5000: episode: 33, duration: 0.271s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 102.692 [2.000, 212.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.168838\n",
      "  450/5000: episode: 34, duration: 0.438s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 103.619 [8.000, 224.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000002, mean_absolute_error: 0.004443, mean_q: 0.407781\n",
      "  457/5000: episode: 35, duration: 0.146s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 84.286 [11.000, 182.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004430, mean_q: 0.026315\n",
      "  475/5000: episode: 36, duration: 0.374s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 127.111 [0.000, 220.000], mean observation: 0.215 [0.000, 36.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.452269\n",
      "  485/5000: episode: 37, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 131.500 [68.000, 209.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.418372\n",
      "  495/5000: episode: 38, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 98.000 [7.000, 223.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.456159\n",
      "  503/5000: episode: 39, duration: 0.165s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 96.125 [13.000, 163.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004434, mean_q: 0.272187\n",
      "  517/5000: episode: 40, duration: 0.296s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 118.286 [8.000, 193.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.571122\n",
      "  528/5000: episode: 41, duration: 0.227s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 95.455 [7.000, 196.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000003, mean_absolute_error: 0.004442, mean_q: 0.524702\n",
      "  537/5000: episode: 42, duration: 0.190s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 132.333 [63.000, 217.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.360738\n",
      "  551/5000: episode: 43, duration: 0.337s, episode steps: 14, steps per second: 42, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 129.714 [9.000, 215.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.596821\n",
      "  558/5000: episode: 44, duration: 0.186s, episode steps: 7, steps per second: 38, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 94.286 [6.000, 157.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000003, mean_absolute_error: 0.004439, mean_q: 0.220511\n",
      "  579/5000: episode: 45, duration: 0.519s, episode steps: 21, steps per second: 40, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 118.429 [7.000, 220.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.771580\n",
      "  591/5000: episode: 46, duration: 0.339s, episode steps: 12, steps per second: 35, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 116.333 [38.000, 214.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.702745\n",
      "  598/5000: episode: 47, duration: 0.137s, episode steps: 7, steps per second: 51, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 143.000 [22.000, 224.000], mean observation: 0.046 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004429, mean_q: 0.065267\n",
      "  617/5000: episode: 48, duration: 0.469s, episode steps: 19, steps per second: 41, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 111.895 [24.000, 221.000], mean observation: 0.219 [0.000, 38.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.773760\n",
      "  626/5000: episode: 49, duration: 0.193s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 125.444 [0.000, 217.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.479752\n",
      "  636/5000: episode: 50, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 109.400 [17.000, 189.000], mean observation: 0.081 [0.000, 20.000], loss: 0.109391, mean_absolute_error: 0.004928, mean_q: 0.532409\n",
      "  643/5000: episode: 51, duration: 0.155s, episode steps: 7, steps per second: 45, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 87.429 [21.000, 203.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000007, mean_absolute_error: 0.004432, mean_q: 0.320135\n",
      "  649/5000: episode: 52, duration: 0.127s, episode steps: 6, steps per second: 47, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 150.333 [49.000, 224.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000004, mean_absolute_error: 0.004435, mean_q: 0.108951\n",
      "  654/5000: episode: 53, duration: 0.123s, episode steps: 5, steps per second: 41, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 88.400 [3.000, 197.000], mean observation: 0.030 [0.000, 10.000], loss: 0.000001, mean_absolute_error: 0.004433, mean_q: 0.028487\n",
      "  661/5000: episode: 54, duration: 0.166s, episode steps: 7, steps per second: 42, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 120.143 [37.000, 223.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.227317\n",
      "  680/5000: episode: 55, duration: 0.424s, episode steps: 19, steps per second: 45, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 101.842 [6.000, 218.000], mean observation: 0.237 [0.000, 38.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.702406\n",
      "  690/5000: episode: 56, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 131.000 [47.000, 199.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004439, mean_q: 0.556593\n",
      "  703/5000: episode: 57, duration: 0.324s, episode steps: 13, steps per second: 40, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 131.154 [24.000, 213.000], mean observation: 0.122 [0.000, 26.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.613063\n",
      "  710/5000: episode: 58, duration: 0.161s, episode steps: 7, steps per second: 43, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 129.429 [37.000, 219.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004434, mean_q: 0.281126\n",
      "  719/5000: episode: 59, duration: 0.189s, episode steps: 9, steps per second: 48, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 112.667 [46.000, 210.000], mean observation: 0.063 [0.000, 18.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.444632\n",
      "  727/5000: episode: 60, duration: 0.187s, episode steps: 8, steps per second: 43, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 130.125 [11.000, 222.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000003, mean_absolute_error: 0.004441, mean_q: 0.322282\n",
      "  749/5000: episode: 61, duration: 0.499s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 101.000 [3.000, 222.000], mean observation: 0.300 [0.000, 44.000], loss: 0.000008, mean_absolute_error: 0.004446, mean_q: 0.751716\n",
      "  761/5000: episode: 62, duration: 0.292s, episode steps: 12, steps per second: 41, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 89.500 [14.000, 217.000], mean observation: 0.109 [0.000, 24.000], loss: 0.089981, mean_absolute_error: 0.004841, mean_q: 0.490688\n",
      "  769/5000: episode: 63, duration: 0.202s, episode steps: 8, steps per second: 40, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 122.125 [18.000, 219.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.282250\n",
      "  781/5000: episode: 64, duration: 0.262s, episode steps: 12, steps per second: 46, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 112.083 [43.000, 215.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000004, mean_absolute_error: 0.004439, mean_q: 0.571557\n",
      "  795/5000: episode: 65, duration: 0.339s, episode steps: 14, steps per second: 41, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 108.571 [3.000, 214.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.593040\n",
      "  803/5000: episode: 66, duration: 0.195s, episode steps: 8, steps per second: 41, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 129.750 [10.000, 218.000], mean observation: 0.055 [0.000, 16.000], loss: 0.000003, mean_absolute_error: 0.004435, mean_q: 0.303221\n",
      "  820/5000: episode: 67, duration: 0.357s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 102.294 [4.000, 170.000], mean observation: 0.185 [0.000, 34.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.741665\n",
      "  843/5000: episode: 68, duration: 0.572s, episode steps: 23, steps per second: 40, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 104.739 [13.000, 217.000], mean observation: 0.317 [0.000, 46.000], loss: 0.044173, mean_absolute_error: 0.004445, mean_q: 0.717686\n",
      "  849/5000: episode: 69, duration: 0.123s, episode steps: 6, steps per second: 49, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 160.500 [103.000, 201.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.165858\n",
      "  867/5000: episode: 70, duration: 0.393s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 72.111 [3.000, 221.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.481573\n",
      "  887/5000: episode: 71, duration: 0.437s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 107.150 [3.000, 223.000], mean observation: 0.245 [0.000, 40.000], loss: 0.045977, mean_absolute_error: 0.004659, mean_q: 0.690733\n",
      "  902/5000: episode: 72, duration: 0.327s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 99.133 [2.000, 171.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.717218\n",
      "  913/5000: episode: 73, duration: 0.262s, episode steps: 11, steps per second: 42, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 89.273 [3.000, 185.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.621329\n",
      "  921/5000: episode: 74, duration: 0.208s, episode steps: 8, steps per second: 38, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 76.750 [13.000, 214.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.439840\n",
      "  937/5000: episode: 75, duration: 0.352s, episode steps: 16, steps per second: 45, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 80.938 [0.000, 155.000], mean observation: 0.176 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.761927\n",
      "  945/5000: episode: 76, duration: 0.203s, episode steps: 8, steps per second: 39, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 100.125 [2.000, 209.000], mean observation: 0.058 [0.000, 16.000], loss: 0.005391, mean_absolute_error: 0.004560, mean_q: 0.486841\n",
      "  952/5000: episode: 77, duration: 0.149s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 115.571 [14.000, 196.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.371696\n",
      "  973/5000: episode: 78, duration: 0.496s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 98.000 [9.000, 224.000], mean observation: 0.282 [0.000, 42.000], loss: 0.049504, mean_absolute_error: 0.004663, mean_q: 0.799350\n",
      "  980/5000: episode: 79, duration: 0.138s, episode steps: 7, steps per second: 51, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 72.000 [0.000, 184.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.419604\n",
      "  994/5000: episode: 80, duration: 0.311s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 97.143 [3.000, 223.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000018, mean_absolute_error: 0.004446, mean_q: 0.735422\n",
      " 1014/5000: episode: 81, duration: 0.451s, episode steps: 20, steps per second: 44, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 106.250 [15.000, 224.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.851664\n",
      " 1020/5000: episode: 82, duration: 0.125s, episode steps: 6, steps per second: 48, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 65.500 [10.000, 152.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.349257\n",
      " 1036/5000: episode: 83, duration: 0.372s, episode steps: 16, steps per second: 43, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 100.875 [16.000, 215.000], mean observation: 0.176 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.805715\n",
      " 1056/5000: episode: 84, duration: 0.496s, episode steps: 20, steps per second: 40, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 112.750 [8.000, 219.000], mean observation: 0.240 [0.000, 40.000], loss: 0.000003, mean_absolute_error: 0.004445, mean_q: 0.858431\n",
      " 1065/5000: episode: 85, duration: 0.181s, episode steps: 9, steps per second: 50, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 132.000 [3.000, 223.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.671029\n",
      " 1072/5000: episode: 86, duration: 0.154s, episode steps: 7, steps per second: 46, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 89.714 [8.000, 211.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.577313\n",
      " 1084/5000: episode: 87, duration: 0.255s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 103.917 [0.000, 196.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.711964\n",
      " 1100/5000: episode: 88, duration: 0.359s, episode steps: 16, steps per second: 45, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 60.375 [0.000, 219.000], mean observation: 0.173 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.769933\n",
      " 1115/5000: episode: 89, duration: 0.327s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 136.600 [5.000, 223.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.804026\n",
      " 1141/5000: episode: 90, duration: 0.549s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.500 [1.000, 219.000], mean observation: 0.412 [0.000, 52.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.876738\n",
      " 1158/5000: episode: 91, duration: 0.363s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 121.941 [0.000, 218.000], mean observation: 0.174 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.787455\n",
      " 1176/5000: episode: 92, duration: 0.390s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 92.556 [12.000, 218.000], mean observation: 0.210 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.815226\n",
      " 1183/5000: episode: 93, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 122.000 [2.000, 208.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.523845\n",
      " 1192/5000: episode: 94, duration: 0.197s, episode steps: 9, steps per second: 46, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 131.889 [34.000, 224.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.655112\n",
      " 1199/5000: episode: 95, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 125.857 [2.000, 210.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.590934\n",
      " 1206/5000: episode: 96, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 101.143 [40.000, 193.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004443, mean_q: 0.568562\n",
      " 1217/5000: episode: 97, duration: 0.241s, episode steps: 11, steps per second: 46, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 107.818 [33.000, 214.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000002, mean_absolute_error: 0.004443, mean_q: 0.764254\n",
      " 1236/5000: episode: 98, duration: 0.405s, episode steps: 19, steps per second: 47, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 110.579 [4.000, 212.000], mean observation: 0.208 [0.000, 38.000], loss: 0.000005, mean_absolute_error: 0.004444, mean_q: 0.793359\n",
      " 1244/5000: episode: 99, duration: 0.165s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 98.375 [4.000, 200.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.572794\n",
      " 1253/5000: episode: 100, duration: 0.193s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 138.444 [37.000, 218.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000002, mean_absolute_error: 0.004442, mean_q: 0.697295\n",
      " 1264/5000: episode: 101, duration: 0.233s, episode steps: 11, steps per second: 47, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 115.909 [20.000, 215.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.680966\n",
      " 1271/5000: episode: 102, duration: 0.153s, episode steps: 7, steps per second: 46, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 105.143 [16.000, 220.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.522238\n",
      " 1292/5000: episode: 103, duration: 0.443s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.762 [17.000, 218.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.829329\n",
      " 1306/5000: episode: 104, duration: 0.300s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 141.429 [10.000, 214.000], mean observation: 0.128 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.798744\n",
      " 1314/5000: episode: 105, duration: 0.160s, episode steps: 8, steps per second: 50, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 132.750 [18.000, 205.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.640446\n",
      " 1325/5000: episode: 106, duration: 0.237s, episode steps: 11, steps per second: 46, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 125.182 [8.000, 194.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.714628\n",
      " 1346/5000: episode: 107, duration: 0.456s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 113.000 [0.000, 205.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.830926\n",
      " 1364/5000: episode: 108, duration: 0.382s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 104.389 [0.000, 213.000], mean observation: 0.213 [0.000, 36.000], loss: 0.058239, mean_absolute_error: 0.004701, mean_q: 0.820320\n",
      " 1382/5000: episode: 109, duration: 0.374s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 108.556 [23.000, 224.000], mean observation: 0.191 [0.000, 36.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.839217\n",
      " 1398/5000: episode: 110, duration: 0.337s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 92.375 [7.000, 210.000], mean observation: 0.174 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.751791\n",
      " 1406/5000: episode: 111, duration: 0.164s, episode steps: 8, steps per second: 49, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 108.875 [42.000, 192.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.599750\n",
      " 1413/5000: episode: 112, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 130.571 [10.000, 214.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000047, mean_absolute_error: 0.004450, mean_q: 0.492525\n",
      " 1422/5000: episode: 113, duration: 0.184s, episode steps: 9, steps per second: 49, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 115.889 [24.000, 220.000], mean observation: 0.065 [0.000, 18.000], loss: 0.000000, mean_absolute_error: 0.004439, mean_q: 0.622311\n",
      " 1436/5000: episode: 114, duration: 0.292s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 107.429 [14.000, 218.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.784258\n",
      " 1445/5000: episode: 115, duration: 0.182s, episode steps: 9, steps per second: 49, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 84.889 [17.000, 223.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.579265\n",
      " 1454/5000: episode: 116, duration: 0.189s, episode steps: 9, steps per second: 48, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 101.222 [41.000, 178.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.622798\n",
      " 1467/5000: episode: 117, duration: 0.291s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 113.308 [31.000, 195.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.800234\n",
      " 1477/5000: episode: 118, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 140.000 [20.000, 224.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.633137\n",
      " 1488/5000: episode: 119, duration: 0.238s, episode steps: 11, steps per second: 46, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 112.727 [17.000, 207.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.663548\n",
      " 1502/5000: episode: 120, duration: 0.305s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 100.357 [2.000, 211.000], mean observation: 0.140 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.762722\n",
      " 1509/5000: episode: 121, duration: 0.146s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 115.429 [12.000, 205.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.463389\n",
      " 1530/5000: episode: 122, duration: 0.452s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 118.905 [7.000, 207.000], mean observation: 0.282 [0.000, 42.000], loss: 0.049503, mean_absolute_error: 0.004663, mean_q: 0.843606\n",
      " 1548/5000: episode: 123, duration: 0.384s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 137.389 [13.000, 220.000], mean observation: 0.214 [0.000, 36.000], loss: 0.000019, mean_absolute_error: 0.004445, mean_q: 0.823696\n",
      " 1570/5000: episode: 124, duration: 0.476s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 93.136 [2.000, 209.000], mean observation: 0.307 [0.000, 44.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.850110\n",
      " 1584/5000: episode: 125, duration: 0.300s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 121.786 [18.000, 223.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.759049\n",
      " 1594/5000: episode: 126, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 102.700 [2.000, 213.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000004, mean_absolute_error: 0.004443, mean_q: 0.664585\n",
      " 1604/5000: episode: 127, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 113.800 [1.000, 202.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.658608\n",
      " 1611/5000: episode: 128, duration: 0.151s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 109.857 [22.000, 197.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.585604\n",
      " 1622/5000: episode: 129, duration: 0.242s, episode steps: 11, steps per second: 45, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 144.182 [59.000, 200.000], mean observation: 0.090 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.767216\n",
      " 1634/5000: episode: 130, duration: 0.264s, episode steps: 12, steps per second: 46, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 123.667 [3.000, 212.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.707570\n",
      " 1641/5000: episode: 131, duration: 0.145s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 57.857 [0.000, 118.000], mean observation: 0.047 [0.000, 14.000], loss: 0.159744, mean_absolute_error: 0.005161, mean_q: 0.454218\n",
      " 1651/5000: episode: 132, duration: 0.220s, episode steps: 10, steps per second: 46, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 116.200 [28.000, 210.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.585611\n",
      " 1676/5000: episode: 133, duration: 0.547s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 114.280 [5.000, 223.000], mean observation: 0.385 [0.000, 50.000], loss: 0.000001, mean_absolute_error: 0.004444, mean_q: 0.854913\n",
      " 1697/5000: episode: 134, duration: 0.454s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 111.143 [15.000, 219.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.737585\n",
      " 1715/5000: episode: 135, duration: 0.388s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 139.389 [28.000, 224.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000002, mean_absolute_error: 0.004442, mean_q: 0.711719\n",
      " 1736/5000: episode: 136, duration: 0.449s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.476 [10.000, 208.000], mean observation: 0.263 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004444, mean_q: 0.836900\n",
      " 1751/5000: episode: 137, duration: 0.325s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 101.533 [3.000, 204.000], mean observation: 0.158 [0.000, 30.000], loss: 0.070670, mean_absolute_error: 0.004758, mean_q: 0.667386\n",
      " 1768/5000: episode: 138, duration: 0.370s, episode steps: 17, steps per second: 46, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 107.176 [2.000, 217.000], mean observation: 0.196 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.649936\n",
      " 1775/5000: episode: 139, duration: 0.146s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 156.000 [41.000, 220.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004432, mean_q: 0.090571\n",
      " 1796/5000: episode: 140, duration: 0.450s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 124.476 [0.000, 222.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.681177\n",
      " 1804/5000: episode: 141, duration: 0.159s, episode steps: 8, steps per second: 50, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 139.750 [54.000, 215.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.249662\n",
      " 1811/5000: episode: 142, duration: 0.153s, episode steps: 7, steps per second: 46, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 90.571 [20.000, 149.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000023, mean_absolute_error: 0.004445, mean_q: 0.034149\n",
      " 1839/5000: episode: 143, duration: 0.598s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 90.036 [16.000, 206.000], mean observation: 0.454 [0.000, 56.000], loss: 0.000004, mean_absolute_error: 0.004443, mean_q: 0.557476\n",
      " 1848/5000: episode: 144, duration: 0.190s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 92.000 [8.000, 200.000], mean observation: 0.069 [0.000, 18.000], loss: 0.013612, mean_absolute_error: 0.004623, mean_q: 0.242499\n",
      " 1855/5000: episode: 145, duration: 0.147s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 135.000 [43.000, 185.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.178565\n",
      " 1876/5000: episode: 146, duration: 0.454s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.905 [8.000, 210.000], mean observation: 0.278 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.774708\n",
      " 1893/5000: episode: 147, duration: 0.357s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 101.824 [6.000, 210.000], mean observation: 0.196 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.725986\n",
      " 1913/5000: episode: 148, duration: 0.425s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 109.650 [3.000, 209.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.770971\n",
      " 1919/5000: episode: 149, duration: 0.126s, episode steps: 6, steps per second: 48, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 113.500 [49.000, 201.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000002, mean_absolute_error: 0.004436, mean_q: 0.191019\n",
      " 1935/5000: episode: 150, duration: 0.344s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 99.375 [3.000, 215.000], mean observation: 0.159 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.720517\n",
      " 1953/5000: episode: 151, duration: 0.377s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 102.389 [1.000, 210.000], mean observation: 0.215 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.702259\n",
      " 1966/5000: episode: 152, duration: 0.274s, episode steps: 13, steps per second: 47, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 111.000 [36.000, 202.000], mean observation: 0.117 [0.000, 26.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.569217\n",
      " 1987/5000: episode: 153, duration: 0.456s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 115.524 [36.000, 224.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.751121\n",
      " 1999/5000: episode: 154, duration: 0.254s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 110.500 [10.000, 213.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000011, mean_absolute_error: 0.004443, mean_q: 0.603227\n",
      " 2015/5000: episode: 155, duration: 0.350s, episode steps: 16, steps per second: 46, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 106.750 [5.000, 222.000], mean observation: 0.175 [0.000, 32.000], loss: 0.000003, mean_absolute_error: 0.004443, mean_q: 0.696326\n",
      " 2027/5000: episode: 156, duration: 0.251s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 146.583 [66.000, 224.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.580440\n",
      " 2038/5000: episode: 157, duration: 0.231s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 132.909 [5.000, 219.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.558255\n",
      " 2054/5000: episode: 158, duration: 0.348s, episode steps: 16, steps per second: 46, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 92.562 [28.000, 184.000], mean observation: 0.176 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.756854\n",
      " 2061/5000: episode: 159, duration: 0.144s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 141.714 [15.000, 203.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000007, mean_absolute_error: 0.004442, mean_q: 0.223283\n",
      " 2081/5000: episode: 160, duration: 0.435s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 90.150 [4.000, 215.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.757513\n",
      " 2089/5000: episode: 161, duration: 0.169s, episode steps: 8, steps per second: 47, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 117.250 [24.000, 218.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000004, mean_absolute_error: 0.004438, mean_q: 0.270341\n",
      " 2098/5000: episode: 162, duration: 0.194s, episode steps: 9, steps per second: 46, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 109.444 [49.000, 207.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.555648\n",
      " 2112/5000: episode: 163, duration: 0.309s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 92.286 [3.000, 218.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000002, mean_absolute_error: 0.004444, mean_q: 0.685732\n",
      " 2126/5000: episode: 164, duration: 0.305s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 116.429 [11.000, 223.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.618498\n",
      " 2133/5000: episode: 165, duration: 0.146s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 111.857 [15.000, 192.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004433, mean_q: 0.190455\n",
      " 2140/5000: episode: 166, duration: 0.152s, episode steps: 7, steps per second: 46, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 100.857 [1.000, 192.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000003, mean_absolute_error: 0.004434, mean_q: 0.172908\n",
      " 2155/5000: episode: 167, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 120.133 [0.000, 224.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.707759\n",
      " 2167/5000: episode: 168, duration: 0.251s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 120.333 [11.000, 218.000], mean observation: 0.108 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.617680\n",
      " 2179/5000: episode: 169, duration: 0.253s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 103.917 [0.000, 193.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.662208\n",
      " 2196/5000: episode: 170, duration: 0.384s, episode steps: 17, steps per second: 44, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 112.941 [6.000, 209.000], mean observation: 0.196 [0.000, 34.000], loss: 0.000013, mean_absolute_error: 0.004446, mean_q: 0.756990\n",
      " 2208/5000: episode: 171, duration: 0.264s, episode steps: 12, steps per second: 45, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 95.917 [4.000, 217.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.718644\n",
      " 2222/5000: episode: 172, duration: 0.324s, episode steps: 14, steps per second: 43, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 107.214 [7.000, 197.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.676842\n",
      " 2228/5000: episode: 173, duration: 0.149s, episode steps: 6, steps per second: 40, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 71.500 [6.000, 161.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000003, mean_absolute_error: 0.004438, mean_q: 0.122561\n",
      " 2236/5000: episode: 174, duration: 0.178s, episode steps: 8, steps per second: 45, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 96.000 [7.000, 220.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.377072\n",
      " 2243/5000: episode: 175, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 66.857 [0.000, 182.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000007, mean_absolute_error: 0.004436, mean_q: 0.386091\n",
      " 2255/5000: episode: 176, duration: 0.262s, episode steps: 12, steps per second: 46, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 104.333 [28.000, 218.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.658803\n",
      " 2278/5000: episode: 177, duration: 0.485s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 112.043 [2.000, 211.000], mean observation: 0.330 [0.000, 46.000], loss: 0.045003, mean_absolute_error: 0.004642, mean_q: 0.835173\n",
      " 2295/5000: episode: 178, duration: 0.364s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 121.706 [10.000, 222.000], mean observation: 0.181 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.725063\n",
      " 2315/5000: episode: 179, duration: 0.422s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 115.550 [2.000, 223.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000003, mean_absolute_error: 0.004442, mean_q: 0.789928\n",
      " 2322/5000: episode: 180, duration: 0.149s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 114.857 [12.000, 218.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000011, mean_absolute_error: 0.004441, mean_q: 0.323900\n",
      " 2333/5000: episode: 181, duration: 0.237s, episode steps: 11, steps per second: 46, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 126.000 [4.000, 218.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000005, mean_absolute_error: 0.004443, mean_q: 0.597182\n",
      " 2343/5000: episode: 182, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 102.900 [6.000, 198.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000003, mean_absolute_error: 0.004444, mean_q: 0.572996\n",
      " 2359/5000: episode: 183, duration: 0.332s, episode steps: 16, steps per second: 48, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 130.750 [50.000, 211.000], mean observation: 0.174 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.658489\n",
      " 2369/5000: episode: 184, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 105.200 [20.000, 223.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004439, mean_q: 0.574921\n",
      " 2376/5000: episode: 185, duration: 0.154s, episode steps: 7, steps per second: 46, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 116.286 [37.000, 200.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000009, mean_absolute_error: 0.004443, mean_q: 0.377000\n",
      " 2384/5000: episode: 186, duration: 0.171s, episode steps: 8, steps per second: 47, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 115.750 [16.000, 168.000], mean observation: 0.058 [0.000, 16.000], loss: 0.140348, mean_absolute_error: 0.005065, mean_q: 0.541794\n",
      " 2390/5000: episode: 187, duration: 0.125s, episode steps: 6, steps per second: 48, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 49.667 [10.000, 138.000], mean observation: 0.037 [0.000, 12.000], loss: 0.000001, mean_absolute_error: 0.004434, mean_q: 0.038851\n",
      " 2398/5000: episode: 188, duration: 0.169s, episode steps: 8, steps per second: 47, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 101.125 [0.000, 191.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.253841\n",
      " 2407/5000: episode: 189, duration: 0.190s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 84.000 [6.000, 184.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.342798\n",
      " 2438/5000: episode: 190, duration: 0.635s, episode steps: 31, steps per second: 49, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 120.548 [13.000, 211.000], mean observation: 0.567 [0.000, 61.000], loss: 0.000012, mean_absolute_error: 0.004444, mean_q: 0.737479\n",
      " 2455/5000: episode: 191, duration: 0.363s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.647 [22.000, 216.000], mean observation: 0.195 [0.000, 34.000], loss: 0.000009, mean_absolute_error: 0.004447, mean_q: 0.731348\n",
      " 2463/5000: episode: 192, duration: 0.205s, episode steps: 8, steps per second: 39, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 117.875 [5.000, 187.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.301790\n",
      " 2478/5000: episode: 193, duration: 0.346s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 113.667 [14.000, 220.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.564868\n",
      " 2485/5000: episode: 194, duration: 0.141s, episode steps: 7, steps per second: 50, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 144.429 [27.000, 219.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000173, mean_absolute_error: 0.004464, mean_q: 0.173859\n",
      " 2498/5000: episode: 195, duration: 0.347s, episode steps: 13, steps per second: 37, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 106.154 [1.000, 171.000], mean observation: 0.106 [0.000, 26.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.532670\n",
      " 2505/5000: episode: 196, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 93.429 [13.000, 207.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.210164\n",
      " 2525/5000: episode: 197, duration: 0.488s, episode steps: 20, steps per second: 41, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 120.200 [29.000, 220.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.820181\n",
      " 2538/5000: episode: 198, duration: 0.272s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 88.846 [0.000, 197.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.430161\n",
      " 2563/5000: episode: 199, duration: 0.604s, episode steps: 25, steps per second: 41, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 141.080 [5.000, 217.000], mean observation: 0.384 [0.000, 50.000], loss: 0.035062, mean_absolute_error: 0.004616, mean_q: 0.722377\n",
      " 2588/5000: episode: 200, duration: 0.545s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 125.960 [21.000, 219.000], mean observation: 0.377 [0.000, 50.000], loss: 0.000156, mean_absolute_error: 0.004454, mean_q: 0.714087\n",
      " 2604/5000: episode: 201, duration: 0.366s, episode steps: 16, steps per second: 44, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 106.562 [16.000, 196.000], mean observation: 0.173 [0.000, 32.000], loss: 0.065825, mean_absolute_error: 0.004735, mean_q: 0.587461\n",
      " 2615/5000: episode: 202, duration: 0.240s, episode steps: 11, steps per second: 46, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 117.182 [34.000, 205.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000041, mean_absolute_error: 0.004448, mean_q: 0.286325\n",
      " 2635/5000: episode: 203, duration: 0.464s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 105.900 [14.000, 211.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000003, mean_absolute_error: 0.004445, mean_q: 0.404629\n",
      " 2650/5000: episode: 204, duration: 0.345s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 100.467 [6.000, 197.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000005, mean_absolute_error: 0.004443, mean_q: 0.616021\n",
      " 2670/5000: episode: 205, duration: 0.424s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 96.350 [3.000, 222.000], mean observation: 0.259 [0.000, 40.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.672056\n",
      " 2684/5000: episode: 206, duration: 0.299s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 115.786 [2.000, 221.000], mean observation: 0.140 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.568880\n",
      " 2695/5000: episode: 207, duration: 0.225s, episode steps: 11, steps per second: 49, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 107.364 [7.000, 181.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.500410\n",
      " 2710/5000: episode: 208, duration: 0.326s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 114.067 [5.000, 217.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000000, mean_absolute_error: 0.004439, mean_q: 0.362901\n",
      " 2724/5000: episode: 209, duration: 0.290s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 112.500 [25.000, 205.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000002, mean_absolute_error: 0.004442, mean_q: 0.405664\n",
      " 2732/5000: episode: 210, duration: 0.162s, episode steps: 8, steps per second: 49, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 104.125 [23.000, 199.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000012, mean_absolute_error: 0.004444, mean_q: 0.108074\n",
      " 2740/5000: episode: 211, duration: 0.173s, episode steps: 8, steps per second: 46, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 128.875 [25.000, 215.000], mean observation: 0.049 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.087419\n",
      " 2761/5000: episode: 212, duration: 0.458s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.286 [2.000, 217.000], mean observation: 0.275 [0.000, 42.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.604182\n",
      " 2790/5000: episode: 213, duration: 0.668s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 107.897 [1.000, 218.000], mean observation: 0.504 [0.000, 58.000], loss: 0.000006, mean_absolute_error: 0.004441, mean_q: 0.717675\n",
      " 2800/5000: episode: 214, duration: 0.261s, episode steps: 10, steps per second: 38, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 118.900 [3.000, 204.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000048, mean_absolute_error: 0.004451, mean_q: 0.237912\n",
      " 2809/5000: episode: 215, duration: 0.209s, episode steps: 9, steps per second: 43, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 111.222 [17.000, 221.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.235923\n",
      " 2817/5000: episode: 216, duration: 0.197s, episode steps: 8, steps per second: 41, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 131.125 [1.000, 221.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000057, mean_absolute_error: 0.004445, mean_q: 0.059991\n",
      " 2828/5000: episode: 217, duration: 0.232s, episode steps: 11, steps per second: 47, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 99.909 [2.000, 221.000], mean observation: 0.094 [0.000, 22.000], loss: 0.000004, mean_absolute_error: 0.004438, mean_q: 0.234754\n",
      " 2837/5000: episode: 218, duration: 0.182s, episode steps: 9, steps per second: 49, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 77.333 [14.000, 223.000], mean observation: 0.065 [0.000, 18.000], loss: 0.000011, mean_absolute_error: 0.004440, mean_q: 0.162406\n",
      " 2849/5000: episode: 219, duration: 0.248s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 127.500 [5.000, 217.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000009, mean_absolute_error: 0.004443, mean_q: 0.253594\n",
      " 2868/5000: episode: 220, duration: 0.405s, episode steps: 19, steps per second: 47, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 117.579 [2.000, 219.000], mean observation: 0.212 [0.000, 38.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.646130\n",
      " 2874/5000: episode: 221, duration: 0.132s, episode steps: 6, steps per second: 45, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 133.000 [45.000, 207.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000011, mean_absolute_error: 0.004437, mean_q: 0.029650\n",
      " 2883/5000: episode: 222, duration: 0.193s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 153.222 [40.000, 223.000], mean observation: 0.069 [0.000, 18.000], loss: 0.002598, mean_absolute_error: 0.004518, mean_q: 0.308576\n",
      " 2893/5000: episode: 223, duration: 0.249s, episode steps: 10, steps per second: 40, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 120.000 [4.000, 198.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.554424\n",
      " 2901/5000: episode: 224, duration: 0.198s, episode steps: 8, steps per second: 40, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 81.875 [3.000, 184.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.331284\n",
      " 2911/5000: episode: 225, duration: 0.236s, episode steps: 10, steps per second: 42, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 84.700 [3.000, 204.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000005, mean_absolute_error: 0.004438, mean_q: 0.453450\n",
      " 2917/5000: episode: 226, duration: 0.124s, episode steps: 6, steps per second: 48, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 96.500 [47.000, 200.000], mean observation: 0.036 [0.000, 12.000], loss: 0.000002, mean_absolute_error: 0.004431, mean_q: 0.147394\n",
      " 2926/5000: episode: 227, duration: 0.193s, episode steps: 9, steps per second: 47, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 130.333 [24.000, 194.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000016, mean_absolute_error: 0.004447, mean_q: 0.656205\n",
      " 2940/5000: episode: 228, duration: 0.300s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 121.714 [34.000, 217.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.725363\n",
      " 2949/5000: episode: 229, duration: 0.208s, episode steps: 9, steps per second: 43, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 93.444 [19.000, 203.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.527283\n",
      " 2958/5000: episode: 230, duration: 0.188s, episode steps: 9, steps per second: 48, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 87.000 [8.000, 185.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000035, mean_absolute_error: 0.004449, mean_q: 0.362787\n",
      " 2976/5000: episode: 231, duration: 0.369s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 121.056 [10.000, 207.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.754838\n",
      " 2993/5000: episode: 232, duration: 0.351s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 99.824 [7.000, 165.000], mean observation: 0.185 [0.000, 34.000], loss: 0.000002, mean_absolute_error: 0.004442, mean_q: 0.753809\n",
      " 3000/5000: episode: 233, duration: 0.138s, episode steps: 7, steps per second: 51, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 104.143 [0.000, 218.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000025, mean_absolute_error: 0.004440, mean_q: 0.175563\n",
      " 3006/5000: episode: 234, duration: 0.121s, episode steps: 6, steps per second: 49, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 116.667 [11.000, 199.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000018, mean_absolute_error: 0.004433, mean_q: 0.150092\n",
      " 3015/5000: episode: 235, duration: 0.178s, episode steps: 9, steps per second: 51, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 111.556 [22.000, 222.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000004, mean_absolute_error: 0.004440, mean_q: 0.475090\n",
      " 3028/5000: episode: 236, duration: 0.276s, episode steps: 13, steps per second: 47, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 131.846 [41.000, 219.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.659904\n",
      " 3051/5000: episode: 237, duration: 0.471s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 116.000 [14.000, 202.000], mean observation: 0.332 [0.000, 46.000], loss: 0.000000, mean_absolute_error: 0.004444, mean_q: 0.816011\n",
      " 3075/5000: episode: 238, duration: 0.490s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 121.458 [0.000, 220.000], mean observation: 0.356 [0.000, 48.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.795614\n",
      " 3089/5000: episode: 239, duration: 0.299s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 153.429 [27.000, 222.000], mean observation: 0.140 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.756557\n",
      " 3104/5000: episode: 240, duration: 0.314s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 129.000 [22.000, 221.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.759046\n",
      " 3114/5000: episode: 241, duration: 0.257s, episode steps: 10, steps per second: 39, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 100.300 [14.000, 190.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.573585\n",
      " 3126/5000: episode: 242, duration: 0.249s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 126.833 [14.000, 198.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000002, mean_absolute_error: 0.004442, mean_q: 0.648284\n",
      " 3139/5000: episode: 243, duration: 0.270s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 114.462 [14.000, 193.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.718401\n",
      " 3146/5000: episode: 244, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 113.429 [43.000, 205.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.459181\n",
      " 3160/5000: episode: 245, duration: 0.295s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 141.857 [48.000, 223.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000002, mean_absolute_error: 0.004444, mean_q: 0.735756\n",
      " 3167/5000: episode: 246, duration: 0.171s, episode steps: 7, steps per second: 41, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 113.429 [0.000, 207.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000002, mean_absolute_error: 0.004436, mean_q: 0.196095\n",
      " 3180/5000: episode: 247, duration: 0.320s, episode steps: 13, steps per second: 41, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 89.538 [18.000, 185.000], mean observation: 0.120 [0.000, 26.000], loss: 0.000006, mean_absolute_error: 0.004445, mean_q: 0.680429\n",
      " 3193/5000: episode: 248, duration: 0.297s, episode steps: 13, steps per second: 44, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 99.000 [26.000, 222.000], mean observation: 0.124 [0.000, 26.000], loss: 0.059443, mean_absolute_error: 0.004772, mean_q: 0.329412\n",
      " 3218/5000: episode: 249, duration: 0.580s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 115.760 [3.000, 212.000], mean observation: 0.385 [0.000, 50.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.766931\n",
      " 3231/5000: episode: 250, duration: 0.273s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 112.000 [5.000, 215.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000003, mean_absolute_error: 0.004441, mean_q: 0.464365\n",
      " 3250/5000: episode: 251, duration: 0.457s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 104.474 [0.000, 203.000], mean observation: 0.215 [0.000, 38.000], loss: 0.000005, mean_absolute_error: 0.004440, mean_q: 0.535930\n",
      " 3266/5000: episode: 252, duration: 0.397s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 118.938 [28.000, 190.000], mean observation: 0.172 [0.000, 32.000], loss: 0.000042, mean_absolute_error: 0.004448, mean_q: 0.546561\n",
      " 3291/5000: episode: 253, duration: 0.580s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 104.800 [8.000, 218.000], mean observation: 0.380 [0.000, 50.000], loss: 0.013925, mean_absolute_error: 0.004549, mean_q: 0.727052\n",
      " 3330/5000: episode: 254, duration: 0.935s, episode steps: 39, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 96.282 [1.000, 221.000], mean observation: 0.791 [0.000, 78.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.799812\n",
      " 3347/5000: episode: 255, duration: 0.421s, episode steps: 17, steps per second: 40, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 95.647 [5.000, 212.000], mean observation: 0.189 [0.000, 34.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.574833\n",
      " 3368/5000: episode: 256, duration: 0.527s, episode steps: 21, steps per second: 40, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 122.143 [11.000, 215.000], mean observation: 0.282 [0.000, 42.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.622513\n",
      " 3377/5000: episode: 257, duration: 0.213s, episode steps: 9, steps per second: 42, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 79.000 [0.000, 174.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000005, mean_absolute_error: 0.004440, mean_q: 0.278541\n",
      " 3384/5000: episode: 258, duration: 0.176s, episode steps: 7, steps per second: 40, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 136.571 [28.000, 214.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000003, mean_absolute_error: 0.004438, mean_q: 0.073700\n",
      " 3395/5000: episode: 259, duration: 0.279s, episode steps: 11, steps per second: 39, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 106.182 [5.000, 214.000], mean observation: 0.095 [0.000, 22.000], loss: 0.092335, mean_absolute_error: 0.004865, mean_q: 0.379641\n",
      " 3409/5000: episode: 260, duration: 0.361s, episode steps: 14, steps per second: 39, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 118.643 [55.000, 209.000], mean observation: 0.137 [0.000, 28.000], loss: 0.000036, mean_absolute_error: 0.004446, mean_q: 0.277446\n",
      " 3420/5000: episode: 261, duration: 0.228s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 98.636 [1.000, 201.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.069519\n",
      " 3428/5000: episode: 262, duration: 0.171s, episode steps: 8, steps per second: 47, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 92.375 [33.000, 182.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000022, mean_absolute_error: 0.004447, mean_q: 0.053600\n",
      " 3440/5000: episode: 263, duration: 0.247s, episode steps: 12, steps per second: 49, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 83.167 [6.000, 194.000], mean observation: 0.106 [0.000, 24.000], loss: 0.000004, mean_absolute_error: 0.004441, mean_q: 0.144637\n",
      " 3453/5000: episode: 264, duration: 0.263s, episode steps: 13, steps per second: 49, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 153.385 [29.000, 197.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000006, mean_absolute_error: 0.004442, mean_q: 0.223030\n",
      " 3474/5000: episode: 265, duration: 0.502s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 87.810 [0.000, 206.000], mean observation: 0.254 [0.000, 42.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.530289\n",
      " 3486/5000: episode: 266, duration: 0.249s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 112.917 [0.000, 198.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.165635\n",
      " 3499/5000: episode: 267, duration: 0.264s, episode steps: 13, steps per second: 49, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 93.385 [1.000, 204.000], mean observation: 0.119 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.130621\n",
      " 3511/5000: episode: 268, duration: 0.297s, episode steps: 12, steps per second: 40, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 109.750 [10.000, 216.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000003, mean_absolute_error: 0.004439, mean_q: 0.244659\n",
      " 3527/5000: episode: 269, duration: 0.383s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 110.938 [2.000, 217.000], mean observation: 0.176 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.188485\n",
      " 3545/5000: episode: 270, duration: 0.395s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 116.000 [5.000, 197.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.273980\n",
      " 3560/5000: episode: 271, duration: 0.382s, episode steps: 15, steps per second: 39, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 128.200 [15.000, 220.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.304030\n",
      " 3577/5000: episode: 272, duration: 0.383s, episode steps: 17, steps per second: 44, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 125.235 [11.000, 216.000], mean observation: 0.194 [0.000, 34.000], loss: 0.000003, mean_absolute_error: 0.004443, mean_q: 0.345995\n",
      " 3594/5000: episode: 273, duration: 0.356s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 126.059 [24.000, 222.000], mean observation: 0.194 [0.000, 34.000], loss: 0.000010, mean_absolute_error: 0.004443, mean_q: 0.363710\n",
      " 3608/5000: episode: 274, duration: 0.293s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 116.786 [11.000, 213.000], mean observation: 0.136 [0.000, 28.000], loss: 0.004143, mean_absolute_error: 0.004515, mean_q: 0.240628\n",
      " 3623/5000: episode: 275, duration: 0.304s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 95.933 [5.000, 212.000], mean observation: 0.139 [0.000, 30.000], loss: 0.000006, mean_absolute_error: 0.004441, mean_q: 0.515985\n",
      " 3635/5000: episode: 276, duration: 0.242s, episode steps: 12, steps per second: 50, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 86.250 [12.000, 219.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004439, mean_q: 0.353567\n",
      " 3642/5000: episode: 277, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 117.429 [28.000, 172.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004435, mean_q: 0.068747\n",
      " 3651/5000: episode: 278, duration: 0.189s, episode steps: 9, steps per second: 48, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 85.778 [11.000, 166.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000000, mean_absolute_error: 0.004438, mean_q: 0.281133\n",
      " 3659/5000: episode: 279, duration: 0.163s, episode steps: 8, steps per second: 49, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 108.375 [42.000, 218.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000392, mean_absolute_error: 0.004469, mean_q: 0.072442\n",
      " 3685/5000: episode: 280, duration: 0.541s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 88.769 [1.000, 216.000], mean observation: 0.413 [0.000, 52.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.576112\n",
      " 3695/5000: episode: 281, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 72.700 [3.000, 162.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.325965\n",
      " 3704/5000: episode: 282, duration: 0.184s, episode steps: 9, steps per second: 49, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 130.222 [44.000, 218.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000004, mean_absolute_error: 0.004440, mean_q: 0.273820\n",
      " 3723/5000: episode: 283, duration: 0.462s, episode steps: 19, steps per second: 41, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 117.842 [17.000, 217.000], mean observation: 0.235 [0.000, 38.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.642973\n",
      " 3731/5000: episode: 284, duration: 0.205s, episode steps: 8, steps per second: 39, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 66.875 [9.000, 153.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000003, mean_absolute_error: 0.004437, mean_q: 0.139084\n",
      " 3743/5000: episode: 285, duration: 0.278s, episode steps: 12, steps per second: 43, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 105.833 [15.000, 210.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000003, mean_absolute_error: 0.004441, mean_q: 0.475505\n",
      " 3755/5000: episode: 286, duration: 0.266s, episode steps: 12, steps per second: 45, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 96.417 [4.000, 203.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.511125\n",
      " 3762/5000: episode: 287, duration: 0.171s, episode steps: 7, steps per second: 41, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 107.143 [1.000, 218.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000021, mean_absolute_error: 0.004442, mean_q: 0.049303\n",
      " 3775/5000: episode: 288, duration: 0.345s, episode steps: 13, steps per second: 38, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 138.077 [38.000, 224.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.629282\n",
      " 3789/5000: episode: 289, duration: 0.379s, episode steps: 14, steps per second: 37, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 98.214 [9.000, 195.000], mean observation: 0.141 [0.000, 28.000], loss: 0.001358, mean_absolute_error: 0.004487, mean_q: 0.566212\n",
      " 3799/5000: episode: 290, duration: 0.262s, episode steps: 10, steps per second: 38, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 96.100 [7.000, 221.000], mean observation: 0.080 [0.000, 20.000], loss: 0.000003, mean_absolute_error: 0.004440, mean_q: 0.410101\n",
      " 3812/5000: episode: 291, duration: 0.276s, episode steps: 13, steps per second: 47, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 102.231 [5.000, 179.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.545656\n",
      " 3827/5000: episode: 292, duration: 0.312s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 117.733 [4.000, 207.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.645098\n",
      " 3837/5000: episode: 293, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 156.000 [61.000, 223.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.318485\n",
      " 3848/5000: episode: 294, duration: 0.226s, episode steps: 11, steps per second: 49, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 148.000 [32.000, 223.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000000, mean_absolute_error: 0.004437, mean_q: 0.408874\n",
      " 3859/5000: episode: 295, duration: 0.227s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 140.818 [35.000, 212.000], mean observation: 0.086 [0.000, 22.000], loss: 0.056878, mean_absolute_error: 0.004653, mean_q: 0.521116\n",
      " 3867/5000: episode: 296, duration: 0.165s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 100.875 [5.000, 171.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.127717\n",
      " 3874/5000: episode: 297, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 161.429 [13.000, 221.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000005, mean_absolute_error: 0.004436, mean_q: 0.043573\n",
      " 3884/5000: episode: 298, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 104.100 [11.000, 184.000], mean observation: 0.081 [0.000, 20.000], loss: 0.076470, mean_absolute_error: 0.004855, mean_q: 0.352585\n",
      " 3915/5000: episode: 299, duration: 0.631s, episode steps: 31, steps per second: 49, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 105.613 [16.000, 224.000], mean observation: 0.541 [0.000, 61.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.759008\n",
      " 3938/5000: episode: 300, duration: 0.551s, episode steps: 23, steps per second: 42, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 124.870 [18.000, 218.000], mean observation: 0.325 [0.000, 46.000], loss: 0.000164, mean_absolute_error: 0.004454, mean_q: 0.677359\n",
      " 3950/5000: episode: 301, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 131.083 [12.000, 216.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.532269\n",
      " 3961/5000: episode: 302, duration: 0.229s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 90.455 [2.000, 207.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000332, mean_absolute_error: 0.004462, mean_q: 0.149392\n",
      " 3968/5000: episode: 303, duration: 0.145s, episode steps: 7, steps per second: 48, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 131.000 [2.000, 221.000], mean observation: 0.037 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004430, mean_q: 0.032661\n",
      " 3981/5000: episode: 304, duration: 0.270s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 111.308 [63.000, 193.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000042, mean_absolute_error: 0.004449, mean_q: 0.546870\n",
      " 3988/5000: episode: 305, duration: 0.139s, episode steps: 7, steps per second: 50, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 99.143 [6.000, 203.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.189858\n",
      " 3996/5000: episode: 306, duration: 0.167s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 114.625 [14.000, 178.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000041, mean_absolute_error: 0.004446, mean_q: 0.065583\n",
      " 4005/5000: episode: 307, duration: 0.179s, episode steps: 9, steps per second: 50, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 72.000 [0.000, 201.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.066591\n",
      " 4018/5000: episode: 308, duration: 0.289s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 129.692 [3.000, 211.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.498591\n",
      " 4032/5000: episode: 309, duration: 0.365s, episode steps: 14, steps per second: 38, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 108.214 [21.000, 224.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000472, mean_absolute_error: 0.004463, mean_q: 0.400831\n",
      " 4043/5000: episode: 310, duration: 0.244s, episode steps: 11, steps per second: 45, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 89.545 [1.000, 217.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.329658\n",
      " 4051/5000: episode: 311, duration: 0.167s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 89.000 [2.000, 172.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.265705\n",
      " 4064/5000: episode: 312, duration: 0.264s, episode steps: 13, steps per second: 49, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 101.308 [26.000, 216.000], mean observation: 0.124 [0.000, 26.000], loss: 0.064258, mean_absolute_error: 0.004765, mean_q: 0.519552\n",
      " 4074/5000: episode: 313, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 99.000 [26.000, 163.000], mean observation: 0.076 [0.000, 20.000], loss: 0.004515, mean_absolute_error: 0.004561, mean_q: 0.296362\n",
      " 4089/5000: episode: 314, duration: 0.315s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 102.667 [2.000, 218.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000013, mean_absolute_error: 0.004444, mean_q: 0.404913\n",
      " 4107/5000: episode: 315, duration: 0.372s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 107.556 [6.000, 221.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000005, mean_absolute_error: 0.004442, mean_q: 0.581512\n",
      " 4114/5000: episode: 316, duration: 0.154s, episode steps: 7, steps per second: 45, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 131.714 [14.000, 221.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004434, mean_q: 0.033384\n",
      " 4130/5000: episode: 317, duration: 0.397s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 109.312 [1.000, 180.000], mean observation: 0.176 [0.000, 32.000], loss: 0.063135, mean_absolute_error: 0.004730, mean_q: 0.461509\n",
      " 4150/5000: episode: 318, duration: 0.443s, episode steps: 20, steps per second: 45, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 108.000 [4.000, 215.000], mean observation: 0.229 [0.000, 40.000], loss: 0.000005, mean_absolute_error: 0.004442, mean_q: 0.289787\n",
      " 4172/5000: episode: 319, duration: 0.462s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 101.364 [7.000, 218.000], mean observation: 0.307 [0.000, 44.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.647473\n",
      " 4177/5000: episode: 320, duration: 0.101s, episode steps: 5, steps per second: 49, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 108.200 [33.000, 164.000], mean observation: 0.030 [0.000, 10.000], loss: 0.000001, mean_absolute_error: 0.004433, mean_q: 0.012048\n",
      " 4191/5000: episode: 321, duration: 0.324s, episode steps: 14, steps per second: 43, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 84.143 [0.000, 218.000], mean observation: 0.128 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.485383\n",
      " 4197/5000: episode: 322, duration: 0.163s, episode steps: 6, steps per second: 37, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 85.667 [7.000, 208.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000003, mean_absolute_error: 0.004434, mean_q: 0.016606\n",
      " 4206/5000: episode: 323, duration: 0.226s, episode steps: 9, steps per second: 40, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 77.333 [25.000, 203.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000002, mean_absolute_error: 0.004436, mean_q: 0.069174\n",
      " 4219/5000: episode: 324, duration: 0.323s, episode steps: 13, steps per second: 40, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 153.231 [14.000, 213.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.214959\n",
      " 4233/5000: episode: 325, duration: 0.348s, episode steps: 14, steps per second: 40, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 120.643 [4.000, 218.000], mean observation: 0.141 [0.000, 28.000], loss: 0.075058, mean_absolute_error: 0.004777, mean_q: 0.578523\n",
      " 4253/5000: episode: 326, duration: 0.497s, episode steps: 20, steps per second: 40, episode reward: 1.000, mean reward: 0.050 [0.000, 1.000], mean action: 109.850 [14.000, 212.000], mean observation: 0.255 [0.000, 39.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.628167\n",
      " 4264/5000: episode: 327, duration: 0.262s, episode steps: 11, steps per second: 42, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 103.364 [18.000, 195.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000003, mean_absolute_error: 0.004440, mean_q: 0.291992\n",
      " 4278/5000: episode: 328, duration: 0.297s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 93.143 [13.000, 222.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000003, mean_absolute_error: 0.004442, mean_q: 0.335511\n",
      " 4296/5000: episode: 329, duration: 0.429s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 115.500 [27.000, 223.000], mean observation: 0.208 [0.000, 36.000], loss: 0.000005, mean_absolute_error: 0.004442, mean_q: 0.614050\n",
      " 4306/5000: episode: 330, duration: 0.230s, episode steps: 10, steps per second: 43, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 112.300 [14.000, 220.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.364030\n",
      " 4313/5000: episode: 331, duration: 0.150s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 56.286 [3.000, 151.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004432, mean_q: 0.076361\n",
      " 4327/5000: episode: 332, duration: 0.363s, episode steps: 14, steps per second: 39, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 138.929 [37.000, 219.000], mean observation: 0.120 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.518733\n",
      " 4344/5000: episode: 333, duration: 0.397s, episode steps: 17, steps per second: 43, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 112.824 [8.000, 205.000], mean observation: 0.193 [0.000, 34.000], loss: 0.000005, mean_absolute_error: 0.004443, mean_q: 0.642130\n",
      " 4352/5000: episode: 334, duration: 0.163s, episode steps: 8, steps per second: 49, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 108.250 [3.000, 187.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000002, mean_absolute_error: 0.004437, mean_q: 0.199678\n",
      " 4367/5000: episode: 335, duration: 0.307s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 114.400 [22.000, 211.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.631062\n",
      " 4381/5000: episode: 336, duration: 0.294s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 143.857 [6.000, 220.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.558176\n",
      " 4388/5000: episode: 337, duration: 0.141s, episode steps: 7, steps per second: 50, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 96.429 [23.000, 208.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.084600\n",
      " 4405/5000: episode: 338, duration: 0.352s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.000 [1.000, 204.000], mean observation: 0.190 [0.000, 34.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.650194\n",
      " 4415/5000: episode: 339, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 96.700 [21.000, 161.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.330896\n",
      " 4429/5000: episode: 340, duration: 0.291s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 119.786 [8.000, 215.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.663551\n",
      " 4439/5000: episode: 341, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 97.000 [13.000, 219.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000003, mean_absolute_error: 0.004436, mean_q: 0.372301\n",
      " 4447/5000: episode: 342, duration: 0.167s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 101.375 [13.000, 209.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000016, mean_absolute_error: 0.004441, mean_q: 0.219729\n",
      " 4459/5000: episode: 343, duration: 0.254s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 107.917 [16.000, 221.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.530918\n",
      " 4482/5000: episode: 344, duration: 0.460s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 102.043 [12.000, 213.000], mean observation: 0.329 [0.000, 46.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.774043\n",
      " 4505/5000: episode: 345, duration: 0.471s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 95.478 [6.000, 221.000], mean observation: 0.332 [0.000, 46.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.723140\n",
      " 4514/5000: episode: 346, duration: 0.180s, episode steps: 9, steps per second: 50, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 72.778 [3.000, 168.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.134083\n",
      " 4528/5000: episode: 347, duration: 0.287s, episode steps: 14, steps per second: 49, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 139.143 [10.000, 219.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.532199\n",
      " 4546/5000: episode: 348, duration: 0.376s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 106.000 [7.000, 224.000], mean observation: 0.204 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.613367\n",
      " 4554/5000: episode: 349, duration: 0.165s, episode steps: 8, steps per second: 48, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 101.000 [22.000, 216.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000003, mean_absolute_error: 0.004439, mean_q: 0.294429\n",
      " 4568/5000: episode: 350, duration: 0.294s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 125.571 [3.000, 200.000], mean observation: 0.141 [0.000, 28.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.584985\n",
      " 4581/5000: episode: 351, duration: 0.261s, episode steps: 13, steps per second: 50, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 137.538 [13.000, 204.000], mean observation: 0.124 [0.000, 26.000], loss: 0.000283, mean_absolute_error: 0.004464, mean_q: 0.477901\n",
      " 4590/5000: episode: 352, duration: 0.177s, episode steps: 9, steps per second: 51, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 100.111 [36.000, 192.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000003, mean_absolute_error: 0.004438, mean_q: 0.243032\n",
      " 4608/5000: episode: 353, duration: 0.378s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 105.056 [7.000, 214.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000000, mean_absolute_error: 0.004442, mean_q: 0.720183\n",
      " 4614/5000: episode: 354, duration: 0.130s, episode steps: 6, steps per second: 46, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 127.000 [96.000, 167.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000003, mean_absolute_error: 0.004435, mean_q: 0.048788\n",
      " 4627/5000: episode: 355, duration: 0.264s, episode steps: 13, steps per second: 49, episode reward: 1.000, mean reward: 0.077 [0.000, 1.000], mean action: 110.462 [10.000, 203.000], mean observation: 0.120 [0.000, 25.000], loss: 0.000004, mean_absolute_error: 0.004443, mean_q: 0.626600\n",
      " 4650/5000: episode: 356, duration: 0.477s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 122.870 [5.000, 218.000], mean observation: 0.317 [0.000, 46.000], loss: 0.000000, mean_absolute_error: 0.004443, mean_q: 0.700563\n",
      " 4658/5000: episode: 357, duration: 0.164s, episode steps: 8, steps per second: 49, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 139.750 [38.000, 195.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000004, mean_absolute_error: 0.004440, mean_q: 0.204215\n",
      " 4664/5000: episode: 358, duration: 0.128s, episode steps: 6, steps per second: 47, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 121.667 [28.000, 215.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.055582\n",
      " 4679/5000: episode: 359, duration: 0.323s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 130.533 [24.000, 216.000], mean observation: 0.156 [0.000, 30.000], loss: 0.000003, mean_absolute_error: 0.004443, mean_q: 0.692461\n",
      " 4690/5000: episode: 360, duration: 0.229s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 109.182 [8.000, 215.000], mean observation: 0.095 [0.000, 22.000], loss: 0.000000, mean_absolute_error: 0.004440, mean_q: 0.421688\n",
      " 4706/5000: episode: 361, duration: 0.340s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 110.875 [4.000, 214.000], mean observation: 0.172 [0.000, 32.000], loss: 0.000000, mean_absolute_error: 0.004441, mean_q: 0.630912\n",
      " 4718/5000: episode: 362, duration: 0.248s, episode steps: 12, steps per second: 48, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 113.083 [30.000, 187.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000002, mean_absolute_error: 0.004441, mean_q: 0.428431\n",
      " 4724/5000: episode: 363, duration: 0.125s, episode steps: 6, steps per second: 48, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 105.000 [10.000, 204.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000001, mean_absolute_error: 0.004433, mean_q: 0.060503\n",
      " 4740/5000: episode: 364, duration: 0.346s, episode steps: 16, steps per second: 46, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 80.688 [6.000, 210.000], mean observation: 0.176 [0.000, 32.000], loss: 0.065978, mean_absolute_error: 0.004735, mean_q: 0.684272\n",
      " 4750/5000: episode: 365, duration: 0.261s, episode steps: 10, steps per second: 38, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 123.500 [8.000, 223.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004439, mean_q: 0.412503\n",
      " 4774/5000: episode: 366, duration: 0.583s, episode steps: 24, steps per second: 41, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 120.917 [14.000, 223.000], mean observation: 0.354 [0.000, 48.000], loss: 0.000001, mean_absolute_error: 0.004443, mean_q: 0.765753\n",
      " 4792/5000: episode: 367, duration: 0.433s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 134.778 [5.000, 210.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.707732\n",
      " 4810/5000: episode: 368, duration: 0.425s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 117.444 [15.000, 203.000], mean observation: 0.216 [0.000, 36.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.677143\n",
      " 4817/5000: episode: 369, duration: 0.186s, episode steps: 7, steps per second: 38, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 102.429 [16.000, 187.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004436, mean_q: 0.147630\n",
      " 4834/5000: episode: 370, duration: 0.378s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 100.824 [8.000, 196.000], mean observation: 0.196 [0.000, 34.000], loss: 0.000046, mean_absolute_error: 0.004448, mean_q: 0.677018\n",
      " 4842/5000: episode: 371, duration: 0.211s, episode steps: 8, steps per second: 38, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 130.125 [43.000, 184.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.364425\n",
      " 4857/5000: episode: 372, duration: 0.335s, episode steps: 15, steps per second: 45, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 104.400 [17.000, 189.000], mean observation: 0.158 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004440, mean_q: 0.601221\n",
      " 4872/5000: episode: 373, duration: 0.381s, episode steps: 15, steps per second: 39, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 146.000 [47.000, 223.000], mean observation: 0.155 [0.000, 30.000], loss: 0.000001, mean_absolute_error: 0.004441, mean_q: 0.504340\n",
      " 4880/5000: episode: 374, duration: 0.186s, episode steps: 8, steps per second: 43, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 108.625 [22.000, 220.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000002, mean_absolute_error: 0.004437, mean_q: 0.342985\n",
      " 4887/5000: episode: 375, duration: 0.148s, episode steps: 7, steps per second: 47, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 82.571 [24.000, 142.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.175772\n",
      " 4893/5000: episode: 376, duration: 0.135s, episode steps: 6, steps per second: 44, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 152.167 [92.000, 186.000], mean observation: 0.038 [0.000, 12.000], loss: 0.000002, mean_absolute_error: 0.004437, mean_q: 0.189215\n",
      " 4901/5000: episode: 377, duration: 0.195s, episode steps: 8, steps per second: 41, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 94.250 [24.000, 180.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000001, mean_absolute_error: 0.004438, mean_q: 0.142072\n",
      " 4911/5000: episode: 378, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 95.800 [7.000, 222.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000001, mean_absolute_error: 0.004437, mean_q: 0.284032\n",
      " 4919/5000: episode: 379, duration: 0.193s, episode steps: 8, steps per second: 41, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 104.500 [5.000, 221.000], mean observation: 0.058 [0.000, 16.000], loss: 0.000018, mean_absolute_error: 0.004445, mean_q: 0.158540\n",
      " 4938/5000: episode: 380, duration: 0.491s, episode steps: 19, steps per second: 39, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 115.526 [0.000, 215.000], mean observation: 0.237 [0.000, 38.000], loss: 0.000001, mean_absolute_error: 0.004442, mean_q: 0.687930\n",
      " 4947/5000: episode: 381, duration: 0.246s, episode steps: 9, steps per second: 37, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 105.667 [4.000, 185.000], mean observation: 0.069 [0.000, 18.000], loss: 0.000007, mean_absolute_error: 0.004440, mean_q: 0.280528\n",
      " 4959/5000: episode: 382, duration: 0.266s, episode steps: 12, steps per second: 45, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 98.167 [6.000, 197.000], mean observation: 0.109 [0.000, 24.000], loss: 0.000013, mean_absolute_error: 0.004443, mean_q: 0.503837\n",
      " 4969/5000: episode: 383, duration: 0.253s, episode steps: 10, steps per second: 39, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 126.500 [12.000, 224.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004440, mean_q: 0.431563\n",
      " 4979/5000: episode: 384, duration: 0.241s, episode steps: 10, steps per second: 42, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 125.100 [13.000, 222.000], mean observation: 0.081 [0.000, 20.000], loss: 0.000002, mean_absolute_error: 0.004438, mean_q: 0.434447\n",
      " 4992/5000: episode: 385, duration: 0.279s, episode steps: 13, steps per second: 47, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 110.385 [6.000, 212.000], mean observation: 0.120 [0.000, 26.000], loss: 0.000004, mean_absolute_error: 0.004439, mean_q: 0.342278\n",
      " 4999/5000: episode: 386, duration: 0.180s, episode steps: 7, steps per second: 39, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 116.286 [11.000, 209.000], mean observation: 0.047 [0.000, 14.000], loss: 0.000001, mean_absolute_error: 0.004434, mean_q: 0.144886\n",
      "done, took 112.203 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 28\n",
      "Episode 2: reward: -1.000, steps: 15\n",
      "Episode 3: reward: -1.000, steps: 9\n",
      "Episode 4: reward: -1.000, steps: 16\n",
      "Episode 5: reward: -1.000, steps: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7339876a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.agents.sarsa import SarsaAgent\n",
    "\n",
    "\n",
    "ENV_NAME = 'Renju'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = RenjuTEST(1, None)\n",
    "np.random.seed(123)\n",
    "nb_actions = env.action_space\n",
    "\n",
    "\n",
    "policy = BoltzmannQPolicy()\n",
    "sarsa = SarsaAgent(model=model_policy, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\n",
    "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "sarsa.fit(env, nb_steps=5000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "sarsa.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 5\n",
      "Episode 2: reward: -1.000, steps: 16\n",
      "Episode 3: reward: -1.000, steps: 7\n",
      "Episode 4: reward: -1.000, steps: 7\n",
      "Episode 5: reward: -1.000, steps: 8\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 10\n",
      "Episode 2: reward: -1.000, steps: 19\n",
      "Episode 3: reward: -1.000, steps: 5\n",
      "Episode 4: reward: -1.000, steps: 7\n",
      "Episode 5: reward: -1.000, steps: 10\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 9\n",
      "Episode 2: reward: -1.000, steps: 13\n",
      "Episode 3: reward: -1.000, steps: 7\n",
      "Episode 4: reward: -1.000, steps: 6\n",
      "Episode 5: reward: -1.000, steps: 9\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 10\n",
      "Episode 2: reward: -1.000, steps: 11\n",
      "Episode 3: reward: -1.000, steps: 21\n",
      "Episode 4: reward: 1.000, steps: 17\n",
      "Episode 5: reward: -1.000, steps: 11\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 20\n",
      "Episode 2: reward: -1.000, steps: 7\n",
      "Episode 3: reward: -1.000, steps: 16\n",
      "Episode 4: reward: -1.000, steps: 20\n",
      "Episode 5: reward: -1.000, steps: 11\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 9\n",
      "Episode 2: reward: -1.000, steps: 23\n",
      "Episode 3: reward: -1.000, steps: 9\n",
      "Episode 4: reward: -1.000, steps: 9\n",
      "Episode 5: reward: -1.000, steps: 14\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 18\n",
      "Episode 2: reward: -1.000, steps: 18\n",
      "Episode 3: reward: -1.000, steps: 13\n",
      "Episode 4: reward: -1.000, steps: 13\n",
      "Episode 5: reward: -1.000, steps: 9\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 9\n",
      "Episode 2: reward: -1.000, steps: 9\n",
      "Episode 3: reward: -1.000, steps: 7\n",
      "Episode 4: reward: -1.000, steps: 13\n",
      "Episode 5: reward: -1.000, steps: 11\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 14\n",
      "Episode 2: reward: -1.000, steps: 11\n",
      "Episode 3: reward: -1.000, steps: 12\n",
      "Episode 4: reward: -1.000, steps: 13\n",
      "Episode 5: reward: -1.000, steps: 15\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 12\n",
      "Episode 2: reward: -1.000, steps: 7\n",
      "Episode 3: reward: -1.000, steps: 14\n",
      "Episode 4: reward: -1.000, steps: 21\n",
      "Episode 5: reward: -1.000, steps: 8\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    sarsa.fit(env, nb_steps=5000, visualize=False, verbose=0)\n",
    "\n",
    "    # After training is done, we save the final weights.\n",
    "    sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME + str(epoch)), overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    sarsa.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 17500 steps ...\n",
      "     8/17500: episode: 1, duration: 0.174s, episode steps: 8, steps per second: 46, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 113.375 [6.000, 224.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    14/17500: episode: 2, duration: 0.092s, episode steps: 6, steps per second: 65, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 67.167 [1.000, 140.000], mean observation: 0.038 [0.000, 12.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    21/17500: episode: 3, duration: 0.104s, episode steps: 7, steps per second: 67, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 122.000 [45.000, 194.000], mean observation: 0.047 [0.000, 14.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    29/17500: episode: 4, duration: 0.124s, episode steps: 8, steps per second: 65, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 120.500 [35.000, 220.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    39/17500: episode: 5, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 122.000 [20.000, 219.000], mean observation: 0.081 [0.000, 20.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    66/17500: episode: 6, duration: 0.398s, episode steps: 27, steps per second: 68, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 93.259 [0.000, 223.000], mean observation: 0.429 [0.000, 54.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    85/17500: episode: 7, duration: 0.277s, episode steps: 19, steps per second: 69, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 77.421 [2.000, 214.000], mean observation: 0.221 [0.000, 38.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "    93/17500: episode: 8, duration: 0.112s, episode steps: 8, steps per second: 71, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 131.875 [53.000, 224.000], mean observation: 0.056 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   104/17500: episode: 9, duration: 0.167s, episode steps: 11, steps per second: 66, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 123.182 [22.000, 211.000], mean observation: 0.095 [0.000, 22.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   112/17500: episode: 10, duration: 0.122s, episode steps: 8, steps per second: 65, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 132.375 [8.000, 217.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   140/17500: episode: 11, duration: 0.414s, episode steps: 28, steps per second: 68, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 128.250 [15.000, 224.000], mean observation: 0.473 [0.000, 56.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   148/17500: episode: 12, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 134.500 [41.000, 217.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   161/17500: episode: 13, duration: 0.241s, episode steps: 13, steps per second: 54, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 145.923 [9.000, 219.000], mean observation: 0.122 [0.000, 26.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   169/17500: episode: 14, duration: 0.149s, episode steps: 8, steps per second: 54, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 88.125 [15.000, 206.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   174/17500: episode: 15, duration: 0.084s, episode steps: 5, steps per second: 59, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 127.600 [17.000, 163.000], mean observation: 0.030 [0.000, 10.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   193/17500: episode: 16, duration: 0.338s, episode steps: 19, steps per second: 56, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 87.263 [7.000, 194.000], mean observation: 0.237 [0.000, 38.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   199/17500: episode: 17, duration: 0.111s, episode steps: 6, steps per second: 54, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 123.833 [38.000, 212.000], mean observation: 0.038 [0.000, 12.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   207/17500: episode: 18, duration: 0.151s, episode steps: 8, steps per second: 53, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 130.875 [64.000, 209.000], mean observation: 0.058 [0.000, 16.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   226/17500: episode: 19, duration: 0.318s, episode steps: 19, steps per second: 60, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 92.263 [18.000, 216.000], mean observation: 0.233 [0.000, 38.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   231/17500: episode: 20, duration: 0.083s, episode steps: 5, steps per second: 60, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 119.000 [61.000, 182.000], mean observation: 0.030 [0.000, 10.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   238/17500: episode: 21, duration: 0.114s, episode steps: 7, steps per second: 61, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 129.000 [12.000, 220.000], mean observation: 0.047 [0.000, 14.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   262/17500: episode: 22, duration: 0.350s, episode steps: 24, steps per second: 68, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 109.875 [5.000, 213.000], mean observation: 0.354 [0.000, 48.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   268/17500: episode: 23, duration: 0.091s, episode steps: 6, steps per second: 66, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 106.167 [15.000, 200.000], mean observation: 0.038 [0.000, 12.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "   273/17500: episode: 24, duration: 0.076s, episode steps: 5, steps per second: 65, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 77.000 [12.000, 188.000], mean observation: 0.030 [0.000, 10.000], loss: --, acc: --, mean_q: --, mean_eps: --\n",
      "done, took 4.424 seconds\n"
     ]
    }
   ],
   "source": [
    "%lprun -f dqn.fit dqn.fit(env, nb_steps=17500, log_interval=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ O _ _ _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "Episode 1: reward: -1.000, steps: 13\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ O _ _ _ O O _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ O O _ O _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ O O O O O _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "------------------------------------------------\n",
      "Episode 2: reward: -1.000, steps: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9656aac198>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RenjuTEST(2, None)\n",
    "dqn.test(env, nb_episodes=2, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.compute_q_values([env.cur_pos])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
