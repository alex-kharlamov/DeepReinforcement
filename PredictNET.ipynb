{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-1.renju') as f:\n",
    "    content = f.readlines()\n",
    "data = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "' '.join(list(filter(lambda a: 'p' not in a, data[34])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('renju-mod', 'w')\n",
    "for elem in data:\n",
    "    list(filter(lambda a: 'p' not in a, elem))\n",
    "    f.write(' '.join(list(filter(lambda a: 'p' not in a, elem))) +'\\n')  # python will convert \\n to os.linesep\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number(letter):\n",
    "    #lst = {'a':0, 'b':1,}\n",
    "\n",
    "    return ord(letter) - ord('a')\n",
    "\n",
    "def get_pos(elem):\n",
    "    w = get_number(elem[0])\n",
    "    h = int(elem[1:]) - 1\n",
    "    return w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[int(0.7 * len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[: int(0.7 * len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def parallel_game(_):\n",
    "    elem = data[np.random.randint(len(data))]\n",
    "    cur_pos = np.zeros((15,15, 3))\n",
    "    game = elem.split()\n",
    "    for iterator in range(1, len(game) - 1):\n",
    "        elem = game[iterator]\n",
    "        w, h = get_pos(elem)\n",
    "        if iterator % 2 == 1:\n",
    "            cur_pos[w][h][0] = 1 \n",
    "        else:\n",
    "            cur_pos[w][h][1] = 1\n",
    "\n",
    "\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if cur_pos[i][j][0] > 0:\n",
    "                    cur_pos[i][j][2] += 1\n",
    "                if cur_pos[i][j][1] > 0:\n",
    "                    cur_pos[i][j][2] += 1\n",
    "\n",
    "\n",
    "        cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "        w, h = get_pos(game[iterator + 1])\n",
    "        ans = h * 15 + w\n",
    "        #print(cur_pos)\n",
    "        return (cur_pos_to_app, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen_mp(data, test=False):\n",
    "    if test == False:\n",
    "        part_size = len(data)//30\n",
    "    else:\n",
    "        part_size = len(data)//255\n",
    "    cpu = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=cpu)\n",
    "    args = [None] * cpu\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        \n",
    "        while(len(neural_games) < part_size):\n",
    "            results = pool.map(parallel_game,  args)\n",
    "            \n",
    "            #print('Ext')\n",
    "            \n",
    "            for elem in results:\n",
    "                if elem is not None:\n",
    "                    neural_games.append(elem[0])\n",
    "                    neural_ans.append(elem[1])\n",
    "            \n",
    "            \n",
    "            if (len(neural_games) >= part_size):\n",
    "                break\n",
    "            \n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        neural_games.resize((len(neural_games), 1, 15,15,3))\n",
    "        pool.close()\n",
    "        yield [neural_games, neural_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen(data, test=False):\n",
    "    part_size = len(data)//30\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        while(len(neural_games) < part_size):\n",
    "            elem = data[np.random.randint(len(data))]\n",
    "            cur_pos = np.zeros((15,15, 3))\n",
    "            game = elem.split()\n",
    "            for iterator in range(1, len(game) - 1):\n",
    "                elem = game[iterator]\n",
    "                w, h = get_pos(elem)\n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos[w][h][0] = 1 \n",
    "                else:\n",
    "                    cur_pos[w][h][1] = 1 \n",
    "                \n",
    "                cur_pos[:, :, 2:] = cur_pos.sum(2).reshape((15,15,1))\n",
    "                        \n",
    "                \n",
    "                \n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "                    w, h = get_pos(game[iterator + 1])\n",
    "                    ans = h * 15 + w\n",
    "                    #print(cur_pos)\n",
    "\n",
    "                    neural_ans.append(ans)\n",
    "                    neural_games.append(cur_pos_to_app)\n",
    "                    if (len(neural_games) >= part_size):\n",
    "                        break\n",
    "\n",
    "\n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        neural_games.resize((len(neural_games), 1, 15,15,3))\n",
    "        yield (neural_games, neural_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen_lr(data, test=False):\n",
    "    part_size = len(data) // 4100\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        while(len(neural_games) < part_size):\n",
    "            elem = data[np.random.randint(len(data))]\n",
    "            cur_pos = np.zeros(225)\n",
    "            game = elem.split()\n",
    "            for iterator in range(1, len(game) - 1):\n",
    "                elem = game[iterator]\n",
    "                w, h = get_pos(elem)\n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos[h * 15 + w] = 1 \n",
    "                else:\n",
    "                    cur_pos[h * 15 + w] = 2 \n",
    "                \n",
    "                cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "                w, h = get_pos(game[iterator + 1])\n",
    "                ans = h * 15 + w\n",
    "                #print(cur_pos)\n",
    "                neural_ans.append(ans)\n",
    "                neural_games.append(cur_pos_to_app)\n",
    "                if (len(neural_games) >= part_size):\n",
    "                    break\n",
    "\n",
    "\n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        #neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        #neural_games.resize((part_size, 1, 15,15,3))\n",
    "        yield (neural_games, neural_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=8,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "#clf = LinearRegression(n_jobs=8)\n",
    "#clf = KNeighborsClassifier(n_jobs = 4, n_neighbors=3)\n",
    "clf = LogisticRegression(n_jobs = 8)\n",
    "#clf = Ridge()\n",
    "\n",
    "X, y = next(data_gen_lr(data))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7 µs ± 1.66 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = clf.predict([tester_1.lr_pos])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979 ns ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "random.randint(0,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 µs ± 19.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.random.randint(0,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = 'LinearClass'\n",
    "_ = joblib.dump(clf, filename, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('LinearClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14522,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14522,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.37090301513672"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020689655172413793"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_test, y_true = next(data_gen_lr(test_data))\n",
    "y_pred = clf.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred[i] = int(y_pred[i])\n",
    "    \n",
    "\"\"\" \n",
    "for elem in clf.predict(X_test):\n",
    "    if elem > int(elem) + 0.5:\n",
    "        y_pred.append(int(elem) + 1)\n",
    "    else:\n",
    "        y_pred.append(int(elem))\n",
    "\"\"\"        \n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 15, 15, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 4)         304       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 11, 4)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 8)           808       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 16)          1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 5, 16)          64        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 32)          4640      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "fczero (Dense)               (None, 1100)              317900    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1100)              4400      \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 550)               605550    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 225)               123975    \n",
      "=================================================================\n",
      "Total params: 1,058,825\n",
      "Trainable params: 1,056,585\n",
      "Non-trainable params: 2,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "model_policy = Sequential()\n",
    "\n",
    "model_policy.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model_policy.add(Convolution2D(4, (5,5), activation='relu'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(Convolution2D(8, (5,5), activation='relu'))\n",
    "model_policy.add(Convolution2D(16, (3,3), activation='relu'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(Convolution2D(32, (3,3), activation='relu'))\n",
    "\n",
    "model_policy.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "model_policy.add(Dense(1100, activation='relu', name='fczero'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(Dense(550, activation='relu', name='fc1'))\n",
    "model_policy.add(Dropout(0.1))\n",
    "model_policy.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model_policy.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model_policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.load_weights(\"weights.11-0.283181.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_policy = load_model(\"zeros_policy_44\")\n",
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1883 - acc: 0.4262 - val_loss: 2.2348 - val_acc: 0.4338\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1722 - acc: 0.4303 - val_loss: 2.2318 - val_acc: 0.4348\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1579 - acc: 0.4341 - val_loss: 2.2337 - val_acc: 0.4338\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1490 - acc: 0.4350 - val_loss: 2.2485 - val_acc: 0.4316\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1436 - acc: 0.4339 - val_loss: 2.2527 - val_acc: 0.4321\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1408 - acc: 0.4360 - val_loss: 2.2499 - val_acc: 0.4311\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1347 - acc: 0.4395 - val_loss: 2.2570 - val_acc: 0.4314\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1284 - acc: 0.4404 - val_loss: 2.2594 - val_acc: 0.4298\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1202 - acc: 0.4403 - val_loss: 2.2556 - val_acc: 0.4313\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1152 - acc: 0.4420 - val_loss: 2.2557 - val_acc: 0.4313\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1138 - acc: 0.4437 - val_loss: 2.2627 - val_acc: 0.4317\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1129 - acc: 0.4425 - val_loss: 2.2607 - val_acc: 0.4302\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1065 - acc: 0.4416 - val_loss: 2.2666 - val_acc: 0.4275\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1087 - acc: 0.4457 - val_loss: 2.2630 - val_acc: 0.4263\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0992 - acc: 0.4453 - val_loss: 2.2634 - val_acc: 0.4278\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0936 - acc: 0.4465 - val_loss: 2.2508 - val_acc: 0.4282\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0889 - acc: 0.4484 - val_loss: 2.2592 - val_acc: 0.4276\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0893 - acc: 0.4502 - val_loss: 2.2527 - val_acc: 0.4309\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0845 - acc: 0.4494 - val_loss: 2.2491 - val_acc: 0.4309\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0881 - acc: 0.4472 - val_loss: 2.2510 - val_acc: 0.4301\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0844 - acc: 0.4488 - val_loss: 2.2556 - val_acc: 0.4283\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0793 - acc: 0.4507 - val_loss: 2.2509 - val_acc: 0.4307\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0792 - acc: 0.4480 - val_loss: 2.2896 - val_acc: 0.4294\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0967 - acc: 0.4467 - val_loss: 2.2674 - val_acc: 0.4274\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0767 - acc: 0.4505 - val_loss: 2.2550 - val_acc: 0.4275\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0639 - acc: 0.4535 - val_loss: 2.2534 - val_acc: 0.4271\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0662 - acc: 0.4528 - val_loss: 2.2577 - val_acc: 0.4299\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0635 - acc: 0.4522 - val_loss: 2.2465 - val_acc: 0.4314\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0654 - acc: 0.4523 - val_loss: 2.2685 - val_acc: 0.4287\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0660 - acc: 0.4534 - val_loss: 2.2611 - val_acc: 0.4333\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0648 - acc: 0.4527 - val_loss: 2.2505 - val_acc: 0.4322\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0502 - acc: 0.4550 - val_loss: 2.2544 - val_acc: 0.4334\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0530 - acc: 0.4546 - val_loss: 2.2512 - val_acc: 0.4319\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0538 - acc: 0.4546 - val_loss: 2.2657 - val_acc: 0.4320\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0482 - acc: 0.4556 - val_loss: 2.2540 - val_acc: 0.4328\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0500 - acc: 0.4548 - val_loss: 2.2529 - val_acc: 0.4334\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0411 - acc: 0.4601 - val_loss: 2.2526 - val_acc: 0.4318\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0388 - acc: 0.4590 - val_loss: 2.2648 - val_acc: 0.4333\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0411 - acc: 0.4584 - val_loss: 2.2500 - val_acc: 0.4323\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0501 - acc: 0.4557 - val_loss: 2.2619 - val_acc: 0.4311\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0513 - acc: 0.4574 - val_loss: 2.2389 - val_acc: 0.4341\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0417 - acc: 0.4584 - val_loss: 2.2602 - val_acc: 0.4326\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0370 - acc: 0.4587 - val_loss: 2.2478 - val_acc: 0.4330\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0295 - acc: 0.4604 - val_loss: 2.2686 - val_acc: 0.4294\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0376 - acc: 0.4596 - val_loss: 2.2439 - val_acc: 0.4356\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0281 - acc: 0.4615 - val_loss: 2.2512 - val_acc: 0.4330\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0241 - acc: 0.4620 - val_loss: 2.2395 - val_acc: 0.4340\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0187 - acc: 0.4628 - val_loss: 2.2726 - val_acc: 0.4348\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0565 - acc: 0.4554 - val_loss: 2.2761 - val_acc: 0.4237\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0408 - acc: 0.4588 - val_loss: 2.2679 - val_acc: 0.4307\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0223 - acc: 0.4635 - val_loss: 2.2437 - val_acc: 0.4331\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0222 - acc: 0.4636 - val_loss: 2.2507 - val_acc: 0.4331\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0131 - acc: 0.4638 - val_loss: 2.2466 - val_acc: 0.4336\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0157 - acc: 0.4633 - val_loss: 2.2552 - val_acc: 0.4315\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0156 - acc: 0.4650 - val_loss: 2.2347 - val_acc: 0.4372\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0093 - acc: 0.4664 - val_loss: 2.2668 - val_acc: 0.4304\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0197 - acc: 0.4623 - val_loss: 2.2622 - val_acc: 0.4319\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0173 - acc: 0.4638 - val_loss: 2.2499 - val_acc: 0.4343\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0071 - acc: 0.4656 - val_loss: 2.2393 - val_acc: 0.4351\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0020 - acc: 0.4671 - val_loss: 2.2823 - val_acc: 0.4271\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0181 - acc: 0.4654 - val_loss: 2.2395 - val_acc: 0.4343\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0022 - acc: 0.4685 - val_loss: 2.2587 - val_acc: 0.4322\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0064 - acc: 0.4658 - val_loss: 2.2499 - val_acc: 0.4328\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0097 - acc: 0.4668 - val_loss: 2.2643 - val_acc: 0.4332\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0032 - acc: 0.4670 - val_loss: 2.2480 - val_acc: 0.4335\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9977 - acc: 0.4671 - val_loss: 2.2872 - val_acc: 0.4298\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0064 - acc: 0.4662 - val_loss: 2.2487 - val_acc: 0.4334\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9957 - acc: 0.4681 - val_loss: 2.2707 - val_acc: 0.4312\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0072 - acc: 0.4663 - val_loss: 2.2674 - val_acc: 0.4291\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9995 - acc: 0.4676 - val_loss: 2.2654 - val_acc: 0.4329\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0028 - acc: 0.4671 - val_loss: 2.2821 - val_acc: 0.4246\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0041 - acc: 0.4672 - val_loss: 2.2543 - val_acc: 0.4341\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9833 - acc: 0.4710 - val_loss: 2.2572 - val_acc: 0.4322\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9782 - acc: 0.4735 - val_loss: 2.2531 - val_acc: 0.4331\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9850 - acc: 0.4724 - val_loss: 2.2561 - val_acc: 0.4329\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9863 - acc: 0.4703 - val_loss: 2.2519 - val_acc: 0.4349\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9848 - acc: 0.4710 - val_loss: 2.2581 - val_acc: 0.4316\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9810 - acc: 0.4728 - val_loss: 2.2608 - val_acc: 0.4330\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9869 - acc: 0.4707 - val_loss: 2.2661 - val_acc: 0.4301\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9909 - acc: 0.4704 - val_loss: 2.2833 - val_acc: 0.4292\n",
      "Epoch 81/100\n",
      "1s - loss: 2.0025 - acc: 0.4682 - val_loss: 2.2774 - val_acc: 0.4287\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9775 - acc: 0.4727 - val_loss: 2.2620 - val_acc: 0.4317\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9743 - acc: 0.4727 - val_loss: 2.2642 - val_acc: 0.4305\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9752 - acc: 0.4725 - val_loss: 2.2642 - val_acc: 0.4325\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9715 - acc: 0.4741 - val_loss: 2.2571 - val_acc: 0.4310\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9662 - acc: 0.4768 - val_loss: 2.2597 - val_acc: 0.4331\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9725 - acc: 0.4732 - val_loss: 2.2691 - val_acc: 0.4302\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9830 - acc: 0.4718 - val_loss: 2.2671 - val_acc: 0.4315\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9683 - acc: 0.4740 - val_loss: 2.2543 - val_acc: 0.4312\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9688 - acc: 0.4757 - val_loss: 2.2894 - val_acc: 0.4302\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9762 - acc: 0.4732 - val_loss: 2.3031 - val_acc: 0.4251\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9767 - acc: 0.4738 - val_loss: 2.2730 - val_acc: 0.4304\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9668 - acc: 0.4765 - val_loss: 2.2620 - val_acc: 0.4312\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9681 - acc: 0.4746 - val_loss: 2.2738 - val_acc: 0.4314\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9622 - acc: 0.4775 - val_loss: 2.2583 - val_acc: 0.4301\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9645 - acc: 0.4772 - val_loss: 2.2672 - val_acc: 0.4317\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9635 - acc: 0.4750 - val_loss: 2.2682 - val_acc: 0.4300\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9611 - acc: 0.4772 - val_loss: 2.2637 - val_acc: 0.4333\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9714 - acc: 0.4763 - val_loss: 2.2818 - val_acc: 0.4280\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9617 - acc: 0.4769 - val_loss: 2.2541 - val_acc: 0.4310\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1926 - acc: 0.4271 - val_loss: 2.1488 - val_acc: 0.4486\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1728 - acc: 0.4303 - val_loss: 2.1544 - val_acc: 0.4496\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1570 - acc: 0.4342 - val_loss: 2.1709 - val_acc: 0.4460\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1501 - acc: 0.4360 - val_loss: 2.1701 - val_acc: 0.4448\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1434 - acc: 0.4392 - val_loss: 2.1737 - val_acc: 0.4437\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1341 - acc: 0.4387 - val_loss: 2.1774 - val_acc: 0.4423\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1335 - acc: 0.4397 - val_loss: 2.1918 - val_acc: 0.4393\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1236 - acc: 0.4414 - val_loss: 2.1957 - val_acc: 0.4381\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1187 - acc: 0.4397 - val_loss: 2.1880 - val_acc: 0.4388\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1129 - acc: 0.4446 - val_loss: 2.1825 - val_acc: 0.4403\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1088 - acc: 0.4447 - val_loss: 2.1945 - val_acc: 0.4415\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1053 - acc: 0.4459 - val_loss: 2.1832 - val_acc: 0.4428\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0999 - acc: 0.4467 - val_loss: 2.1841 - val_acc: 0.4441\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0978 - acc: 0.4454 - val_loss: 2.2105 - val_acc: 0.4394\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0959 - acc: 0.4478 - val_loss: 2.1954 - val_acc: 0.4389\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0883 - acc: 0.4486 - val_loss: 2.1933 - val_acc: 0.4404\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0852 - acc: 0.4510 - val_loss: 2.1929 - val_acc: 0.4369\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0848 - acc: 0.4487 - val_loss: 2.1902 - val_acc: 0.4385\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0745 - acc: 0.4530 - val_loss: 2.1925 - val_acc: 0.4398\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0768 - acc: 0.4529 - val_loss: 2.1925 - val_acc: 0.4392\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0721 - acc: 0.4509 - val_loss: 2.1861 - val_acc: 0.4415\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0708 - acc: 0.4538 - val_loss: 2.1948 - val_acc: 0.4383\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0649 - acc: 0.4540 - val_loss: 2.1914 - val_acc: 0.4407\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0643 - acc: 0.4539 - val_loss: 2.1841 - val_acc: 0.4424\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0625 - acc: 0.4550 - val_loss: 2.1926 - val_acc: 0.4397\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0619 - acc: 0.4529 - val_loss: 2.1932 - val_acc: 0.4439\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0902 - acc: 0.4488 - val_loss: 2.2283 - val_acc: 0.4305\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0741 - acc: 0.4532 - val_loss: 2.1980 - val_acc: 0.4397\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0496 - acc: 0.4566 - val_loss: 2.1936 - val_acc: 0.4406\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0522 - acc: 0.4591 - val_loss: 2.1759 - val_acc: 0.4443\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0486 - acc: 0.4566 - val_loss: 2.1948 - val_acc: 0.4401\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0467 - acc: 0.4596 - val_loss: 2.1689 - val_acc: 0.4457\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0448 - acc: 0.4582 - val_loss: 2.1732 - val_acc: 0.4434\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0397 - acc: 0.4581 - val_loss: 2.1743 - val_acc: 0.4433\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0456 - acc: 0.4585 - val_loss: 2.1907 - val_acc: 0.4408\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0388 - acc: 0.4593 - val_loss: 2.1695 - val_acc: 0.4445\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0393 - acc: 0.4587 - val_loss: 2.2014 - val_acc: 0.4437\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0456 - acc: 0.4571 - val_loss: 2.1932 - val_acc: 0.4413\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0403 - acc: 0.4594 - val_loss: 2.1852 - val_acc: 0.4444\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0337 - acc: 0.4612 - val_loss: 2.1728 - val_acc: 0.4461\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0345 - acc: 0.4600 - val_loss: 2.1554 - val_acc: 0.4500\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0230 - acc: 0.4633 - val_loss: 2.1740 - val_acc: 0.4443\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0208 - acc: 0.4641 - val_loss: 2.1595 - val_acc: 0.4466\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0238 - acc: 0.4623 - val_loss: 2.1688 - val_acc: 0.4483\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0358 - acc: 0.4614 - val_loss: 2.1645 - val_acc: 0.4456\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0195 - acc: 0.4631 - val_loss: 2.2106 - val_acc: 0.4404\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0581 - acc: 0.4572 - val_loss: 2.1956 - val_acc: 0.4382\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0326 - acc: 0.4616 - val_loss: 2.1902 - val_acc: 0.4427\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0175 - acc: 0.4654 - val_loss: 2.1646 - val_acc: 0.4446\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0132 - acc: 0.4649 - val_loss: 2.1683 - val_acc: 0.4448\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0135 - acc: 0.4657 - val_loss: 2.1618 - val_acc: 0.4457\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0006 - acc: 0.4687 - val_loss: 2.1599 - val_acc: 0.4478\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0040 - acc: 0.4678 - val_loss: 2.1634 - val_acc: 0.4454\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0045 - acc: 0.4669 - val_loss: 2.1745 - val_acc: 0.4441\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0048 - acc: 0.4671 - val_loss: 2.1676 - val_acc: 0.4455\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0047 - acc: 0.4665 - val_loss: 2.1793 - val_acc: 0.4452\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0092 - acc: 0.4668 - val_loss: 2.1632 - val_acc: 0.4449\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9983 - acc: 0.4685 - val_loss: 2.1757 - val_acc: 0.4439\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9937 - acc: 0.4682 - val_loss: 2.1709 - val_acc: 0.4459\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9982 - acc: 0.4677 - val_loss: 2.1761 - val_acc: 0.4443\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9961 - acc: 0.4699 - val_loss: 2.1629 - val_acc: 0.4452\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9935 - acc: 0.4701 - val_loss: 2.1958 - val_acc: 0.4414\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9964 - acc: 0.4704 - val_loss: 2.1672 - val_acc: 0.4433\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9947 - acc: 0.4703 - val_loss: 2.1943 - val_acc: 0.4428\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9989 - acc: 0.4706 - val_loss: 2.1870 - val_acc: 0.4438\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0051 - acc: 0.4677 - val_loss: 2.1983 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9890 - acc: 0.4694 - val_loss: 2.2446 - val_acc: 0.4332\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0055 - acc: 0.4694 - val_loss: 2.1958 - val_acc: 0.4416\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9893 - acc: 0.4703 - val_loss: 2.1844 - val_acc: 0.4431\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9939 - acc: 0.4708 - val_loss: 2.1704 - val_acc: 0.4465\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9790 - acc: 0.4724 - val_loss: 2.1668 - val_acc: 0.4452\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9779 - acc: 0.4750 - val_loss: 2.1691 - val_acc: 0.4473\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9732 - acc: 0.4731 - val_loss: 2.1791 - val_acc: 0.4441\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9760 - acc: 0.4744 - val_loss: 2.1724 - val_acc: 0.4475\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9845 - acc: 0.4703 - val_loss: 2.2036 - val_acc: 0.4407\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9865 - acc: 0.4718 - val_loss: 2.1691 - val_acc: 0.4472\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9719 - acc: 0.4764 - val_loss: 2.1699 - val_acc: 0.4450\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9656 - acc: 0.4762 - val_loss: 2.1701 - val_acc: 0.4453\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9705 - acc: 0.4745 - val_loss: 2.1786 - val_acc: 0.4444\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9668 - acc: 0.4756 - val_loss: 2.1690 - val_acc: 0.4467\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9661 - acc: 0.4765 - val_loss: 2.1842 - val_acc: 0.4433\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9637 - acc: 0.4788 - val_loss: 2.1765 - val_acc: 0.4466\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9651 - acc: 0.4788 - val_loss: 2.2091 - val_acc: 0.4373\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9852 - acc: 0.4711 - val_loss: 2.1751 - val_acc: 0.4446\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9627 - acc: 0.4784 - val_loss: 2.1653 - val_acc: 0.4440\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9639 - acc: 0.4759 - val_loss: 2.1970 - val_acc: 0.4421\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9637 - acc: 0.4762 - val_loss: 2.1786 - val_acc: 0.4439\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9635 - acc: 0.4767 - val_loss: 2.1899 - val_acc: 0.4450\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9698 - acc: 0.4762 - val_loss: 2.1975 - val_acc: 0.4416\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9608 - acc: 0.4782 - val_loss: 2.1891 - val_acc: 0.4428\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9688 - acc: 0.4750 - val_loss: 2.2229 - val_acc: 0.4374\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9652 - acc: 0.4779 - val_loss: 2.1820 - val_acc: 0.4460\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9543 - acc: 0.4798 - val_loss: 2.1756 - val_acc: 0.4438\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9427 - acc: 0.4817 - val_loss: 2.1830 - val_acc: 0.4452\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9777 - acc: 0.4767 - val_loss: 2.2306 - val_acc: 0.4370\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9636 - acc: 0.4778 - val_loss: 2.1859 - val_acc: 0.4442\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9623 - acc: 0.4779 - val_loss: 2.1960 - val_acc: 0.4409\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9566 - acc: 0.4787 - val_loss: 2.1816 - val_acc: 0.4430\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9575 - acc: 0.4770 - val_loss: 2.1840 - val_acc: 0.4444\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9472 - acc: 0.4815 - val_loss: 2.1709 - val_acc: 0.4445\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2069 - acc: 0.4261 - val_loss: 2.2212 - val_acc: 0.4322\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1908 - acc: 0.4275 - val_loss: 2.2240 - val_acc: 0.4315\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1806 - acc: 0.4309 - val_loss: 2.2345 - val_acc: 0.4311\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1732 - acc: 0.4292 - val_loss: 2.2308 - val_acc: 0.4314\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1603 - acc: 0.4333 - val_loss: 2.2489 - val_acc: 0.4260\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1515 - acc: 0.4359 - val_loss: 2.2590 - val_acc: 0.4257\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1508 - acc: 0.4364 - val_loss: 2.2493 - val_acc: 0.4256\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1434 - acc: 0.4380 - val_loss: 2.2579 - val_acc: 0.4270\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1368 - acc: 0.4389 - val_loss: 2.2608 - val_acc: 0.4262\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1343 - acc: 0.4405 - val_loss: 2.2722 - val_acc: 0.4241\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1277 - acc: 0.4408 - val_loss: 2.2748 - val_acc: 0.4222\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1249 - acc: 0.4409 - val_loss: 2.2812 - val_acc: 0.4224\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1211 - acc: 0.4406 - val_loss: 2.2475 - val_acc: 0.4281\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1156 - acc: 0.4433 - val_loss: 2.2633 - val_acc: 0.4249\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1104 - acc: 0.4440 - val_loss: 2.3007 - val_acc: 0.4203\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1198 - acc: 0.4411 - val_loss: 2.2539 - val_acc: 0.4273\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1055 - acc: 0.4446 - val_loss: 2.2477 - val_acc: 0.4286\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1005 - acc: 0.4474 - val_loss: 2.2434 - val_acc: 0.4270\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0972 - acc: 0.4465 - val_loss: 2.2574 - val_acc: 0.4246\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0996 - acc: 0.4469 - val_loss: 2.2610 - val_acc: 0.4246\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0946 - acc: 0.4467 - val_loss: 2.2613 - val_acc: 0.4239\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0966 - acc: 0.4471 - val_loss: 2.2443 - val_acc: 0.4286\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0924 - acc: 0.4488 - val_loss: 2.2467 - val_acc: 0.4267\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0816 - acc: 0.4484 - val_loss: 2.2551 - val_acc: 0.4261\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0846 - acc: 0.4504 - val_loss: 2.2565 - val_acc: 0.4269\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0886 - acc: 0.4483 - val_loss: 2.2522 - val_acc: 0.4249\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0873 - acc: 0.4478 - val_loss: 2.2756 - val_acc: 0.4233\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0802 - acc: 0.4509 - val_loss: 2.2528 - val_acc: 0.4272\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0755 - acc: 0.4529 - val_loss: 2.2535 - val_acc: 0.4271\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0744 - acc: 0.4520 - val_loss: 2.2764 - val_acc: 0.4254\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0739 - acc: 0.4524 - val_loss: 2.2706 - val_acc: 0.4253\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0914 - acc: 0.4508 - val_loss: 2.2449 - val_acc: 0.4268\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0681 - acc: 0.4539 - val_loss: 2.2436 - val_acc: 0.4279\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0610 - acc: 0.4554 - val_loss: 2.2413 - val_acc: 0.4290\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0607 - acc: 0.4558 - val_loss: 2.2436 - val_acc: 0.4270\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0566 - acc: 0.4560 - val_loss: 2.2242 - val_acc: 0.4296\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0527 - acc: 0.4564 - val_loss: 2.2296 - val_acc: 0.4283\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0508 - acc: 0.4564 - val_loss: 2.2744 - val_acc: 0.4232\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0569 - acc: 0.4565 - val_loss: 2.2414 - val_acc: 0.4273\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0473 - acc: 0.4589 - val_loss: 2.3055 - val_acc: 0.4195\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0562 - acc: 0.4565 - val_loss: 2.2314 - val_acc: 0.4309\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0486 - acc: 0.4576 - val_loss: 2.2362 - val_acc: 0.4319\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0554 - acc: 0.4563 - val_loss: 2.2415 - val_acc: 0.4281\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0367 - acc: 0.4599 - val_loss: 2.2318 - val_acc: 0.4293\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0402 - acc: 0.4584 - val_loss: 2.2435 - val_acc: 0.4295\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0460 - acc: 0.4575 - val_loss: 2.2229 - val_acc: 0.4324\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0359 - acc: 0.4620 - val_loss: 2.2311 - val_acc: 0.4311\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0349 - acc: 0.4600 - val_loss: 2.2242 - val_acc: 0.4320\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0367 - acc: 0.4578 - val_loss: 2.2342 - val_acc: 0.4308\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0344 - acc: 0.4611 - val_loss: 2.2526 - val_acc: 0.4304\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0482 - acc: 0.4565 - val_loss: 2.2184 - val_acc: 0.4317\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0269 - acc: 0.4633 - val_loss: 2.2283 - val_acc: 0.4325\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0281 - acc: 0.4634 - val_loss: 2.2179 - val_acc: 0.4326\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0192 - acc: 0.4634 - val_loss: 2.2300 - val_acc: 0.4293\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0238 - acc: 0.4634 - val_loss: 2.2165 - val_acc: 0.4335\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0209 - acc: 0.4631 - val_loss: 2.2302 - val_acc: 0.4335\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0283 - acc: 0.4628 - val_loss: 2.2212 - val_acc: 0.4325\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0159 - acc: 0.4638 - val_loss: 2.2246 - val_acc: 0.4330\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0195 - acc: 0.4644 - val_loss: 2.2558 - val_acc: 0.4313\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0241 - acc: 0.4613 - val_loss: 2.2440 - val_acc: 0.4295\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0370 - acc: 0.4598 - val_loss: 2.2268 - val_acc: 0.4346\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0221 - acc: 0.4617 - val_loss: 2.2180 - val_acc: 0.4346\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0061 - acc: 0.4669 - val_loss: 2.2186 - val_acc: 0.4334\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0201 - acc: 0.4641 - val_loss: 2.2250 - val_acc: 0.4302\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0040 - acc: 0.4674 - val_loss: 2.2321 - val_acc: 0.4294\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0069 - acc: 0.4675 - val_loss: 2.2287 - val_acc: 0.4317\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0121 - acc: 0.4644 - val_loss: 2.2189 - val_acc: 0.4335\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0127 - acc: 0.4668 - val_loss: 2.2400 - val_acc: 0.4315\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0072 - acc: 0.4671 - val_loss: 2.2379 - val_acc: 0.4279\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0067 - acc: 0.4680 - val_loss: 2.2259 - val_acc: 0.4345\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0082 - acc: 0.4679 - val_loss: 2.2255 - val_acc: 0.4315\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9971 - acc: 0.4689 - val_loss: 2.2159 - val_acc: 0.4339\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9940 - acc: 0.4700 - val_loss: 2.2191 - val_acc: 0.4349\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9946 - acc: 0.4695 - val_loss: 2.2162 - val_acc: 0.4351\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9926 - acc: 0.4703 - val_loss: 2.2417 - val_acc: 0.4281\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9982 - acc: 0.4695 - val_loss: 2.2242 - val_acc: 0.4337\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9927 - acc: 0.4686 - val_loss: 2.2280 - val_acc: 0.4331\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9914 - acc: 0.4709 - val_loss: 2.2348 - val_acc: 0.4336\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9966 - acc: 0.4710 - val_loss: 2.2258 - val_acc: 0.4320\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9966 - acc: 0.4678 - val_loss: 2.2238 - val_acc: 0.4322\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9826 - acc: 0.4724 - val_loss: 2.2364 - val_acc: 0.4306\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9917 - acc: 0.4704 - val_loss: 2.2380 - val_acc: 0.4321\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9851 - acc: 0.4722 - val_loss: 2.2293 - val_acc: 0.4312\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9822 - acc: 0.4727 - val_loss: 2.2253 - val_acc: 0.4335\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9804 - acc: 0.4703 - val_loss: 2.2563 - val_acc: 0.4299\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9918 - acc: 0.4701 - val_loss: 2.2286 - val_acc: 0.4314\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9784 - acc: 0.4750 - val_loss: 2.2645 - val_acc: 0.4246\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9903 - acc: 0.4716 - val_loss: 2.2271 - val_acc: 0.4328\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9773 - acc: 0.4747 - val_loss: 2.2792 - val_acc: 0.4256\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9871 - acc: 0.4712 - val_loss: 2.2246 - val_acc: 0.4342\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9763 - acc: 0.4732 - val_loss: 2.2480 - val_acc: 0.4301\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9864 - acc: 0.4723 - val_loss: 2.2351 - val_acc: 0.4320\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9782 - acc: 0.4725 - val_loss: 2.2513 - val_acc: 0.4279\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9729 - acc: 0.4738 - val_loss: 2.2459 - val_acc: 0.4329\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9830 - acc: 0.4712 - val_loss: 2.2394 - val_acc: 0.4314\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9704 - acc: 0.4766 - val_loss: 2.2304 - val_acc: 0.4327\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9623 - acc: 0.4783 - val_loss: 2.2346 - val_acc: 0.4339\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9723 - acc: 0.4763 - val_loss: 2.2280 - val_acc: 0.4338\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9719 - acc: 0.4729 - val_loss: 2.2478 - val_acc: 0.4270\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9872 - acc: 0.4703 - val_loss: 2.2292 - val_acc: 0.4304\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2107 - acc: 0.4245 - val_loss: 2.1875 - val_acc: 0.4445\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1853 - acc: 0.4304 - val_loss: 2.1916 - val_acc: 0.4425\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1723 - acc: 0.4322 - val_loss: 2.1896 - val_acc: 0.4435\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1603 - acc: 0.4350 - val_loss: 2.1909 - val_acc: 0.4416\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1545 - acc: 0.4331 - val_loss: 2.2005 - val_acc: 0.4419\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1493 - acc: 0.4357 - val_loss: 2.1937 - val_acc: 0.4392\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1451 - acc: 0.4379 - val_loss: 2.2072 - val_acc: 0.4406\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1357 - acc: 0.4381 - val_loss: 2.1999 - val_acc: 0.4416\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1283 - acc: 0.4394 - val_loss: 2.1968 - val_acc: 0.4420\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1272 - acc: 0.4389 - val_loss: 2.2034 - val_acc: 0.4396\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1216 - acc: 0.4414 - val_loss: 2.1989 - val_acc: 0.4406\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1137 - acc: 0.4443 - val_loss: 2.2043 - val_acc: 0.4396\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1130 - acc: 0.4421 - val_loss: 2.1988 - val_acc: 0.4414\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1093 - acc: 0.4429 - val_loss: 2.2117 - val_acc: 0.4389\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1096 - acc: 0.4432 - val_loss: 2.2039 - val_acc: 0.4409\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1040 - acc: 0.4459 - val_loss: 2.1975 - val_acc: 0.4411\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1008 - acc: 0.4449 - val_loss: 2.1955 - val_acc: 0.4414\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0979 - acc: 0.4450 - val_loss: 2.1943 - val_acc: 0.4434\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0884 - acc: 0.4467 - val_loss: 2.1967 - val_acc: 0.4408\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0880 - acc: 0.4467 - val_loss: 2.2011 - val_acc: 0.4428\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0877 - acc: 0.4468 - val_loss: 2.1921 - val_acc: 0.4408\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0786 - acc: 0.4494 - val_loss: 2.1947 - val_acc: 0.4443\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0812 - acc: 0.4486 - val_loss: 2.1944 - val_acc: 0.4425\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0803 - acc: 0.4481 - val_loss: 2.1981 - val_acc: 0.4440\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0740 - acc: 0.4514 - val_loss: 2.1947 - val_acc: 0.4425\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0717 - acc: 0.4509 - val_loss: 2.1896 - val_acc: 0.4437\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0665 - acc: 0.4528 - val_loss: 2.2021 - val_acc: 0.4399\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0728 - acc: 0.4512 - val_loss: 2.1971 - val_acc: 0.4445\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0678 - acc: 0.4532 - val_loss: 2.1901 - val_acc: 0.4422\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0672 - acc: 0.4517 - val_loss: 2.1987 - val_acc: 0.4442\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0651 - acc: 0.4517 - val_loss: 2.1972 - val_acc: 0.4427\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0658 - acc: 0.4543 - val_loss: 2.1897 - val_acc: 0.4453\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0588 - acc: 0.4544 - val_loss: 2.1853 - val_acc: 0.4451\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0504 - acc: 0.4563 - val_loss: 2.1929 - val_acc: 0.4430\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0489 - acc: 0.4562 - val_loss: 2.1848 - val_acc: 0.4445\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0466 - acc: 0.4569 - val_loss: 2.1943 - val_acc: 0.4458\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0498 - acc: 0.4563 - val_loss: 2.2172 - val_acc: 0.4378\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0555 - acc: 0.4541 - val_loss: 2.1965 - val_acc: 0.4444\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0423 - acc: 0.4559 - val_loss: 2.1808 - val_acc: 0.4432\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0452 - acc: 0.4580 - val_loss: 2.1957 - val_acc: 0.4427\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0501 - acc: 0.4554 - val_loss: 2.1874 - val_acc: 0.4423\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0387 - acc: 0.4570 - val_loss: 2.1889 - val_acc: 0.4453\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0390 - acc: 0.4585 - val_loss: 2.1886 - val_acc: 0.4423\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0382 - acc: 0.4579 - val_loss: 2.1948 - val_acc: 0.4449\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0351 - acc: 0.4606 - val_loss: 2.1950 - val_acc: 0.4424\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0300 - acc: 0.4586 - val_loss: 2.1920 - val_acc: 0.4457\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0475 - acc: 0.4581 - val_loss: 2.1942 - val_acc: 0.4430\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0282 - acc: 0.4600 - val_loss: 2.1877 - val_acc: 0.4457\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0204 - acc: 0.4622 - val_loss: 2.1860 - val_acc: 0.4444\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0197 - acc: 0.4636 - val_loss: 2.1780 - val_acc: 0.4439\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0202 - acc: 0.4636 - val_loss: 2.2099 - val_acc: 0.4428\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0274 - acc: 0.4594 - val_loss: 2.1883 - val_acc: 0.4435\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0171 - acc: 0.4607 - val_loss: 2.1855 - val_acc: 0.4438\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0154 - acc: 0.4625 - val_loss: 2.1861 - val_acc: 0.4467\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0146 - acc: 0.4633 - val_loss: 2.1866 - val_acc: 0.4434\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0168 - acc: 0.4636 - val_loss: 2.1855 - val_acc: 0.4471\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0161 - acc: 0.4643 - val_loss: 2.1912 - val_acc: 0.4433\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0079 - acc: 0.4667 - val_loss: 2.1742 - val_acc: 0.4464\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0034 - acc: 0.4654 - val_loss: 2.1938 - val_acc: 0.4448\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0118 - acc: 0.4649 - val_loss: 2.1890 - val_acc: 0.4470\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0059 - acc: 0.4657 - val_loss: 2.1899 - val_acc: 0.4438\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9991 - acc: 0.4677 - val_loss: 2.1834 - val_acc: 0.4452\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9977 - acc: 0.4671 - val_loss: 2.1902 - val_acc: 0.4430\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9996 - acc: 0.4642 - val_loss: 2.1778 - val_acc: 0.4465\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9930 - acc: 0.4695 - val_loss: 2.1840 - val_acc: 0.4473\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9964 - acc: 0.4665 - val_loss: 2.1903 - val_acc: 0.4450\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0035 - acc: 0.4670 - val_loss: 2.2063 - val_acc: 0.4436\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0026 - acc: 0.4648 - val_loss: 2.1910 - val_acc: 0.4439\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0007 - acc: 0.4666 - val_loss: 2.2222 - val_acc: 0.4409\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0100 - acc: 0.4665 - val_loss: 2.1915 - val_acc: 0.4435\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9959 - acc: 0.4679 - val_loss: 2.2073 - val_acc: 0.4417\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9951 - acc: 0.4673 - val_loss: 2.1961 - val_acc: 0.4445\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9927 - acc: 0.4676 - val_loss: 2.1878 - val_acc: 0.4444\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9884 - acc: 0.4696 - val_loss: 2.2013 - val_acc: 0.4442\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9868 - acc: 0.4686 - val_loss: 2.2039 - val_acc: 0.4417\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9893 - acc: 0.4685 - val_loss: 2.1883 - val_acc: 0.4461\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9863 - acc: 0.4704 - val_loss: 2.2081 - val_acc: 0.4390\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9855 - acc: 0.4711 - val_loss: 2.1883 - val_acc: 0.4441\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9756 - acc: 0.4726 - val_loss: 2.2029 - val_acc: 0.4403\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9860 - acc: 0.4700 - val_loss: 2.2007 - val_acc: 0.4458\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9881 - acc: 0.4692 - val_loss: 2.2007 - val_acc: 0.4409\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9792 - acc: 0.4714 - val_loss: 2.2072 - val_acc: 0.4434\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9829 - acc: 0.4708 - val_loss: 2.1932 - val_acc: 0.4421\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9801 - acc: 0.4698 - val_loss: 2.2118 - val_acc: 0.4434\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9878 - acc: 0.4718 - val_loss: 2.2157 - val_acc: 0.4378\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9832 - acc: 0.4703 - val_loss: 2.2056 - val_acc: 0.4464\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9748 - acc: 0.4755 - val_loss: 2.1928 - val_acc: 0.4442\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9635 - acc: 0.4762 - val_loss: 2.1849 - val_acc: 0.4473\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9624 - acc: 0.4765 - val_loss: 2.1963 - val_acc: 0.4433\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9569 - acc: 0.4768 - val_loss: 2.1902 - val_acc: 0.4467\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9656 - acc: 0.4742 - val_loss: 2.2094 - val_acc: 0.4395\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9735 - acc: 0.4745 - val_loss: 2.2063 - val_acc: 0.4464\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9742 - acc: 0.4734 - val_loss: 2.2166 - val_acc: 0.4376\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9630 - acc: 0.4776 - val_loss: 2.2065 - val_acc: 0.4440\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9629 - acc: 0.4768 - val_loss: 2.2047 - val_acc: 0.4389\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9575 - acc: 0.4766 - val_loss: 2.1996 - val_acc: 0.4437\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9543 - acc: 0.4772 - val_loss: 2.1894 - val_acc: 0.4452\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9561 - acc: 0.4773 - val_loss: 2.1888 - val_acc: 0.4451\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9506 - acc: 0.4776 - val_loss: 2.1975 - val_acc: 0.4425\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9595 - acc: 0.4783 - val_loss: 2.2158 - val_acc: 0.4401\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2165 - acc: 0.4215 - val_loss: 2.1734 - val_acc: 0.4411\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1877 - acc: 0.4277 - val_loss: 2.1716 - val_acc: 0.4420\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1803 - acc: 0.4287 - val_loss: 2.1857 - val_acc: 0.4418\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1723 - acc: 0.4303 - val_loss: 2.1889 - val_acc: 0.4418\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1624 - acc: 0.4345 - val_loss: 2.1829 - val_acc: 0.4392\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1558 - acc: 0.4332 - val_loss: 2.2082 - val_acc: 0.4388\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1531 - acc: 0.4359 - val_loss: 2.1858 - val_acc: 0.4417\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1478 - acc: 0.4346 - val_loss: 2.1845 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1415 - acc: 0.4366 - val_loss: 2.2309 - val_acc: 0.4313\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1366 - acc: 0.4379 - val_loss: 2.2058 - val_acc: 0.4371\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1258 - acc: 0.4393 - val_loss: 2.1933 - val_acc: 0.4398\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1235 - acc: 0.4419 - val_loss: 2.1967 - val_acc: 0.4404\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1192 - acc: 0.4420 - val_loss: 2.2078 - val_acc: 0.4374\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1204 - acc: 0.4393 - val_loss: 2.2112 - val_acc: 0.4375\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1125 - acc: 0.4428 - val_loss: 2.1997 - val_acc: 0.4385\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1137 - acc: 0.4428 - val_loss: 2.2219 - val_acc: 0.4356\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1100 - acc: 0.4438 - val_loss: 2.1979 - val_acc: 0.4390\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1020 - acc: 0.4438 - val_loss: 2.2066 - val_acc: 0.4371\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1083 - acc: 0.4449 - val_loss: 2.2087 - val_acc: 0.4358\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1036 - acc: 0.4449 - val_loss: 2.2270 - val_acc: 0.4353\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1023 - acc: 0.4432 - val_loss: 2.2026 - val_acc: 0.4375\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0934 - acc: 0.4472 - val_loss: 2.2251 - val_acc: 0.4345\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0883 - acc: 0.4500 - val_loss: 2.2042 - val_acc: 0.4374\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0829 - acc: 0.4483 - val_loss: 2.1992 - val_acc: 0.4394\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0777 - acc: 0.4501 - val_loss: 2.2107 - val_acc: 0.4351\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0885 - acc: 0.4491 - val_loss: 2.2364 - val_acc: 0.4351\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0845 - acc: 0.4503 - val_loss: 2.2019 - val_acc: 0.4361\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0744 - acc: 0.4527 - val_loss: 2.2035 - val_acc: 0.4409\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0772 - acc: 0.4526 - val_loss: 2.1950 - val_acc: 0.4413\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0929 - acc: 0.4471 - val_loss: 2.2510 - val_acc: 0.4335\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0803 - acc: 0.4517 - val_loss: 2.1910 - val_acc: 0.4394\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0773 - acc: 0.4502 - val_loss: 2.1977 - val_acc: 0.4427\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0627 - acc: 0.4542 - val_loss: 2.1850 - val_acc: 0.4406\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0641 - acc: 0.4532 - val_loss: 2.2156 - val_acc: 0.4392\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0561 - acc: 0.4547 - val_loss: 2.1775 - val_acc: 0.4444\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0505 - acc: 0.4559 - val_loss: 2.1742 - val_acc: 0.4437\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0483 - acc: 0.4576 - val_loss: 2.1854 - val_acc: 0.4426\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0516 - acc: 0.4562 - val_loss: 2.1741 - val_acc: 0.4435\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0534 - acc: 0.4555 - val_loss: 2.2562 - val_acc: 0.4334\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0593 - acc: 0.4554 - val_loss: 2.1893 - val_acc: 0.4432\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0507 - acc: 0.4580 - val_loss: 2.2091 - val_acc: 0.4411\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0500 - acc: 0.4556 - val_loss: 2.1870 - val_acc: 0.4412\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0502 - acc: 0.4552 - val_loss: 2.1824 - val_acc: 0.4447\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0454 - acc: 0.4592 - val_loss: 2.1752 - val_acc: 0.4434\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0360 - acc: 0.4595 - val_loss: 2.1888 - val_acc: 0.4438\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0416 - acc: 0.4588 - val_loss: 2.1971 - val_acc: 0.4414\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0451 - acc: 0.4564 - val_loss: 2.1763 - val_acc: 0.4437\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0400 - acc: 0.4600 - val_loss: 2.2100 - val_acc: 0.4417\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0661 - acc: 0.4552 - val_loss: 2.2062 - val_acc: 0.4398\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0417 - acc: 0.4582 - val_loss: 2.2104 - val_acc: 0.4416\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0480 - acc: 0.4582 - val_loss: 2.1866 - val_acc: 0.4437\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0404 - acc: 0.4591 - val_loss: 2.2155 - val_acc: 0.4384\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0424 - acc: 0.4590 - val_loss: 2.1802 - val_acc: 0.4450\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0328 - acc: 0.4628 - val_loss: 2.2178 - val_acc: 0.4409\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0224 - acc: 0.4628 - val_loss: 2.1988 - val_acc: 0.4390\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0322 - acc: 0.4612 - val_loss: 2.1968 - val_acc: 0.4440\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0220 - acc: 0.4622 - val_loss: 2.1870 - val_acc: 0.4408\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0155 - acc: 0.4649 - val_loss: 2.1781 - val_acc: 0.4445\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0101 - acc: 0.4655 - val_loss: 2.1801 - val_acc: 0.4449\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0307 - acc: 0.4632 - val_loss: 2.1861 - val_acc: 0.4431\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0154 - acc: 0.4644 - val_loss: 2.1804 - val_acc: 0.4459\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0287 - acc: 0.4619 - val_loss: 2.1953 - val_acc: 0.4419\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0141 - acc: 0.4660 - val_loss: 2.1830 - val_acc: 0.4438\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0116 - acc: 0.4669 - val_loss: 2.1685 - val_acc: 0.4482\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0069 - acc: 0.4660 - val_loss: 2.1852 - val_acc: 0.4423\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0067 - acc: 0.4665 - val_loss: 2.1765 - val_acc: 0.4468\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0049 - acc: 0.4667 - val_loss: 2.1783 - val_acc: 0.4431\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9994 - acc: 0.4681 - val_loss: 2.1689 - val_acc: 0.4465\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0031 - acc: 0.4673 - val_loss: 2.1694 - val_acc: 0.4448\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0068 - acc: 0.4667 - val_loss: 2.2203 - val_acc: 0.4420\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0267 - acc: 0.4617 - val_loss: 2.1753 - val_acc: 0.4445\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0098 - acc: 0.4661 - val_loss: 2.2264 - val_acc: 0.4403\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0081 - acc: 0.4673 - val_loss: 2.2304 - val_acc: 0.4349\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0195 - acc: 0.4634 - val_loss: 2.1708 - val_acc: 0.4469\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9899 - acc: 0.4706 - val_loss: 2.1779 - val_acc: 0.4444\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9888 - acc: 0.4690 - val_loss: 2.1639 - val_acc: 0.4476\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9836 - acc: 0.4706 - val_loss: 2.1829 - val_acc: 0.4439\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9881 - acc: 0.4713 - val_loss: 2.1708 - val_acc: 0.4449\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9828 - acc: 0.4718 - val_loss: 2.1697 - val_acc: 0.4451\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9826 - acc: 0.4712 - val_loss: 2.1892 - val_acc: 0.4449\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9889 - acc: 0.4709 - val_loss: 2.2108 - val_acc: 0.4383\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0026 - acc: 0.4669 - val_loss: 2.2018 - val_acc: 0.4403\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9905 - acc: 0.4709 - val_loss: 2.1665 - val_acc: 0.4461\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9815 - acc: 0.4713 - val_loss: 2.2200 - val_acc: 0.4408\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9923 - acc: 0.4700 - val_loss: 2.1751 - val_acc: 0.4434\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9915 - acc: 0.4709 - val_loss: 2.2147 - val_acc: 0.4410\n",
      "Epoch 87/100\n",
      "1s - loss: 2.0043 - acc: 0.4681 - val_loss: 2.1898 - val_acc: 0.4424\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9917 - acc: 0.4690 - val_loss: 2.1973 - val_acc: 0.4435\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9854 - acc: 0.4722 - val_loss: 2.1752 - val_acc: 0.4427\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9776 - acc: 0.4728 - val_loss: 2.2015 - val_acc: 0.4439\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9842 - acc: 0.4720 - val_loss: 2.1975 - val_acc: 0.4397\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9774 - acc: 0.4734 - val_loss: 2.2035 - val_acc: 0.4422\n",
      "Epoch 93/100\n",
      "1s - loss: 2.0047 - acc: 0.4700 - val_loss: 2.1844 - val_acc: 0.4395\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9757 - acc: 0.4737 - val_loss: 2.1900 - val_acc: 0.4455\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9826 - acc: 0.4738 - val_loss: 2.1875 - val_acc: 0.4428\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9766 - acc: 0.4750 - val_loss: 2.1969 - val_acc: 0.4462\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9839 - acc: 0.4755 - val_loss: 2.1798 - val_acc: 0.4440\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9720 - acc: 0.4743 - val_loss: 2.2043 - val_acc: 0.4421\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9770 - acc: 0.4740 - val_loss: 2.1698 - val_acc: 0.4442\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9593 - acc: 0.4782 - val_loss: 2.1789 - val_acc: 0.4443\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2037 - acc: 0.4257 - val_loss: 2.2169 - val_acc: 0.4349\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1913 - acc: 0.4300 - val_loss: 2.1940 - val_acc: 0.4383\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1679 - acc: 0.4332 - val_loss: 2.1951 - val_acc: 0.4368\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1601 - acc: 0.4342 - val_loss: 2.1995 - val_acc: 0.4345\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1522 - acc: 0.4368 - val_loss: 2.2090 - val_acc: 0.4354\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1439 - acc: 0.4350 - val_loss: 2.2082 - val_acc: 0.4356\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1379 - acc: 0.4398 - val_loss: 2.2189 - val_acc: 0.4366\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1309 - acc: 0.4406 - val_loss: 2.2174 - val_acc: 0.4372\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1242 - acc: 0.4409 - val_loss: 2.2063 - val_acc: 0.4355\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1215 - acc: 0.4420 - val_loss: 2.2171 - val_acc: 0.4345\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1264 - acc: 0.4403 - val_loss: 2.2114 - val_acc: 0.4356\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1120 - acc: 0.4452 - val_loss: 2.2141 - val_acc: 0.4341\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1069 - acc: 0.4456 - val_loss: 2.2135 - val_acc: 0.4351\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1042 - acc: 0.4455 - val_loss: 2.2081 - val_acc: 0.4362\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0956 - acc: 0.4465 - val_loss: 2.2345 - val_acc: 0.4358\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1351 - acc: 0.4413 - val_loss: 2.2507 - val_acc: 0.4269\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1229 - acc: 0.4398 - val_loss: 2.2197 - val_acc: 0.4342\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0934 - acc: 0.4481 - val_loss: 2.2155 - val_acc: 0.4355\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0860 - acc: 0.4495 - val_loss: 2.2133 - val_acc: 0.4346\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0862 - acc: 0.4485 - val_loss: 2.1993 - val_acc: 0.4387\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0791 - acc: 0.4515 - val_loss: 2.2261 - val_acc: 0.4335\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0906 - acc: 0.4500 - val_loss: 2.2262 - val_acc: 0.4333\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0751 - acc: 0.4535 - val_loss: 2.2253 - val_acc: 0.4350\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0860 - acc: 0.4487 - val_loss: 2.1995 - val_acc: 0.4373\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0685 - acc: 0.4531 - val_loss: 2.2165 - val_acc: 0.4346\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0668 - acc: 0.4545 - val_loss: 2.2127 - val_acc: 0.4377\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0663 - acc: 0.4544 - val_loss: 2.2040 - val_acc: 0.4354\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0567 - acc: 0.4552 - val_loss: 2.2100 - val_acc: 0.4367\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0609 - acc: 0.4556 - val_loss: 2.2038 - val_acc: 0.4366\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0529 - acc: 0.4559 - val_loss: 2.2031 - val_acc: 0.4369\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0539 - acc: 0.4560 - val_loss: 2.1996 - val_acc: 0.4393\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0505 - acc: 0.4574 - val_loss: 2.1934 - val_acc: 0.4377\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0484 - acc: 0.4576 - val_loss: 2.1958 - val_acc: 0.4379\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0456 - acc: 0.4575 - val_loss: 2.1977 - val_acc: 0.4374\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0399 - acc: 0.4576 - val_loss: 2.1957 - val_acc: 0.4378\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0397 - acc: 0.4600 - val_loss: 2.1990 - val_acc: 0.4373\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0413 - acc: 0.4596 - val_loss: 2.1965 - val_acc: 0.4390\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0338 - acc: 0.4604 - val_loss: 2.2020 - val_acc: 0.4396\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0380 - acc: 0.4598 - val_loss: 2.2009 - val_acc: 0.4383\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0314 - acc: 0.4626 - val_loss: 2.2020 - val_acc: 0.4371\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0287 - acc: 0.4627 - val_loss: 2.2104 - val_acc: 0.4363\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0517 - acc: 0.4570 - val_loss: 2.1922 - val_acc: 0.4394\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0333 - acc: 0.4606 - val_loss: 2.2063 - val_acc: 0.4373\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0332 - acc: 0.4620 - val_loss: 2.1968 - val_acc: 0.4375\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0320 - acc: 0.4614 - val_loss: 2.2142 - val_acc: 0.4333\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0216 - acc: 0.4618 - val_loss: 2.2087 - val_acc: 0.4396\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0321 - acc: 0.4623 - val_loss: 2.2024 - val_acc: 0.4353\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0178 - acc: 0.4652 - val_loss: 2.1934 - val_acc: 0.4388\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0149 - acc: 0.4643 - val_loss: 2.2040 - val_acc: 0.4368\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0146 - acc: 0.4659 - val_loss: 2.2313 - val_acc: 0.4337\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0275 - acc: 0.4626 - val_loss: 2.2247 - val_acc: 0.4364\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0302 - acc: 0.4611 - val_loss: 2.2007 - val_acc: 0.4397\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0123 - acc: 0.4666 - val_loss: 2.2270 - val_acc: 0.4386\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0610 - acc: 0.4573 - val_loss: 2.2469 - val_acc: 0.4272\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0496 - acc: 0.4571 - val_loss: 2.2184 - val_acc: 0.4332\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0268 - acc: 0.4609 - val_loss: 2.2059 - val_acc: 0.4370\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0069 - acc: 0.4668 - val_loss: 2.2053 - val_acc: 0.4361\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9980 - acc: 0.4686 - val_loss: 2.1948 - val_acc: 0.4361\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9967 - acc: 0.4686 - val_loss: 2.2094 - val_acc: 0.4382\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0002 - acc: 0.4684 - val_loss: 2.2018 - val_acc: 0.4355\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9969 - acc: 0.4687 - val_loss: 2.1968 - val_acc: 0.4399\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9997 - acc: 0.4700 - val_loss: 2.2090 - val_acc: 0.4362\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9936 - acc: 0.4684 - val_loss: 2.1937 - val_acc: 0.4400\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9882 - acc: 0.4716 - val_loss: 2.1959 - val_acc: 0.4385\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9862 - acc: 0.4708 - val_loss: 2.2012 - val_acc: 0.4396\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9960 - acc: 0.4698 - val_loss: 2.2029 - val_acc: 0.4365\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9903 - acc: 0.4714 - val_loss: 2.1945 - val_acc: 0.4399\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9826 - acc: 0.4727 - val_loss: 2.2067 - val_acc: 0.4376\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9829 - acc: 0.4724 - val_loss: 2.2049 - val_acc: 0.4349\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9884 - acc: 0.4722 - val_loss: 2.2142 - val_acc: 0.4376\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9933 - acc: 0.4703 - val_loss: 2.2115 - val_acc: 0.4328\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9897 - acc: 0.4711 - val_loss: 2.2013 - val_acc: 0.4395\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9871 - acc: 0.4705 - val_loss: 2.2124 - val_acc: 0.4359\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9863 - acc: 0.4724 - val_loss: 2.1975 - val_acc: 0.4389\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9757 - acc: 0.4733 - val_loss: 2.2063 - val_acc: 0.4352\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9739 - acc: 0.4748 - val_loss: 2.2034 - val_acc: 0.4384\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9778 - acc: 0.4740 - val_loss: 2.1949 - val_acc: 0.4399\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9708 - acc: 0.4739 - val_loss: 2.1992 - val_acc: 0.4373\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9715 - acc: 0.4744 - val_loss: 2.2190 - val_acc: 0.4353\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9882 - acc: 0.4719 - val_loss: 2.2268 - val_acc: 0.4360\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9726 - acc: 0.4754 - val_loss: 2.2201 - val_acc: 0.4346\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9798 - acc: 0.4726 - val_loss: 2.2146 - val_acc: 0.4360\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9691 - acc: 0.4760 - val_loss: 2.1965 - val_acc: 0.4387\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9616 - acc: 0.4765 - val_loss: 2.1980 - val_acc: 0.4361\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9611 - acc: 0.4782 - val_loss: 2.1901 - val_acc: 0.4407\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9715 - acc: 0.4759 - val_loss: 2.2146 - val_acc: 0.4378\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9727 - acc: 0.4758 - val_loss: 2.2024 - val_acc: 0.4386\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9663 - acc: 0.4780 - val_loss: 2.2226 - val_acc: 0.4384\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9928 - acc: 0.4712 - val_loss: 2.2202 - val_acc: 0.4351\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9670 - acc: 0.4768 - val_loss: 2.2045 - val_acc: 0.4408\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9523 - acc: 0.4790 - val_loss: 2.2064 - val_acc: 0.4375\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9554 - acc: 0.4793 - val_loss: 2.1991 - val_acc: 0.4398\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9546 - acc: 0.4786 - val_loss: 2.1966 - val_acc: 0.4382\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9588 - acc: 0.4777 - val_loss: 2.1994 - val_acc: 0.4398\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9487 - acc: 0.4824 - val_loss: 2.2169 - val_acc: 0.4382\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9662 - acc: 0.4763 - val_loss: 2.2252 - val_acc: 0.4351\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9602 - acc: 0.4765 - val_loss: 2.1965 - val_acc: 0.4389\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9480 - acc: 0.4801 - val_loss: 2.2123 - val_acc: 0.4360\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9660 - acc: 0.4783 - val_loss: 2.2217 - val_acc: 0.4364\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9637 - acc: 0.4750 - val_loss: 2.2513 - val_acc: 0.4325\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2053 - acc: 0.4219 - val_loss: 2.1946 - val_acc: 0.4455\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1805 - acc: 0.4287 - val_loss: 2.1934 - val_acc: 0.4452\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1667 - acc: 0.4299 - val_loss: 2.1919 - val_acc: 0.4430\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1515 - acc: 0.4317 - val_loss: 2.1993 - val_acc: 0.4423\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1417 - acc: 0.4341 - val_loss: 2.2239 - val_acc: 0.4379\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1402 - acc: 0.4359 - val_loss: 2.2093 - val_acc: 0.4406\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1334 - acc: 0.4373 - val_loss: 2.2044 - val_acc: 0.4404\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1283 - acc: 0.4373 - val_loss: 2.2114 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1171 - acc: 0.4411 - val_loss: 2.2073 - val_acc: 0.4409\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1165 - acc: 0.4394 - val_loss: 2.2149 - val_acc: 0.4382\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1109 - acc: 0.4408 - val_loss: 2.2169 - val_acc: 0.4378\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1118 - acc: 0.4423 - val_loss: 2.2299 - val_acc: 0.4374\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1029 - acc: 0.4430 - val_loss: 2.2601 - val_acc: 0.4321\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1163 - acc: 0.4409 - val_loss: 2.2298 - val_acc: 0.4389\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0940 - acc: 0.4437 - val_loss: 2.2248 - val_acc: 0.4370\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0890 - acc: 0.4474 - val_loss: 2.2270 - val_acc: 0.4366\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0855 - acc: 0.4474 - val_loss: 2.2165 - val_acc: 0.4376\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0838 - acc: 0.4456 - val_loss: 2.2372 - val_acc: 0.4378\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0797 - acc: 0.4492 - val_loss: 2.2415 - val_acc: 0.4366\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0787 - acc: 0.4483 - val_loss: 2.2279 - val_acc: 0.4374\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0770 - acc: 0.4490 - val_loss: 2.2286 - val_acc: 0.4372\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0712 - acc: 0.4499 - val_loss: 2.2227 - val_acc: 0.4381\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0698 - acc: 0.4510 - val_loss: 2.2345 - val_acc: 0.4353\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0638 - acc: 0.4500 - val_loss: 2.2234 - val_acc: 0.4352\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0662 - acc: 0.4497 - val_loss: 2.2199 - val_acc: 0.4390\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0590 - acc: 0.4507 - val_loss: 2.2399 - val_acc: 0.4354\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0528 - acc: 0.4542 - val_loss: 2.2161 - val_acc: 0.4400\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0515 - acc: 0.4535 - val_loss: 2.2397 - val_acc: 0.4353\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0548 - acc: 0.4524 - val_loss: 2.2297 - val_acc: 0.4366\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0503 - acc: 0.4544 - val_loss: 2.2320 - val_acc: 0.4362\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0519 - acc: 0.4555 - val_loss: 2.2208 - val_acc: 0.4380\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0455 - acc: 0.4550 - val_loss: 2.2167 - val_acc: 0.4406\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0441 - acc: 0.4553 - val_loss: 2.2351 - val_acc: 0.4366\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0580 - acc: 0.4534 - val_loss: 2.2183 - val_acc: 0.4399\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0390 - acc: 0.4567 - val_loss: 2.2206 - val_acc: 0.4380\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0348 - acc: 0.4583 - val_loss: 2.2151 - val_acc: 0.4413\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0331 - acc: 0.4596 - val_loss: 2.2184 - val_acc: 0.4392\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0344 - acc: 0.4587 - val_loss: 2.2241 - val_acc: 0.4363\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0345 - acc: 0.4567 - val_loss: 2.2120 - val_acc: 0.4414\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0316 - acc: 0.4590 - val_loss: 2.2066 - val_acc: 0.4410\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0254 - acc: 0.4599 - val_loss: 2.2087 - val_acc: 0.4401\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0239 - acc: 0.4609 - val_loss: 2.2263 - val_acc: 0.4370\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0446 - acc: 0.4560 - val_loss: 2.2242 - val_acc: 0.4363\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0306 - acc: 0.4595 - val_loss: 2.2168 - val_acc: 0.4387\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0166 - acc: 0.4640 - val_loss: 2.2104 - val_acc: 0.4401\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0144 - acc: 0.4608 - val_loss: 2.2123 - val_acc: 0.4406\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0172 - acc: 0.4609 - val_loss: 2.2110 - val_acc: 0.4396\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0122 - acc: 0.4617 - val_loss: 2.2015 - val_acc: 0.4414\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0121 - acc: 0.4622 - val_loss: 2.2101 - val_acc: 0.4387\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0099 - acc: 0.4620 - val_loss: 2.2207 - val_acc: 0.4399\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0220 - acc: 0.4601 - val_loss: 2.2138 - val_acc: 0.4419\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0084 - acc: 0.4644 - val_loss: 2.2095 - val_acc: 0.4402\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0059 - acc: 0.4642 - val_loss: 2.2157 - val_acc: 0.4410\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0028 - acc: 0.4631 - val_loss: 2.2090 - val_acc: 0.4410\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0018 - acc: 0.4640 - val_loss: 2.2064 - val_acc: 0.4403\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0009 - acc: 0.4662 - val_loss: 2.2071 - val_acc: 0.4416\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9923 - acc: 0.4672 - val_loss: 2.2180 - val_acc: 0.4403\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0016 - acc: 0.4653 - val_loss: 2.2178 - val_acc: 0.4387\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9962 - acc: 0.4670 - val_loss: 2.2061 - val_acc: 0.4418\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9957 - acc: 0.4672 - val_loss: 2.2192 - val_acc: 0.4398\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9949 - acc: 0.4680 - val_loss: 2.2056 - val_acc: 0.4413\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9970 - acc: 0.4652 - val_loss: 2.2275 - val_acc: 0.4386\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0016 - acc: 0.4668 - val_loss: 2.2100 - val_acc: 0.4396\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9924 - acc: 0.4670 - val_loss: 2.2369 - val_acc: 0.4386\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9955 - acc: 0.4662 - val_loss: 2.2216 - val_acc: 0.4391\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9958 - acc: 0.4667 - val_loss: 2.2274 - val_acc: 0.4389\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9926 - acc: 0.4662 - val_loss: 2.2053 - val_acc: 0.4411\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9813 - acc: 0.4676 - val_loss: 2.2007 - val_acc: 0.4427\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9808 - acc: 0.4695 - val_loss: 2.2022 - val_acc: 0.4427\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9868 - acc: 0.4683 - val_loss: 2.2152 - val_acc: 0.4392\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9717 - acc: 0.4709 - val_loss: 2.2009 - val_acc: 0.4390\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9700 - acc: 0.4729 - val_loss: 2.2063 - val_acc: 0.4422\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9708 - acc: 0.4723 - val_loss: 2.2159 - val_acc: 0.4407\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9722 - acc: 0.4718 - val_loss: 2.2276 - val_acc: 0.4392\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9764 - acc: 0.4711 - val_loss: 2.2082 - val_acc: 0.4424\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9767 - acc: 0.4708 - val_loss: 2.2168 - val_acc: 0.4422\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9745 - acc: 0.4705 - val_loss: 2.2180 - val_acc: 0.4392\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9757 - acc: 0.4725 - val_loss: 2.2061 - val_acc: 0.4430\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9681 - acc: 0.4714 - val_loss: 2.2079 - val_acc: 0.4413\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9769 - acc: 0.4712 - val_loss: 2.2191 - val_acc: 0.4386\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9756 - acc: 0.4714 - val_loss: 2.2075 - val_acc: 0.4416\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9728 - acc: 0.4727 - val_loss: 2.2131 - val_acc: 0.4414\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9573 - acc: 0.4750 - val_loss: 2.2146 - val_acc: 0.4400\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9620 - acc: 0.4744 - val_loss: 2.2133 - val_acc: 0.4399\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9606 - acc: 0.4735 - val_loss: 2.2168 - val_acc: 0.4395\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9622 - acc: 0.4736 - val_loss: 2.2174 - val_acc: 0.4402\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9649 - acc: 0.4751 - val_loss: 2.2191 - val_acc: 0.4390\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9587 - acc: 0.4756 - val_loss: 2.2162 - val_acc: 0.4431\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9586 - acc: 0.4755 - val_loss: 2.2095 - val_acc: 0.4410\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9623 - acc: 0.4749 - val_loss: 2.2239 - val_acc: 0.4399\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9590 - acc: 0.4753 - val_loss: 2.2559 - val_acc: 0.4328\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9654 - acc: 0.4727 - val_loss: 2.2208 - val_acc: 0.4392\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9602 - acc: 0.4732 - val_loss: 2.2046 - val_acc: 0.4423\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9511 - acc: 0.4770 - val_loss: 2.2082 - val_acc: 0.4409\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9417 - acc: 0.4786 - val_loss: 2.2082 - val_acc: 0.4412\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9576 - acc: 0.4750 - val_loss: 2.2110 - val_acc: 0.4399\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9459 - acc: 0.4784 - val_loss: 2.1982 - val_acc: 0.4429\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9415 - acc: 0.4800 - val_loss: 2.2028 - val_acc: 0.4399\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9373 - acc: 0.4801 - val_loss: 2.2030 - val_acc: 0.4443\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9386 - acc: 0.4801 - val_loss: 2.2155 - val_acc: 0.4377\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1791 - acc: 0.4310 - val_loss: 2.2121 - val_acc: 0.4390\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1598 - acc: 0.4334 - val_loss: 2.2202 - val_acc: 0.4379\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1481 - acc: 0.4372 - val_loss: 2.2294 - val_acc: 0.4386\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1541 - acc: 0.4367 - val_loss: 2.2341 - val_acc: 0.4344\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1312 - acc: 0.4400 - val_loss: 2.2261 - val_acc: 0.4364\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1222 - acc: 0.4405 - val_loss: 2.2264 - val_acc: 0.4364\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1201 - acc: 0.4429 - val_loss: 2.2252 - val_acc: 0.4374\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1090 - acc: 0.4444 - val_loss: 2.2350 - val_acc: 0.4356\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1049 - acc: 0.4446 - val_loss: 2.2386 - val_acc: 0.4342\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1008 - acc: 0.4457 - val_loss: 2.2542 - val_acc: 0.4339\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1286 - acc: 0.4402 - val_loss: 2.2656 - val_acc: 0.4282\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1116 - acc: 0.4454 - val_loss: 2.2410 - val_acc: 0.4310\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0927 - acc: 0.4476 - val_loss: 2.2337 - val_acc: 0.4349\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0854 - acc: 0.4479 - val_loss: 2.2395 - val_acc: 0.4339\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0804 - acc: 0.4498 - val_loss: 2.2327 - val_acc: 0.4326\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0738 - acc: 0.4505 - val_loss: 2.2438 - val_acc: 0.4323\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0741 - acc: 0.4506 - val_loss: 2.2451 - val_acc: 0.4323\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0732 - acc: 0.4535 - val_loss: 2.2403 - val_acc: 0.4320\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0657 - acc: 0.4530 - val_loss: 2.2481 - val_acc: 0.4310\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0656 - acc: 0.4528 - val_loss: 2.2456 - val_acc: 0.4311\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0594 - acc: 0.4534 - val_loss: 2.2364 - val_acc: 0.4344\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0599 - acc: 0.4531 - val_loss: 2.2425 - val_acc: 0.4322\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0582 - acc: 0.4533 - val_loss: 2.2402 - val_acc: 0.4353\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0587 - acc: 0.4542 - val_loss: 2.2467 - val_acc: 0.4318\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0493 - acc: 0.4556 - val_loss: 2.2402 - val_acc: 0.4345\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0429 - acc: 0.4584 - val_loss: 2.2433 - val_acc: 0.4346\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0453 - acc: 0.4571 - val_loss: 2.2363 - val_acc: 0.4356\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0684 - acc: 0.4527 - val_loss: 2.2703 - val_acc: 0.4271\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0505 - acc: 0.4553 - val_loss: 2.2410 - val_acc: 0.4339\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0367 - acc: 0.4600 - val_loss: 2.2224 - val_acc: 0.4385\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0339 - acc: 0.4596 - val_loss: 2.2358 - val_acc: 0.4351\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0269 - acc: 0.4623 - val_loss: 2.2261 - val_acc: 0.4367\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0258 - acc: 0.4612 - val_loss: 2.2394 - val_acc: 0.4352\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0274 - acc: 0.4623 - val_loss: 2.2577 - val_acc: 0.4310\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0332 - acc: 0.4602 - val_loss: 2.2507 - val_acc: 0.4361\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0450 - acc: 0.4586 - val_loss: 2.2449 - val_acc: 0.4348\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0258 - acc: 0.4617 - val_loss: 2.2513 - val_acc: 0.4337\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0185 - acc: 0.4624 - val_loss: 2.2325 - val_acc: 0.4372\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0150 - acc: 0.4644 - val_loss: 2.2479 - val_acc: 0.4367\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0183 - acc: 0.4618 - val_loss: 2.2468 - val_acc: 0.4331\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0073 - acc: 0.4666 - val_loss: 2.2287 - val_acc: 0.4376\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0092 - acc: 0.4669 - val_loss: 2.2402 - val_acc: 0.4355\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0099 - acc: 0.4629 - val_loss: 2.2333 - val_acc: 0.4360\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0081 - acc: 0.4642 - val_loss: 2.2271 - val_acc: 0.4373\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0045 - acc: 0.4672 - val_loss: 2.2412 - val_acc: 0.4358\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0060 - acc: 0.4668 - val_loss: 2.2265 - val_acc: 0.4394\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9990 - acc: 0.4683 - val_loss: 2.2207 - val_acc: 0.4381\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0035 - acc: 0.4677 - val_loss: 2.2761 - val_acc: 0.4329\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0211 - acc: 0.4632 - val_loss: 2.2316 - val_acc: 0.4371\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0045 - acc: 0.4665 - val_loss: 2.2417 - val_acc: 0.4367\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0079 - acc: 0.4627 - val_loss: 2.2390 - val_acc: 0.4347\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9996 - acc: 0.4669 - val_loss: 2.2199 - val_acc: 0.4380\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9894 - acc: 0.4684 - val_loss: 2.2192 - val_acc: 0.4406\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9868 - acc: 0.4690 - val_loss: 2.2188 - val_acc: 0.4401\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9875 - acc: 0.4689 - val_loss: 2.2180 - val_acc: 0.4375\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9927 - acc: 0.4675 - val_loss: 2.2308 - val_acc: 0.4388\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0100 - acc: 0.4633 - val_loss: 2.2419 - val_acc: 0.4372\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9971 - acc: 0.4674 - val_loss: 2.2306 - val_acc: 0.4383\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9910 - acc: 0.4686 - val_loss: 2.2490 - val_acc: 0.4340\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9951 - acc: 0.4702 - val_loss: 2.2629 - val_acc: 0.4334\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9944 - acc: 0.4692 - val_loss: 2.2467 - val_acc: 0.4368\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9832 - acc: 0.4694 - val_loss: 2.2379 - val_acc: 0.4391\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9785 - acc: 0.4730 - val_loss: 2.2367 - val_acc: 0.4392\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9811 - acc: 0.4702 - val_loss: 2.2295 - val_acc: 0.4391\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9787 - acc: 0.4728 - val_loss: 2.2371 - val_acc: 0.4372\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9732 - acc: 0.4726 - val_loss: 2.2567 - val_acc: 0.4340\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9730 - acc: 0.4752 - val_loss: 2.2253 - val_acc: 0.4387\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9655 - acc: 0.4752 - val_loss: 2.2305 - val_acc: 0.4380\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9784 - acc: 0.4731 - val_loss: 2.2325 - val_acc: 0.4379\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9653 - acc: 0.4766 - val_loss: 2.2475 - val_acc: 0.4325\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9692 - acc: 0.4763 - val_loss: 2.2394 - val_acc: 0.4369\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9678 - acc: 0.4759 - val_loss: 2.2447 - val_acc: 0.4338\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9666 - acc: 0.4752 - val_loss: 2.2373 - val_acc: 0.4389\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9564 - acc: 0.4767 - val_loss: 2.2223 - val_acc: 0.4371\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9576 - acc: 0.4769 - val_loss: 2.2091 - val_acc: 0.4418\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9604 - acc: 0.4756 - val_loss: 2.2636 - val_acc: 0.4358\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0062 - acc: 0.4655 - val_loss: 2.2563 - val_acc: 0.4314\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9834 - acc: 0.4727 - val_loss: 2.2343 - val_acc: 0.4373\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9610 - acc: 0.4760 - val_loss: 2.2632 - val_acc: 0.4317\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9640 - acc: 0.4757 - val_loss: 2.2372 - val_acc: 0.4364\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9555 - acc: 0.4787 - val_loss: 2.2308 - val_acc: 0.4390\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9600 - acc: 0.4776 - val_loss: 2.2365 - val_acc: 0.4368\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9564 - acc: 0.4770 - val_loss: 2.2281 - val_acc: 0.4382\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9534 - acc: 0.4777 - val_loss: 2.2259 - val_acc: 0.4367\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9565 - acc: 0.4774 - val_loss: 2.2329 - val_acc: 0.4375\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9473 - acc: 0.4777 - val_loss: 2.2287 - val_acc: 0.4380\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9477 - acc: 0.4797 - val_loss: 2.2339 - val_acc: 0.4380\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9480 - acc: 0.4777 - val_loss: 2.2349 - val_acc: 0.4329\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9506 - acc: 0.4766 - val_loss: 2.2342 - val_acc: 0.4385\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9552 - acc: 0.4796 - val_loss: 2.2523 - val_acc: 0.4355\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9773 - acc: 0.4754 - val_loss: 2.2515 - val_acc: 0.4316\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9552 - acc: 0.4781 - val_loss: 2.2362 - val_acc: 0.4322\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9414 - acc: 0.4788 - val_loss: 2.2337 - val_acc: 0.4363\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9380 - acc: 0.4801 - val_loss: 2.2250 - val_acc: 0.4382\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9266 - acc: 0.4846 - val_loss: 2.2354 - val_acc: 0.4378\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9310 - acc: 0.4836 - val_loss: 2.2146 - val_acc: 0.4379\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9237 - acc: 0.4851 - val_loss: 2.2224 - val_acc: 0.4367\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9208 - acc: 0.4842 - val_loss: 2.2225 - val_acc: 0.4374\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9263 - acc: 0.4849 - val_loss: 2.2320 - val_acc: 0.4371\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9246 - acc: 0.4853 - val_loss: 2.2374 - val_acc: 0.4376\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2123 - acc: 0.4235 - val_loss: 2.1972 - val_acc: 0.4364\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1896 - acc: 0.4283 - val_loss: 2.2002 - val_acc: 0.4349\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1780 - acc: 0.4299 - val_loss: 2.2006 - val_acc: 0.4381\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1668 - acc: 0.4309 - val_loss: 2.2154 - val_acc: 0.4335\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1597 - acc: 0.4332 - val_loss: 2.2048 - val_acc: 0.4364\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1492 - acc: 0.4356 - val_loss: 2.1993 - val_acc: 0.4372\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1458 - acc: 0.4359 - val_loss: 2.2076 - val_acc: 0.4355\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1409 - acc: 0.4369 - val_loss: 2.2109 - val_acc: 0.4354\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1343 - acc: 0.4389 - val_loss: 2.2080 - val_acc: 0.4378\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1312 - acc: 0.4379 - val_loss: 2.2396 - val_acc: 0.4308\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1292 - acc: 0.4399 - val_loss: 2.2289 - val_acc: 0.4326\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1217 - acc: 0.4420 - val_loss: 2.2111 - val_acc: 0.4350\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1157 - acc: 0.4431 - val_loss: 2.2168 - val_acc: 0.4328\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1164 - acc: 0.4438 - val_loss: 2.2190 - val_acc: 0.4333\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1060 - acc: 0.4462 - val_loss: 2.2161 - val_acc: 0.4335\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1032 - acc: 0.4440 - val_loss: 2.2162 - val_acc: 0.4356\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1062 - acc: 0.4459 - val_loss: 2.2068 - val_acc: 0.4349\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0936 - acc: 0.4482 - val_loss: 2.2069 - val_acc: 0.4355\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0974 - acc: 0.4471 - val_loss: 2.2185 - val_acc: 0.4342\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0971 - acc: 0.4468 - val_loss: 2.2188 - val_acc: 0.4355\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0935 - acc: 0.4493 - val_loss: 2.2047 - val_acc: 0.4383\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0888 - acc: 0.4482 - val_loss: 2.2081 - val_acc: 0.4363\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0841 - acc: 0.4517 - val_loss: 2.2095 - val_acc: 0.4353\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0824 - acc: 0.4511 - val_loss: 2.2065 - val_acc: 0.4365\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0849 - acc: 0.4505 - val_loss: 2.2169 - val_acc: 0.4345\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0844 - acc: 0.4497 - val_loss: 2.2049 - val_acc: 0.4384\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0772 - acc: 0.4505 - val_loss: 2.2030 - val_acc: 0.4376\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0705 - acc: 0.4532 - val_loss: 2.1961 - val_acc: 0.4373\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0667 - acc: 0.4543 - val_loss: 2.1932 - val_acc: 0.4405\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0662 - acc: 0.4541 - val_loss: 2.1973 - val_acc: 0.4366\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0632 - acc: 0.4552 - val_loss: 2.1981 - val_acc: 0.4377\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0610 - acc: 0.4545 - val_loss: 2.1977 - val_acc: 0.4374\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0733 - acc: 0.4528 - val_loss: 2.2060 - val_acc: 0.4361\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0758 - acc: 0.4521 - val_loss: 2.2047 - val_acc: 0.4377\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0643 - acc: 0.4545 - val_loss: 2.1976 - val_acc: 0.4371\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0544 - acc: 0.4559 - val_loss: 2.1953 - val_acc: 0.4376\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0521 - acc: 0.4578 - val_loss: 2.1967 - val_acc: 0.4394\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0496 - acc: 0.4562 - val_loss: 2.2126 - val_acc: 0.4360\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0571 - acc: 0.4577 - val_loss: 2.2064 - val_acc: 0.4381\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0490 - acc: 0.4593 - val_loss: 2.2018 - val_acc: 0.4383\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0486 - acc: 0.4571 - val_loss: 2.2100 - val_acc: 0.4356\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0428 - acc: 0.4576 - val_loss: 2.1942 - val_acc: 0.4385\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0441 - acc: 0.4584 - val_loss: 2.1999 - val_acc: 0.4377\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0378 - acc: 0.4605 - val_loss: 2.1918 - val_acc: 0.4402\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0412 - acc: 0.4596 - val_loss: 2.2045 - val_acc: 0.4370\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0440 - acc: 0.4577 - val_loss: 2.2070 - val_acc: 0.4357\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0362 - acc: 0.4592 - val_loss: 2.2085 - val_acc: 0.4372\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0364 - acc: 0.4594 - val_loss: 2.1978 - val_acc: 0.4399\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0369 - acc: 0.4611 - val_loss: 2.1931 - val_acc: 0.4377\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0414 - acc: 0.4608 - val_loss: 2.2194 - val_acc: 0.4346\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0343 - acc: 0.4617 - val_loss: 2.1969 - val_acc: 0.4383\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0289 - acc: 0.4634 - val_loss: 2.1928 - val_acc: 0.4404\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0231 - acc: 0.4629 - val_loss: 2.1867 - val_acc: 0.4407\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0194 - acc: 0.4650 - val_loss: 2.1964 - val_acc: 0.4384\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0202 - acc: 0.4649 - val_loss: 2.1951 - val_acc: 0.4413\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0187 - acc: 0.4650 - val_loss: 2.2043 - val_acc: 0.4386\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0231 - acc: 0.4655 - val_loss: 2.1908 - val_acc: 0.4400\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0212 - acc: 0.4637 - val_loss: 2.2122 - val_acc: 0.4369\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0253 - acc: 0.4641 - val_loss: 2.2033 - val_acc: 0.4391\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0212 - acc: 0.4655 - val_loss: 2.2377 - val_acc: 0.4337\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0200 - acc: 0.4638 - val_loss: 2.2156 - val_acc: 0.4394\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0333 - acc: 0.4607 - val_loss: 2.2242 - val_acc: 0.4333\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0225 - acc: 0.4652 - val_loss: 2.1932 - val_acc: 0.4393\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0173 - acc: 0.4642 - val_loss: 2.2142 - val_acc: 0.4355\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0153 - acc: 0.4646 - val_loss: 2.1911 - val_acc: 0.4398\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0062 - acc: 0.4670 - val_loss: 2.1950 - val_acc: 0.4397\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0036 - acc: 0.4670 - val_loss: 2.1965 - val_acc: 0.4358\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0015 - acc: 0.4702 - val_loss: 2.1915 - val_acc: 0.4395\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0071 - acc: 0.4676 - val_loss: 2.1847 - val_acc: 0.4408\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0022 - acc: 0.4687 - val_loss: 2.1825 - val_acc: 0.4412\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9879 - acc: 0.4722 - val_loss: 2.1864 - val_acc: 0.4408\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9893 - acc: 0.4714 - val_loss: 2.1949 - val_acc: 0.4394\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9965 - acc: 0.4717 - val_loss: 2.1915 - val_acc: 0.4418\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9914 - acc: 0.4698 - val_loss: 2.2005 - val_acc: 0.4385\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9911 - acc: 0.4722 - val_loss: 2.1876 - val_acc: 0.4387\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9885 - acc: 0.4735 - val_loss: 2.2267 - val_acc: 0.4352\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9943 - acc: 0.4706 - val_loss: 2.1905 - val_acc: 0.4402\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9891 - acc: 0.4731 - val_loss: 2.2293 - val_acc: 0.4352\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9909 - acc: 0.4727 - val_loss: 2.2158 - val_acc: 0.4325\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9911 - acc: 0.4705 - val_loss: 2.2246 - val_acc: 0.4344\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9888 - acc: 0.4728 - val_loss: 2.2200 - val_acc: 0.4364\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0009 - acc: 0.4689 - val_loss: 2.2173 - val_acc: 0.4360\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9908 - acc: 0.4715 - val_loss: 2.2013 - val_acc: 0.4389\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9921 - acc: 0.4680 - val_loss: 2.2188 - val_acc: 0.4369\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9904 - acc: 0.4714 - val_loss: 2.2328 - val_acc: 0.4400\n",
      "Epoch 86/100\n",
      "1s - loss: 2.0267 - acc: 0.4639 - val_loss: 2.2503 - val_acc: 0.4283\n",
      "Epoch 87/100\n",
      "1s - loss: 2.0059 - acc: 0.4673 - val_loss: 2.1891 - val_acc: 0.4391\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9718 - acc: 0.4741 - val_loss: 2.2017 - val_acc: 0.4373\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9811 - acc: 0.4723 - val_loss: 2.2151 - val_acc: 0.4345\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9882 - acc: 0.4697 - val_loss: 2.1929 - val_acc: 0.4383\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9739 - acc: 0.4755 - val_loss: 2.1994 - val_acc: 0.4389\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9685 - acc: 0.4762 - val_loss: 2.1931 - val_acc: 0.4394\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9709 - acc: 0.4748 - val_loss: 2.2053 - val_acc: 0.4381\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9754 - acc: 0.4754 - val_loss: 2.1965 - val_acc: 0.4414\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9851 - acc: 0.4710 - val_loss: 2.2137 - val_acc: 0.4379\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9886 - acc: 0.4718 - val_loss: 2.1871 - val_acc: 0.4424\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9667 - acc: 0.4752 - val_loss: 2.1994 - val_acc: 0.4393\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9626 - acc: 0.4781 - val_loss: 2.1967 - val_acc: 0.4406\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9745 - acc: 0.4757 - val_loss: 2.1973 - val_acc: 0.4387\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9619 - acc: 0.4764 - val_loss: 2.1981 - val_acc: 0.4386\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2040 - acc: 0.4257 - val_loss: 2.2084 - val_acc: 0.4351\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1896 - acc: 0.4288 - val_loss: 2.2041 - val_acc: 0.4336\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1697 - acc: 0.4321 - val_loss: 2.2008 - val_acc: 0.4354\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1631 - acc: 0.4343 - val_loss: 2.2051 - val_acc: 0.4352\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1535 - acc: 0.4343 - val_loss: 2.2086 - val_acc: 0.4359\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1469 - acc: 0.4379 - val_loss: 2.2138 - val_acc: 0.4343\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1438 - acc: 0.4369 - val_loss: 2.2305 - val_acc: 0.4332\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1401 - acc: 0.4388 - val_loss: 2.2212 - val_acc: 0.4306\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1281 - acc: 0.4400 - val_loss: 2.2194 - val_acc: 0.4321\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1224 - acc: 0.4411 - val_loss: 2.2274 - val_acc: 0.4319\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1209 - acc: 0.4436 - val_loss: 2.2832 - val_acc: 0.4185\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1408 - acc: 0.4389 - val_loss: 2.2217 - val_acc: 0.4325\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1096 - acc: 0.4430 - val_loss: 2.2093 - val_acc: 0.4319\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1092 - acc: 0.4443 - val_loss: 2.2241 - val_acc: 0.4313\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1049 - acc: 0.4456 - val_loss: 2.2116 - val_acc: 0.4315\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1016 - acc: 0.4466 - val_loss: 2.2236 - val_acc: 0.4328\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1026 - acc: 0.4456 - val_loss: 2.2152 - val_acc: 0.4336\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0938 - acc: 0.4483 - val_loss: 2.2298 - val_acc: 0.4326\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0890 - acc: 0.4496 - val_loss: 2.2235 - val_acc: 0.4305\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0854 - acc: 0.4505 - val_loss: 2.2448 - val_acc: 0.4294\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0994 - acc: 0.4482 - val_loss: 2.2254 - val_acc: 0.4295\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0835 - acc: 0.4494 - val_loss: 2.2323 - val_acc: 0.4298\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0813 - acc: 0.4511 - val_loss: 2.2221 - val_acc: 0.4324\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0738 - acc: 0.4513 - val_loss: 2.2271 - val_acc: 0.4312\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0821 - acc: 0.4508 - val_loss: 2.2301 - val_acc: 0.4282\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0696 - acc: 0.4527 - val_loss: 2.2158 - val_acc: 0.4291\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0674 - acc: 0.4505 - val_loss: 2.2503 - val_acc: 0.4286\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0982 - acc: 0.4482 - val_loss: 2.2379 - val_acc: 0.4247\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0753 - acc: 0.4507 - val_loss: 2.2293 - val_acc: 0.4303\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0656 - acc: 0.4534 - val_loss: 2.2085 - val_acc: 0.4310\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0662 - acc: 0.4519 - val_loss: 2.2226 - val_acc: 0.4302\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0567 - acc: 0.4535 - val_loss: 2.2132 - val_acc: 0.4304\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0514 - acc: 0.4565 - val_loss: 2.2320 - val_acc: 0.4282\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0540 - acc: 0.4560 - val_loss: 2.2260 - val_acc: 0.4277\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0518 - acc: 0.4571 - val_loss: 2.2473 - val_acc: 0.4313\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0873 - acc: 0.4501 - val_loss: 2.2461 - val_acc: 0.4247\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0687 - acc: 0.4544 - val_loss: 2.2252 - val_acc: 0.4303\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0436 - acc: 0.4587 - val_loss: 2.2218 - val_acc: 0.4309\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0422 - acc: 0.4587 - val_loss: 2.2230 - val_acc: 0.4315\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0420 - acc: 0.4604 - val_loss: 2.2202 - val_acc: 0.4299\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0462 - acc: 0.4585 - val_loss: 2.2284 - val_acc: 0.4275\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0508 - acc: 0.4570 - val_loss: 2.2295 - val_acc: 0.4312\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0367 - acc: 0.4591 - val_loss: 2.2184 - val_acc: 0.4297\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0383 - acc: 0.4590 - val_loss: 2.2125 - val_acc: 0.4313\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0335 - acc: 0.4612 - val_loss: 2.2146 - val_acc: 0.4295\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0385 - acc: 0.4584 - val_loss: 2.2301 - val_acc: 0.4323\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0301 - acc: 0.4624 - val_loss: 2.2126 - val_acc: 0.4313\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0312 - acc: 0.4627 - val_loss: 2.2431 - val_acc: 0.4275\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0339 - acc: 0.4608 - val_loss: 2.2105 - val_acc: 0.4329\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0294 - acc: 0.4602 - val_loss: 2.2284 - val_acc: 0.4304\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0303 - acc: 0.4625 - val_loss: 2.2093 - val_acc: 0.4328\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0235 - acc: 0.4631 - val_loss: 2.2317 - val_acc: 0.4259\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0202 - acc: 0.4648 - val_loss: 2.2102 - val_acc: 0.4334\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0199 - acc: 0.4633 - val_loss: 2.2391 - val_acc: 0.4274\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0189 - acc: 0.4652 - val_loss: 2.2190 - val_acc: 0.4315\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0126 - acc: 0.4648 - val_loss: 2.2135 - val_acc: 0.4285\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0063 - acc: 0.4680 - val_loss: 2.2129 - val_acc: 0.4326\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0050 - acc: 0.4670 - val_loss: 2.2099 - val_acc: 0.4313\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0051 - acc: 0.4659 - val_loss: 2.2188 - val_acc: 0.4329\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0015 - acc: 0.4681 - val_loss: 2.2152 - val_acc: 0.4295\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0026 - acc: 0.4699 - val_loss: 2.2262 - val_acc: 0.4325\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0164 - acc: 0.4645 - val_loss: 2.2185 - val_acc: 0.4274\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0133 - acc: 0.4645 - val_loss: 2.2456 - val_acc: 0.4283\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0156 - acc: 0.4646 - val_loss: 2.2296 - val_acc: 0.4258\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0158 - acc: 0.4666 - val_loss: 2.2342 - val_acc: 0.4319\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0033 - acc: 0.4656 - val_loss: 2.2220 - val_acc: 0.4303\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0015 - acc: 0.4663 - val_loss: 2.2107 - val_acc: 0.4332\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9927 - acc: 0.4685 - val_loss: 2.2026 - val_acc: 0.4332\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9953 - acc: 0.4681 - val_loss: 2.2290 - val_acc: 0.4327\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0018 - acc: 0.4681 - val_loss: 2.2046 - val_acc: 0.4316\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9898 - acc: 0.4692 - val_loss: 2.2311 - val_acc: 0.4325\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9843 - acc: 0.4698 - val_loss: 2.2021 - val_acc: 0.4333\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9842 - acc: 0.4704 - val_loss: 2.2166 - val_acc: 0.4312\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9769 - acc: 0.4725 - val_loss: 2.2208 - val_acc: 0.4316\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9878 - acc: 0.4711 - val_loss: 2.2241 - val_acc: 0.4287\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9769 - acc: 0.4739 - val_loss: 2.2009 - val_acc: 0.4347\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9872 - acc: 0.4709 - val_loss: 2.2484 - val_acc: 0.4233\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0190 - acc: 0.4626 - val_loss: 2.2001 - val_acc: 0.4342\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9893 - acc: 0.4706 - val_loss: 2.2396 - val_acc: 0.4307\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9856 - acc: 0.4719 - val_loss: 2.2046 - val_acc: 0.4330\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9805 - acc: 0.4711 - val_loss: 2.2381 - val_acc: 0.4320\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9823 - acc: 0.4724 - val_loss: 2.2542 - val_acc: 0.4199\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9790 - acc: 0.4736 - val_loss: 2.2283 - val_acc: 0.4310\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9800 - acc: 0.4726 - val_loss: 2.2413 - val_acc: 0.4260\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9832 - acc: 0.4714 - val_loss: 2.2202 - val_acc: 0.4313\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9683 - acc: 0.4746 - val_loss: 2.2171 - val_acc: 0.4306\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9616 - acc: 0.4776 - val_loss: 2.2145 - val_acc: 0.4324\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9579 - acc: 0.4772 - val_loss: 2.2008 - val_acc: 0.4325\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9544 - acc: 0.4775 - val_loss: 2.2058 - val_acc: 0.4323\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9576 - acc: 0.4766 - val_loss: 2.2368 - val_acc: 0.4313\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9766 - acc: 0.4733 - val_loss: 2.2175 - val_acc: 0.4284\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9692 - acc: 0.4750 - val_loss: 2.2413 - val_acc: 0.4321\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9694 - acc: 0.4766 - val_loss: 2.2162 - val_acc: 0.4282\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9685 - acc: 0.4756 - val_loss: 2.2387 - val_acc: 0.4310\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9825 - acc: 0.4726 - val_loss: 2.2123 - val_acc: 0.4286\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9665 - acc: 0.4763 - val_loss: 2.2507 - val_acc: 0.4309\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9691 - acc: 0.4752 - val_loss: 2.2132 - val_acc: 0.4287\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9594 - acc: 0.4772 - val_loss: 2.2291 - val_acc: 0.4318\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9519 - acc: 0.4797 - val_loss: 2.2385 - val_acc: 0.4258\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9593 - acc: 0.4771 - val_loss: 2.2254 - val_acc: 0.4334\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2339 - acc: 0.4195 - val_loss: 2.1819 - val_acc: 0.4437\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2155 - acc: 0.4233 - val_loss: 2.1646 - val_acc: 0.4478\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1964 - acc: 0.4269 - val_loss: 2.1565 - val_acc: 0.4487\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1896 - acc: 0.4264 - val_loss: 2.1686 - val_acc: 0.4471\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1863 - acc: 0.4297 - val_loss: 2.1781 - val_acc: 0.4453\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1728 - acc: 0.4312 - val_loss: 2.1812 - val_acc: 0.4442\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1713 - acc: 0.4311 - val_loss: 2.1774 - val_acc: 0.4455\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1627 - acc: 0.4343 - val_loss: 2.1809 - val_acc: 0.4448\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1543 - acc: 0.4361 - val_loss: 2.1713 - val_acc: 0.4464\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1470 - acc: 0.4360 - val_loss: 2.1739 - val_acc: 0.4445\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1457 - acc: 0.4367 - val_loss: 2.1674 - val_acc: 0.4456\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1390 - acc: 0.4380 - val_loss: 2.1658 - val_acc: 0.4472\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1380 - acc: 0.4379 - val_loss: 2.1809 - val_acc: 0.4456\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1456 - acc: 0.4370 - val_loss: 2.1709 - val_acc: 0.4453\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1271 - acc: 0.4422 - val_loss: 2.1725 - val_acc: 0.4449\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1257 - acc: 0.4430 - val_loss: 2.1719 - val_acc: 0.4438\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1274 - acc: 0.4410 - val_loss: 2.1806 - val_acc: 0.4469\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1215 - acc: 0.4419 - val_loss: 2.1697 - val_acc: 0.4443\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1148 - acc: 0.4443 - val_loss: 2.1731 - val_acc: 0.4452\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1147 - acc: 0.4428 - val_loss: 2.1889 - val_acc: 0.4441\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1136 - acc: 0.4424 - val_loss: 2.1651 - val_acc: 0.4477\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1029 - acc: 0.4457 - val_loss: 2.1788 - val_acc: 0.4453\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1064 - acc: 0.4467 - val_loss: 2.1692 - val_acc: 0.4464\n",
      "Epoch 24/100\n",
      "1s - loss: 2.1066 - acc: 0.4449 - val_loss: 2.1681 - val_acc: 0.4476\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1046 - acc: 0.4465 - val_loss: 2.1652 - val_acc: 0.4473\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0975 - acc: 0.4491 - val_loss: 2.1670 - val_acc: 0.4484\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0919 - acc: 0.4495 - val_loss: 2.1622 - val_acc: 0.4482\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0940 - acc: 0.4476 - val_loss: 2.1650 - val_acc: 0.4472\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0906 - acc: 0.4484 - val_loss: 2.1629 - val_acc: 0.4472\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0863 - acc: 0.4492 - val_loss: 2.1830 - val_acc: 0.4470\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0979 - acc: 0.4460 - val_loss: 2.1797 - val_acc: 0.4460\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0909 - acc: 0.4490 - val_loss: 2.1765 - val_acc: 0.4469\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0796 - acc: 0.4504 - val_loss: 2.1900 - val_acc: 0.4421\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0819 - acc: 0.4497 - val_loss: 2.1750 - val_acc: 0.4472\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0738 - acc: 0.4523 - val_loss: 2.1614 - val_acc: 0.4481\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0762 - acc: 0.4527 - val_loss: 2.1753 - val_acc: 0.4475\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0792 - acc: 0.4511 - val_loss: 2.1622 - val_acc: 0.4483\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0667 - acc: 0.4524 - val_loss: 2.1647 - val_acc: 0.4493\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0709 - acc: 0.4519 - val_loss: 2.1720 - val_acc: 0.4458\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0649 - acc: 0.4537 - val_loss: 2.1569 - val_acc: 0.4479\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0658 - acc: 0.4537 - val_loss: 2.1894 - val_acc: 0.4441\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0758 - acc: 0.4525 - val_loss: 2.1923 - val_acc: 0.4407\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0743 - acc: 0.4535 - val_loss: 2.1761 - val_acc: 0.4456\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0647 - acc: 0.4546 - val_loss: 2.1601 - val_acc: 0.4507\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0606 - acc: 0.4568 - val_loss: 2.1805 - val_acc: 0.4445\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0628 - acc: 0.4540 - val_loss: 2.1601 - val_acc: 0.4492\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0559 - acc: 0.4550 - val_loss: 2.1581 - val_acc: 0.4503\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0500 - acc: 0.4582 - val_loss: 2.1541 - val_acc: 0.4504\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0467 - acc: 0.4579 - val_loss: 2.1613 - val_acc: 0.4485\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0421 - acc: 0.4609 - val_loss: 2.1597 - val_acc: 0.4489\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0445 - acc: 0.4582 - val_loss: 2.1656 - val_acc: 0.4482\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0421 - acc: 0.4580 - val_loss: 2.1802 - val_acc: 0.4446\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0557 - acc: 0.4557 - val_loss: 2.1877 - val_acc: 0.4474\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0700 - acc: 0.4523 - val_loss: 2.1808 - val_acc: 0.4444\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0504 - acc: 0.4578 - val_loss: 2.1765 - val_acc: 0.4466\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0467 - acc: 0.4589 - val_loss: 2.1681 - val_acc: 0.4470\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0424 - acc: 0.4572 - val_loss: 2.1773 - val_acc: 0.4476\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0473 - acc: 0.4584 - val_loss: 2.1961 - val_acc: 0.4429\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0453 - acc: 0.4584 - val_loss: 2.1789 - val_acc: 0.4468\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0361 - acc: 0.4616 - val_loss: 2.1640 - val_acc: 0.4475\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0287 - acc: 0.4600 - val_loss: 2.1600 - val_acc: 0.4486\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0273 - acc: 0.4631 - val_loss: 2.1632 - val_acc: 0.4475\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0317 - acc: 0.4595 - val_loss: 2.1683 - val_acc: 0.4483\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0361 - acc: 0.4621 - val_loss: 2.1605 - val_acc: 0.4515\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0231 - acc: 0.4642 - val_loss: 2.1614 - val_acc: 0.4494\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0252 - acc: 0.4613 - val_loss: 2.1609 - val_acc: 0.4480\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0243 - acc: 0.4648 - val_loss: 2.1702 - val_acc: 0.4467\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0327 - acc: 0.4620 - val_loss: 2.1575 - val_acc: 0.4508\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0188 - acc: 0.4628 - val_loss: 2.1674 - val_acc: 0.4497\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0160 - acc: 0.4666 - val_loss: 2.1583 - val_acc: 0.4494\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0404 - acc: 0.4600 - val_loss: 2.1944 - val_acc: 0.4433\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0350 - acc: 0.4641 - val_loss: 2.1614 - val_acc: 0.4485\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0187 - acc: 0.4649 - val_loss: 2.1774 - val_acc: 0.4448\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0091 - acc: 0.4674 - val_loss: 2.1564 - val_acc: 0.4511\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0126 - acc: 0.4655 - val_loss: 2.1938 - val_acc: 0.4428\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0061 - acc: 0.4688 - val_loss: 2.1584 - val_acc: 0.4487\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0085 - acc: 0.4653 - val_loss: 2.1675 - val_acc: 0.4465\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0172 - acc: 0.4645 - val_loss: 2.1556 - val_acc: 0.4514\n",
      "Epoch 79/100\n",
      "1s - loss: 2.0175 - acc: 0.4644 - val_loss: 2.1847 - val_acc: 0.4460\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0113 - acc: 0.4654 - val_loss: 2.1605 - val_acc: 0.4507\n",
      "Epoch 81/100\n",
      "1s - loss: 2.0057 - acc: 0.4665 - val_loss: 2.1542 - val_acc: 0.4512\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9946 - acc: 0.4713 - val_loss: 2.1684 - val_acc: 0.4480\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9970 - acc: 0.4679 - val_loss: 2.1743 - val_acc: 0.4498\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0105 - acc: 0.4661 - val_loss: 2.1592 - val_acc: 0.4501\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0000 - acc: 0.4683 - val_loss: 2.1664 - val_acc: 0.4497\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9966 - acc: 0.4689 - val_loss: 2.2013 - val_acc: 0.4439\n",
      "Epoch 87/100\n",
      "1s - loss: 2.0064 - acc: 0.4670 - val_loss: 2.1796 - val_acc: 0.4475\n",
      "Epoch 88/100\n",
      "1s - loss: 2.0027 - acc: 0.4682 - val_loss: 2.1828 - val_acc: 0.4461\n",
      "Epoch 89/100\n",
      "1s - loss: 2.0002 - acc: 0.4696 - val_loss: 2.1759 - val_acc: 0.4475\n",
      "Epoch 90/100\n",
      "1s - loss: 2.0067 - acc: 0.4668 - val_loss: 2.2133 - val_acc: 0.4401\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9996 - acc: 0.4696 - val_loss: 2.1607 - val_acc: 0.4494\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9858 - acc: 0.4752 - val_loss: 2.1803 - val_acc: 0.4441\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9848 - acc: 0.4747 - val_loss: 2.1546 - val_acc: 0.4526\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9794 - acc: 0.4731 - val_loss: 2.1731 - val_acc: 0.4456\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9815 - acc: 0.4742 - val_loss: 2.1545 - val_acc: 0.4516\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9829 - acc: 0.4731 - val_loss: 2.1733 - val_acc: 0.4454\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9839 - acc: 0.4710 - val_loss: 2.1650 - val_acc: 0.4474\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9851 - acc: 0.4729 - val_loss: 2.1693 - val_acc: 0.4481\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9878 - acc: 0.4708 - val_loss: 2.1708 - val_acc: 0.4487\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9888 - acc: 0.4703 - val_loss: 2.1680 - val_acc: 0.4473\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1972 - acc: 0.4275 - val_loss: 2.2076 - val_acc: 0.4376\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1766 - acc: 0.4279 - val_loss: 2.2180 - val_acc: 0.4340\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1628 - acc: 0.4315 - val_loss: 2.2241 - val_acc: 0.4311\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1527 - acc: 0.4344 - val_loss: 2.2475 - val_acc: 0.4283\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1520 - acc: 0.4338 - val_loss: 2.2281 - val_acc: 0.4296\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1388 - acc: 0.4360 - val_loss: 2.2419 - val_acc: 0.4286\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1346 - acc: 0.4378 - val_loss: 2.2292 - val_acc: 0.4293\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1275 - acc: 0.4380 - val_loss: 2.2423 - val_acc: 0.4282\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1515 - acc: 0.4322 - val_loss: 2.2590 - val_acc: 0.4243\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1283 - acc: 0.4398 - val_loss: 2.2372 - val_acc: 0.4300\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1115 - acc: 0.4414 - val_loss: 2.2347 - val_acc: 0.4291\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1067 - acc: 0.4442 - val_loss: 2.2481 - val_acc: 0.4283\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1050 - acc: 0.4429 - val_loss: 2.2371 - val_acc: 0.4296\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1068 - acc: 0.4451 - val_loss: 2.2415 - val_acc: 0.4296\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0961 - acc: 0.4448 - val_loss: 2.2415 - val_acc: 0.4318\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0941 - acc: 0.4451 - val_loss: 2.2418 - val_acc: 0.4294\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1204 - acc: 0.4391 - val_loss: 2.2623 - val_acc: 0.4255\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0959 - acc: 0.4455 - val_loss: 2.2343 - val_acc: 0.4274\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0815 - acc: 0.4482 - val_loss: 2.2377 - val_acc: 0.4293\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0814 - acc: 0.4479 - val_loss: 2.2708 - val_acc: 0.4231\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0863 - acc: 0.4474 - val_loss: 2.2315 - val_acc: 0.4306\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0689 - acc: 0.4509 - val_loss: 2.2460 - val_acc: 0.4260\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0733 - acc: 0.4492 - val_loss: 2.2337 - val_acc: 0.4283\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0673 - acc: 0.4505 - val_loss: 2.2347 - val_acc: 0.4288\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0688 - acc: 0.4499 - val_loss: 2.2256 - val_acc: 0.4328\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0609 - acc: 0.4545 - val_loss: 2.2413 - val_acc: 0.4289\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0609 - acc: 0.4538 - val_loss: 2.2214 - val_acc: 0.4303\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0553 - acc: 0.4544 - val_loss: 2.2247 - val_acc: 0.4312\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0506 - acc: 0.4544 - val_loss: 2.2155 - val_acc: 0.4328\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0512 - acc: 0.4553 - val_loss: 2.2181 - val_acc: 0.4336\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0732 - acc: 0.4506 - val_loss: 2.2515 - val_acc: 0.4259\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0540 - acc: 0.4548 - val_loss: 2.2186 - val_acc: 0.4318\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0420 - acc: 0.4556 - val_loss: 2.2211 - val_acc: 0.4323\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0400 - acc: 0.4574 - val_loss: 2.2582 - val_acc: 0.4249\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0457 - acc: 0.4565 - val_loss: 2.2209 - val_acc: 0.4319\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0373 - acc: 0.4566 - val_loss: 2.2252 - val_acc: 0.4307\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0365 - acc: 0.4571 - val_loss: 2.2232 - val_acc: 0.4312\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0341 - acc: 0.4577 - val_loss: 2.2245 - val_acc: 0.4304\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0383 - acc: 0.4576 - val_loss: 2.2240 - val_acc: 0.4328\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0357 - acc: 0.4589 - val_loss: 2.2077 - val_acc: 0.4345\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0245 - acc: 0.4623 - val_loss: 2.2074 - val_acc: 0.4330\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0233 - acc: 0.4603 - val_loss: 2.2154 - val_acc: 0.4312\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0210 - acc: 0.4606 - val_loss: 2.2190 - val_acc: 0.4327\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0221 - acc: 0.4598 - val_loss: 2.2236 - val_acc: 0.4293\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0243 - acc: 0.4608 - val_loss: 2.2134 - val_acc: 0.4332\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0205 - acc: 0.4612 - val_loss: 2.2071 - val_acc: 0.4346\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0107 - acc: 0.4632 - val_loss: 2.2107 - val_acc: 0.4325\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0166 - acc: 0.4618 - val_loss: 2.2183 - val_acc: 0.4312\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0241 - acc: 0.4616 - val_loss: 2.3283 - val_acc: 0.4167\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0487 - acc: 0.4550 - val_loss: 2.2175 - val_acc: 0.4328\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0153 - acc: 0.4635 - val_loss: 2.2163 - val_acc: 0.4338\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0107 - acc: 0.4627 - val_loss: 2.2246 - val_acc: 0.4323\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0088 - acc: 0.4655 - val_loss: 2.2160 - val_acc: 0.4319\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0077 - acc: 0.4649 - val_loss: 2.2122 - val_acc: 0.4335\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0024 - acc: 0.4651 - val_loss: 2.2096 - val_acc: 0.4341\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9969 - acc: 0.4677 - val_loss: 2.2149 - val_acc: 0.4336\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0027 - acc: 0.4652 - val_loss: 2.2108 - val_acc: 0.4345\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0030 - acc: 0.4647 - val_loss: 2.2208 - val_acc: 0.4323\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0029 - acc: 0.4655 - val_loss: 2.2275 - val_acc: 0.4327\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9988 - acc: 0.4665 - val_loss: 2.2119 - val_acc: 0.4340\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9918 - acc: 0.4677 - val_loss: 2.2150 - val_acc: 0.4357\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0146 - acc: 0.4633 - val_loss: 2.2195 - val_acc: 0.4330\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0000 - acc: 0.4666 - val_loss: 2.2138 - val_acc: 0.4340\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9866 - acc: 0.4699 - val_loss: 2.2193 - val_acc: 0.4316\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9851 - acc: 0.4699 - val_loss: 2.2201 - val_acc: 0.4328\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9955 - acc: 0.4662 - val_loss: 2.2163 - val_acc: 0.4337\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9879 - acc: 0.4696 - val_loss: 2.2217 - val_acc: 0.4339\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0146 - acc: 0.4653 - val_loss: 2.2437 - val_acc: 0.4282\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9977 - acc: 0.4684 - val_loss: 2.2123 - val_acc: 0.4345\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9827 - acc: 0.4712 - val_loss: 2.2356 - val_acc: 0.4306\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9810 - acc: 0.4695 - val_loss: 2.2106 - val_acc: 0.4336\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9752 - acc: 0.4708 - val_loss: 2.2083 - val_acc: 0.4332\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9742 - acc: 0.4743 - val_loss: 2.2142 - val_acc: 0.4318\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9792 - acc: 0.4697 - val_loss: 2.2139 - val_acc: 0.4341\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9630 - acc: 0.4739 - val_loss: 2.2040 - val_acc: 0.4332\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9694 - acc: 0.4718 - val_loss: 2.2191 - val_acc: 0.4340\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9711 - acc: 0.4715 - val_loss: 2.2084 - val_acc: 0.4357\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9679 - acc: 0.4715 - val_loss: 2.2115 - val_acc: 0.4341\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9640 - acc: 0.4733 - val_loss: 2.2146 - val_acc: 0.4339\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9679 - acc: 0.4722 - val_loss: 2.2292 - val_acc: 0.4308\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9667 - acc: 0.4723 - val_loss: 2.2279 - val_acc: 0.4334\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9751 - acc: 0.4717 - val_loss: 2.2538 - val_acc: 0.4277\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9737 - acc: 0.4711 - val_loss: 2.2222 - val_acc: 0.4341\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9635 - acc: 0.4755 - val_loss: 2.2230 - val_acc: 0.4311\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9602 - acc: 0.4733 - val_loss: 2.2177 - val_acc: 0.4346\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9722 - acc: 0.4730 - val_loss: 2.2140 - val_acc: 0.4330\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9595 - acc: 0.4757 - val_loss: 2.2247 - val_acc: 0.4337\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9668 - acc: 0.4745 - val_loss: 2.2110 - val_acc: 0.4330\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9512 - acc: 0.4785 - val_loss: 2.2401 - val_acc: 0.4335\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9940 - acc: 0.4699 - val_loss: 2.2392 - val_acc: 0.4275\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9733 - acc: 0.4739 - val_loss: 2.2194 - val_acc: 0.4372\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9499 - acc: 0.4759 - val_loss: 2.2123 - val_acc: 0.4322\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9481 - acc: 0.4777 - val_loss: 2.2200 - val_acc: 0.4328\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9462 - acc: 0.4777 - val_loss: 2.2175 - val_acc: 0.4322\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9437 - acc: 0.4788 - val_loss: 2.2169 - val_acc: 0.4322\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9394 - acc: 0.4800 - val_loss: 2.2122 - val_acc: 0.4316\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9433 - acc: 0.4793 - val_loss: 2.2241 - val_acc: 0.4319\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9433 - acc: 0.4789 - val_loss: 2.2108 - val_acc: 0.4347\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9409 - acc: 0.4802 - val_loss: 2.2163 - val_acc: 0.4349\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9442 - acc: 0.4773 - val_loss: 2.2159 - val_acc: 0.4327\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2616 - acc: 0.4150 - val_loss: 2.1952 - val_acc: 0.4434\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2369 - acc: 0.4178 - val_loss: 2.1964 - val_acc: 0.4427\n",
      "Epoch 3/100\n",
      "1s - loss: 2.2255 - acc: 0.4216 - val_loss: 2.2130 - val_acc: 0.4406\n",
      "Epoch 4/100\n",
      "1s - loss: 2.2117 - acc: 0.4220 - val_loss: 2.2285 - val_acc: 0.4390\n",
      "Epoch 5/100\n",
      "1s - loss: 2.2187 - acc: 0.4222 - val_loss: 2.2166 - val_acc: 0.4392\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1944 - acc: 0.4276 - val_loss: 2.2081 - val_acc: 0.4403\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1907 - acc: 0.4275 - val_loss: 2.2164 - val_acc: 0.4394\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1858 - acc: 0.4283 - val_loss: 2.2233 - val_acc: 0.4406\n",
      "Epoch 9/100\n",
      "1s - loss: 2.2056 - acc: 0.4262 - val_loss: 2.2384 - val_acc: 0.4371\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1858 - acc: 0.4262 - val_loss: 2.1994 - val_acc: 0.4408\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1644 - acc: 0.4328 - val_loss: 2.2131 - val_acc: 0.4401\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1588 - acc: 0.4330 - val_loss: 2.2111 - val_acc: 0.4409\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1582 - acc: 0.4346 - val_loss: 2.2019 - val_acc: 0.4411\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1522 - acc: 0.4356 - val_loss: 2.2099 - val_acc: 0.4405\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1498 - acc: 0.4365 - val_loss: 2.2008 - val_acc: 0.4398\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1498 - acc: 0.4361 - val_loss: 2.2194 - val_acc: 0.4422\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1509 - acc: 0.4365 - val_loss: 2.1966 - val_acc: 0.4423\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1417 - acc: 0.4366 - val_loss: 2.2171 - val_acc: 0.4398\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1380 - acc: 0.4386 - val_loss: 2.2159 - val_acc: 0.4386\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1453 - acc: 0.4378 - val_loss: 2.2549 - val_acc: 0.4393\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1981 - acc: 0.4289 - val_loss: 2.2634 - val_acc: 0.4276\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1983 - acc: 0.4265 - val_loss: 2.2753 - val_acc: 0.4289\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1744 - acc: 0.4322 - val_loss: 2.2263 - val_acc: 0.4401\n",
      "Epoch 24/100\n",
      "1s - loss: 2.1434 - acc: 0.4387 - val_loss: 2.2154 - val_acc: 0.4409\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1262 - acc: 0.4409 - val_loss: 2.1977 - val_acc: 0.4438\n",
      "Epoch 26/100\n",
      "1s - loss: 2.1167 - acc: 0.4430 - val_loss: 2.2096 - val_acc: 0.4425\n",
      "Epoch 27/100\n",
      "1s - loss: 2.1149 - acc: 0.4437 - val_loss: 2.1999 - val_acc: 0.4438\n",
      "Epoch 28/100\n",
      "1s - loss: 2.1107 - acc: 0.4438 - val_loss: 2.1953 - val_acc: 0.4429\n",
      "Epoch 29/100\n",
      "1s - loss: 2.1071 - acc: 0.4450 - val_loss: 2.1892 - val_acc: 0.4440\n",
      "Epoch 30/100\n",
      "1s - loss: 2.1133 - acc: 0.4444 - val_loss: 2.2002 - val_acc: 0.4401\n",
      "Epoch 31/100\n",
      "1s - loss: 2.1016 - acc: 0.4473 - val_loss: 2.1906 - val_acc: 0.4433\n",
      "Epoch 32/100\n",
      "1s - loss: 2.1053 - acc: 0.4464 - val_loss: 2.2239 - val_acc: 0.4383\n",
      "Epoch 33/100\n",
      "1s - loss: 2.1123 - acc: 0.4449 - val_loss: 2.1979 - val_acc: 0.4431\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0993 - acc: 0.4459 - val_loss: 2.2132 - val_acc: 0.4388\n",
      "Epoch 35/100\n",
      "1s - loss: 2.1059 - acc: 0.4477 - val_loss: 2.2011 - val_acc: 0.4442\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0964 - acc: 0.4477 - val_loss: 2.2209 - val_acc: 0.4420\n",
      "Epoch 37/100\n",
      "1s - loss: 2.1123 - acc: 0.4458 - val_loss: 2.1955 - val_acc: 0.4440\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0978 - acc: 0.4491 - val_loss: 2.2168 - val_acc: 0.4406\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0918 - acc: 0.4512 - val_loss: 2.1971 - val_acc: 0.4459\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0964 - acc: 0.4469 - val_loss: 2.2214 - val_acc: 0.4400\n",
      "Epoch 41/100\n",
      "1s - loss: 2.1014 - acc: 0.4471 - val_loss: 2.1975 - val_acc: 0.4427\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0869 - acc: 0.4512 - val_loss: 2.2069 - val_acc: 0.4420\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0841 - acc: 0.4510 - val_loss: 2.1891 - val_acc: 0.4429\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0787 - acc: 0.4532 - val_loss: 2.2016 - val_acc: 0.4436\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0723 - acc: 0.4530 - val_loss: 2.1923 - val_acc: 0.4448\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0717 - acc: 0.4539 - val_loss: 2.2047 - val_acc: 0.4422\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0808 - acc: 0.4502 - val_loss: 2.2091 - val_acc: 0.4412\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0843 - acc: 0.4505 - val_loss: 2.2199 - val_acc: 0.4389\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0815 - acc: 0.4525 - val_loss: 2.1948 - val_acc: 0.4461\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0693 - acc: 0.4572 - val_loss: 2.2098 - val_acc: 0.4420\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0671 - acc: 0.4560 - val_loss: 2.1899 - val_acc: 0.4441\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0633 - acc: 0.4571 - val_loss: 2.2237 - val_acc: 0.4391\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0783 - acc: 0.4506 - val_loss: 2.1953 - val_acc: 0.4436\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0616 - acc: 0.4568 - val_loss: 2.2031 - val_acc: 0.4453\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0665 - acc: 0.4556 - val_loss: 2.1996 - val_acc: 0.4437\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0648 - acc: 0.4545 - val_loss: 2.2153 - val_acc: 0.4410\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0597 - acc: 0.4576 - val_loss: 2.1895 - val_acc: 0.4459\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0629 - acc: 0.4559 - val_loss: 2.2239 - val_acc: 0.4369\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0811 - acc: 0.4519 - val_loss: 2.1860 - val_acc: 0.4466\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0607 - acc: 0.4558 - val_loss: 2.2012 - val_acc: 0.4416\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0486 - acc: 0.4590 - val_loss: 2.1803 - val_acc: 0.4474\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0387 - acc: 0.4626 - val_loss: 2.1948 - val_acc: 0.4450\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0447 - acc: 0.4606 - val_loss: 2.1952 - val_acc: 0.4458\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0508 - acc: 0.4602 - val_loss: 2.2086 - val_acc: 0.4427\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0439 - acc: 0.4596 - val_loss: 2.1952 - val_acc: 0.4453\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0371 - acc: 0.4629 - val_loss: 2.2106 - val_acc: 0.4428\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0379 - acc: 0.4609 - val_loss: 2.1948 - val_acc: 0.4458\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0427 - acc: 0.4612 - val_loss: 2.1936 - val_acc: 0.4447\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0300 - acc: 0.4635 - val_loss: 2.1946 - val_acc: 0.4450\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0483 - acc: 0.4595 - val_loss: 2.2212 - val_acc: 0.4396\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0514 - acc: 0.4593 - val_loss: 2.2111 - val_acc: 0.4422\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0540 - acc: 0.4571 - val_loss: 2.2163 - val_acc: 0.4399\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0404 - acc: 0.4594 - val_loss: 2.1915 - val_acc: 0.4432\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0418 - acc: 0.4609 - val_loss: 2.2171 - val_acc: 0.4410\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0375 - acc: 0.4622 - val_loss: 2.2018 - val_acc: 0.4452\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0327 - acc: 0.4632 - val_loss: 2.2127 - val_acc: 0.4401\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0256 - acc: 0.4646 - val_loss: 2.1931 - val_acc: 0.4468\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0270 - acc: 0.4658 - val_loss: 2.2144 - val_acc: 0.4400\n",
      "Epoch 79/100\n",
      "1s - loss: 2.0328 - acc: 0.4625 - val_loss: 2.2008 - val_acc: 0.4431\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0305 - acc: 0.4660 - val_loss: 2.2169 - val_acc: 0.4433\n",
      "Epoch 81/100\n",
      "1s - loss: 2.0322 - acc: 0.4624 - val_loss: 2.2024 - val_acc: 0.4445\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0283 - acc: 0.4633 - val_loss: 2.2191 - val_acc: 0.4433\n",
      "Epoch 83/100\n",
      "1s - loss: 2.0358 - acc: 0.4614 - val_loss: 2.2032 - val_acc: 0.4431\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0303 - acc: 0.4642 - val_loss: 2.2173 - val_acc: 0.4418\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0250 - acc: 0.4649 - val_loss: 2.2239 - val_acc: 0.4421\n",
      "Epoch 86/100\n",
      "1s - loss: 2.0292 - acc: 0.4627 - val_loss: 2.2052 - val_acc: 0.4434\n",
      "Epoch 87/100\n",
      "1s - loss: 2.0088 - acc: 0.4682 - val_loss: 2.1972 - val_acc: 0.4462\n",
      "Epoch 88/100\n",
      "1s - loss: 2.0132 - acc: 0.4668 - val_loss: 2.2034 - val_acc: 0.4420\n",
      "Epoch 89/100\n",
      "1s - loss: 2.0303 - acc: 0.4623 - val_loss: 2.2040 - val_acc: 0.4438\n",
      "Epoch 90/100\n",
      "1s - loss: 2.0185 - acc: 0.4665 - val_loss: 2.2025 - val_acc: 0.4431\n",
      "Epoch 91/100\n",
      "1s - loss: 2.0039 - acc: 0.4703 - val_loss: 2.1904 - val_acc: 0.4450\n",
      "Epoch 92/100\n",
      "1s - loss: 2.0096 - acc: 0.4687 - val_loss: 2.2312 - val_acc: 0.4411\n",
      "Epoch 93/100\n",
      "1s - loss: 2.0507 - acc: 0.4602 - val_loss: 2.2156 - val_acc: 0.4391\n",
      "Epoch 94/100\n",
      "1s - loss: 2.0233 - acc: 0.4658 - val_loss: 2.2035 - val_acc: 0.4435\n",
      "Epoch 95/100\n",
      "1s - loss: 2.0078 - acc: 0.4690 - val_loss: 2.2037 - val_acc: 0.4448\n",
      "Epoch 96/100\n",
      "1s - loss: 2.0063 - acc: 0.4679 - val_loss: 2.2117 - val_acc: 0.4434\n",
      "Epoch 97/100\n",
      "1s - loss: 2.0163 - acc: 0.4669 - val_loss: 2.1907 - val_acc: 0.4471\n",
      "Epoch 98/100\n",
      "1s - loss: 2.0036 - acc: 0.4695 - val_loss: 2.2294 - val_acc: 0.4390\n",
      "Epoch 99/100\n",
      "1s - loss: 2.0127 - acc: 0.4679 - val_loss: 2.2077 - val_acc: 0.4447\n",
      "Epoch 100/100\n",
      "1s - loss: 2.0127 - acc: 0.4670 - val_loss: 2.2271 - val_acc: 0.4381\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1841 - acc: 0.4292 - val_loss: 2.2449 - val_acc: 0.4306\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1501 - acc: 0.4364 - val_loss: 2.2450 - val_acc: 0.4328\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1416 - acc: 0.4367 - val_loss: 2.2491 - val_acc: 0.4311\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1283 - acc: 0.4399 - val_loss: 2.2623 - val_acc: 0.4292\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1203 - acc: 0.4414 - val_loss: 2.2809 - val_acc: 0.4295\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1217 - acc: 0.4441 - val_loss: 2.2725 - val_acc: 0.4266\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1078 - acc: 0.4464 - val_loss: 2.2710 - val_acc: 0.4283\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1030 - acc: 0.4467 - val_loss: 2.2740 - val_acc: 0.4272\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0967 - acc: 0.4472 - val_loss: 2.2946 - val_acc: 0.4216\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0927 - acc: 0.4485 - val_loss: 2.2834 - val_acc: 0.4258\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0828 - acc: 0.4495 - val_loss: 2.2788 - val_acc: 0.4265\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0808 - acc: 0.4495 - val_loss: 2.3298 - val_acc: 0.4199\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0823 - acc: 0.4515 - val_loss: 2.2856 - val_acc: 0.4248\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0699 - acc: 0.4537 - val_loss: 2.2824 - val_acc: 0.4249\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0704 - acc: 0.4516 - val_loss: 2.2862 - val_acc: 0.4250\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0656 - acc: 0.4535 - val_loss: 2.2858 - val_acc: 0.4250\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0636 - acc: 0.4537 - val_loss: 2.2825 - val_acc: 0.4241\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0609 - acc: 0.4553 - val_loss: 2.2900 - val_acc: 0.4251\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0628 - acc: 0.4559 - val_loss: 2.2798 - val_acc: 0.4251\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0533 - acc: 0.4560 - val_loss: 2.2851 - val_acc: 0.4243\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0461 - acc: 0.4582 - val_loss: 2.2919 - val_acc: 0.4240\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0525 - acc: 0.4566 - val_loss: 2.2796 - val_acc: 0.4280\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0424 - acc: 0.4602 - val_loss: 2.2753 - val_acc: 0.4257\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0405 - acc: 0.4602 - val_loss: 2.2818 - val_acc: 0.4245\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0406 - acc: 0.4596 - val_loss: 2.2721 - val_acc: 0.4268\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0341 - acc: 0.4594 - val_loss: 2.3072 - val_acc: 0.4218\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0386 - acc: 0.4612 - val_loss: 2.2828 - val_acc: 0.4253\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0418 - acc: 0.4576 - val_loss: 2.2884 - val_acc: 0.4242\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0299 - acc: 0.4608 - val_loss: 2.2952 - val_acc: 0.4228\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0373 - acc: 0.4607 - val_loss: 2.2804 - val_acc: 0.4259\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0230 - acc: 0.4637 - val_loss: 2.2637 - val_acc: 0.4279\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0160 - acc: 0.4654 - val_loss: 2.2674 - val_acc: 0.4260\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0167 - acc: 0.4665 - val_loss: 2.3204 - val_acc: 0.4216\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0214 - acc: 0.4642 - val_loss: 2.3197 - val_acc: 0.4226\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0442 - acc: 0.4611 - val_loss: 2.2955 - val_acc: 0.4227\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0206 - acc: 0.4653 - val_loss: 2.2644 - val_acc: 0.4273\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0180 - acc: 0.4640 - val_loss: 2.2655 - val_acc: 0.4288\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0170 - acc: 0.4645 - val_loss: 2.2753 - val_acc: 0.4241\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0100 - acc: 0.4666 - val_loss: 2.2900 - val_acc: 0.4223\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0041 - acc: 0.4668 - val_loss: 2.2595 - val_acc: 0.4276\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0057 - acc: 0.4660 - val_loss: 2.2713 - val_acc: 0.4264\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9983 - acc: 0.4697 - val_loss: 2.2582 - val_acc: 0.4286\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9945 - acc: 0.4709 - val_loss: 2.2635 - val_acc: 0.4270\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9989 - acc: 0.4700 - val_loss: 2.2762 - val_acc: 0.4243\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0185 - acc: 0.4635 - val_loss: 2.2730 - val_acc: 0.4263\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9929 - acc: 0.4718 - val_loss: 2.2639 - val_acc: 0.4264\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9912 - acc: 0.4720 - val_loss: 2.3081 - val_acc: 0.4220\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9961 - acc: 0.4711 - val_loss: 2.2676 - val_acc: 0.4253\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9908 - acc: 0.4706 - val_loss: 2.2816 - val_acc: 0.4268\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9894 - acc: 0.4721 - val_loss: 2.2970 - val_acc: 0.4258\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0074 - acc: 0.4666 - val_loss: 2.2643 - val_acc: 0.4284\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9979 - acc: 0.4690 - val_loss: 2.2666 - val_acc: 0.4262\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9800 - acc: 0.4727 - val_loss: 2.2607 - val_acc: 0.4287\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9836 - acc: 0.4741 - val_loss: 2.2564 - val_acc: 0.4296\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9783 - acc: 0.4744 - val_loss: 2.2576 - val_acc: 0.4303\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9777 - acc: 0.4737 - val_loss: 2.2821 - val_acc: 0.4299\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9824 - acc: 0.4727 - val_loss: 2.2557 - val_acc: 0.4290\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9755 - acc: 0.4743 - val_loss: 2.2542 - val_acc: 0.4280\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9741 - acc: 0.4748 - val_loss: 2.2564 - val_acc: 0.4309\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9832 - acc: 0.4702 - val_loss: 2.2841 - val_acc: 0.4252\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9769 - acc: 0.4738 - val_loss: 2.2623 - val_acc: 0.4325\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9719 - acc: 0.4728 - val_loss: 2.2655 - val_acc: 0.4299\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9683 - acc: 0.4752 - val_loss: 2.2562 - val_acc: 0.4309\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9627 - acc: 0.4770 - val_loss: 2.2757 - val_acc: 0.4296\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9721 - acc: 0.4760 - val_loss: 2.2739 - val_acc: 0.4294\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9689 - acc: 0.4768 - val_loss: 2.2571 - val_acc: 0.4322\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9677 - acc: 0.4769 - val_loss: 2.2691 - val_acc: 0.4287\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9706 - acc: 0.4761 - val_loss: 2.2750 - val_acc: 0.4259\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9667 - acc: 0.4778 - val_loss: 2.2555 - val_acc: 0.4328\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9706 - acc: 0.4761 - val_loss: 2.2580 - val_acc: 0.4306\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9583 - acc: 0.4789 - val_loss: 2.2819 - val_acc: 0.4281\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9498 - acc: 0.4789 - val_loss: 2.2583 - val_acc: 0.4295\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9609 - acc: 0.4786 - val_loss: 2.2773 - val_acc: 0.4267\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9624 - acc: 0.4773 - val_loss: 2.2709 - val_acc: 0.4279\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9557 - acc: 0.4779 - val_loss: 2.3042 - val_acc: 0.4257\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9612 - acc: 0.4764 - val_loss: 2.2610 - val_acc: 0.4298\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9479 - acc: 0.4797 - val_loss: 2.2658 - val_acc: 0.4290\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9472 - acc: 0.4804 - val_loss: 2.2682 - val_acc: 0.4292\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9544 - acc: 0.4784 - val_loss: 2.2953 - val_acc: 0.4268\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9465 - acc: 0.4809 - val_loss: 2.2565 - val_acc: 0.4323\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9480 - acc: 0.4811 - val_loss: 2.2757 - val_acc: 0.4299\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9466 - acc: 0.4817 - val_loss: 2.2458 - val_acc: 0.4332\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9333 - acc: 0.4842 - val_loss: 2.2786 - val_acc: 0.4278\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9503 - acc: 0.4819 - val_loss: 2.2590 - val_acc: 0.4309\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9455 - acc: 0.4814 - val_loss: 2.2743 - val_acc: 0.4273\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9438 - acc: 0.4811 - val_loss: 2.2713 - val_acc: 0.4300\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9547 - acc: 0.4777 - val_loss: 2.2913 - val_acc: 0.4278\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9391 - acc: 0.4832 - val_loss: 2.2499 - val_acc: 0.4354\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9241 - acc: 0.4860 - val_loss: 2.2572 - val_acc: 0.4321\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9230 - acc: 0.4862 - val_loss: 2.2500 - val_acc: 0.4342\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9236 - acc: 0.4853 - val_loss: 2.2781 - val_acc: 0.4317\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9339 - acc: 0.4851 - val_loss: 2.2526 - val_acc: 0.4340\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9215 - acc: 0.4847 - val_loss: 2.2724 - val_acc: 0.4297\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9290 - acc: 0.4849 - val_loss: 2.2788 - val_acc: 0.4287\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9344 - acc: 0.4840 - val_loss: 2.2705 - val_acc: 0.4295\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9272 - acc: 0.4853 - val_loss: 2.2592 - val_acc: 0.4328\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9272 - acc: 0.4880 - val_loss: 2.2737 - val_acc: 0.4269\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9171 - acc: 0.4879 - val_loss: 2.2600 - val_acc: 0.4332\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9183 - acc: 0.4878 - val_loss: 2.2601 - val_acc: 0.4275\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9187 - acc: 0.4877 - val_loss: 2.2541 - val_acc: 0.4330\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2211 - acc: 0.4225 - val_loss: 2.1655 - val_acc: 0.4438\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2000 - acc: 0.4277 - val_loss: 2.1618 - val_acc: 0.4433\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1876 - acc: 0.4304 - val_loss: 2.1767 - val_acc: 0.4426\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1780 - acc: 0.4319 - val_loss: 2.1678 - val_acc: 0.4436\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1668 - acc: 0.4330 - val_loss: 2.1817 - val_acc: 0.4395\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1653 - acc: 0.4324 - val_loss: 2.2125 - val_acc: 0.4366\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1665 - acc: 0.4336 - val_loss: 2.1944 - val_acc: 0.4388\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1621 - acc: 0.4343 - val_loss: 2.1971 - val_acc: 0.4379\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1495 - acc: 0.4370 - val_loss: 2.1775 - val_acc: 0.4407\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1435 - acc: 0.4384 - val_loss: 2.1785 - val_acc: 0.4400\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1355 - acc: 0.4390 - val_loss: 2.1965 - val_acc: 0.4369\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1387 - acc: 0.4387 - val_loss: 2.1880 - val_acc: 0.4384\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1346 - acc: 0.4409 - val_loss: 2.2028 - val_acc: 0.4361\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1257 - acc: 0.4406 - val_loss: 2.1973 - val_acc: 0.4385\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1197 - acc: 0.4416 - val_loss: 2.1756 - val_acc: 0.4424\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1155 - acc: 0.4425 - val_loss: 2.1859 - val_acc: 0.4396\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1097 - acc: 0.4444 - val_loss: 2.1781 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1054 - acc: 0.4457 - val_loss: 2.2179 - val_acc: 0.4358\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1399 - acc: 0.4414 - val_loss: 2.2236 - val_acc: 0.4323\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1278 - acc: 0.4405 - val_loss: 2.1987 - val_acc: 0.4391\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1014 - acc: 0.4485 - val_loss: 2.1897 - val_acc: 0.4379\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0949 - acc: 0.4482 - val_loss: 2.1751 - val_acc: 0.4418\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0944 - acc: 0.4475 - val_loss: 2.1886 - val_acc: 0.4413\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0876 - acc: 0.4485 - val_loss: 2.1802 - val_acc: 0.4405\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0862 - acc: 0.4479 - val_loss: 2.1915 - val_acc: 0.4398\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0819 - acc: 0.4508 - val_loss: 2.1765 - val_acc: 0.4448\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0821 - acc: 0.4505 - val_loss: 2.1741 - val_acc: 0.4440\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0764 - acc: 0.4521 - val_loss: 2.1782 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0817 - acc: 0.4507 - val_loss: 2.1816 - val_acc: 0.4409\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0898 - acc: 0.4494 - val_loss: 2.1799 - val_acc: 0.4402\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0750 - acc: 0.4540 - val_loss: 2.1825 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0710 - acc: 0.4519 - val_loss: 2.2010 - val_acc: 0.4400\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0778 - acc: 0.4538 - val_loss: 2.1736 - val_acc: 0.4427\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0662 - acc: 0.4555 - val_loss: 2.2146 - val_acc: 0.4378\n",
      "Epoch 35/100\n",
      "1s - loss: 2.1017 - acc: 0.4481 - val_loss: 2.1862 - val_acc: 0.4395\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0802 - acc: 0.4523 - val_loss: 2.1949 - val_acc: 0.4391\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0587 - acc: 0.4574 - val_loss: 2.1772 - val_acc: 0.4444\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0609 - acc: 0.4556 - val_loss: 2.1748 - val_acc: 0.4419\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0509 - acc: 0.4584 - val_loss: 2.1795 - val_acc: 0.4413\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0585 - acc: 0.4569 - val_loss: 2.1774 - val_acc: 0.4427\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0499 - acc: 0.4582 - val_loss: 2.1668 - val_acc: 0.4452\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0482 - acc: 0.4584 - val_loss: 2.1880 - val_acc: 0.4423\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0533 - acc: 0.4569 - val_loss: 2.1724 - val_acc: 0.4451\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0505 - acc: 0.4590 - val_loss: 2.1833 - val_acc: 0.4425\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0480 - acc: 0.4583 - val_loss: 2.1918 - val_acc: 0.4389\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0562 - acc: 0.4560 - val_loss: 2.1807 - val_acc: 0.4418\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0445 - acc: 0.4601 - val_loss: 2.1821 - val_acc: 0.4429\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0655 - acc: 0.4552 - val_loss: 2.2120 - val_acc: 0.4373\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0528 - acc: 0.4589 - val_loss: 2.1746 - val_acc: 0.4449\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0612 - acc: 0.4555 - val_loss: 2.1935 - val_acc: 0.4424\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0306 - acc: 0.4628 - val_loss: 2.1846 - val_acc: 0.4395\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0501 - acc: 0.4573 - val_loss: 2.1906 - val_acc: 0.4429\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0428 - acc: 0.4597 - val_loss: 2.1861 - val_acc: 0.4448\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0290 - acc: 0.4637 - val_loss: 2.1687 - val_acc: 0.4442\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0282 - acc: 0.4625 - val_loss: 2.1695 - val_acc: 0.4438\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0228 - acc: 0.4647 - val_loss: 2.1668 - val_acc: 0.4455\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0172 - acc: 0.4646 - val_loss: 2.1719 - val_acc: 0.4456\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0150 - acc: 0.4651 - val_loss: 2.1732 - val_acc: 0.4421\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0186 - acc: 0.4644 - val_loss: 2.1903 - val_acc: 0.4430\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0314 - acc: 0.4631 - val_loss: 2.1700 - val_acc: 0.4436\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0231 - acc: 0.4643 - val_loss: 2.1824 - val_acc: 0.4437\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0185 - acc: 0.4658 - val_loss: 2.1836 - val_acc: 0.4420\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0269 - acc: 0.4629 - val_loss: 2.2163 - val_acc: 0.4382\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0692 - acc: 0.4532 - val_loss: 2.1921 - val_acc: 0.4392\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0389 - acc: 0.4601 - val_loss: 2.2160 - val_acc: 0.4385\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0218 - acc: 0.4630 - val_loss: 2.1686 - val_acc: 0.4450\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0063 - acc: 0.4676 - val_loss: 2.1733 - val_acc: 0.4433\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0195 - acc: 0.4647 - val_loss: 2.1713 - val_acc: 0.4450\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0163 - acc: 0.4665 - val_loss: 2.1780 - val_acc: 0.4421\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0052 - acc: 0.4683 - val_loss: 2.1648 - val_acc: 0.4461\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0102 - acc: 0.4674 - val_loss: 2.1888 - val_acc: 0.4408\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0089 - acc: 0.4684 - val_loss: 2.1650 - val_acc: 0.4462\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0074 - acc: 0.4684 - val_loss: 2.1875 - val_acc: 0.4429\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0140 - acc: 0.4667 - val_loss: 2.1777 - val_acc: 0.4416\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0045 - acc: 0.4699 - val_loss: 2.1726 - val_acc: 0.4450\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9923 - acc: 0.4728 - val_loss: 2.1702 - val_acc: 0.4434\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9931 - acc: 0.4702 - val_loss: 2.1793 - val_acc: 0.4432\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9895 - acc: 0.4738 - val_loss: 2.1786 - val_acc: 0.4430\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9879 - acc: 0.4735 - val_loss: 2.1889 - val_acc: 0.4452\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0150 - acc: 0.4678 - val_loss: 2.2006 - val_acc: 0.4360\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9980 - acc: 0.4714 - val_loss: 2.1822 - val_acc: 0.4434\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0139 - acc: 0.4667 - val_loss: 2.1861 - val_acc: 0.4408\n",
      "Epoch 83/100\n",
      "1s - loss: 2.0098 - acc: 0.4692 - val_loss: 2.2152 - val_acc: 0.4401\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0087 - acc: 0.4665 - val_loss: 2.1937 - val_acc: 0.4404\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0040 - acc: 0.4680 - val_loss: 2.1971 - val_acc: 0.4434\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9872 - acc: 0.4738 - val_loss: 2.1721 - val_acc: 0.4442\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9762 - acc: 0.4755 - val_loss: 2.1859 - val_acc: 0.4408\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9854 - acc: 0.4736 - val_loss: 2.1752 - val_acc: 0.4438\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9996 - acc: 0.4683 - val_loss: 2.1982 - val_acc: 0.4413\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9879 - acc: 0.4710 - val_loss: 2.1818 - val_acc: 0.4449\n",
      "Epoch 91/100\n",
      "1s - loss: 2.0031 - acc: 0.4705 - val_loss: 2.1962 - val_acc: 0.4430\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9877 - acc: 0.4735 - val_loss: 2.1708 - val_acc: 0.4450\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9822 - acc: 0.4733 - val_loss: 2.1851 - val_acc: 0.4398\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9905 - acc: 0.4709 - val_loss: 2.1660 - val_acc: 0.4466\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9753 - acc: 0.4734 - val_loss: 2.1884 - val_acc: 0.4432\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9696 - acc: 0.4763 - val_loss: 2.1703 - val_acc: 0.4460\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9670 - acc: 0.4771 - val_loss: 2.1854 - val_acc: 0.4428\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9717 - acc: 0.4766 - val_loss: 2.1742 - val_acc: 0.4438\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9814 - acc: 0.4732 - val_loss: 2.2012 - val_acc: 0.4405\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9779 - acc: 0.4764 - val_loss: 2.1710 - val_acc: 0.4438\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1807 - acc: 0.4271 - val_loss: 2.1730 - val_acc: 0.4451\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1619 - acc: 0.4323 - val_loss: 2.1818 - val_acc: 0.4450\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1488 - acc: 0.4342 - val_loss: 2.1774 - val_acc: 0.4445\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1416 - acc: 0.4353 - val_loss: 2.1890 - val_acc: 0.4431\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1307 - acc: 0.4374 - val_loss: 2.2022 - val_acc: 0.4428\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1258 - acc: 0.4387 - val_loss: 2.1851 - val_acc: 0.4432\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1216 - acc: 0.4384 - val_loss: 2.1938 - val_acc: 0.4398\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1184 - acc: 0.4403 - val_loss: 2.2076 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1153 - acc: 0.4420 - val_loss: 2.1935 - val_acc: 0.4410\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1026 - acc: 0.4441 - val_loss: 2.1858 - val_acc: 0.4445\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1042 - acc: 0.4451 - val_loss: 2.2072 - val_acc: 0.4395\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1029 - acc: 0.4459 - val_loss: 2.1919 - val_acc: 0.4425\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0893 - acc: 0.4469 - val_loss: 2.1849 - val_acc: 0.4424\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0900 - acc: 0.4479 - val_loss: 2.2187 - val_acc: 0.4401\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0870 - acc: 0.4476 - val_loss: 2.2001 - val_acc: 0.4429\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0832 - acc: 0.4477 - val_loss: 2.1933 - val_acc: 0.4419\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0762 - acc: 0.4498 - val_loss: 2.2094 - val_acc: 0.4413\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0969 - acc: 0.4479 - val_loss: 2.2017 - val_acc: 0.4421\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0762 - acc: 0.4493 - val_loss: 2.1990 - val_acc: 0.4427\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0700 - acc: 0.4510 - val_loss: 2.1844 - val_acc: 0.4465\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0647 - acc: 0.4515 - val_loss: 2.1925 - val_acc: 0.4428\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0613 - acc: 0.4533 - val_loss: 2.1809 - val_acc: 0.4453\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0604 - acc: 0.4544 - val_loss: 2.1908 - val_acc: 0.4429\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0584 - acc: 0.4536 - val_loss: 2.1842 - val_acc: 0.4454\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0578 - acc: 0.4531 - val_loss: 2.2157 - val_acc: 0.4395\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0580 - acc: 0.4536 - val_loss: 2.1984 - val_acc: 0.4420\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0488 - acc: 0.4556 - val_loss: 2.1896 - val_acc: 0.4433\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0470 - acc: 0.4567 - val_loss: 2.1932 - val_acc: 0.4451\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0410 - acc: 0.4580 - val_loss: 2.1885 - val_acc: 0.4451\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0388 - acc: 0.4576 - val_loss: 2.2011 - val_acc: 0.4444\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0620 - acc: 0.4525 - val_loss: 2.1947 - val_acc: 0.4426\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0407 - acc: 0.4568 - val_loss: 2.1880 - val_acc: 0.4456\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0305 - acc: 0.4605 - val_loss: 2.1809 - val_acc: 0.4468\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0297 - acc: 0.4609 - val_loss: 2.1818 - val_acc: 0.4458\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0259 - acc: 0.4598 - val_loss: 2.1848 - val_acc: 0.4460\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0274 - acc: 0.4606 - val_loss: 2.1980 - val_acc: 0.4416\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0238 - acc: 0.4604 - val_loss: 2.1762 - val_acc: 0.4457\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0202 - acc: 0.4620 - val_loss: 2.1776 - val_acc: 0.4473\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0198 - acc: 0.4630 - val_loss: 2.1859 - val_acc: 0.4463\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0191 - acc: 0.4636 - val_loss: 2.1881 - val_acc: 0.4446\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0227 - acc: 0.4614 - val_loss: 2.1795 - val_acc: 0.4454\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0185 - acc: 0.4636 - val_loss: 2.1827 - val_acc: 0.4456\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0147 - acc: 0.4607 - val_loss: 2.1889 - val_acc: 0.4440\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0074 - acc: 0.4645 - val_loss: 2.1840 - val_acc: 0.4452\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0080 - acc: 0.4642 - val_loss: 2.2311 - val_acc: 0.4369\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0220 - acc: 0.4615 - val_loss: 2.1831 - val_acc: 0.4441\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0025 - acc: 0.4668 - val_loss: 2.1972 - val_acc: 0.4442\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0151 - acc: 0.4642 - val_loss: 2.2012 - val_acc: 0.4413\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0115 - acc: 0.4638 - val_loss: 2.1914 - val_acc: 0.4435\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0065 - acc: 0.4639 - val_loss: 2.1900 - val_acc: 0.4452\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0015 - acc: 0.4664 - val_loss: 2.1790 - val_acc: 0.4458\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9986 - acc: 0.4662 - val_loss: 2.1866 - val_acc: 0.4442\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0058 - acc: 0.4637 - val_loss: 2.1971 - val_acc: 0.4430\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0035 - acc: 0.4644 - val_loss: 2.1806 - val_acc: 0.4455\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9951 - acc: 0.4684 - val_loss: 2.1747 - val_acc: 0.4453\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9890 - acc: 0.4698 - val_loss: 2.1747 - val_acc: 0.4464\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9870 - acc: 0.4689 - val_loss: 2.1844 - val_acc: 0.4447\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9837 - acc: 0.4697 - val_loss: 2.1820 - val_acc: 0.4446\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9889 - acc: 0.4698 - val_loss: 2.1794 - val_acc: 0.4456\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9863 - acc: 0.4697 - val_loss: 2.1845 - val_acc: 0.4425\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9824 - acc: 0.4711 - val_loss: 2.1762 - val_acc: 0.4436\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9799 - acc: 0.4710 - val_loss: 2.1800 - val_acc: 0.4464\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9786 - acc: 0.4713 - val_loss: 2.2149 - val_acc: 0.4421\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0326 - acc: 0.4618 - val_loss: 2.2177 - val_acc: 0.4390\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0184 - acc: 0.4640 - val_loss: 2.1936 - val_acc: 0.4434\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9876 - acc: 0.4686 - val_loss: 2.2174 - val_acc: 0.4409\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0060 - acc: 0.4665 - val_loss: 2.1901 - val_acc: 0.4403\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9797 - acc: 0.4720 - val_loss: 2.1834 - val_acc: 0.4462\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9762 - acc: 0.4731 - val_loss: 2.1741 - val_acc: 0.4454\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9683 - acc: 0.4750 - val_loss: 2.1764 - val_acc: 0.4465\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9684 - acc: 0.4732 - val_loss: 2.1840 - val_acc: 0.4450\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9741 - acc: 0.4730 - val_loss: 2.1760 - val_acc: 0.4456\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9608 - acc: 0.4755 - val_loss: 2.1787 - val_acc: 0.4441\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9662 - acc: 0.4748 - val_loss: 2.2130 - val_acc: 0.4409\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9861 - acc: 0.4696 - val_loss: 2.1858 - val_acc: 0.4418\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9659 - acc: 0.4755 - val_loss: 2.1733 - val_acc: 0.4441\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9544 - acc: 0.4776 - val_loss: 2.1868 - val_acc: 0.4421\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9564 - acc: 0.4772 - val_loss: 2.1891 - val_acc: 0.4414\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9608 - acc: 0.4756 - val_loss: 2.1776 - val_acc: 0.4435\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9554 - acc: 0.4765 - val_loss: 2.1802 - val_acc: 0.4451\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9584 - acc: 0.4759 - val_loss: 2.1861 - val_acc: 0.4434\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9614 - acc: 0.4755 - val_loss: 2.2038 - val_acc: 0.4432\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9684 - acc: 0.4749 - val_loss: 2.1918 - val_acc: 0.4415\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9516 - acc: 0.4789 - val_loss: 2.1983 - val_acc: 0.4432\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9557 - acc: 0.4763 - val_loss: 2.1935 - val_acc: 0.4432\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9537 - acc: 0.4774 - val_loss: 2.1922 - val_acc: 0.4441\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9519 - acc: 0.4781 - val_loss: 2.1902 - val_acc: 0.4427\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9550 - acc: 0.4766 - val_loss: 2.2037 - val_acc: 0.4409\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9555 - acc: 0.4766 - val_loss: 2.1887 - val_acc: 0.4455\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9479 - acc: 0.4794 - val_loss: 2.1869 - val_acc: 0.4452\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9430 - acc: 0.4809 - val_loss: 2.1898 - val_acc: 0.4411\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9435 - acc: 0.4793 - val_loss: 2.1956 - val_acc: 0.4444\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9434 - acc: 0.4798 - val_loss: 2.1881 - val_acc: 0.4440\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9466 - acc: 0.4806 - val_loss: 2.2060 - val_acc: 0.4416\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9448 - acc: 0.4783 - val_loss: 2.1856 - val_acc: 0.4454\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9358 - acc: 0.4805 - val_loss: 2.2054 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9345 - acc: 0.4817 - val_loss: 2.1900 - val_acc: 0.4447\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9430 - acc: 0.4800 - val_loss: 2.2115 - val_acc: 0.4419\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9402 - acc: 0.4797 - val_loss: 2.1856 - val_acc: 0.4435\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9330 - acc: 0.4818 - val_loss: 2.2223 - val_acc: 0.4406\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2126 - acc: 0.4235 - val_loss: 2.1852 - val_acc: 0.4439\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1850 - acc: 0.4281 - val_loss: 2.1869 - val_acc: 0.4444\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1706 - acc: 0.4314 - val_loss: 2.2004 - val_acc: 0.4422\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1627 - acc: 0.4318 - val_loss: 2.2008 - val_acc: 0.4424\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1548 - acc: 0.4343 - val_loss: 2.1941 - val_acc: 0.4430\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1457 - acc: 0.4355 - val_loss: 2.1894 - val_acc: 0.4449\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1417 - acc: 0.4339 - val_loss: 2.1923 - val_acc: 0.4428\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1323 - acc: 0.4387 - val_loss: 2.1984 - val_acc: 0.4407\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1296 - acc: 0.4385 - val_loss: 2.1984 - val_acc: 0.4407\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1248 - acc: 0.4395 - val_loss: 2.2034 - val_acc: 0.4394\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1220 - acc: 0.4396 - val_loss: 2.2032 - val_acc: 0.4391\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1147 - acc: 0.4402 - val_loss: 2.2120 - val_acc: 0.4400\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1121 - acc: 0.4412 - val_loss: 2.2033 - val_acc: 0.4394\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1043 - acc: 0.4411 - val_loss: 2.2027 - val_acc: 0.4384\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1046 - acc: 0.4444 - val_loss: 2.2151 - val_acc: 0.4373\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1020 - acc: 0.4426 - val_loss: 2.2145 - val_acc: 0.4383\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1065 - acc: 0.4430 - val_loss: 2.2037 - val_acc: 0.4416\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0931 - acc: 0.4459 - val_loss: 2.1973 - val_acc: 0.4410\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0867 - acc: 0.4472 - val_loss: 2.1986 - val_acc: 0.4406\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0871 - acc: 0.4481 - val_loss: 2.2014 - val_acc: 0.4384\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0867 - acc: 0.4482 - val_loss: 2.1939 - val_acc: 0.4423\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0828 - acc: 0.4470 - val_loss: 2.2095 - val_acc: 0.4402\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0924 - acc: 0.4478 - val_loss: 2.2094 - val_acc: 0.4383\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0771 - acc: 0.4479 - val_loss: 2.2203 - val_acc: 0.4384\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0867 - acc: 0.4473 - val_loss: 2.2120 - val_acc: 0.4375\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0704 - acc: 0.4515 - val_loss: 2.2017 - val_acc: 0.4393\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0684 - acc: 0.4509 - val_loss: 2.2012 - val_acc: 0.4393\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0651 - acc: 0.4502 - val_loss: 2.2174 - val_acc: 0.4372\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0642 - acc: 0.4511 - val_loss: 2.1989 - val_acc: 0.4394\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0599 - acc: 0.4520 - val_loss: 2.2048 - val_acc: 0.4385\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0548 - acc: 0.4530 - val_loss: 2.2000 - val_acc: 0.4419\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0571 - acc: 0.4526 - val_loss: 2.2122 - val_acc: 0.4371\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0505 - acc: 0.4551 - val_loss: 2.2023 - val_acc: 0.4402\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0480 - acc: 0.4561 - val_loss: 2.1986 - val_acc: 0.4392\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0463 - acc: 0.4561 - val_loss: 2.1982 - val_acc: 0.4396\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0476 - acc: 0.4553 - val_loss: 2.2027 - val_acc: 0.4384\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0461 - acc: 0.4561 - val_loss: 2.1899 - val_acc: 0.4435\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0389 - acc: 0.4581 - val_loss: 2.2103 - val_acc: 0.4398\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0413 - acc: 0.4579 - val_loss: 2.2012 - val_acc: 0.4420\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0350 - acc: 0.4575 - val_loss: 2.1927 - val_acc: 0.4423\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0347 - acc: 0.4570 - val_loss: 2.2096 - val_acc: 0.4372\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0327 - acc: 0.4580 - val_loss: 2.1919 - val_acc: 0.4397\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0315 - acc: 0.4584 - val_loss: 2.2085 - val_acc: 0.4401\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0356 - acc: 0.4569 - val_loss: 2.2108 - val_acc: 0.4389\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0330 - acc: 0.4577 - val_loss: 2.2119 - val_acc: 0.4392\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0258 - acc: 0.4591 - val_loss: 2.1993 - val_acc: 0.4411\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0231 - acc: 0.4606 - val_loss: 2.2012 - val_acc: 0.4411\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0220 - acc: 0.4610 - val_loss: 2.1842 - val_acc: 0.4425\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0147 - acc: 0.4620 - val_loss: 2.2021 - val_acc: 0.4423\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0149 - acc: 0.4599 - val_loss: 2.1821 - val_acc: 0.4442\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0153 - acc: 0.4603 - val_loss: 2.1837 - val_acc: 0.4413\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0108 - acc: 0.4643 - val_loss: 2.1904 - val_acc: 0.4440\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0125 - acc: 0.4626 - val_loss: 2.1828 - val_acc: 0.4415\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0077 - acc: 0.4634 - val_loss: 2.2004 - val_acc: 0.4416\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0086 - acc: 0.4626 - val_loss: 2.1894 - val_acc: 0.4427\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0062 - acc: 0.4643 - val_loss: 2.1887 - val_acc: 0.4439\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0123 - acc: 0.4658 - val_loss: 2.2004 - val_acc: 0.4392\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0048 - acc: 0.4637 - val_loss: 2.1951 - val_acc: 0.4413\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0078 - acc: 0.4635 - val_loss: 2.2241 - val_acc: 0.4363\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0104 - acc: 0.4637 - val_loss: 2.1879 - val_acc: 0.4424\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9974 - acc: 0.4661 - val_loss: 2.1940 - val_acc: 0.4409\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9933 - acc: 0.4668 - val_loss: 2.1836 - val_acc: 0.4445\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9935 - acc: 0.4675 - val_loss: 2.2045 - val_acc: 0.4382\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9966 - acc: 0.4661 - val_loss: 2.1900 - val_acc: 0.4414\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9972 - acc: 0.4667 - val_loss: 2.1903 - val_acc: 0.4421\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9889 - acc: 0.4679 - val_loss: 2.1883 - val_acc: 0.4411\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9845 - acc: 0.4697 - val_loss: 2.2128 - val_acc: 0.4377\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9948 - acc: 0.4672 - val_loss: 2.1875 - val_acc: 0.4444\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9895 - acc: 0.4671 - val_loss: 2.2024 - val_acc: 0.4401\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9850 - acc: 0.4686 - val_loss: 2.1870 - val_acc: 0.4423\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9828 - acc: 0.4709 - val_loss: 2.1823 - val_acc: 0.4427\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9801 - acc: 0.4714 - val_loss: 2.1876 - val_acc: 0.4418\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9910 - acc: 0.4693 - val_loss: 2.2096 - val_acc: 0.4389\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9931 - acc: 0.4668 - val_loss: 2.2141 - val_acc: 0.4410\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0051 - acc: 0.4663 - val_loss: 2.1945 - val_acc: 0.4409\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9881 - acc: 0.4675 - val_loss: 2.2072 - val_acc: 0.4394\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9832 - acc: 0.4709 - val_loss: 2.1894 - val_acc: 0.4413\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9742 - acc: 0.4716 - val_loss: 2.1829 - val_acc: 0.4435\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9732 - acc: 0.4713 - val_loss: 2.1881 - val_acc: 0.4435\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9693 - acc: 0.4720 - val_loss: 2.1844 - val_acc: 0.4427\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9725 - acc: 0.4721 - val_loss: 2.2140 - val_acc: 0.4388\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9771 - acc: 0.4711 - val_loss: 2.1912 - val_acc: 0.4432\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9686 - acc: 0.4757 - val_loss: 2.2012 - val_acc: 0.4402\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9724 - acc: 0.4724 - val_loss: 2.1922 - val_acc: 0.4444\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9664 - acc: 0.4747 - val_loss: 2.2086 - val_acc: 0.4398\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9683 - acc: 0.4730 - val_loss: 2.1931 - val_acc: 0.4415\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9618 - acc: 0.4752 - val_loss: 2.2080 - val_acc: 0.4390\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9660 - acc: 0.4760 - val_loss: 2.2017 - val_acc: 0.4412\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9671 - acc: 0.4752 - val_loss: 2.2004 - val_acc: 0.4434\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9604 - acc: 0.4750 - val_loss: 2.1906 - val_acc: 0.4432\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9558 - acc: 0.4766 - val_loss: 2.1836 - val_acc: 0.4442\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9545 - acc: 0.4772 - val_loss: 2.2003 - val_acc: 0.4427\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9552 - acc: 0.4767 - val_loss: 2.1910 - val_acc: 0.4445\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9520 - acc: 0.4752 - val_loss: 2.1990 - val_acc: 0.4419\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9496 - acc: 0.4785 - val_loss: 2.1873 - val_acc: 0.4454\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9501 - acc: 0.4784 - val_loss: 2.2131 - val_acc: 0.4408\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9588 - acc: 0.4780 - val_loss: 2.1963 - val_acc: 0.4422\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9514 - acc: 0.4788 - val_loss: 2.2014 - val_acc: 0.4428\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9493 - acc: 0.4799 - val_loss: 2.2265 - val_acc: 0.4368\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9519 - acc: 0.4763 - val_loss: 2.2045 - val_acc: 0.4413\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1860 - acc: 0.4283 - val_loss: 2.1940 - val_acc: 0.4396\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1626 - acc: 0.4323 - val_loss: 2.1981 - val_acc: 0.4366\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1521 - acc: 0.4337 - val_loss: 2.1914 - val_acc: 0.4396\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1429 - acc: 0.4370 - val_loss: 2.1980 - val_acc: 0.4385\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1322 - acc: 0.4380 - val_loss: 2.2081 - val_acc: 0.4364\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1307 - acc: 0.4395 - val_loss: 2.2143 - val_acc: 0.4343\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1198 - acc: 0.4415 - val_loss: 2.2159 - val_acc: 0.4336\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1153 - acc: 0.4422 - val_loss: 2.2215 - val_acc: 0.4352\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1077 - acc: 0.4441 - val_loss: 2.2231 - val_acc: 0.4307\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1078 - acc: 0.4429 - val_loss: 2.2208 - val_acc: 0.4318\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0986 - acc: 0.4447 - val_loss: 2.2205 - val_acc: 0.4339\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0966 - acc: 0.4450 - val_loss: 2.2145 - val_acc: 0.4347\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0977 - acc: 0.4453 - val_loss: 2.2188 - val_acc: 0.4328\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0892 - acc: 0.4471 - val_loss: 2.2279 - val_acc: 0.4314\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0875 - acc: 0.4468 - val_loss: 2.2154 - val_acc: 0.4336\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0796 - acc: 0.4492 - val_loss: 2.2137 - val_acc: 0.4347\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0814 - acc: 0.4481 - val_loss: 2.2070 - val_acc: 0.4322\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0747 - acc: 0.4516 - val_loss: 2.2145 - val_acc: 0.4322\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0716 - acc: 0.4503 - val_loss: 2.2156 - val_acc: 0.4329\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0654 - acc: 0.4509 - val_loss: 2.2195 - val_acc: 0.4311\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0662 - acc: 0.4509 - val_loss: 2.2215 - val_acc: 0.4321\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0649 - acc: 0.4532 - val_loss: 2.2207 - val_acc: 0.4318\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0601 - acc: 0.4553 - val_loss: 2.2163 - val_acc: 0.4318\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0601 - acc: 0.4533 - val_loss: 2.2187 - val_acc: 0.4303\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0522 - acc: 0.4546 - val_loss: 2.2157 - val_acc: 0.4323\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0470 - acc: 0.4562 - val_loss: 2.2143 - val_acc: 0.4344\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0482 - acc: 0.4573 - val_loss: 2.2042 - val_acc: 0.4332\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0433 - acc: 0.4572 - val_loss: 2.2196 - val_acc: 0.4332\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0487 - acc: 0.4557 - val_loss: 2.2103 - val_acc: 0.4354\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0415 - acc: 0.4576 - val_loss: 2.2130 - val_acc: 0.4358\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0378 - acc: 0.4601 - val_loss: 2.1969 - val_acc: 0.4352\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0342 - acc: 0.4589 - val_loss: 2.2038 - val_acc: 0.4355\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0325 - acc: 0.4589 - val_loss: 2.2046 - val_acc: 0.4373\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0346 - acc: 0.4589 - val_loss: 2.2123 - val_acc: 0.4366\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0364 - acc: 0.4596 - val_loss: 2.1997 - val_acc: 0.4373\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0281 - acc: 0.4595 - val_loss: 2.1996 - val_acc: 0.4391\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0261 - acc: 0.4622 - val_loss: 2.2036 - val_acc: 0.4366\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0288 - acc: 0.4607 - val_loss: 2.1926 - val_acc: 0.4411\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0180 - acc: 0.4620 - val_loss: 2.1932 - val_acc: 0.4376\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0182 - acc: 0.4637 - val_loss: 2.1973 - val_acc: 0.4386\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0183 - acc: 0.4628 - val_loss: 2.1980 - val_acc: 0.4378\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0158 - acc: 0.4629 - val_loss: 2.1975 - val_acc: 0.4383\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0133 - acc: 0.4643 - val_loss: 2.2087 - val_acc: 0.4375\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0154 - acc: 0.4630 - val_loss: 2.2016 - val_acc: 0.4364\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0129 - acc: 0.4618 - val_loss: 2.1974 - val_acc: 0.4370\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0102 - acc: 0.4633 - val_loss: 2.2102 - val_acc: 0.4357\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0095 - acc: 0.4657 - val_loss: 2.1927 - val_acc: 0.4391\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0039 - acc: 0.4653 - val_loss: 2.1968 - val_acc: 0.4368\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0034 - acc: 0.4638 - val_loss: 2.1995 - val_acc: 0.4393\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0014 - acc: 0.4680 - val_loss: 2.1980 - val_acc: 0.4376\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9986 - acc: 0.4682 - val_loss: 2.1942 - val_acc: 0.4404\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9949 - acc: 0.4678 - val_loss: 2.1966 - val_acc: 0.4376\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9960 - acc: 0.4674 - val_loss: 2.2005 - val_acc: 0.4388\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9969 - acc: 0.4677 - val_loss: 2.1951 - val_acc: 0.4367\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9970 - acc: 0.4676 - val_loss: 2.1865 - val_acc: 0.4407\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9928 - acc: 0.4687 - val_loss: 2.2089 - val_acc: 0.4328\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9924 - acc: 0.4678 - val_loss: 2.1896 - val_acc: 0.4394\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9862 - acc: 0.4695 - val_loss: 2.2057 - val_acc: 0.4357\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9868 - acc: 0.4700 - val_loss: 2.1909 - val_acc: 0.4388\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9832 - acc: 0.4706 - val_loss: 2.2086 - val_acc: 0.4340\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9897 - acc: 0.4679 - val_loss: 2.1891 - val_acc: 0.4389\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9912 - acc: 0.4683 - val_loss: 2.2016 - val_acc: 0.4377\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9843 - acc: 0.4690 - val_loss: 2.1953 - val_acc: 0.4387\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9775 - acc: 0.4717 - val_loss: 2.1992 - val_acc: 0.4342\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9768 - acc: 0.4717 - val_loss: 2.1952 - val_acc: 0.4388\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9807 - acc: 0.4709 - val_loss: 2.2033 - val_acc: 0.4373\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9785 - acc: 0.4720 - val_loss: 2.1970 - val_acc: 0.4380\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9742 - acc: 0.4730 - val_loss: 2.1980 - val_acc: 0.4401\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9690 - acc: 0.4739 - val_loss: 2.1863 - val_acc: 0.4382\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9699 - acc: 0.4736 - val_loss: 2.1945 - val_acc: 0.4381\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9637 - acc: 0.4769 - val_loss: 2.1928 - val_acc: 0.4387\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9708 - acc: 0.4739 - val_loss: 2.2042 - val_acc: 0.4350\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9634 - acc: 0.4750 - val_loss: 2.2074 - val_acc: 0.4382\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9719 - acc: 0.4742 - val_loss: 2.2169 - val_acc: 0.4344\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9715 - acc: 0.4734 - val_loss: 2.2062 - val_acc: 0.4366\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9692 - acc: 0.4741 - val_loss: 2.2025 - val_acc: 0.4393\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9638 - acc: 0.4757 - val_loss: 2.2060 - val_acc: 0.4376\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9645 - acc: 0.4743 - val_loss: 2.2134 - val_acc: 0.4395\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9646 - acc: 0.4740 - val_loss: 2.2256 - val_acc: 0.4322\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9657 - acc: 0.4747 - val_loss: 2.2145 - val_acc: 0.4393\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9632 - acc: 0.4761 - val_loss: 2.2017 - val_acc: 0.4372\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9579 - acc: 0.4760 - val_loss: 2.2008 - val_acc: 0.4402\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9624 - acc: 0.4762 - val_loss: 2.2052 - val_acc: 0.4355\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9446 - acc: 0.4807 - val_loss: 2.1972 - val_acc: 0.4380\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9506 - acc: 0.4775 - val_loss: 2.2255 - val_acc: 0.4312\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9535 - acc: 0.4766 - val_loss: 2.2006 - val_acc: 0.4405\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9620 - acc: 0.4760 - val_loss: 2.2197 - val_acc: 0.4347\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9442 - acc: 0.4807 - val_loss: 2.2020 - val_acc: 0.4385\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9446 - acc: 0.4792 - val_loss: 2.2012 - val_acc: 0.4371\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9452 - acc: 0.4778 - val_loss: 2.2001 - val_acc: 0.4398\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9420 - acc: 0.4811 - val_loss: 2.1923 - val_acc: 0.4380\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9402 - acc: 0.4801 - val_loss: 2.1861 - val_acc: 0.4393\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9256 - acc: 0.4827 - val_loss: 2.1923 - val_acc: 0.4368\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9327 - acc: 0.4807 - val_loss: 2.1893 - val_acc: 0.4411\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9302 - acc: 0.4817 - val_loss: 2.1987 - val_acc: 0.4366\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9338 - acc: 0.4812 - val_loss: 2.1930 - val_acc: 0.4389\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9284 - acc: 0.4830 - val_loss: 2.1959 - val_acc: 0.4373\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9344 - acc: 0.4821 - val_loss: 2.2127 - val_acc: 0.4345\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9279 - acc: 0.4845 - val_loss: 2.1903 - val_acc: 0.4370\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9243 - acc: 0.4831 - val_loss: 2.2145 - val_acc: 0.4370\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1956 - acc: 0.4273 - val_loss: 2.2221 - val_acc: 0.4365\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1707 - acc: 0.4327 - val_loss: 2.2236 - val_acc: 0.4334\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1607 - acc: 0.4330 - val_loss: 2.2241 - val_acc: 0.4315\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1508 - acc: 0.4345 - val_loss: 2.2310 - val_acc: 0.4313\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1424 - acc: 0.4360 - val_loss: 2.2283 - val_acc: 0.4335\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1354 - acc: 0.4382 - val_loss: 2.2330 - val_acc: 0.4295\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1285 - acc: 0.4387 - val_loss: 2.2345 - val_acc: 0.4323\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1228 - acc: 0.4397 - val_loss: 2.2394 - val_acc: 0.4287\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1215 - acc: 0.4410 - val_loss: 2.2419 - val_acc: 0.4323\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1211 - acc: 0.4413 - val_loss: 2.2427 - val_acc: 0.4329\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1084 - acc: 0.4433 - val_loss: 2.2492 - val_acc: 0.4302\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1027 - acc: 0.4443 - val_loss: 2.2467 - val_acc: 0.4312\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0972 - acc: 0.4460 - val_loss: 2.2495 - val_acc: 0.4278\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0937 - acc: 0.4458 - val_loss: 2.2531 - val_acc: 0.4302\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0947 - acc: 0.4465 - val_loss: 2.2481 - val_acc: 0.4294\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0880 - acc: 0.4478 - val_loss: 2.2458 - val_acc: 0.4298\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0858 - acc: 0.4482 - val_loss: 2.2400 - val_acc: 0.4302\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0796 - acc: 0.4494 - val_loss: 2.2446 - val_acc: 0.4284\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0739 - acc: 0.4511 - val_loss: 2.2313 - val_acc: 0.4314\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0724 - acc: 0.4509 - val_loss: 2.2367 - val_acc: 0.4332\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0709 - acc: 0.4514 - val_loss: 2.2565 - val_acc: 0.4296\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0795 - acc: 0.4508 - val_loss: 2.2520 - val_acc: 0.4278\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0728 - acc: 0.4512 - val_loss: 2.2381 - val_acc: 0.4309\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0709 - acc: 0.4505 - val_loss: 2.2503 - val_acc: 0.4336\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0724 - acc: 0.4518 - val_loss: 2.2310 - val_acc: 0.4334\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0652 - acc: 0.4523 - val_loss: 2.2338 - val_acc: 0.4343\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0619 - acc: 0.4546 - val_loss: 2.2355 - val_acc: 0.4342\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0571 - acc: 0.4535 - val_loss: 2.2421 - val_acc: 0.4307\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0567 - acc: 0.4535 - val_loss: 2.2344 - val_acc: 0.4314\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0467 - acc: 0.4565 - val_loss: 2.2347 - val_acc: 0.4315\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0468 - acc: 0.4573 - val_loss: 2.2437 - val_acc: 0.4307\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0440 - acc: 0.4570 - val_loss: 2.2352 - val_acc: 0.4346\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0423 - acc: 0.4578 - val_loss: 2.2244 - val_acc: 0.4342\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0384 - acc: 0.4587 - val_loss: 2.2391 - val_acc: 0.4323\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0369 - acc: 0.4582 - val_loss: 2.2165 - val_acc: 0.4366\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0380 - acc: 0.4584 - val_loss: 2.2276 - val_acc: 0.4358\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0307 - acc: 0.4614 - val_loss: 2.2239 - val_acc: 0.4338\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0434 - acc: 0.4577 - val_loss: 2.2199 - val_acc: 0.4352\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0249 - acc: 0.4617 - val_loss: 2.2191 - val_acc: 0.4339\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0337 - acc: 0.4599 - val_loss: 2.2361 - val_acc: 0.4369\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0335 - acc: 0.4600 - val_loss: 2.2694 - val_acc: 0.4271\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0463 - acc: 0.4579 - val_loss: 2.2280 - val_acc: 0.4352\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0174 - acc: 0.4622 - val_loss: 2.2244 - val_acc: 0.4324\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0156 - acc: 0.4629 - val_loss: 2.2219 - val_acc: 0.4352\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0172 - acc: 0.4641 - val_loss: 2.2179 - val_acc: 0.4361\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0194 - acc: 0.4627 - val_loss: 2.2336 - val_acc: 0.4323\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0150 - acc: 0.4649 - val_loss: 2.2155 - val_acc: 0.4343\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0157 - acc: 0.4637 - val_loss: 2.2260 - val_acc: 0.4354\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0112 - acc: 0.4663 - val_loss: 2.2170 - val_acc: 0.4356\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0026 - acc: 0.4664 - val_loss: 2.2220 - val_acc: 0.4341\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0031 - acc: 0.4648 - val_loss: 2.2255 - val_acc: 0.4329\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0029 - acc: 0.4664 - val_loss: 2.2258 - val_acc: 0.4333\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0050 - acc: 0.4652 - val_loss: 2.2232 - val_acc: 0.4347\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9991 - acc: 0.4668 - val_loss: 2.2345 - val_acc: 0.4372\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0208 - acc: 0.4624 - val_loss: 2.2342 - val_acc: 0.4312\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0135 - acc: 0.4638 - val_loss: 2.2284 - val_acc: 0.4341\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0043 - acc: 0.4671 - val_loss: 2.2443 - val_acc: 0.4321\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0070 - acc: 0.4661 - val_loss: 2.2212 - val_acc: 0.4332\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9995 - acc: 0.4664 - val_loss: 2.2203 - val_acc: 0.4325\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9989 - acc: 0.4688 - val_loss: 2.2561 - val_acc: 0.4328\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0089 - acc: 0.4650 - val_loss: 2.2240 - val_acc: 0.4333\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9905 - acc: 0.4690 - val_loss: 2.2356 - val_acc: 0.4323\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9879 - acc: 0.4701 - val_loss: 2.2171 - val_acc: 0.4348\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9812 - acc: 0.4726 - val_loss: 2.2258 - val_acc: 0.4337\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9832 - acc: 0.4707 - val_loss: 2.2359 - val_acc: 0.4304\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9905 - acc: 0.4693 - val_loss: 2.2240 - val_acc: 0.4342\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9798 - acc: 0.4727 - val_loss: 2.2193 - val_acc: 0.4315\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9835 - acc: 0.4701 - val_loss: 2.2506 - val_acc: 0.4319\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9953 - acc: 0.4689 - val_loss: 2.2182 - val_acc: 0.4330\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9758 - acc: 0.4716 - val_loss: 2.2247 - val_acc: 0.4332\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9774 - acc: 0.4728 - val_loss: 2.2326 - val_acc: 0.4293\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9825 - acc: 0.4704 - val_loss: 2.2788 - val_acc: 0.4303\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0276 - acc: 0.4608 - val_loss: 2.2454 - val_acc: 0.4279\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9810 - acc: 0.4711 - val_loss: 2.2245 - val_acc: 0.4344\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9716 - acc: 0.4740 - val_loss: 2.2373 - val_acc: 0.4324\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9785 - acc: 0.4722 - val_loss: 2.2410 - val_acc: 0.4336\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9764 - acc: 0.4728 - val_loss: 2.2212 - val_acc: 0.4317\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9724 - acc: 0.4731 - val_loss: 2.2316 - val_acc: 0.4315\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9651 - acc: 0.4754 - val_loss: 2.2220 - val_acc: 0.4328\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9590 - acc: 0.4757 - val_loss: 2.2319 - val_acc: 0.4330\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9608 - acc: 0.4757 - val_loss: 2.2293 - val_acc: 0.4336\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9623 - acc: 0.4758 - val_loss: 2.2349 - val_acc: 0.4304\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9662 - acc: 0.4748 - val_loss: 2.2366 - val_acc: 0.4324\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9673 - acc: 0.4743 - val_loss: 2.2293 - val_acc: 0.4320\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9635 - acc: 0.4751 - val_loss: 2.2476 - val_acc: 0.4305\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9725 - acc: 0.4745 - val_loss: 2.2483 - val_acc: 0.4283\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9642 - acc: 0.4739 - val_loss: 2.2400 - val_acc: 0.4314\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9523 - acc: 0.4792 - val_loss: 2.2345 - val_acc: 0.4307\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9620 - acc: 0.4750 - val_loss: 2.2328 - val_acc: 0.4301\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9462 - acc: 0.4787 - val_loss: 2.2285 - val_acc: 0.4329\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9532 - acc: 0.4809 - val_loss: 2.2331 - val_acc: 0.4325\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9552 - acc: 0.4774 - val_loss: 2.2466 - val_acc: 0.4314\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9599 - acc: 0.4752 - val_loss: 2.2332 - val_acc: 0.4338\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9455 - acc: 0.4779 - val_loss: 2.2282 - val_acc: 0.4315\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9431 - acc: 0.4806 - val_loss: 2.2337 - val_acc: 0.4315\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9449 - acc: 0.4803 - val_loss: 2.2250 - val_acc: 0.4357\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9416 - acc: 0.4808 - val_loss: 2.2309 - val_acc: 0.4321\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9383 - acc: 0.4827 - val_loss: 2.2292 - val_acc: 0.4320\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9405 - acc: 0.4813 - val_loss: 2.2428 - val_acc: 0.4307\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9392 - acc: 0.4796 - val_loss: 2.2355 - val_acc: 0.4313\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2016 - acc: 0.4267 - val_loss: 2.2091 - val_acc: 0.4367\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1818 - acc: 0.4298 - val_loss: 2.1860 - val_acc: 0.4408\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1728 - acc: 0.4324 - val_loss: 2.2002 - val_acc: 0.4360\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1638 - acc: 0.4340 - val_loss: 2.2265 - val_acc: 0.4340\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1665 - acc: 0.4353 - val_loss: 2.1991 - val_acc: 0.4387\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1417 - acc: 0.4382 - val_loss: 2.2060 - val_acc: 0.4373\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1373 - acc: 0.4402 - val_loss: 2.1940 - val_acc: 0.4392\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1402 - acc: 0.4398 - val_loss: 2.1926 - val_acc: 0.4399\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1305 - acc: 0.4408 - val_loss: 2.2042 - val_acc: 0.4379\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1223 - acc: 0.4417 - val_loss: 2.1915 - val_acc: 0.4387\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1170 - acc: 0.4447 - val_loss: 2.2054 - val_acc: 0.4375\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1168 - acc: 0.4440 - val_loss: 2.2069 - val_acc: 0.4360\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1108 - acc: 0.4434 - val_loss: 2.2346 - val_acc: 0.4315\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1216 - acc: 0.4421 - val_loss: 2.2036 - val_acc: 0.4400\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1005 - acc: 0.4458 - val_loss: 2.2015 - val_acc: 0.4372\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1115 - acc: 0.4442 - val_loss: 2.2063 - val_acc: 0.4399\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0992 - acc: 0.4483 - val_loss: 2.2078 - val_acc: 0.4369\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0980 - acc: 0.4466 - val_loss: 2.2008 - val_acc: 0.4397\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0953 - acc: 0.4484 - val_loss: 2.2084 - val_acc: 0.4374\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0911 - acc: 0.4479 - val_loss: 2.1993 - val_acc: 0.4382\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0842 - acc: 0.4500 - val_loss: 2.1948 - val_acc: 0.4393\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0753 - acc: 0.4501 - val_loss: 2.2061 - val_acc: 0.4380\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0971 - acc: 0.4482 - val_loss: 2.2015 - val_acc: 0.4391\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0777 - acc: 0.4516 - val_loss: 2.2014 - val_acc: 0.4364\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0710 - acc: 0.4541 - val_loss: 2.1901 - val_acc: 0.4411\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0711 - acc: 0.4511 - val_loss: 2.1962 - val_acc: 0.4370\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0737 - acc: 0.4511 - val_loss: 2.1914 - val_acc: 0.4400\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0717 - acc: 0.4500 - val_loss: 2.2003 - val_acc: 0.4382\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0637 - acc: 0.4558 - val_loss: 2.1913 - val_acc: 0.4404\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0550 - acc: 0.4566 - val_loss: 2.1919 - val_acc: 0.4393\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0559 - acc: 0.4559 - val_loss: 2.1969 - val_acc: 0.4405\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0555 - acc: 0.4566 - val_loss: 2.1866 - val_acc: 0.4416\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0560 - acc: 0.4570 - val_loss: 2.2341 - val_acc: 0.4330\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0804 - acc: 0.4520 - val_loss: 2.1931 - val_acc: 0.4375\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0472 - acc: 0.4597 - val_loss: 2.2135 - val_acc: 0.4383\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0671 - acc: 0.4548 - val_loss: 2.2139 - val_acc: 0.4370\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0546 - acc: 0.4590 - val_loss: 2.1998 - val_acc: 0.4384\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0558 - acc: 0.4570 - val_loss: 2.1978 - val_acc: 0.4396\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0507 - acc: 0.4580 - val_loss: 2.1937 - val_acc: 0.4391\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0397 - acc: 0.4605 - val_loss: 2.1987 - val_acc: 0.4398\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0433 - acc: 0.4579 - val_loss: 2.1934 - val_acc: 0.4398\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0584 - acc: 0.4561 - val_loss: 2.1968 - val_acc: 0.4403\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0357 - acc: 0.4628 - val_loss: 2.2046 - val_acc: 0.4386\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0400 - acc: 0.4593 - val_loss: 2.1859 - val_acc: 0.4407\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0315 - acc: 0.4603 - val_loss: 2.1787 - val_acc: 0.4388\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0234 - acc: 0.4640 - val_loss: 2.1801 - val_acc: 0.4428\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0154 - acc: 0.4637 - val_loss: 2.1812 - val_acc: 0.4431\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0203 - acc: 0.4651 - val_loss: 2.1850 - val_acc: 0.4407\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0155 - acc: 0.4653 - val_loss: 2.1830 - val_acc: 0.4420\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0207 - acc: 0.4647 - val_loss: 2.2003 - val_acc: 0.4397\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0280 - acc: 0.4638 - val_loss: 2.1894 - val_acc: 0.4437\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0205 - acc: 0.4627 - val_loss: 2.1904 - val_acc: 0.4410\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0205 - acc: 0.4641 - val_loss: 2.1811 - val_acc: 0.4425\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0170 - acc: 0.4626 - val_loss: 2.1890 - val_acc: 0.4413\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0157 - acc: 0.4661 - val_loss: 2.2152 - val_acc: 0.4394\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0376 - acc: 0.4623 - val_loss: 2.2359 - val_acc: 0.4365\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0410 - acc: 0.4607 - val_loss: 2.2039 - val_acc: 0.4410\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0185 - acc: 0.4655 - val_loss: 2.1797 - val_acc: 0.4442\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9980 - acc: 0.4685 - val_loss: 2.1851 - val_acc: 0.4422\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0010 - acc: 0.4697 - val_loss: 2.1751 - val_acc: 0.4442\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0020 - acc: 0.4670 - val_loss: 2.1859 - val_acc: 0.4413\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0019 - acc: 0.4698 - val_loss: 2.1837 - val_acc: 0.4436\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0031 - acc: 0.4682 - val_loss: 2.1897 - val_acc: 0.4404\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0040 - acc: 0.4682 - val_loss: 2.1845 - val_acc: 0.4440\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0058 - acc: 0.4684 - val_loss: 2.1903 - val_acc: 0.4408\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0031 - acc: 0.4686 - val_loss: 2.1870 - val_acc: 0.4453\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9960 - acc: 0.4696 - val_loss: 2.1904 - val_acc: 0.4393\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0059 - acc: 0.4655 - val_loss: 2.1887 - val_acc: 0.4426\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0026 - acc: 0.4678 - val_loss: 2.1972 - val_acc: 0.4407\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9951 - acc: 0.4708 - val_loss: 2.1801 - val_acc: 0.4455\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9941 - acc: 0.4701 - val_loss: 2.2087 - val_acc: 0.4371\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9986 - acc: 0.4702 - val_loss: 2.2357 - val_acc: 0.4348\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0240 - acc: 0.4655 - val_loss: 2.1912 - val_acc: 0.4400\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9841 - acc: 0.4732 - val_loss: 2.1836 - val_acc: 0.4419\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9765 - acc: 0.4717 - val_loss: 2.1921 - val_acc: 0.4398\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9839 - acc: 0.4730 - val_loss: 2.1830 - val_acc: 0.4424\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9843 - acc: 0.4719 - val_loss: 2.1928 - val_acc: 0.4409\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9911 - acc: 0.4706 - val_loss: 2.1817 - val_acc: 0.4429\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9730 - acc: 0.4755 - val_loss: 2.1858 - val_acc: 0.4414\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9694 - acc: 0.4735 - val_loss: 2.1854 - val_acc: 0.4427\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9699 - acc: 0.4751 - val_loss: 2.2005 - val_acc: 0.4382\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9795 - acc: 0.4722 - val_loss: 2.1904 - val_acc: 0.4413\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9792 - acc: 0.4742 - val_loss: 2.2059 - val_acc: 0.4411\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9736 - acc: 0.4742 - val_loss: 2.2237 - val_acc: 0.4359\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9847 - acc: 0.4739 - val_loss: 2.2217 - val_acc: 0.4393\n",
      "Epoch 86/100\n",
      "1s - loss: 2.0085 - acc: 0.4673 - val_loss: 2.2193 - val_acc: 0.4366\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9777 - acc: 0.4744 - val_loss: 2.1976 - val_acc: 0.4412\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9640 - acc: 0.4779 - val_loss: 2.1923 - val_acc: 0.4416\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9648 - acc: 0.4777 - val_loss: 2.1970 - val_acc: 0.4409\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9691 - acc: 0.4744 - val_loss: 2.1995 - val_acc: 0.4389\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9679 - acc: 0.4745 - val_loss: 2.2094 - val_acc: 0.4401\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9703 - acc: 0.4750 - val_loss: 2.1906 - val_acc: 0.4422\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9635 - acc: 0.4767 - val_loss: 2.2225 - val_acc: 0.4372\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9689 - acc: 0.4763 - val_loss: 2.2002 - val_acc: 0.4399\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9762 - acc: 0.4744 - val_loss: 2.2037 - val_acc: 0.4388\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9501 - acc: 0.4804 - val_loss: 2.2354 - val_acc: 0.4335\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9742 - acc: 0.4758 - val_loss: 2.2251 - val_acc: 0.4402\n",
      "Epoch 98/100\n",
      "1s - loss: 2.0073 - acc: 0.4691 - val_loss: 2.2571 - val_acc: 0.4292\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9726 - acc: 0.4744 - val_loss: 2.2074 - val_acc: 0.4389\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9479 - acc: 0.4818 - val_loss: 2.1978 - val_acc: 0.4418\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1788 - acc: 0.4294 - val_loss: 2.2018 - val_acc: 0.4435\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1585 - acc: 0.4340 - val_loss: 2.1999 - val_acc: 0.4415\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1462 - acc: 0.4360 - val_loss: 2.2095 - val_acc: 0.4386\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1380 - acc: 0.4393 - val_loss: 2.2095 - val_acc: 0.4402\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1346 - acc: 0.4367 - val_loss: 2.2101 - val_acc: 0.4402\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1231 - acc: 0.4414 - val_loss: 2.2181 - val_acc: 0.4370\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1170 - acc: 0.4433 - val_loss: 2.2147 - val_acc: 0.4402\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1100 - acc: 0.4428 - val_loss: 2.2111 - val_acc: 0.4403\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1009 - acc: 0.4454 - val_loss: 2.2219 - val_acc: 0.4365\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1008 - acc: 0.4468 - val_loss: 2.2204 - val_acc: 0.4385\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0909 - acc: 0.4471 - val_loss: 2.2225 - val_acc: 0.4360\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0973 - acc: 0.4453 - val_loss: 2.2180 - val_acc: 0.4368\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0876 - acc: 0.4482 - val_loss: 2.2216 - val_acc: 0.4381\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0811 - acc: 0.4493 - val_loss: 2.2254 - val_acc: 0.4353\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0780 - acc: 0.4499 - val_loss: 2.2206 - val_acc: 0.4357\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0737 - acc: 0.4508 - val_loss: 2.2205 - val_acc: 0.4345\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0710 - acc: 0.4515 - val_loss: 2.2330 - val_acc: 0.4356\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0797 - acc: 0.4513 - val_loss: 2.2231 - val_acc: 0.4364\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0677 - acc: 0.4514 - val_loss: 2.2340 - val_acc: 0.4366\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0688 - acc: 0.4508 - val_loss: 2.2300 - val_acc: 0.4349\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0593 - acc: 0.4544 - val_loss: 2.2283 - val_acc: 0.4372\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0624 - acc: 0.4527 - val_loss: 2.2244 - val_acc: 0.4362\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0574 - acc: 0.4546 - val_loss: 2.2200 - val_acc: 0.4384\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0522 - acc: 0.4553 - val_loss: 2.2305 - val_acc: 0.4370\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0502 - acc: 0.4570 - val_loss: 2.2272 - val_acc: 0.4357\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0410 - acc: 0.4583 - val_loss: 2.2167 - val_acc: 0.4397\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0431 - acc: 0.4579 - val_loss: 2.2470 - val_acc: 0.4336\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0469 - acc: 0.4571 - val_loss: 2.2170 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0365 - acc: 0.4578 - val_loss: 2.2171 - val_acc: 0.4377\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0340 - acc: 0.4598 - val_loss: 2.2274 - val_acc: 0.4391\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0560 - acc: 0.4539 - val_loss: 2.2196 - val_acc: 0.4374\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0296 - acc: 0.4619 - val_loss: 2.2174 - val_acc: 0.4397\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0289 - acc: 0.4611 - val_loss: 2.2425 - val_acc: 0.4346\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0349 - acc: 0.4573 - val_loss: 2.2137 - val_acc: 0.4389\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0262 - acc: 0.4622 - val_loss: 2.2175 - val_acc: 0.4367\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0218 - acc: 0.4625 - val_loss: 2.2218 - val_acc: 0.4353\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0255 - acc: 0.4630 - val_loss: 2.2571 - val_acc: 0.4334\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0233 - acc: 0.4628 - val_loss: 2.2059 - val_acc: 0.4388\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0117 - acc: 0.4672 - val_loss: 2.2193 - val_acc: 0.4378\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0127 - acc: 0.4646 - val_loss: 2.2157 - val_acc: 0.4368\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0087 - acc: 0.4659 - val_loss: 2.2287 - val_acc: 0.4372\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0163 - acc: 0.4654 - val_loss: 2.2128 - val_acc: 0.4373\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0056 - acc: 0.4656 - val_loss: 2.2163 - val_acc: 0.4391\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0056 - acc: 0.4677 - val_loss: 2.2234 - val_acc: 0.4394\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0109 - acc: 0.4648 - val_loss: 2.2234 - val_acc: 0.4402\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0114 - acc: 0.4651 - val_loss: 2.2310 - val_acc: 0.4379\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0066 - acc: 0.4667 - val_loss: 2.2253 - val_acc: 0.4361\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9959 - acc: 0.4692 - val_loss: 2.2600 - val_acc: 0.4319\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9987 - acc: 0.4686 - val_loss: 2.2112 - val_acc: 0.4409\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9923 - acc: 0.4697 - val_loss: 2.2055 - val_acc: 0.4414\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9895 - acc: 0.4697 - val_loss: 2.2105 - val_acc: 0.4403\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9863 - acc: 0.4720 - val_loss: 2.2085 - val_acc: 0.4411\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9839 - acc: 0.4716 - val_loss: 2.2074 - val_acc: 0.4397\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9825 - acc: 0.4729 - val_loss: 2.2059 - val_acc: 0.4398\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9788 - acc: 0.4714 - val_loss: 2.2243 - val_acc: 0.4387\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9900 - acc: 0.4704 - val_loss: 2.2082 - val_acc: 0.4398\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9818 - acc: 0.4716 - val_loss: 2.2426 - val_acc: 0.4364\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9850 - acc: 0.4700 - val_loss: 2.2229 - val_acc: 0.4390\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9928 - acc: 0.4709 - val_loss: 2.2420 - val_acc: 0.4355\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9894 - acc: 0.4701 - val_loss: 2.2261 - val_acc: 0.4402\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9780 - acc: 0.4747 - val_loss: 2.2217 - val_acc: 0.4392\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9759 - acc: 0.4744 - val_loss: 2.2262 - val_acc: 0.4405\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9801 - acc: 0.4741 - val_loss: 2.2116 - val_acc: 0.4407\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9757 - acc: 0.4720 - val_loss: 2.2200 - val_acc: 0.4397\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9822 - acc: 0.4727 - val_loss: 2.2189 - val_acc: 0.4372\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9797 - acc: 0.4726 - val_loss: 2.2191 - val_acc: 0.4384\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9763 - acc: 0.4717 - val_loss: 2.2143 - val_acc: 0.4378\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9657 - acc: 0.4767 - val_loss: 2.2137 - val_acc: 0.4399\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9753 - acc: 0.4725 - val_loss: 2.2288 - val_acc: 0.4377\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9667 - acc: 0.4750 - val_loss: 2.2196 - val_acc: 0.4372\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9734 - acc: 0.4739 - val_loss: 2.2314 - val_acc: 0.4374\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9617 - acc: 0.4772 - val_loss: 2.2067 - val_acc: 0.4413\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9563 - acc: 0.4770 - val_loss: 2.2239 - val_acc: 0.4399\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9656 - acc: 0.4754 - val_loss: 2.2064 - val_acc: 0.4404\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9614 - acc: 0.4771 - val_loss: 2.2017 - val_acc: 0.4420\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9515 - acc: 0.4790 - val_loss: 2.2079 - val_acc: 0.4405\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9562 - acc: 0.4790 - val_loss: 2.2692 - val_acc: 0.4303\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9637 - acc: 0.4781 - val_loss: 2.2328 - val_acc: 0.4382\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9618 - acc: 0.4784 - val_loss: 2.2786 - val_acc: 0.4256\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9537 - acc: 0.4772 - val_loss: 2.2175 - val_acc: 0.4383\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9580 - acc: 0.4777 - val_loss: 2.2376 - val_acc: 0.4361\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9456 - acc: 0.4809 - val_loss: 2.2117 - val_acc: 0.4401\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9494 - acc: 0.4798 - val_loss: 2.2419 - val_acc: 0.4332\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9475 - acc: 0.4800 - val_loss: 2.2183 - val_acc: 0.4391\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9509 - acc: 0.4796 - val_loss: 2.2197 - val_acc: 0.4392\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9464 - acc: 0.4803 - val_loss: 2.2321 - val_acc: 0.4346\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9563 - acc: 0.4762 - val_loss: 2.2146 - val_acc: 0.4403\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9393 - acc: 0.4810 - val_loss: 2.2134 - val_acc: 0.4405\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9298 - acc: 0.4850 - val_loss: 2.2293 - val_acc: 0.4399\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9421 - acc: 0.4814 - val_loss: 2.2182 - val_acc: 0.4394\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9373 - acc: 0.4836 - val_loss: 2.2105 - val_acc: 0.4425\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9224 - acc: 0.4882 - val_loss: 2.2090 - val_acc: 0.4416\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9260 - acc: 0.4848 - val_loss: 2.2203 - val_acc: 0.4381\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9289 - acc: 0.4838 - val_loss: 2.2118 - val_acc: 0.4392\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9326 - acc: 0.4823 - val_loss: 2.2245 - val_acc: 0.4405\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9431 - acc: 0.4776 - val_loss: 2.2223 - val_acc: 0.4387\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9385 - acc: 0.4823 - val_loss: 2.2307 - val_acc: 0.4392\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9794 - acc: 0.4726 - val_loss: 2.2877 - val_acc: 0.4240\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9624 - acc: 0.4773 - val_loss: 2.2256 - val_acc: 0.4379\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9358 - acc: 0.4842 - val_loss: 2.2209 - val_acc: 0.4382\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2087 - acc: 0.4239 - val_loss: 2.2165 - val_acc: 0.4386\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1874 - acc: 0.4263 - val_loss: 2.2239 - val_acc: 0.4355\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1726 - acc: 0.4299 - val_loss: 2.2292 - val_acc: 0.4337\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1659 - acc: 0.4311 - val_loss: 2.2286 - val_acc: 0.4322\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1586 - acc: 0.4321 - val_loss: 2.2344 - val_acc: 0.4309\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1500 - acc: 0.4353 - val_loss: 2.2430 - val_acc: 0.4307\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1451 - acc: 0.4370 - val_loss: 2.2463 - val_acc: 0.4290\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1361 - acc: 0.4373 - val_loss: 2.2555 - val_acc: 0.4273\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1383 - acc: 0.4381 - val_loss: 2.2441 - val_acc: 0.4278\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1292 - acc: 0.4381 - val_loss: 2.2620 - val_acc: 0.4274\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1281 - acc: 0.4384 - val_loss: 2.2682 - val_acc: 0.4274\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1319 - acc: 0.4389 - val_loss: 2.2492 - val_acc: 0.4295\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1156 - acc: 0.4412 - val_loss: 2.2715 - val_acc: 0.4270\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1184 - acc: 0.4420 - val_loss: 2.2516 - val_acc: 0.4316\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1096 - acc: 0.4432 - val_loss: 2.2809 - val_acc: 0.4248\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1059 - acc: 0.4450 - val_loss: 2.2616 - val_acc: 0.4252\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1024 - acc: 0.4441 - val_loss: 2.2793 - val_acc: 0.4219\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0952 - acc: 0.4459 - val_loss: 2.2496 - val_acc: 0.4273\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0934 - acc: 0.4464 - val_loss: 2.2733 - val_acc: 0.4270\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0960 - acc: 0.4461 - val_loss: 2.2534 - val_acc: 0.4271\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0877 - acc: 0.4478 - val_loss: 2.2603 - val_acc: 0.4263\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0866 - acc: 0.4461 - val_loss: 2.2600 - val_acc: 0.4267\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0804 - acc: 0.4483 - val_loss: 2.2637 - val_acc: 0.4242\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0822 - acc: 0.4464 - val_loss: 2.2403 - val_acc: 0.4287\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0748 - acc: 0.4494 - val_loss: 2.2612 - val_acc: 0.4247\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0727 - acc: 0.4488 - val_loss: 2.2445 - val_acc: 0.4286\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0756 - acc: 0.4517 - val_loss: 2.2837 - val_acc: 0.4238\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0733 - acc: 0.4495 - val_loss: 2.2491 - val_acc: 0.4241\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0721 - acc: 0.4490 - val_loss: 2.2811 - val_acc: 0.4238\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0718 - acc: 0.4512 - val_loss: 2.2455 - val_acc: 0.4286\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0722 - acc: 0.4493 - val_loss: 2.2983 - val_acc: 0.4243\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0644 - acc: 0.4506 - val_loss: 2.2413 - val_acc: 0.4316\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0679 - acc: 0.4516 - val_loss: 2.2515 - val_acc: 0.4295\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0618 - acc: 0.4537 - val_loss: 2.2292 - val_acc: 0.4346\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0532 - acc: 0.4555 - val_loss: 2.2639 - val_acc: 0.4278\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0518 - acc: 0.4552 - val_loss: 2.2461 - val_acc: 0.4300\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0574 - acc: 0.4529 - val_loss: 2.2721 - val_acc: 0.4274\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0468 - acc: 0.4573 - val_loss: 2.2475 - val_acc: 0.4302\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0484 - acc: 0.4560 - val_loss: 2.2711 - val_acc: 0.4297\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0488 - acc: 0.4545 - val_loss: 2.2385 - val_acc: 0.4281\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0439 - acc: 0.4582 - val_loss: 2.2867 - val_acc: 0.4288\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0522 - acc: 0.4549 - val_loss: 2.2443 - val_acc: 0.4294\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0463 - acc: 0.4575 - val_loss: 2.2847 - val_acc: 0.4287\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0422 - acc: 0.4590 - val_loss: 2.2306 - val_acc: 0.4330\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0402 - acc: 0.4570 - val_loss: 2.2588 - val_acc: 0.4307\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0320 - acc: 0.4587 - val_loss: 2.2406 - val_acc: 0.4301\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0377 - acc: 0.4578 - val_loss: 2.2538 - val_acc: 0.4297\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0295 - acc: 0.4604 - val_loss: 2.2205 - val_acc: 0.4363\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0229 - acc: 0.4613 - val_loss: 2.2334 - val_acc: 0.4333\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0201 - acc: 0.4625 - val_loss: 2.2324 - val_acc: 0.4323\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0142 - acc: 0.4622 - val_loss: 2.2414 - val_acc: 0.4329\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0252 - acc: 0.4605 - val_loss: 2.2416 - val_acc: 0.4308\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0234 - acc: 0.4612 - val_loss: 2.2480 - val_acc: 0.4278\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0255 - acc: 0.4597 - val_loss: 2.2421 - val_acc: 0.4296\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0204 - acc: 0.4636 - val_loss: 2.2355 - val_acc: 0.4311\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0149 - acc: 0.4631 - val_loss: 2.2472 - val_acc: 0.4278\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0217 - acc: 0.4616 - val_loss: 2.2260 - val_acc: 0.4345\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0121 - acc: 0.4635 - val_loss: 2.2304 - val_acc: 0.4340\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0068 - acc: 0.4653 - val_loss: 2.2303 - val_acc: 0.4325\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0078 - acc: 0.4651 - val_loss: 2.2333 - val_acc: 0.4345\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0196 - acc: 0.4629 - val_loss: 2.2235 - val_acc: 0.4344\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0110 - acc: 0.4639 - val_loss: 2.2324 - val_acc: 0.4330\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9994 - acc: 0.4678 - val_loss: 2.2166 - val_acc: 0.4362\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0011 - acc: 0.4666 - val_loss: 2.2531 - val_acc: 0.4308\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0159 - acc: 0.4644 - val_loss: 2.2818 - val_acc: 0.4270\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0265 - acc: 0.4619 - val_loss: 2.2695 - val_acc: 0.4315\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0060 - acc: 0.4664 - val_loss: 2.2339 - val_acc: 0.4338\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0048 - acc: 0.4639 - val_loss: 2.2708 - val_acc: 0.4307\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0128 - acc: 0.4652 - val_loss: 2.2746 - val_acc: 0.4263\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0143 - acc: 0.4617 - val_loss: 2.2438 - val_acc: 0.4347\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9973 - acc: 0.4665 - val_loss: 2.2281 - val_acc: 0.4346\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9900 - acc: 0.4694 - val_loss: 2.2183 - val_acc: 0.4364\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9837 - acc: 0.4696 - val_loss: 2.2149 - val_acc: 0.4366\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9959 - acc: 0.4656 - val_loss: 2.2603 - val_acc: 0.4326\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9995 - acc: 0.4657 - val_loss: 2.2241 - val_acc: 0.4353\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9904 - acc: 0.4679 - val_loss: 2.2574 - val_acc: 0.4328\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9941 - acc: 0.4670 - val_loss: 2.2300 - val_acc: 0.4328\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9837 - acc: 0.4706 - val_loss: 2.2275 - val_acc: 0.4353\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9792 - acc: 0.4702 - val_loss: 2.2521 - val_acc: 0.4300\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9822 - acc: 0.4715 - val_loss: 2.2109 - val_acc: 0.4385\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9773 - acc: 0.4727 - val_loss: 2.2413 - val_acc: 0.4332\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9874 - acc: 0.4699 - val_loss: 2.2170 - val_acc: 0.4374\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9889 - acc: 0.4692 - val_loss: 2.2413 - val_acc: 0.4331\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9749 - acc: 0.4733 - val_loss: 2.2314 - val_acc: 0.4379\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9785 - acc: 0.4736 - val_loss: 2.2543 - val_acc: 0.4319\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9855 - acc: 0.4692 - val_loss: 2.2256 - val_acc: 0.4363\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9757 - acc: 0.4719 - val_loss: 2.2597 - val_acc: 0.4331\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9731 - acc: 0.4720 - val_loss: 2.2273 - val_acc: 0.4340\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9666 - acc: 0.4725 - val_loss: 2.2464 - val_acc: 0.4341\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9687 - acc: 0.4741 - val_loss: 2.2277 - val_acc: 0.4352\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9582 - acc: 0.4765 - val_loss: 2.2411 - val_acc: 0.4322\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9704 - acc: 0.4731 - val_loss: 2.2301 - val_acc: 0.4355\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9634 - acc: 0.4763 - val_loss: 2.2366 - val_acc: 0.4321\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9552 - acc: 0.4772 - val_loss: 2.2271 - val_acc: 0.4362\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9621 - acc: 0.4756 - val_loss: 2.2347 - val_acc: 0.4340\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9635 - acc: 0.4756 - val_loss: 2.2231 - val_acc: 0.4349\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9609 - acc: 0.4773 - val_loss: 2.2331 - val_acc: 0.4336\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9587 - acc: 0.4766 - val_loss: 2.2205 - val_acc: 0.4381\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9580 - acc: 0.4779 - val_loss: 2.2738 - val_acc: 0.4298\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9757 - acc: 0.4746 - val_loss: 2.2393 - val_acc: 0.4349\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2046 - acc: 0.4235 - val_loss: 2.2129 - val_acc: 0.4436\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1846 - acc: 0.4283 - val_loss: 2.2154 - val_acc: 0.4413\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1681 - acc: 0.4310 - val_loss: 2.2244 - val_acc: 0.4388\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1601 - acc: 0.4332 - val_loss: 2.2396 - val_acc: 0.4363\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1544 - acc: 0.4344 - val_loss: 2.2362 - val_acc: 0.4387\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1435 - acc: 0.4364 - val_loss: 2.2282 - val_acc: 0.4388\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1369 - acc: 0.4373 - val_loss: 2.2303 - val_acc: 0.4377\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1327 - acc: 0.4382 - val_loss: 2.2394 - val_acc: 0.4379\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1307 - acc: 0.4384 - val_loss: 2.2219 - val_acc: 0.4404\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1215 - acc: 0.4414 - val_loss: 2.2348 - val_acc: 0.4381\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1164 - acc: 0.4401 - val_loss: 2.2395 - val_acc: 0.4394\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1076 - acc: 0.4448 - val_loss: 2.2448 - val_acc: 0.4380\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1227 - acc: 0.4410 - val_loss: 2.2448 - val_acc: 0.4351\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1031 - acc: 0.4438 - val_loss: 2.2505 - val_acc: 0.4358\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1031 - acc: 0.4445 - val_loss: 2.2513 - val_acc: 0.4361\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0936 - acc: 0.4450 - val_loss: 2.2328 - val_acc: 0.4373\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0918 - acc: 0.4478 - val_loss: 2.2402 - val_acc: 0.4361\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0871 - acc: 0.4488 - val_loss: 2.2344 - val_acc: 0.4361\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0861 - acc: 0.4504 - val_loss: 2.2336 - val_acc: 0.4385\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0851 - acc: 0.4491 - val_loss: 2.2351 - val_acc: 0.4384\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0807 - acc: 0.4503 - val_loss: 2.2269 - val_acc: 0.4390\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0789 - acc: 0.4489 - val_loss: 2.2369 - val_acc: 0.4374\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0712 - acc: 0.4514 - val_loss: 2.2237 - val_acc: 0.4400\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0708 - acc: 0.4519 - val_loss: 2.2271 - val_acc: 0.4379\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0672 - acc: 0.4527 - val_loss: 2.2204 - val_acc: 0.4382\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0661 - acc: 0.4530 - val_loss: 2.2287 - val_acc: 0.4398\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0666 - acc: 0.4535 - val_loss: 2.2290 - val_acc: 0.4400\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0637 - acc: 0.4541 - val_loss: 2.2358 - val_acc: 0.4356\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0635 - acc: 0.4543 - val_loss: 2.2267 - val_acc: 0.4392\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0569 - acc: 0.4552 - val_loss: 2.2216 - val_acc: 0.4395\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0546 - acc: 0.4555 - val_loss: 2.2134 - val_acc: 0.4423\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0475 - acc: 0.4570 - val_loss: 2.2216 - val_acc: 0.4414\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0454 - acc: 0.4603 - val_loss: 2.2276 - val_acc: 0.4381\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0490 - acc: 0.4561 - val_loss: 2.2172 - val_acc: 0.4396\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0418 - acc: 0.4566 - val_loss: 2.2088 - val_acc: 0.4398\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0415 - acc: 0.4558 - val_loss: 2.2129 - val_acc: 0.4413\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0372 - acc: 0.4612 - val_loss: 2.2165 - val_acc: 0.4422\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0413 - acc: 0.4575 - val_loss: 2.2236 - val_acc: 0.4377\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0368 - acc: 0.4595 - val_loss: 2.2269 - val_acc: 0.4361\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0336 - acc: 0.4605 - val_loss: 2.2154 - val_acc: 0.4395\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0268 - acc: 0.4616 - val_loss: 2.2116 - val_acc: 0.4400\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0315 - acc: 0.4598 - val_loss: 2.2212 - val_acc: 0.4405\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0309 - acc: 0.4600 - val_loss: 2.2209 - val_acc: 0.4395\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0278 - acc: 0.4612 - val_loss: 2.2279 - val_acc: 0.4386\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0301 - acc: 0.4614 - val_loss: 2.2095 - val_acc: 0.4396\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0235 - acc: 0.4625 - val_loss: 2.2347 - val_acc: 0.4389\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0340 - acc: 0.4586 - val_loss: 2.2165 - val_acc: 0.4395\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0245 - acc: 0.4623 - val_loss: 2.2136 - val_acc: 0.4421\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0183 - acc: 0.4626 - val_loss: 2.2204 - val_acc: 0.4399\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0126 - acc: 0.4636 - val_loss: 2.2132 - val_acc: 0.4429\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0096 - acc: 0.4668 - val_loss: 2.2155 - val_acc: 0.4413\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0254 - acc: 0.4620 - val_loss: 2.2228 - val_acc: 0.4411\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0158 - acc: 0.4648 - val_loss: 2.2148 - val_acc: 0.4395\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0137 - acc: 0.4634 - val_loss: 2.2212 - val_acc: 0.4416\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0107 - acc: 0.4647 - val_loss: 2.2152 - val_acc: 0.4393\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0088 - acc: 0.4654 - val_loss: 2.2179 - val_acc: 0.4424\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0055 - acc: 0.4665 - val_loss: 2.2334 - val_acc: 0.4381\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0298 - acc: 0.4624 - val_loss: 2.2351 - val_acc: 0.4397\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0072 - acc: 0.4653 - val_loss: 2.2108 - val_acc: 0.4425\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9985 - acc: 0.4678 - val_loss: 2.2124 - val_acc: 0.4411\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0008 - acc: 0.4661 - val_loss: 2.2040 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9955 - acc: 0.4682 - val_loss: 2.2276 - val_acc: 0.4375\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9906 - acc: 0.4694 - val_loss: 2.2159 - val_acc: 0.4406\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9934 - acc: 0.4689 - val_loss: 2.2241 - val_acc: 0.4372\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0013 - acc: 0.4657 - val_loss: 2.2158 - val_acc: 0.4412\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9904 - acc: 0.4697 - val_loss: 2.2274 - val_acc: 0.4397\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9980 - acc: 0.4684 - val_loss: 2.2117 - val_acc: 0.4407\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9846 - acc: 0.4703 - val_loss: 2.2099 - val_acc: 0.4427\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9884 - acc: 0.4683 - val_loss: 2.2142 - val_acc: 0.4396\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9902 - acc: 0.4688 - val_loss: 2.2283 - val_acc: 0.4376\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9872 - acc: 0.4688 - val_loss: 2.2147 - val_acc: 0.4406\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9802 - acc: 0.4731 - val_loss: 2.2215 - val_acc: 0.4408\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9806 - acc: 0.4727 - val_loss: 2.2301 - val_acc: 0.4369\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9867 - acc: 0.4723 - val_loss: 2.2378 - val_acc: 0.4339\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9858 - acc: 0.4711 - val_loss: 2.2239 - val_acc: 0.4385\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9784 - acc: 0.4737 - val_loss: 2.2255 - val_acc: 0.4392\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9745 - acc: 0.4749 - val_loss: 2.2215 - val_acc: 0.4397\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9733 - acc: 0.4747 - val_loss: 2.2337 - val_acc: 0.4349\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9748 - acc: 0.4745 - val_loss: 2.2251 - val_acc: 0.4374\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9771 - acc: 0.4739 - val_loss: 2.2170 - val_acc: 0.4388\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9642 - acc: 0.4766 - val_loss: 2.2158 - val_acc: 0.4407\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9621 - acc: 0.4776 - val_loss: 2.2355 - val_acc: 0.4378\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9689 - acc: 0.4739 - val_loss: 2.2300 - val_acc: 0.4354\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9720 - acc: 0.4734 - val_loss: 2.2365 - val_acc: 0.4372\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9659 - acc: 0.4755 - val_loss: 2.2253 - val_acc: 0.4381\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9780 - acc: 0.4722 - val_loss: 2.2178 - val_acc: 0.4418\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9661 - acc: 0.4748 - val_loss: 2.2054 - val_acc: 0.4427\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9561 - acc: 0.4771 - val_loss: 2.2115 - val_acc: 0.4417\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9600 - acc: 0.4765 - val_loss: 2.2226 - val_acc: 0.4390\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9576 - acc: 0.4773 - val_loss: 2.2261 - val_acc: 0.4376\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9607 - acc: 0.4753 - val_loss: 2.2218 - val_acc: 0.4402\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9558 - acc: 0.4772 - val_loss: 2.2176 - val_acc: 0.4418\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9511 - acc: 0.4763 - val_loss: 2.2297 - val_acc: 0.4396\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9985 - acc: 0.4668 - val_loss: 2.2310 - val_acc: 0.4364\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9650 - acc: 0.4751 - val_loss: 2.2206 - val_acc: 0.4374\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9566 - acc: 0.4775 - val_loss: 2.2348 - val_acc: 0.4389\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9538 - acc: 0.4788 - val_loss: 2.2259 - val_acc: 0.4389\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9612 - acc: 0.4750 - val_loss: 2.2324 - val_acc: 0.4369\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9497 - acc: 0.4784 - val_loss: 2.2224 - val_acc: 0.4384\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9470 - acc: 0.4801 - val_loss: 2.2206 - val_acc: 0.4401\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1845 - acc: 0.4315 - val_loss: 2.1519 - val_acc: 0.4456\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1612 - acc: 0.4365 - val_loss: 2.1668 - val_acc: 0.4423\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1536 - acc: 0.4385 - val_loss: 2.1648 - val_acc: 0.4433\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1403 - acc: 0.4402 - val_loss: 2.1717 - val_acc: 0.4400\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1291 - acc: 0.4445 - val_loss: 2.1894 - val_acc: 0.4371\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1244 - acc: 0.4446 - val_loss: 2.1717 - val_acc: 0.4410\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1190 - acc: 0.4442 - val_loss: 2.1910 - val_acc: 0.4404\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1106 - acc: 0.4473 - val_loss: 2.1933 - val_acc: 0.4379\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1424 - acc: 0.4421 - val_loss: 2.2129 - val_acc: 0.4339\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1153 - acc: 0.4453 - val_loss: 2.2015 - val_acc: 0.4371\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0953 - acc: 0.4508 - val_loss: 2.1922 - val_acc: 0.4363\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0936 - acc: 0.4497 - val_loss: 2.2082 - val_acc: 0.4322\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0856 - acc: 0.4515 - val_loss: 2.1937 - val_acc: 0.4356\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0880 - acc: 0.4518 - val_loss: 2.2067 - val_acc: 0.4334\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0796 - acc: 0.4530 - val_loss: 2.2063 - val_acc: 0.4352\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0745 - acc: 0.4551 - val_loss: 2.1977 - val_acc: 0.4369\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0747 - acc: 0.4562 - val_loss: 2.2115 - val_acc: 0.4327\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0669 - acc: 0.4549 - val_loss: 2.1912 - val_acc: 0.4366\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0682 - acc: 0.4548 - val_loss: 2.2021 - val_acc: 0.4348\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0636 - acc: 0.4559 - val_loss: 2.1967 - val_acc: 0.4338\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0617 - acc: 0.4566 - val_loss: 2.2119 - val_acc: 0.4331\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0597 - acc: 0.4595 - val_loss: 2.1869 - val_acc: 0.4378\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0544 - acc: 0.4592 - val_loss: 2.1926 - val_acc: 0.4346\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0548 - acc: 0.4590 - val_loss: 2.2099 - val_acc: 0.4333\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0583 - acc: 0.4561 - val_loss: 2.1957 - val_acc: 0.4341\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0540 - acc: 0.4589 - val_loss: 2.1983 - val_acc: 0.4359\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0450 - acc: 0.4620 - val_loss: 2.1870 - val_acc: 0.4353\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0434 - acc: 0.4608 - val_loss: 2.1947 - val_acc: 0.4379\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0415 - acc: 0.4609 - val_loss: 2.1881 - val_acc: 0.4371\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0367 - acc: 0.4617 - val_loss: 2.1802 - val_acc: 0.4389\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0325 - acc: 0.4631 - val_loss: 2.2106 - val_acc: 0.4365\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0372 - acc: 0.4635 - val_loss: 2.2174 - val_acc: 0.4309\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0424 - acc: 0.4607 - val_loss: 2.2130 - val_acc: 0.4345\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0424 - acc: 0.4622 - val_loss: 2.2143 - val_acc: 0.4326\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0556 - acc: 0.4573 - val_loss: 2.2100 - val_acc: 0.4340\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0309 - acc: 0.4644 - val_loss: 2.1813 - val_acc: 0.4382\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0220 - acc: 0.4662 - val_loss: 2.1954 - val_acc: 0.4362\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0205 - acc: 0.4662 - val_loss: 2.1882 - val_acc: 0.4356\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0238 - acc: 0.4667 - val_loss: 2.1956 - val_acc: 0.4335\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0315 - acc: 0.4638 - val_loss: 2.1804 - val_acc: 0.4351\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0113 - acc: 0.4676 - val_loss: 2.1772 - val_acc: 0.4364\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0081 - acc: 0.4671 - val_loss: 2.1861 - val_acc: 0.4362\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0056 - acc: 0.4684 - val_loss: 2.1776 - val_acc: 0.4385\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0036 - acc: 0.4694 - val_loss: 2.1777 - val_acc: 0.4390\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0085 - acc: 0.4700 - val_loss: 2.1849 - val_acc: 0.4377\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0076 - acc: 0.4708 - val_loss: 2.1793 - val_acc: 0.4366\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9987 - acc: 0.4728 - val_loss: 2.1779 - val_acc: 0.4384\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0011 - acc: 0.4723 - val_loss: 2.1714 - val_acc: 0.4381\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0041 - acc: 0.4710 - val_loss: 2.1777 - val_acc: 0.4390\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9958 - acc: 0.4728 - val_loss: 2.1725 - val_acc: 0.4383\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9967 - acc: 0.4723 - val_loss: 2.1899 - val_acc: 0.4357\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9994 - acc: 0.4730 - val_loss: 2.1721 - val_acc: 0.4385\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9957 - acc: 0.4714 - val_loss: 2.2228 - val_acc: 0.4299\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0059 - acc: 0.4703 - val_loss: 2.1769 - val_acc: 0.4359\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9917 - acc: 0.4726 - val_loss: 2.1929 - val_acc: 0.4348\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9876 - acc: 0.4735 - val_loss: 2.1798 - val_acc: 0.4365\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9871 - acc: 0.4729 - val_loss: 2.2012 - val_acc: 0.4336\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9946 - acc: 0.4721 - val_loss: 2.1965 - val_acc: 0.4330\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9963 - acc: 0.4713 - val_loss: 2.2129 - val_acc: 0.4335\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9928 - acc: 0.4720 - val_loss: 2.1933 - val_acc: 0.4325\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9872 - acc: 0.4746 - val_loss: 2.1995 - val_acc: 0.4357\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9871 - acc: 0.4753 - val_loss: 2.1851 - val_acc: 0.4341\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9727 - acc: 0.4773 - val_loss: 2.1730 - val_acc: 0.4361\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9709 - acc: 0.4788 - val_loss: 2.2138 - val_acc: 0.4314\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9845 - acc: 0.4749 - val_loss: 2.2025 - val_acc: 0.4353\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9866 - acc: 0.4743 - val_loss: 2.1806 - val_acc: 0.4364\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9790 - acc: 0.4758 - val_loss: 2.1799 - val_acc: 0.4372\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9734 - acc: 0.4769 - val_loss: 2.1712 - val_acc: 0.4358\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9608 - acc: 0.4789 - val_loss: 2.1682 - val_acc: 0.4378\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9618 - acc: 0.4791 - val_loss: 2.1675 - val_acc: 0.4375\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9672 - acc: 0.4788 - val_loss: 2.2049 - val_acc: 0.4349\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9857 - acc: 0.4753 - val_loss: 2.1817 - val_acc: 0.4356\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9750 - acc: 0.4765 - val_loss: 2.1882 - val_acc: 0.4347\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9635 - acc: 0.4795 - val_loss: 2.1738 - val_acc: 0.4367\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9590 - acc: 0.4820 - val_loss: 2.1836 - val_acc: 0.4359\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9547 - acc: 0.4808 - val_loss: 2.1763 - val_acc: 0.4389\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9536 - acc: 0.4802 - val_loss: 2.1859 - val_acc: 0.4354\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9558 - acc: 0.4832 - val_loss: 2.1787 - val_acc: 0.4344\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9484 - acc: 0.4840 - val_loss: 2.1814 - val_acc: 0.4378\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9497 - acc: 0.4807 - val_loss: 2.1707 - val_acc: 0.4374\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9495 - acc: 0.4822 - val_loss: 2.1885 - val_acc: 0.4356\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9576 - acc: 0.4815 - val_loss: 2.1788 - val_acc: 0.4368\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9508 - acc: 0.4801 - val_loss: 2.1859 - val_acc: 0.4358\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9564 - acc: 0.4831 - val_loss: 2.1775 - val_acc: 0.4365\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9481 - acc: 0.4836 - val_loss: 2.2135 - val_acc: 0.4335\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9513 - acc: 0.4814 - val_loss: 2.1821 - val_acc: 0.4354\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9572 - acc: 0.4815 - val_loss: 2.1960 - val_acc: 0.4361\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9456 - acc: 0.4851 - val_loss: 2.1992 - val_acc: 0.4316\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9420 - acc: 0.4824 - val_loss: 2.1920 - val_acc: 0.4360\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9569 - acc: 0.4801 - val_loss: 2.1915 - val_acc: 0.4349\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9390 - acc: 0.4839 - val_loss: 2.1825 - val_acc: 0.4365\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9395 - acc: 0.4855 - val_loss: 2.2308 - val_acc: 0.4276\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9478 - acc: 0.4823 - val_loss: 2.1823 - val_acc: 0.4361\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9448 - acc: 0.4824 - val_loss: 2.1873 - val_acc: 0.4345\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9453 - acc: 0.4838 - val_loss: 2.1813 - val_acc: 0.4378\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9300 - acc: 0.4865 - val_loss: 2.2046 - val_acc: 0.4324\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9550 - acc: 0.4822 - val_loss: 2.1979 - val_acc: 0.4348\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9316 - acc: 0.4868 - val_loss: 2.1841 - val_acc: 0.4370\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9239 - acc: 0.4883 - val_loss: 2.1689 - val_acc: 0.4401\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9275 - acc: 0.4883 - val_loss: 2.1951 - val_acc: 0.4359\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1727 - acc: 0.4311 - val_loss: 2.2135 - val_acc: 0.4392\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1538 - acc: 0.4362 - val_loss: 2.2068 - val_acc: 0.4427\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1420 - acc: 0.4361 - val_loss: 2.2071 - val_acc: 0.4413\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1257 - acc: 0.4404 - val_loss: 2.2294 - val_acc: 0.4418\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1220 - acc: 0.4400 - val_loss: 2.2194 - val_acc: 0.4403\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1161 - acc: 0.4422 - val_loss: 2.2372 - val_acc: 0.4376\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1039 - acc: 0.4424 - val_loss: 2.2379 - val_acc: 0.4393\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1297 - acc: 0.4393 - val_loss: 2.2604 - val_acc: 0.4312\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1074 - acc: 0.4446 - val_loss: 2.2380 - val_acc: 0.4389\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0856 - acc: 0.4490 - val_loss: 2.2272 - val_acc: 0.4392\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0840 - acc: 0.4466 - val_loss: 2.2288 - val_acc: 0.4381\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1100 - acc: 0.4456 - val_loss: 2.2433 - val_acc: 0.4324\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0920 - acc: 0.4475 - val_loss: 2.2363 - val_acc: 0.4364\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0726 - acc: 0.4499 - val_loss: 2.2217 - val_acc: 0.4407\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0664 - acc: 0.4511 - val_loss: 2.2164 - val_acc: 0.4404\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0653 - acc: 0.4521 - val_loss: 2.2197 - val_acc: 0.4403\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0586 - acc: 0.4527 - val_loss: 2.2256 - val_acc: 0.4401\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0590 - acc: 0.4547 - val_loss: 2.2236 - val_acc: 0.4402\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0519 - acc: 0.4543 - val_loss: 2.2207 - val_acc: 0.4399\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0504 - acc: 0.4554 - val_loss: 2.2206 - val_acc: 0.4402\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0425 - acc: 0.4582 - val_loss: 2.2538 - val_acc: 0.4354\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0465 - acc: 0.4571 - val_loss: 2.2228 - val_acc: 0.4398\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0437 - acc: 0.4574 - val_loss: 2.2270 - val_acc: 0.4379\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0463 - acc: 0.4563 - val_loss: 2.2194 - val_acc: 0.4400\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0377 - acc: 0.4571 - val_loss: 2.2178 - val_acc: 0.4410\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0316 - acc: 0.4590 - val_loss: 2.2030 - val_acc: 0.4425\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0303 - acc: 0.4592 - val_loss: 2.2142 - val_acc: 0.4403\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0298 - acc: 0.4623 - val_loss: 2.2163 - val_acc: 0.4422\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0289 - acc: 0.4595 - val_loss: 2.2154 - val_acc: 0.4421\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0235 - acc: 0.4611 - val_loss: 2.2183 - val_acc: 0.4399\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0290 - acc: 0.4599 - val_loss: 2.2186 - val_acc: 0.4406\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0212 - acc: 0.4636 - val_loss: 2.2056 - val_acc: 0.4428\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0191 - acc: 0.4637 - val_loss: 2.2063 - val_acc: 0.4404\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0159 - acc: 0.4635 - val_loss: 2.2153 - val_acc: 0.4428\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0150 - acc: 0.4644 - val_loss: 2.2147 - val_acc: 0.4421\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0147 - acc: 0.4621 - val_loss: 2.2102 - val_acc: 0.4407\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0079 - acc: 0.4651 - val_loss: 2.2042 - val_acc: 0.4442\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0043 - acc: 0.4650 - val_loss: 2.2514 - val_acc: 0.4339\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0206 - acc: 0.4624 - val_loss: 2.1992 - val_acc: 0.4432\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0008 - acc: 0.4657 - val_loss: 2.1968 - val_acc: 0.4444\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9937 - acc: 0.4693 - val_loss: 2.2172 - val_acc: 0.4410\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9993 - acc: 0.4687 - val_loss: 2.2033 - val_acc: 0.4448\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9905 - acc: 0.4691 - val_loss: 2.1969 - val_acc: 0.4436\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9929 - acc: 0.4685 - val_loss: 2.2175 - val_acc: 0.4423\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9896 - acc: 0.4695 - val_loss: 2.2199 - val_acc: 0.4413\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9912 - acc: 0.4673 - val_loss: 2.2119 - val_acc: 0.4430\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9964 - acc: 0.4692 - val_loss: 2.1995 - val_acc: 0.4433\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9891 - acc: 0.4698 - val_loss: 2.2089 - val_acc: 0.4433\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9850 - acc: 0.4707 - val_loss: 2.2020 - val_acc: 0.4443\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0149 - acc: 0.4635 - val_loss: 2.2087 - val_acc: 0.4410\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9872 - acc: 0.4704 - val_loss: 2.2004 - val_acc: 0.4441\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9763 - acc: 0.4721 - val_loss: 2.2057 - val_acc: 0.4431\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9758 - acc: 0.4710 - val_loss: 2.1983 - val_acc: 0.4438\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9773 - acc: 0.4700 - val_loss: 2.2033 - val_acc: 0.4433\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9704 - acc: 0.4725 - val_loss: 2.2228 - val_acc: 0.4413\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9769 - acc: 0.4727 - val_loss: 2.1967 - val_acc: 0.4438\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9681 - acc: 0.4747 - val_loss: 2.2061 - val_acc: 0.4436\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9688 - acc: 0.4730 - val_loss: 2.2124 - val_acc: 0.4408\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9736 - acc: 0.4733 - val_loss: 2.2042 - val_acc: 0.4420\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9754 - acc: 0.4735 - val_loss: 2.1986 - val_acc: 0.4439\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9708 - acc: 0.4728 - val_loss: 2.2000 - val_acc: 0.4437\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9677 - acc: 0.4743 - val_loss: 2.2126 - val_acc: 0.4395\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9702 - acc: 0.4726 - val_loss: 2.2217 - val_acc: 0.4415\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9649 - acc: 0.4747 - val_loss: 2.1987 - val_acc: 0.4429\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9604 - acc: 0.4763 - val_loss: 2.2390 - val_acc: 0.4379\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9657 - acc: 0.4744 - val_loss: 2.2112 - val_acc: 0.4411\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9617 - acc: 0.4738 - val_loss: 2.2399 - val_acc: 0.4385\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9588 - acc: 0.4752 - val_loss: 2.1984 - val_acc: 0.4443\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9482 - acc: 0.4787 - val_loss: 2.2254 - val_acc: 0.4390\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9502 - acc: 0.4788 - val_loss: 2.2182 - val_acc: 0.4428\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9850 - acc: 0.4708 - val_loss: 2.2435 - val_acc: 0.4363\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9641 - acc: 0.4754 - val_loss: 2.1990 - val_acc: 0.4441\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9411 - acc: 0.4806 - val_loss: 2.2393 - val_acc: 0.4368\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9479 - acc: 0.4797 - val_loss: 2.1912 - val_acc: 0.4442\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9432 - acc: 0.4808 - val_loss: 2.1981 - val_acc: 0.4431\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9392 - acc: 0.4798 - val_loss: 2.2043 - val_acc: 0.4418\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9409 - acc: 0.4812 - val_loss: 2.1993 - val_acc: 0.4437\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9374 - acc: 0.4820 - val_loss: 2.2058 - val_acc: 0.4429\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9311 - acc: 0.4833 - val_loss: 2.2343 - val_acc: 0.4403\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9420 - acc: 0.4808 - val_loss: 2.2027 - val_acc: 0.4431\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9323 - acc: 0.4829 - val_loss: 2.2137 - val_acc: 0.4408\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9319 - acc: 0.4830 - val_loss: 2.2082 - val_acc: 0.4423\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9450 - acc: 0.4808 - val_loss: 2.2316 - val_acc: 0.4392\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9527 - acc: 0.4784 - val_loss: 2.2470 - val_acc: 0.4336\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9723 - acc: 0.4743 - val_loss: 2.2438 - val_acc: 0.4353\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9461 - acc: 0.4802 - val_loss: 2.2187 - val_acc: 0.4441\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9783 - acc: 0.4745 - val_loss: 2.2426 - val_acc: 0.4347\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9540 - acc: 0.4766 - val_loss: 2.2071 - val_acc: 0.4421\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9290 - acc: 0.4832 - val_loss: 2.2301 - val_acc: 0.4380\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9285 - acc: 0.4834 - val_loss: 2.2104 - val_acc: 0.4434\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9275 - acc: 0.4825 - val_loss: 2.2319 - val_acc: 0.4380\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9273 - acc: 0.4830 - val_loss: 2.1989 - val_acc: 0.4437\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9145 - acc: 0.4869 - val_loss: 2.2037 - val_acc: 0.4419\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9148 - acc: 0.4876 - val_loss: 2.1992 - val_acc: 0.4431\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9088 - acc: 0.4884 - val_loss: 2.2077 - val_acc: 0.4423\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9127 - acc: 0.4874 - val_loss: 2.2073 - val_acc: 0.4433\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9181 - acc: 0.4874 - val_loss: 2.2461 - val_acc: 0.4349\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9218 - acc: 0.4840 - val_loss: 2.2106 - val_acc: 0.4422\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9217 - acc: 0.4834 - val_loss: 2.2274 - val_acc: 0.4395\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9241 - acc: 0.4835 - val_loss: 2.2108 - val_acc: 0.4422\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1733 - acc: 0.4320 - val_loss: 2.1897 - val_acc: 0.4386\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1517 - acc: 0.4356 - val_loss: 2.1945 - val_acc: 0.4386\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1413 - acc: 0.4362 - val_loss: 2.1955 - val_acc: 0.4378\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1289 - acc: 0.4385 - val_loss: 2.2002 - val_acc: 0.4362\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1248 - acc: 0.4394 - val_loss: 2.2015 - val_acc: 0.4330\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1112 - acc: 0.4408 - val_loss: 2.1968 - val_acc: 0.4377\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1090 - acc: 0.4454 - val_loss: 2.2136 - val_acc: 0.4331\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1051 - acc: 0.4435 - val_loss: 2.2073 - val_acc: 0.4336\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0985 - acc: 0.4445 - val_loss: 2.2127 - val_acc: 0.4332\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0927 - acc: 0.4455 - val_loss: 2.2071 - val_acc: 0.4347\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0909 - acc: 0.4468 - val_loss: 2.2177 - val_acc: 0.4335\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0860 - acc: 0.4492 - val_loss: 2.2136 - val_acc: 0.4312\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0787 - acc: 0.4493 - val_loss: 2.2134 - val_acc: 0.4336\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0769 - acc: 0.4508 - val_loss: 2.2115 - val_acc: 0.4320\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0735 - acc: 0.4515 - val_loss: 2.2019 - val_acc: 0.4355\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0724 - acc: 0.4515 - val_loss: 2.2042 - val_acc: 0.4345\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0636 - acc: 0.4519 - val_loss: 2.2055 - val_acc: 0.4368\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0592 - acc: 0.4530 - val_loss: 2.2022 - val_acc: 0.4362\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0550 - acc: 0.4551 - val_loss: 2.2076 - val_acc: 0.4313\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0521 - acc: 0.4552 - val_loss: 2.2107 - val_acc: 0.4339\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0513 - acc: 0.4556 - val_loss: 2.2266 - val_acc: 0.4275\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0492 - acc: 0.4554 - val_loss: 2.2201 - val_acc: 0.4319\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0500 - acc: 0.4562 - val_loss: 2.2120 - val_acc: 0.4296\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0451 - acc: 0.4572 - val_loss: 2.2178 - val_acc: 0.4333\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0393 - acc: 0.4573 - val_loss: 2.2181 - val_acc: 0.4318\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0384 - acc: 0.4579 - val_loss: 2.2200 - val_acc: 0.4308\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0345 - acc: 0.4594 - val_loss: 2.2155 - val_acc: 0.4311\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0301 - acc: 0.4587 - val_loss: 2.2036 - val_acc: 0.4353\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0272 - acc: 0.4611 - val_loss: 2.2025 - val_acc: 0.4335\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0235 - acc: 0.4607 - val_loss: 2.2160 - val_acc: 0.4333\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0243 - acc: 0.4614 - val_loss: 2.2000 - val_acc: 0.4341\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0226 - acc: 0.4626 - val_loss: 2.2268 - val_acc: 0.4331\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0476 - acc: 0.4580 - val_loss: 2.2076 - val_acc: 0.4352\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0218 - acc: 0.4611 - val_loss: 2.2037 - val_acc: 0.4335\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0167 - acc: 0.4628 - val_loss: 2.2140 - val_acc: 0.4326\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0159 - acc: 0.4628 - val_loss: 2.2208 - val_acc: 0.4329\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0159 - acc: 0.4627 - val_loss: 2.2015 - val_acc: 0.4350\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0118 - acc: 0.4643 - val_loss: 2.2118 - val_acc: 0.4341\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0072 - acc: 0.4663 - val_loss: 2.2000 - val_acc: 0.4341\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0077 - acc: 0.4663 - val_loss: 2.2116 - val_acc: 0.4308\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0048 - acc: 0.4648 - val_loss: 2.1925 - val_acc: 0.4384\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9979 - acc: 0.4679 - val_loss: 2.2070 - val_acc: 0.4338\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9977 - acc: 0.4681 - val_loss: 2.1992 - val_acc: 0.4382\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9997 - acc: 0.4675 - val_loss: 2.2150 - val_acc: 0.4305\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9977 - acc: 0.4662 - val_loss: 2.2070 - val_acc: 0.4349\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9943 - acc: 0.4682 - val_loss: 2.1929 - val_acc: 0.4372\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9872 - acc: 0.4722 - val_loss: 2.1913 - val_acc: 0.4352\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9867 - acc: 0.4711 - val_loss: 2.1963 - val_acc: 0.4360\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9861 - acc: 0.4713 - val_loss: 2.1915 - val_acc: 0.4370\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9800 - acc: 0.4722 - val_loss: 2.2118 - val_acc: 0.4318\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9883 - acc: 0.4704 - val_loss: 2.2019 - val_acc: 0.4333\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9800 - acc: 0.4708 - val_loss: 2.1923 - val_acc: 0.4370\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9804 - acc: 0.4717 - val_loss: 2.1913 - val_acc: 0.4379\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9842 - acc: 0.4690 - val_loss: 2.1991 - val_acc: 0.4349\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9769 - acc: 0.4724 - val_loss: 2.1929 - val_acc: 0.4385\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9755 - acc: 0.4731 - val_loss: 2.2113 - val_acc: 0.4319\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9762 - acc: 0.4731 - val_loss: 2.1910 - val_acc: 0.4376\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9722 - acc: 0.4741 - val_loss: 2.1992 - val_acc: 0.4349\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9685 - acc: 0.4749 - val_loss: 2.1946 - val_acc: 0.4375\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9714 - acc: 0.4731 - val_loss: 2.2051 - val_acc: 0.4345\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9684 - acc: 0.4743 - val_loss: 2.1858 - val_acc: 0.4379\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9636 - acc: 0.4743 - val_loss: 2.2096 - val_acc: 0.4353\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9703 - acc: 0.4732 - val_loss: 2.2004 - val_acc: 0.4362\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9681 - acc: 0.4734 - val_loss: 2.2005 - val_acc: 0.4369\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9689 - acc: 0.4744 - val_loss: 2.2458 - val_acc: 0.4290\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9765 - acc: 0.4724 - val_loss: 2.1966 - val_acc: 0.4372\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9649 - acc: 0.4755 - val_loss: 2.2010 - val_acc: 0.4346\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9587 - acc: 0.4760 - val_loss: 2.1961 - val_acc: 0.4374\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9567 - acc: 0.4777 - val_loss: 2.1915 - val_acc: 0.4355\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9589 - acc: 0.4760 - val_loss: 2.1925 - val_acc: 0.4373\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9682 - acc: 0.4764 - val_loss: 2.2069 - val_acc: 0.4340\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9575 - acc: 0.4767 - val_loss: 2.1963 - val_acc: 0.4385\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9533 - acc: 0.4774 - val_loss: 2.2087 - val_acc: 0.4336\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9534 - acc: 0.4800 - val_loss: 2.1947 - val_acc: 0.4375\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9442 - acc: 0.4799 - val_loss: 2.1852 - val_acc: 0.4384\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9470 - acc: 0.4794 - val_loss: 2.1889 - val_acc: 0.4406\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9458 - acc: 0.4793 - val_loss: 2.1963 - val_acc: 0.4374\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9482 - acc: 0.4787 - val_loss: 2.2128 - val_acc: 0.4354\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9551 - acc: 0.4776 - val_loss: 2.2124 - val_acc: 0.4339\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9548 - acc: 0.4772 - val_loss: 2.2184 - val_acc: 0.4335\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9500 - acc: 0.4797 - val_loss: 2.2018 - val_acc: 0.4377\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9405 - acc: 0.4801 - val_loss: 2.1957 - val_acc: 0.4377\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9378 - acc: 0.4815 - val_loss: 2.1986 - val_acc: 0.4380\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9379 - acc: 0.4796 - val_loss: 2.2038 - val_acc: 0.4331\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9371 - acc: 0.4840 - val_loss: 2.1936 - val_acc: 0.4366\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9308 - acc: 0.4831 - val_loss: 2.2055 - val_acc: 0.4352\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9347 - acc: 0.4816 - val_loss: 2.2097 - val_acc: 0.4338\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9348 - acc: 0.4811 - val_loss: 2.1995 - val_acc: 0.4369\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9271 - acc: 0.4845 - val_loss: 2.2103 - val_acc: 0.4356\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9335 - acc: 0.4821 - val_loss: 2.1998 - val_acc: 0.4382\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9405 - acc: 0.4819 - val_loss: 2.1923 - val_acc: 0.4357\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9306 - acc: 0.4811 - val_loss: 2.1936 - val_acc: 0.4381\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9227 - acc: 0.4843 - val_loss: 2.1906 - val_acc: 0.4405\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9152 - acc: 0.4849 - val_loss: 2.1912 - val_acc: 0.4393\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9107 - acc: 0.4888 - val_loss: 2.1997 - val_acc: 0.4391\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9224 - acc: 0.4844 - val_loss: 2.1880 - val_acc: 0.4397\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9295 - acc: 0.4846 - val_loss: 2.2327 - val_acc: 0.4332\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9314 - acc: 0.4820 - val_loss: 2.2073 - val_acc: 0.4376\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9261 - acc: 0.4837 - val_loss: 2.2334 - val_acc: 0.4313\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9279 - acc: 0.4827 - val_loss: 2.1902 - val_acc: 0.4379\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2197 - acc: 0.4230 - val_loss: 2.2144 - val_acc: 0.4375\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1869 - acc: 0.4293 - val_loss: 2.2098 - val_acc: 0.4391\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1742 - acc: 0.4298 - val_loss: 2.2067 - val_acc: 0.4382\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1685 - acc: 0.4328 - val_loss: 2.2179 - val_acc: 0.4378\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1606 - acc: 0.4326 - val_loss: 2.2109 - val_acc: 0.4378\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1509 - acc: 0.4360 - val_loss: 2.2158 - val_acc: 0.4384\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1495 - acc: 0.4358 - val_loss: 2.2152 - val_acc: 0.4387\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1435 - acc: 0.4379 - val_loss: 2.2310 - val_acc: 0.4327\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1361 - acc: 0.4383 - val_loss: 2.2199 - val_acc: 0.4363\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1416 - acc: 0.4391 - val_loss: 2.2260 - val_acc: 0.4332\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1265 - acc: 0.4398 - val_loss: 2.2346 - val_acc: 0.4370\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1480 - acc: 0.4371 - val_loss: 2.2428 - val_acc: 0.4307\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1256 - acc: 0.4417 - val_loss: 2.2277 - val_acc: 0.4361\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1168 - acc: 0.4414 - val_loss: 2.2328 - val_acc: 0.4330\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1115 - acc: 0.4415 - val_loss: 2.2322 - val_acc: 0.4348\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1090 - acc: 0.4430 - val_loss: 2.2334 - val_acc: 0.4325\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0969 - acc: 0.4462 - val_loss: 2.2398 - val_acc: 0.4324\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0999 - acc: 0.4451 - val_loss: 2.2191 - val_acc: 0.4342\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0905 - acc: 0.4469 - val_loss: 2.2231 - val_acc: 0.4362\n",
      "Epoch 20/100\n",
      "2s - loss: 2.0881 - acc: 0.4482 - val_loss: 2.2262 - val_acc: 0.4357\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0920 - acc: 0.4484 - val_loss: 2.2316 - val_acc: 0.4354\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0906 - acc: 0.4472 - val_loss: 2.2338 - val_acc: 0.4333\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0915 - acc: 0.4483 - val_loss: 2.2463 - val_acc: 0.4317\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0975 - acc: 0.4462 - val_loss: 2.2834 - val_acc: 0.4261\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1337 - acc: 0.4392 - val_loss: 2.2338 - val_acc: 0.4362\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0863 - acc: 0.4506 - val_loss: 2.2335 - val_acc: 0.4321\n",
      "Epoch 27/100\n",
      "2s - loss: 2.0791 - acc: 0.4498 - val_loss: 2.2456 - val_acc: 0.4335\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0814 - acc: 0.4508 - val_loss: 2.2331 - val_acc: 0.4365\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0921 - acc: 0.4476 - val_loss: 2.2261 - val_acc: 0.4343\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0728 - acc: 0.4502 - val_loss: 2.2282 - val_acc: 0.4339\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0691 - acc: 0.4526 - val_loss: 2.2617 - val_acc: 0.4295\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0812 - acc: 0.4512 - val_loss: 2.2235 - val_acc: 0.4357\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0600 - acc: 0.4528 - val_loss: 2.2273 - val_acc: 0.4361\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0597 - acc: 0.4561 - val_loss: 2.2248 - val_acc: 0.4364\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0621 - acc: 0.4541 - val_loss: 2.2166 - val_acc: 0.4371\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0610 - acc: 0.4526 - val_loss: 2.2403 - val_acc: 0.4352\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0848 - acc: 0.4502 - val_loss: 2.2306 - val_acc: 0.4344\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0619 - acc: 0.4525 - val_loss: 2.2293 - val_acc: 0.4337\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0685 - acc: 0.4524 - val_loss: 2.2289 - val_acc: 0.4374\n",
      "Epoch 40/100\n",
      "2s - loss: 2.0657 - acc: 0.4532 - val_loss: 2.2433 - val_acc: 0.4351\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0551 - acc: 0.4551 - val_loss: 2.2193 - val_acc: 0.4367\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0508 - acc: 0.4556 - val_loss: 2.2295 - val_acc: 0.4360\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0483 - acc: 0.4556 - val_loss: 2.2630 - val_acc: 0.4295\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0640 - acc: 0.4536 - val_loss: 2.2525 - val_acc: 0.4325\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0717 - acc: 0.4531 - val_loss: 2.2295 - val_acc: 0.4357\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0514 - acc: 0.4567 - val_loss: 2.2332 - val_acc: 0.4313\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0464 - acc: 0.4565 - val_loss: 2.2204 - val_acc: 0.4367\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0383 - acc: 0.4582 - val_loss: 2.2229 - val_acc: 0.4352\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0306 - acc: 0.4611 - val_loss: 2.2188 - val_acc: 0.4372\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0327 - acc: 0.4592 - val_loss: 2.2166 - val_acc: 0.4363\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0203 - acc: 0.4633 - val_loss: 2.2470 - val_acc: 0.4298\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0294 - acc: 0.4615 - val_loss: 2.2121 - val_acc: 0.4376\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0175 - acc: 0.4630 - val_loss: 2.2283 - val_acc: 0.4360\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0223 - acc: 0.4642 - val_loss: 2.2177 - val_acc: 0.4348\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0250 - acc: 0.4601 - val_loss: 2.2524 - val_acc: 0.4313\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0318 - acc: 0.4611 - val_loss: 2.2360 - val_acc: 0.4341\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0256 - acc: 0.4625 - val_loss: 2.2427 - val_acc: 0.4363\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0522 - acc: 0.4569 - val_loss: 2.2239 - val_acc: 0.4369\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0231 - acc: 0.4632 - val_loss: 2.2437 - val_acc: 0.4350\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0354 - acc: 0.4607 - val_loss: 2.2289 - val_acc: 0.4369\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0200 - acc: 0.4640 - val_loss: 2.2273 - val_acc: 0.4336\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0179 - acc: 0.4633 - val_loss: 2.2350 - val_acc: 0.4355\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0118 - acc: 0.4650 - val_loss: 2.2183 - val_acc: 0.4371\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0146 - acc: 0.4636 - val_loss: 2.2126 - val_acc: 0.4360\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0057 - acc: 0.4665 - val_loss: 2.2179 - val_acc: 0.4373\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0051 - acc: 0.4666 - val_loss: 2.2287 - val_acc: 0.4374\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0096 - acc: 0.4654 - val_loss: 2.2127 - val_acc: 0.4340\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0033 - acc: 0.4666 - val_loss: 2.2135 - val_acc: 0.4364\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9992 - acc: 0.4679 - val_loss: 2.2174 - val_acc: 0.4354\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0028 - acc: 0.4668 - val_loss: 2.2224 - val_acc: 0.4360\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0063 - acc: 0.4678 - val_loss: 2.2261 - val_acc: 0.4337\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0034 - acc: 0.4671 - val_loss: 2.2297 - val_acc: 0.4341\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9996 - acc: 0.4652 - val_loss: 2.2332 - val_acc: 0.4344\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0198 - acc: 0.4647 - val_loss: 2.2292 - val_acc: 0.4346\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0042 - acc: 0.4648 - val_loss: 2.2487 - val_acc: 0.4315\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0054 - acc: 0.4669 - val_loss: 2.2154 - val_acc: 0.4381\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9914 - acc: 0.4693 - val_loss: 2.2240 - val_acc: 0.4355\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9861 - acc: 0.4711 - val_loss: 2.2177 - val_acc: 0.4357\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9899 - acc: 0.4688 - val_loss: 2.2303 - val_acc: 0.4333\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9801 - acc: 0.4714 - val_loss: 2.2237 - val_acc: 0.4372\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9730 - acc: 0.4728 - val_loss: 2.2223 - val_acc: 0.4374\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9912 - acc: 0.4684 - val_loss: 2.2362 - val_acc: 0.4329\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9863 - acc: 0.4716 - val_loss: 2.2415 - val_acc: 0.4366\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0107 - acc: 0.4670 - val_loss: 2.2586 - val_acc: 0.4306\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0010 - acc: 0.4659 - val_loss: 2.2296 - val_acc: 0.4345\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9749 - acc: 0.4730 - val_loss: 2.2494 - val_acc: 0.4325\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9763 - acc: 0.4743 - val_loss: 2.2212 - val_acc: 0.4363\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9704 - acc: 0.4742 - val_loss: 2.2674 - val_acc: 0.4278\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9819 - acc: 0.4710 - val_loss: 2.2307 - val_acc: 0.4377\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9788 - acc: 0.4727 - val_loss: 2.2352 - val_acc: 0.4364\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9657 - acc: 0.4773 - val_loss: 2.2387 - val_acc: 0.4317\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9743 - acc: 0.4743 - val_loss: 2.2257 - val_acc: 0.4342\n",
      "Epoch 93/100\n",
      "1s - loss: 2.0000 - acc: 0.4691 - val_loss: 2.2507 - val_acc: 0.4304\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9949 - acc: 0.4691 - val_loss: 2.2407 - val_acc: 0.4352\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9781 - acc: 0.4721 - val_loss: 2.2480 - val_acc: 0.4337\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9980 - acc: 0.4692 - val_loss: 2.2382 - val_acc: 0.4318\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9792 - acc: 0.4727 - val_loss: 2.2503 - val_acc: 0.4293\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9717 - acc: 0.4759 - val_loss: 2.2458 - val_acc: 0.4344\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9854 - acc: 0.4727 - val_loss: 2.2282 - val_acc: 0.4323\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9576 - acc: 0.4762 - val_loss: 2.2293 - val_acc: 0.4369\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2121 - acc: 0.4239 - val_loss: 2.2040 - val_acc: 0.4404\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1885 - acc: 0.4277 - val_loss: 2.2002 - val_acc: 0.4424\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1764 - acc: 0.4296 - val_loss: 2.2103 - val_acc: 0.4397\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1695 - acc: 0.4314 - val_loss: 2.2074 - val_acc: 0.4388\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1574 - acc: 0.4345 - val_loss: 2.2308 - val_acc: 0.4382\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1551 - acc: 0.4327 - val_loss: 2.2085 - val_acc: 0.4396\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1452 - acc: 0.4367 - val_loss: 2.2445 - val_acc: 0.4375\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1413 - acc: 0.4366 - val_loss: 2.2200 - val_acc: 0.4393\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1371 - acc: 0.4376 - val_loss: 2.2139 - val_acc: 0.4394\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1259 - acc: 0.4407 - val_loss: 2.2210 - val_acc: 0.4380\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1297 - acc: 0.4396 - val_loss: 2.2120 - val_acc: 0.4383\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1221 - acc: 0.4410 - val_loss: 2.2179 - val_acc: 0.4384\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1129 - acc: 0.4415 - val_loss: 2.2341 - val_acc: 0.4355\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1091 - acc: 0.4436 - val_loss: 2.2204 - val_acc: 0.4385\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1125 - acc: 0.4421 - val_loss: 2.2090 - val_acc: 0.4413\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1034 - acc: 0.4451 - val_loss: 2.2183 - val_acc: 0.4388\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1001 - acc: 0.4423 - val_loss: 2.2150 - val_acc: 0.4398\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1017 - acc: 0.4452 - val_loss: 2.2104 - val_acc: 0.4419\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0913 - acc: 0.4491 - val_loss: 2.2002 - val_acc: 0.4417\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0885 - acc: 0.4487 - val_loss: 2.2164 - val_acc: 0.4397\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0846 - acc: 0.4485 - val_loss: 2.2025 - val_acc: 0.4402\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0840 - acc: 0.4478 - val_loss: 2.2016 - val_acc: 0.4412\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0781 - acc: 0.4507 - val_loss: 2.2046 - val_acc: 0.4417\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0760 - acc: 0.4526 - val_loss: 2.1994 - val_acc: 0.4414\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0735 - acc: 0.4513 - val_loss: 2.2023 - val_acc: 0.4418\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0740 - acc: 0.4494 - val_loss: 2.1928 - val_acc: 0.4439\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0697 - acc: 0.4522 - val_loss: 2.2321 - val_acc: 0.4373\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0716 - acc: 0.4517 - val_loss: 2.2071 - val_acc: 0.4429\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0660 - acc: 0.4536 - val_loss: 2.1970 - val_acc: 0.4433\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0605 - acc: 0.4539 - val_loss: 2.1995 - val_acc: 0.4439\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0582 - acc: 0.4563 - val_loss: 2.2079 - val_acc: 0.4418\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0594 - acc: 0.4531 - val_loss: 2.1988 - val_acc: 0.4428\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0572 - acc: 0.4536 - val_loss: 2.2139 - val_acc: 0.4408\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0785 - acc: 0.4485 - val_loss: 2.2096 - val_acc: 0.4428\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0635 - acc: 0.4548 - val_loss: 2.2100 - val_acc: 0.4402\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0569 - acc: 0.4541 - val_loss: 2.2078 - val_acc: 0.4413\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0511 - acc: 0.4536 - val_loss: 2.1989 - val_acc: 0.4430\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0470 - acc: 0.4560 - val_loss: 2.1999 - val_acc: 0.4424\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0379 - acc: 0.4579 - val_loss: 2.2109 - val_acc: 0.4401\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0447 - acc: 0.4563 - val_loss: 2.1947 - val_acc: 0.4435\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0369 - acc: 0.4585 - val_loss: 2.2038 - val_acc: 0.4433\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0357 - acc: 0.4585 - val_loss: 2.2063 - val_acc: 0.4408\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0313 - acc: 0.4601 - val_loss: 2.2021 - val_acc: 0.4448\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0342 - acc: 0.4590 - val_loss: 2.2146 - val_acc: 0.4392\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0391 - acc: 0.4597 - val_loss: 2.2364 - val_acc: 0.4387\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0392 - acc: 0.4575 - val_loss: 2.1948 - val_acc: 0.4432\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0236 - acc: 0.4611 - val_loss: 2.2151 - val_acc: 0.4418\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0289 - acc: 0.4598 - val_loss: 2.1934 - val_acc: 0.4424\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0214 - acc: 0.4628 - val_loss: 2.1936 - val_acc: 0.4458\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0191 - acc: 0.4616 - val_loss: 2.1954 - val_acc: 0.4447\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0168 - acc: 0.4617 - val_loss: 2.1973 - val_acc: 0.4439\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0204 - acc: 0.4621 - val_loss: 2.2163 - val_acc: 0.4410\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0259 - acc: 0.4604 - val_loss: 2.2074 - val_acc: 0.4431\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0175 - acc: 0.4647 - val_loss: 2.2107 - val_acc: 0.4406\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0197 - acc: 0.4617 - val_loss: 2.1937 - val_acc: 0.4445\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0091 - acc: 0.4652 - val_loss: 2.2081 - val_acc: 0.4406\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0090 - acc: 0.4633 - val_loss: 2.1974 - val_acc: 0.4422\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0068 - acc: 0.4655 - val_loss: 2.1960 - val_acc: 0.4419\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0036 - acc: 0.4671 - val_loss: 2.1943 - val_acc: 0.4451\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9998 - acc: 0.4669 - val_loss: 2.1921 - val_acc: 0.4449\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9989 - acc: 0.4674 - val_loss: 2.2016 - val_acc: 0.4432\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0020 - acc: 0.4669 - val_loss: 2.1945 - val_acc: 0.4408\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0036 - acc: 0.4666 - val_loss: 2.2132 - val_acc: 0.4429\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0056 - acc: 0.4656 - val_loss: 2.2350 - val_acc: 0.4347\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0144 - acc: 0.4641 - val_loss: 2.2148 - val_acc: 0.4428\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0028 - acc: 0.4667 - val_loss: 2.2203 - val_acc: 0.4386\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9946 - acc: 0.4692 - val_loss: 2.2325 - val_acc: 0.4409\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0392 - acc: 0.4594 - val_loss: 2.2410 - val_acc: 0.4307\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0220 - acc: 0.4638 - val_loss: 2.2104 - val_acc: 0.4427\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9956 - acc: 0.4687 - val_loss: 2.1985 - val_acc: 0.4410\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9868 - acc: 0.4699 - val_loss: 2.1939 - val_acc: 0.4445\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9894 - acc: 0.4697 - val_loss: 2.1998 - val_acc: 0.4411\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9902 - acc: 0.4693 - val_loss: 2.2249 - val_acc: 0.4406\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0008 - acc: 0.4671 - val_loss: 2.2095 - val_acc: 0.4384\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9833 - acc: 0.4712 - val_loss: 2.2032 - val_acc: 0.4447\n",
      "Epoch 76/100\n",
      "2s - loss: 1.9790 - acc: 0.4708 - val_loss: 2.2069 - val_acc: 0.4387\n",
      "Epoch 77/100\n",
      "2s - loss: 1.9884 - acc: 0.4683 - val_loss: 2.2133 - val_acc: 0.4425\n",
      "Epoch 78/100\n",
      "2s - loss: 1.9795 - acc: 0.4731 - val_loss: 2.1928 - val_acc: 0.4419\n",
      "Epoch 79/100\n",
      "2s - loss: 1.9756 - acc: 0.4756 - val_loss: 2.2162 - val_acc: 0.4457\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9840 - acc: 0.4713 - val_loss: 2.2024 - val_acc: 0.4429\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9774 - acc: 0.4728 - val_loss: 2.2190 - val_acc: 0.4392\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9845 - acc: 0.4700 - val_loss: 2.1971 - val_acc: 0.4419\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9746 - acc: 0.4724 - val_loss: 2.2157 - val_acc: 0.4414\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9925 - acc: 0.4711 - val_loss: 2.1956 - val_acc: 0.4413\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9691 - acc: 0.4736 - val_loss: 2.1967 - val_acc: 0.4454\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9621 - acc: 0.4766 - val_loss: 2.1833 - val_acc: 0.4442\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9636 - acc: 0.4756 - val_loss: 2.1998 - val_acc: 0.4441\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9654 - acc: 0.4778 - val_loss: 2.2238 - val_acc: 0.4377\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9909 - acc: 0.4697 - val_loss: 2.2347 - val_acc: 0.4402\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9743 - acc: 0.4735 - val_loss: 2.2217 - val_acc: 0.4380\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9644 - acc: 0.4759 - val_loss: 2.2146 - val_acc: 0.4421\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9623 - acc: 0.4771 - val_loss: 2.2190 - val_acc: 0.4384\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9599 - acc: 0.4775 - val_loss: 2.2016 - val_acc: 0.4406\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9470 - acc: 0.4819 - val_loss: 2.2055 - val_acc: 0.4401\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9506 - acc: 0.4800 - val_loss: 2.2006 - val_acc: 0.4436\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9503 - acc: 0.4788 - val_loss: 2.2374 - val_acc: 0.4356\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9636 - acc: 0.4775 - val_loss: 2.2208 - val_acc: 0.4403\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9699 - acc: 0.4758 - val_loss: 2.2375 - val_acc: 0.4355\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9617 - acc: 0.4769 - val_loss: 2.2029 - val_acc: 0.4412\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9567 - acc: 0.4784 - val_loss: 2.2242 - val_acc: 0.4378\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2333 - acc: 0.4209 - val_loss: 2.2370 - val_acc: 0.4308\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2072 - acc: 0.4229 - val_loss: 2.2362 - val_acc: 0.4301\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1916 - acc: 0.4270 - val_loss: 2.2545 - val_acc: 0.4258\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1878 - acc: 0.4289 - val_loss: 2.2354 - val_acc: 0.4277\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1744 - acc: 0.4307 - val_loss: 2.2440 - val_acc: 0.4266\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1678 - acc: 0.4306 - val_loss: 2.2516 - val_acc: 0.4275\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1626 - acc: 0.4342 - val_loss: 2.2549 - val_acc: 0.4246\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1564 - acc: 0.4344 - val_loss: 2.2497 - val_acc: 0.4253\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1536 - acc: 0.4353 - val_loss: 2.2470 - val_acc: 0.4269\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1449 - acc: 0.4386 - val_loss: 2.2475 - val_acc: 0.4255\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1421 - acc: 0.4377 - val_loss: 2.2480 - val_acc: 0.4231\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1387 - acc: 0.4390 - val_loss: 2.2489 - val_acc: 0.4268\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1311 - acc: 0.4394 - val_loss: 2.2433 - val_acc: 0.4265\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1278 - acc: 0.4386 - val_loss: 2.2491 - val_acc: 0.4252\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1212 - acc: 0.4415 - val_loss: 2.2529 - val_acc: 0.4252\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1198 - acc: 0.4420 - val_loss: 2.2485 - val_acc: 0.4242\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1160 - acc: 0.4440 - val_loss: 2.2441 - val_acc: 0.4249\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1135 - acc: 0.4430 - val_loss: 2.2433 - val_acc: 0.4270\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1091 - acc: 0.4428 - val_loss: 2.2400 - val_acc: 0.4286\n",
      "Epoch 20/100\n",
      "2s - loss: 2.1074 - acc: 0.4454 - val_loss: 2.2494 - val_acc: 0.4247\n",
      "Epoch 21/100\n",
      "2s - loss: 2.1043 - acc: 0.4453 - val_loss: 2.2464 - val_acc: 0.4273\n",
      "Epoch 22/100\n",
      "2s - loss: 2.1000 - acc: 0.4478 - val_loss: 2.2569 - val_acc: 0.4262\n",
      "Epoch 23/100\n",
      "2s - loss: 2.1157 - acc: 0.4422 - val_loss: 2.2513 - val_acc: 0.4244\n",
      "Epoch 24/100\n",
      "2s - loss: 2.1036 - acc: 0.4460 - val_loss: 2.2499 - val_acc: 0.4286\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1004 - acc: 0.4462 - val_loss: 2.2603 - val_acc: 0.4254\n",
      "Epoch 26/100\n",
      "1s - loss: 2.1012 - acc: 0.4475 - val_loss: 2.2419 - val_acc: 0.4284\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0884 - acc: 0.4490 - val_loss: 2.2439 - val_acc: 0.4269\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0943 - acc: 0.4481 - val_loss: 2.2633 - val_acc: 0.4273\n",
      "Epoch 29/100\n",
      "1s - loss: 2.1117 - acc: 0.4456 - val_loss: 2.2401 - val_acc: 0.4267\n",
      "Epoch 30/100\n",
      "2s - loss: 2.0834 - acc: 0.4502 - val_loss: 2.2455 - val_acc: 0.4297\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0813 - acc: 0.4503 - val_loss: 2.2390 - val_acc: 0.4269\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0741 - acc: 0.4544 - val_loss: 2.2357 - val_acc: 0.4289\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0842 - acc: 0.4508 - val_loss: 2.2422 - val_acc: 0.4290\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0700 - acc: 0.4538 - val_loss: 2.2353 - val_acc: 0.4301\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0799 - acc: 0.4520 - val_loss: 2.2416 - val_acc: 0.4294\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0713 - acc: 0.4538 - val_loss: 2.2504 - val_acc: 0.4237\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0773 - acc: 0.4519 - val_loss: 2.2458 - val_acc: 0.4284\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0753 - acc: 0.4529 - val_loss: 2.2373 - val_acc: 0.4308\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0749 - acc: 0.4554 - val_loss: 2.2478 - val_acc: 0.4263\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0723 - acc: 0.4537 - val_loss: 2.2294 - val_acc: 0.4303\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0620 - acc: 0.4563 - val_loss: 2.2427 - val_acc: 0.4303\n",
      "Epoch 42/100\n",
      "2s - loss: 2.0631 - acc: 0.4541 - val_loss: 2.2320 - val_acc: 0.4289\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0575 - acc: 0.4571 - val_loss: 2.2360 - val_acc: 0.4320\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0606 - acc: 0.4563 - val_loss: 2.2303 - val_acc: 0.4309\n",
      "Epoch 45/100\n",
      "2s - loss: 2.0489 - acc: 0.4584 - val_loss: 2.2276 - val_acc: 0.4302\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0465 - acc: 0.4600 - val_loss: 2.2249 - val_acc: 0.4349\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0434 - acc: 0.4595 - val_loss: 2.2213 - val_acc: 0.4306\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0511 - acc: 0.4557 - val_loss: 2.2349 - val_acc: 0.4292\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0435 - acc: 0.4611 - val_loss: 2.2285 - val_acc: 0.4315\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0468 - acc: 0.4600 - val_loss: 2.2352 - val_acc: 0.4321\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0573 - acc: 0.4572 - val_loss: 2.2332 - val_acc: 0.4341\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0422 - acc: 0.4612 - val_loss: 2.2314 - val_acc: 0.4299\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0319 - acc: 0.4627 - val_loss: 2.2253 - val_acc: 0.4321\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0370 - acc: 0.4614 - val_loss: 2.2305 - val_acc: 0.4320\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0445 - acc: 0.4612 - val_loss: 2.2232 - val_acc: 0.4327\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0420 - acc: 0.4595 - val_loss: 2.2276 - val_acc: 0.4321\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0317 - acc: 0.4640 - val_loss: 2.2278 - val_acc: 0.4327\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0318 - acc: 0.4642 - val_loss: 2.2363 - val_acc: 0.4291\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0347 - acc: 0.4618 - val_loss: 2.2315 - val_acc: 0.4321\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0377 - acc: 0.4596 - val_loss: 2.2359 - val_acc: 0.4305\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0263 - acc: 0.4646 - val_loss: 2.2461 - val_acc: 0.4267\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0314 - acc: 0.4626 - val_loss: 2.2417 - val_acc: 0.4294\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0301 - acc: 0.4612 - val_loss: 2.2247 - val_acc: 0.4326\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0299 - acc: 0.4624 - val_loss: 2.2364 - val_acc: 0.4309\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0207 - acc: 0.4645 - val_loss: 2.2362 - val_acc: 0.4266\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0193 - acc: 0.4659 - val_loss: 2.2484 - val_acc: 0.4286\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0242 - acc: 0.4639 - val_loss: 2.2486 - val_acc: 0.4280\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0201 - acc: 0.4654 - val_loss: 2.2504 - val_acc: 0.4308\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0310 - acc: 0.4635 - val_loss: 2.2615 - val_acc: 0.4277\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0225 - acc: 0.4649 - val_loss: 2.2336 - val_acc: 0.4304\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0108 - acc: 0.4690 - val_loss: 2.2541 - val_acc: 0.4287\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0263 - acc: 0.4626 - val_loss: 2.2516 - val_acc: 0.4260\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0162 - acc: 0.4640 - val_loss: 2.2367 - val_acc: 0.4321\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0110 - acc: 0.4674 - val_loss: 2.2419 - val_acc: 0.4296\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0156 - acc: 0.4669 - val_loss: 2.2429 - val_acc: 0.4297\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0223 - acc: 0.4650 - val_loss: 2.2426 - val_acc: 0.4303\n",
      "Epoch 77/100\n",
      "2s - loss: 2.0139 - acc: 0.4662 - val_loss: 2.2357 - val_acc: 0.4311\n",
      "Epoch 78/100\n",
      "2s - loss: 2.0119 - acc: 0.4678 - val_loss: 2.2410 - val_acc: 0.4306\n",
      "Epoch 79/100\n",
      "2s - loss: 2.0002 - acc: 0.4688 - val_loss: 2.2405 - val_acc: 0.4283\n",
      "Epoch 80/100\n",
      "2s - loss: 2.0070 - acc: 0.4690 - val_loss: 2.2322 - val_acc: 0.4328\n",
      "Epoch 81/100\n",
      "2s - loss: 2.0014 - acc: 0.4693 - val_loss: 2.2379 - val_acc: 0.4304\n",
      "Epoch 82/100\n",
      "2s - loss: 2.0173 - acc: 0.4640 - val_loss: 2.2497 - val_acc: 0.4300\n",
      "Epoch 83/100\n",
      "2s - loss: 2.0050 - acc: 0.4672 - val_loss: 2.2299 - val_acc: 0.4334\n",
      "Epoch 84/100\n",
      "2s - loss: 2.0012 - acc: 0.4701 - val_loss: 2.2415 - val_acc: 0.4328\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9878 - acc: 0.4717 - val_loss: 2.2271 - val_acc: 0.4330\n",
      "Epoch 86/100\n",
      "2s - loss: 1.9880 - acc: 0.4732 - val_loss: 2.2577 - val_acc: 0.4311\n",
      "Epoch 87/100\n",
      "2s - loss: 1.9998 - acc: 0.4707 - val_loss: 2.2558 - val_acc: 0.4300\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9938 - acc: 0.4722 - val_loss: 2.2318 - val_acc: 0.4329\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9865 - acc: 0.4743 - val_loss: 2.2467 - val_acc: 0.4306\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9865 - acc: 0.4734 - val_loss: 2.2344 - val_acc: 0.4316\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9870 - acc: 0.4731 - val_loss: 2.2556 - val_acc: 0.4275\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9888 - acc: 0.4741 - val_loss: 2.2319 - val_acc: 0.4310\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9853 - acc: 0.4746 - val_loss: 2.2485 - val_acc: 0.4281\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9828 - acc: 0.4743 - val_loss: 2.2392 - val_acc: 0.4310\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9812 - acc: 0.4720 - val_loss: 2.2357 - val_acc: 0.4308\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9778 - acc: 0.4739 - val_loss: 2.2379 - val_acc: 0.4294\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9763 - acc: 0.4756 - val_loss: 2.2358 - val_acc: 0.4317\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9780 - acc: 0.4741 - val_loss: 2.2480 - val_acc: 0.4303\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9851 - acc: 0.4724 - val_loss: 2.2314 - val_acc: 0.4320\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9833 - acc: 0.4720 - val_loss: 2.2551 - val_acc: 0.4293\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1870 - acc: 0.4302 - val_loss: 2.2049 - val_acc: 0.4350\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1607 - acc: 0.4353 - val_loss: 2.2138 - val_acc: 0.4327\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1440 - acc: 0.4378 - val_loss: 2.2182 - val_acc: 0.4329\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1376 - acc: 0.4376 - val_loss: 2.2160 - val_acc: 0.4316\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1283 - acc: 0.4413 - val_loss: 2.2428 - val_acc: 0.4268\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1249 - acc: 0.4408 - val_loss: 2.2214 - val_acc: 0.4310\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1145 - acc: 0.4429 - val_loss: 2.2366 - val_acc: 0.4263\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1056 - acc: 0.4457 - val_loss: 2.2587 - val_acc: 0.4275\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1417 - acc: 0.4395 - val_loss: 2.3139 - val_acc: 0.4129\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1338 - acc: 0.4395 - val_loss: 2.2525 - val_acc: 0.4249\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1042 - acc: 0.4466 - val_loss: 2.2417 - val_acc: 0.4275\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0880 - acc: 0.4486 - val_loss: 2.2583 - val_acc: 0.4266\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0847 - acc: 0.4491 - val_loss: 2.2396 - val_acc: 0.4257\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0826 - acc: 0.4506 - val_loss: 2.2508 - val_acc: 0.4236\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0771 - acc: 0.4500 - val_loss: 2.2542 - val_acc: 0.4244\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0756 - acc: 0.4513 - val_loss: 2.2433 - val_acc: 0.4259\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0681 - acc: 0.4528 - val_loss: 2.2438 - val_acc: 0.4273\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0728 - acc: 0.4536 - val_loss: 2.2371 - val_acc: 0.4277\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0605 - acc: 0.4550 - val_loss: 2.2265 - val_acc: 0.4278\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0589 - acc: 0.4563 - val_loss: 2.2616 - val_acc: 0.4226\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0585 - acc: 0.4557 - val_loss: 2.2423 - val_acc: 0.4271\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0517 - acc: 0.4560 - val_loss: 2.2390 - val_acc: 0.4288\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0509 - acc: 0.4560 - val_loss: 2.2343 - val_acc: 0.4296\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0469 - acc: 0.4578 - val_loss: 2.2446 - val_acc: 0.4272\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0426 - acc: 0.4578 - val_loss: 2.2455 - val_acc: 0.4275\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0374 - acc: 0.4584 - val_loss: 2.2701 - val_acc: 0.4227\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0510 - acc: 0.4570 - val_loss: 2.2256 - val_acc: 0.4294\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0380 - acc: 0.4609 - val_loss: 2.2456 - val_acc: 0.4269\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0341 - acc: 0.4610 - val_loss: 2.2297 - val_acc: 0.4278\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0322 - acc: 0.4588 - val_loss: 2.2381 - val_acc: 0.4277\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0293 - acc: 0.4600 - val_loss: 2.2298 - val_acc: 0.4318\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0246 - acc: 0.4601 - val_loss: 2.2305 - val_acc: 0.4266\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0311 - acc: 0.4612 - val_loss: 2.2387 - val_acc: 0.4296\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0302 - acc: 0.4624 - val_loss: 2.2245 - val_acc: 0.4293\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0242 - acc: 0.4611 - val_loss: 2.2471 - val_acc: 0.4261\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0179 - acc: 0.4626 - val_loss: 2.2198 - val_acc: 0.4316\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0171 - acc: 0.4633 - val_loss: 2.2356 - val_acc: 0.4304\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0169 - acc: 0.4668 - val_loss: 2.2250 - val_acc: 0.4301\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0139 - acc: 0.4634 - val_loss: 2.2188 - val_acc: 0.4320\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0083 - acc: 0.4676 - val_loss: 2.2266 - val_acc: 0.4301\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0080 - acc: 0.4671 - val_loss: 2.2246 - val_acc: 0.4304\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0024 - acc: 0.4671 - val_loss: 2.2232 - val_acc: 0.4313\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0001 - acc: 0.4665 - val_loss: 2.2205 - val_acc: 0.4313\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0079 - acc: 0.4646 - val_loss: 2.2403 - val_acc: 0.4305\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0089 - acc: 0.4653 - val_loss: 2.2057 - val_acc: 0.4335\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0029 - acc: 0.4687 - val_loss: 2.2257 - val_acc: 0.4310\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0013 - acc: 0.4678 - val_loss: 2.2108 - val_acc: 0.4310\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9982 - acc: 0.4689 - val_loss: 2.2246 - val_acc: 0.4305\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9940 - acc: 0.4712 - val_loss: 2.2157 - val_acc: 0.4308\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0003 - acc: 0.4681 - val_loss: 2.2383 - val_acc: 0.4322\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9978 - acc: 0.4703 - val_loss: 2.2158 - val_acc: 0.4302\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9991 - acc: 0.4690 - val_loss: 2.2304 - val_acc: 0.4313\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9854 - acc: 0.4723 - val_loss: 2.2113 - val_acc: 0.4324\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9808 - acc: 0.4713 - val_loss: 2.2280 - val_acc: 0.4311\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9788 - acc: 0.4720 - val_loss: 2.2133 - val_acc: 0.4312\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9827 - acc: 0.4725 - val_loss: 2.2173 - val_acc: 0.4315\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9768 - acc: 0.4754 - val_loss: 2.2145 - val_acc: 0.4326\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9699 - acc: 0.4769 - val_loss: 2.2115 - val_acc: 0.4335\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9800 - acc: 0.4723 - val_loss: 2.2071 - val_acc: 0.4336\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9689 - acc: 0.4745 - val_loss: 2.2160 - val_acc: 0.4328\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9735 - acc: 0.4741 - val_loss: 2.2162 - val_acc: 0.4326\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9675 - acc: 0.4763 - val_loss: 2.2406 - val_acc: 0.4291\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9792 - acc: 0.4726 - val_loss: 2.2054 - val_acc: 0.4349\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9792 - acc: 0.4722 - val_loss: 2.2338 - val_acc: 0.4317\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9757 - acc: 0.4738 - val_loss: 2.2077 - val_acc: 0.4334\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9636 - acc: 0.4759 - val_loss: 2.2165 - val_acc: 0.4328\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9676 - acc: 0.4744 - val_loss: 2.2091 - val_acc: 0.4333\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9654 - acc: 0.4752 - val_loss: 2.2463 - val_acc: 0.4295\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9844 - acc: 0.4722 - val_loss: 2.2092 - val_acc: 0.4327\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9581 - acc: 0.4757 - val_loss: 2.2166 - val_acc: 0.4311\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9525 - acc: 0.4778 - val_loss: 2.2068 - val_acc: 0.4317\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9587 - acc: 0.4774 - val_loss: 2.2156 - val_acc: 0.4331\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9506 - acc: 0.4805 - val_loss: 2.2208 - val_acc: 0.4305\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9492 - acc: 0.4798 - val_loss: 2.2082 - val_acc: 0.4333\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9586 - acc: 0.4781 - val_loss: 2.2228 - val_acc: 0.4310\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9499 - acc: 0.4796 - val_loss: 2.2156 - val_acc: 0.4332\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9430 - acc: 0.4813 - val_loss: 2.2153 - val_acc: 0.4312\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9409 - acc: 0.4806 - val_loss: 2.2118 - val_acc: 0.4313\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9392 - acc: 0.4815 - val_loss: 2.2199 - val_acc: 0.4332\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9450 - acc: 0.4811 - val_loss: 2.2290 - val_acc: 0.4336\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9501 - acc: 0.4789 - val_loss: 2.2344 - val_acc: 0.4282\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9591 - acc: 0.4766 - val_loss: 2.2281 - val_acc: 0.4323\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9451 - acc: 0.4800 - val_loss: 2.2246 - val_acc: 0.4308\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9390 - acc: 0.4819 - val_loss: 2.2129 - val_acc: 0.4347\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9400 - acc: 0.4833 - val_loss: 2.2292 - val_acc: 0.4284\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9398 - acc: 0.4840 - val_loss: 2.2256 - val_acc: 0.4300\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9335 - acc: 0.4836 - val_loss: 2.2170 - val_acc: 0.4327\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9356 - acc: 0.4827 - val_loss: 2.2216 - val_acc: 0.4341\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9351 - acc: 0.4831 - val_loss: 2.2263 - val_acc: 0.4299\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9420 - acc: 0.4806 - val_loss: 2.2138 - val_acc: 0.4320\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9271 - acc: 0.4853 - val_loss: 2.2127 - val_acc: 0.4327\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9265 - acc: 0.4849 - val_loss: 2.2175 - val_acc: 0.4311\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9235 - acc: 0.4871 - val_loss: 2.2142 - val_acc: 0.4335\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9287 - acc: 0.4854 - val_loss: 2.2224 - val_acc: 0.4325\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9317 - acc: 0.4848 - val_loss: 2.2144 - val_acc: 0.4341\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9326 - acc: 0.4848 - val_loss: 2.2434 - val_acc: 0.4268\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9314 - acc: 0.4837 - val_loss: 2.2201 - val_acc: 0.4318\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9243 - acc: 0.4862 - val_loss: 2.2370 - val_acc: 0.4287\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9229 - acc: 0.4872 - val_loss: 2.2131 - val_acc: 0.4323\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9198 - acc: 0.4859 - val_loss: 2.2283 - val_acc: 0.4312\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1931 - acc: 0.4243 - val_loss: 2.2221 - val_acc: 0.4364\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1648 - acc: 0.4310 - val_loss: 2.2189 - val_acc: 0.4382\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1532 - acc: 0.4331 - val_loss: 2.2319 - val_acc: 0.4359\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1529 - acc: 0.4321 - val_loss: 2.2277 - val_acc: 0.4341\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1341 - acc: 0.4368 - val_loss: 2.2273 - val_acc: 0.4355\n",
      "Epoch 6/100\n",
      "2s - loss: 2.1289 - acc: 0.4394 - val_loss: 2.2383 - val_acc: 0.4339\n",
      "Epoch 7/100\n",
      "2s - loss: 2.1224 - acc: 0.4393 - val_loss: 2.2261 - val_acc: 0.4342\n",
      "Epoch 8/100\n",
      "2s - loss: 2.1168 - acc: 0.4398 - val_loss: 2.2436 - val_acc: 0.4335\n",
      "Epoch 9/100\n",
      "2s - loss: 2.1121 - acc: 0.4410 - val_loss: 2.2420 - val_acc: 0.4331\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1085 - acc: 0.4424 - val_loss: 2.2413 - val_acc: 0.4340\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1015 - acc: 0.4443 - val_loss: 2.2445 - val_acc: 0.4343\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0996 - acc: 0.4445 - val_loss: 2.2478 - val_acc: 0.4316\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0936 - acc: 0.4461 - val_loss: 2.2559 - val_acc: 0.4306\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0870 - acc: 0.4452 - val_loss: 2.2382 - val_acc: 0.4311\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0809 - acc: 0.4475 - val_loss: 2.2490 - val_acc: 0.4284\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0791 - acc: 0.4480 - val_loss: 2.2488 - val_acc: 0.4320\n",
      "Epoch 17/100\n",
      "2s - loss: 2.0772 - acc: 0.4480 - val_loss: 2.2556 - val_acc: 0.4299\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0792 - acc: 0.4476 - val_loss: 2.2495 - val_acc: 0.4294\n",
      "Epoch 19/100\n",
      "2s - loss: 2.0689 - acc: 0.4493 - val_loss: 2.2513 - val_acc: 0.4284\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0674 - acc: 0.4506 - val_loss: 2.2342 - val_acc: 0.4327\n",
      "Epoch 21/100\n",
      "2s - loss: 2.0625 - acc: 0.4515 - val_loss: 2.2346 - val_acc: 0.4315\n",
      "Epoch 22/100\n",
      "2s - loss: 2.0662 - acc: 0.4524 - val_loss: 2.2473 - val_acc: 0.4291\n",
      "Epoch 23/100\n",
      "2s - loss: 2.0604 - acc: 0.4535 - val_loss: 2.2440 - val_acc: 0.4312\n",
      "Epoch 24/100\n",
      "2s - loss: 2.0676 - acc: 0.4500 - val_loss: 2.2406 - val_acc: 0.4301\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0574 - acc: 0.4526 - val_loss: 2.2825 - val_acc: 0.4283\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0616 - acc: 0.4533 - val_loss: 2.2395 - val_acc: 0.4330\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0537 - acc: 0.4546 - val_loss: 2.2284 - val_acc: 0.4352\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0499 - acc: 0.4529 - val_loss: 2.2746 - val_acc: 0.4257\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0477 - acc: 0.4543 - val_loss: 2.2346 - val_acc: 0.4317\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0481 - acc: 0.4560 - val_loss: 2.2389 - val_acc: 0.4332\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0421 - acc: 0.4562 - val_loss: 2.2336 - val_acc: 0.4323\n",
      "Epoch 32/100\n",
      "2s - loss: 2.0404 - acc: 0.4565 - val_loss: 2.2436 - val_acc: 0.4301\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0362 - acc: 0.4571 - val_loss: 2.2277 - val_acc: 0.4342\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0405 - acc: 0.4573 - val_loss: 2.2292 - val_acc: 0.4325\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0316 - acc: 0.4586 - val_loss: 2.2313 - val_acc: 0.4320\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0344 - acc: 0.4594 - val_loss: 2.2305 - val_acc: 0.4310\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0257 - acc: 0.4609 - val_loss: 2.2299 - val_acc: 0.4327\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0234 - acc: 0.4606 - val_loss: 2.2311 - val_acc: 0.4348\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0191 - acc: 0.4619 - val_loss: 2.2312 - val_acc: 0.4336\n",
      "Epoch 40/100\n",
      "2s - loss: 2.0205 - acc: 0.4618 - val_loss: 2.2393 - val_acc: 0.4330\n",
      "Epoch 41/100\n",
      "2s - loss: 2.0143 - acc: 0.4626 - val_loss: 2.2423 - val_acc: 0.4308\n",
      "Epoch 42/100\n",
      "2s - loss: 2.0200 - acc: 0.4609 - val_loss: 2.2430 - val_acc: 0.4340\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0346 - acc: 0.4595 - val_loss: 2.2424 - val_acc: 0.4314\n",
      "Epoch 44/100\n",
      "2s - loss: 2.0133 - acc: 0.4631 - val_loss: 2.2268 - val_acc: 0.4345\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0107 - acc: 0.4638 - val_loss: 2.2193 - val_acc: 0.4359\n",
      "Epoch 46/100\n",
      "2s - loss: 2.0112 - acc: 0.4625 - val_loss: 2.2294 - val_acc: 0.4349\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0156 - acc: 0.4646 - val_loss: 2.2287 - val_acc: 0.4328\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0052 - acc: 0.4643 - val_loss: 2.2209 - val_acc: 0.4348\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0000 - acc: 0.4643 - val_loss: 2.2318 - val_acc: 0.4350\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0024 - acc: 0.4647 - val_loss: 2.2286 - val_acc: 0.4331\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9946 - acc: 0.4679 - val_loss: 2.2318 - val_acc: 0.4347\n",
      "Epoch 52/100\n",
      "2s - loss: 1.9972 - acc: 0.4664 - val_loss: 2.2312 - val_acc: 0.4345\n",
      "Epoch 53/100\n",
      "2s - loss: 2.0070 - acc: 0.4650 - val_loss: 2.2333 - val_acc: 0.4357\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0009 - acc: 0.4676 - val_loss: 2.2538 - val_acc: 0.4320\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0130 - acc: 0.4625 - val_loss: 2.2306 - val_acc: 0.4373\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0066 - acc: 0.4633 - val_loss: 2.2581 - val_acc: 0.4307\n",
      "Epoch 57/100\n",
      "2s - loss: 2.0045 - acc: 0.4657 - val_loss: 2.2284 - val_acc: 0.4347\n",
      "Epoch 58/100\n",
      "2s - loss: 1.9909 - acc: 0.4695 - val_loss: 2.2424 - val_acc: 0.4322\n",
      "Epoch 59/100\n",
      "2s - loss: 1.9885 - acc: 0.4687 - val_loss: 2.2212 - val_acc: 0.4370\n",
      "Epoch 60/100\n",
      "2s - loss: 1.9895 - acc: 0.4680 - val_loss: 2.2270 - val_acc: 0.4345\n",
      "Epoch 61/100\n",
      "2s - loss: 1.9812 - acc: 0.4694 - val_loss: 2.2129 - val_acc: 0.4377\n",
      "Epoch 62/100\n",
      "2s - loss: 1.9811 - acc: 0.4710 - val_loss: 2.2189 - val_acc: 0.4365\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9733 - acc: 0.4723 - val_loss: 2.2143 - val_acc: 0.4380\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9728 - acc: 0.4721 - val_loss: 2.2334 - val_acc: 0.4343\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9807 - acc: 0.4713 - val_loss: 2.2662 - val_acc: 0.4287\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9802 - acc: 0.4694 - val_loss: 2.2613 - val_acc: 0.4294\n",
      "Epoch 67/100\n",
      "2s - loss: 1.9794 - acc: 0.4709 - val_loss: 2.2210 - val_acc: 0.4357\n",
      "Epoch 68/100\n",
      "2s - loss: 1.9711 - acc: 0.4715 - val_loss: 2.2355 - val_acc: 0.4341\n",
      "Epoch 69/100\n",
      "2s - loss: 1.9794 - acc: 0.4718 - val_loss: 2.2363 - val_acc: 0.4334\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9825 - acc: 0.4711 - val_loss: 2.2473 - val_acc: 0.4332\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9841 - acc: 0.4698 - val_loss: 2.2400 - val_acc: 0.4333\n",
      "Epoch 72/100\n",
      "2s - loss: 1.9724 - acc: 0.4722 - val_loss: 2.2702 - val_acc: 0.4306\n",
      "Epoch 73/100\n",
      "2s - loss: 1.9707 - acc: 0.4735 - val_loss: 2.2180 - val_acc: 0.4360\n",
      "Epoch 74/100\n",
      "2s - loss: 1.9565 - acc: 0.4759 - val_loss: 2.2359 - val_acc: 0.4330\n",
      "Epoch 75/100\n",
      "2s - loss: 1.9569 - acc: 0.4768 - val_loss: 2.2212 - val_acc: 0.4363\n",
      "Epoch 76/100\n",
      "2s - loss: 1.9531 - acc: 0.4763 - val_loss: 2.2249 - val_acc: 0.4356\n",
      "Epoch 77/100\n",
      "2s - loss: 1.9497 - acc: 0.4789 - val_loss: 2.2178 - val_acc: 0.4378\n",
      "Epoch 78/100\n",
      "2s - loss: 1.9513 - acc: 0.4766 - val_loss: 2.2226 - val_acc: 0.4365\n",
      "Epoch 79/100\n",
      "2s - loss: 1.9508 - acc: 0.4786 - val_loss: 2.2330 - val_acc: 0.4358\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9573 - acc: 0.4757 - val_loss: 2.2274 - val_acc: 0.4354\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9487 - acc: 0.4767 - val_loss: 2.2316 - val_acc: 0.4352\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9520 - acc: 0.4793 - val_loss: 2.2517 - val_acc: 0.4303\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9520 - acc: 0.4772 - val_loss: 2.2224 - val_acc: 0.4371\n",
      "Epoch 84/100\n",
      "2s - loss: 1.9447 - acc: 0.4784 - val_loss: 2.2566 - val_acc: 0.4325\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9552 - acc: 0.4761 - val_loss: 2.2651 - val_acc: 0.4337\n",
      "Epoch 86/100\n",
      "2s - loss: 1.9973 - acc: 0.4668 - val_loss: 2.2945 - val_acc: 0.4256\n",
      "Epoch 87/100\n",
      "2s - loss: 1.9945 - acc: 0.4678 - val_loss: 2.2615 - val_acc: 0.4324\n",
      "Epoch 88/100\n",
      "2s - loss: 1.9539 - acc: 0.4752 - val_loss: 2.2247 - val_acc: 0.4363\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9510 - acc: 0.4780 - val_loss: 2.2299 - val_acc: 0.4344\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9332 - acc: 0.4818 - val_loss: 2.2279 - val_acc: 0.4372\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9424 - acc: 0.4806 - val_loss: 2.2552 - val_acc: 0.4318\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9500 - acc: 0.4773 - val_loss: 2.2232 - val_acc: 0.4379\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9434 - acc: 0.4788 - val_loss: 2.2430 - val_acc: 0.4333\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9401 - acc: 0.4803 - val_loss: 2.2592 - val_acc: 0.4316\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9461 - acc: 0.4775 - val_loss: 2.2590 - val_acc: 0.4322\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9435 - acc: 0.4787 - val_loss: 2.2560 - val_acc: 0.4295\n",
      "Epoch 97/100\n",
      "2s - loss: 1.9483 - acc: 0.4784 - val_loss: 2.2359 - val_acc: 0.4343\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9337 - acc: 0.4810 - val_loss: 2.2203 - val_acc: 0.4357\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9407 - acc: 0.4801 - val_loss: 2.2388 - val_acc: 0.4318\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9294 - acc: 0.4826 - val_loss: 2.2283 - val_acc: 0.4360\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1968 - acc: 0.4271 - val_loss: 2.1927 - val_acc: 0.4437\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1804 - acc: 0.4308 - val_loss: 2.1882 - val_acc: 0.4431\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1621 - acc: 0.4336 - val_loss: 2.1907 - val_acc: 0.4419\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1528 - acc: 0.4365 - val_loss: 2.1931 - val_acc: 0.4415\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1431 - acc: 0.4368 - val_loss: 2.1902 - val_acc: 0.4405\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1388 - acc: 0.4384 - val_loss: 2.2028 - val_acc: 0.4395\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1302 - acc: 0.4405 - val_loss: 2.2058 - val_acc: 0.4390\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1378 - acc: 0.4398 - val_loss: 2.2053 - val_acc: 0.4390\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1250 - acc: 0.4413 - val_loss: 2.2014 - val_acc: 0.4389\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1139 - acc: 0.4429 - val_loss: 2.1985 - val_acc: 0.4412\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1111 - acc: 0.4445 - val_loss: 2.2022 - val_acc: 0.4399\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1060 - acc: 0.4449 - val_loss: 2.2005 - val_acc: 0.4393\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0995 - acc: 0.4467 - val_loss: 2.2103 - val_acc: 0.4381\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1009 - acc: 0.4449 - val_loss: 2.2174 - val_acc: 0.4370\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1021 - acc: 0.4458 - val_loss: 2.2097 - val_acc: 0.4400\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0943 - acc: 0.4464 - val_loss: 2.2100 - val_acc: 0.4362\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0885 - acc: 0.4483 - val_loss: 2.2037 - val_acc: 0.4391\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0809 - acc: 0.4499 - val_loss: 2.2044 - val_acc: 0.4391\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0774 - acc: 0.4509 - val_loss: 2.2190 - val_acc: 0.4381\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0772 - acc: 0.4497 - val_loss: 2.1984 - val_acc: 0.4396\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0714 - acc: 0.4521 - val_loss: 2.2248 - val_acc: 0.4374\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0898 - acc: 0.4491 - val_loss: 2.2136 - val_acc: 0.4373\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0675 - acc: 0.4538 - val_loss: 2.2003 - val_acc: 0.4390\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0674 - acc: 0.4543 - val_loss: 2.2044 - val_acc: 0.4380\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0627 - acc: 0.4551 - val_loss: 2.2007 - val_acc: 0.4412\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0583 - acc: 0.4578 - val_loss: 2.2013 - val_acc: 0.4408\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0602 - acc: 0.4561 - val_loss: 2.2075 - val_acc: 0.4381\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0570 - acc: 0.4568 - val_loss: 2.2035 - val_acc: 0.4428\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0643 - acc: 0.4543 - val_loss: 2.2023 - val_acc: 0.4399\n",
      "Epoch 30/100\n",
      "2s - loss: 2.0502 - acc: 0.4578 - val_loss: 2.2113 - val_acc: 0.4394\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0513 - acc: 0.4568 - val_loss: 2.1836 - val_acc: 0.4435\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0495 - acc: 0.4580 - val_loss: 2.2033 - val_acc: 0.4398\n",
      "Epoch 33/100\n",
      "2s - loss: 2.0406 - acc: 0.4600 - val_loss: 2.1909 - val_acc: 0.4401\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0420 - acc: 0.4602 - val_loss: 2.1871 - val_acc: 0.4435\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0396 - acc: 0.4614 - val_loss: 2.2039 - val_acc: 0.4377\n",
      "Epoch 36/100\n",
      "2s - loss: 2.0360 - acc: 0.4589 - val_loss: 2.2148 - val_acc: 0.4421\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0707 - acc: 0.4542 - val_loss: 2.2318 - val_acc: 0.4323\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0662 - acc: 0.4555 - val_loss: 2.2058 - val_acc: 0.4405\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0395 - acc: 0.4601 - val_loss: 2.2314 - val_acc: 0.4377\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0547 - acc: 0.4571 - val_loss: 2.1895 - val_acc: 0.4402\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0257 - acc: 0.4614 - val_loss: 2.1930 - val_acc: 0.4412\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0253 - acc: 0.4624 - val_loss: 2.1870 - val_acc: 0.4430\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0193 - acc: 0.4638 - val_loss: 2.1903 - val_acc: 0.4427\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0194 - acc: 0.4638 - val_loss: 2.1826 - val_acc: 0.4429\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0186 - acc: 0.4664 - val_loss: 2.1806 - val_acc: 0.4432\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0120 - acc: 0.4655 - val_loss: 2.1728 - val_acc: 0.4464\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0095 - acc: 0.4663 - val_loss: 2.1794 - val_acc: 0.4415\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0125 - acc: 0.4653 - val_loss: 2.1933 - val_acc: 0.4453\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0166 - acc: 0.4666 - val_loss: 2.1817 - val_acc: 0.4435\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0096 - acc: 0.4664 - val_loss: 2.1840 - val_acc: 0.4432\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0026 - acc: 0.4668 - val_loss: 2.1855 - val_acc: 0.4437\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0034 - acc: 0.4680 - val_loss: 2.1872 - val_acc: 0.4415\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0027 - acc: 0.4683 - val_loss: 2.1807 - val_acc: 0.4422\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9969 - acc: 0.4670 - val_loss: 2.1945 - val_acc: 0.4440\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0061 - acc: 0.4654 - val_loss: 2.1945 - val_acc: 0.4399\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0090 - acc: 0.4665 - val_loss: 2.1866 - val_acc: 0.4416\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9989 - acc: 0.4674 - val_loss: 2.1852 - val_acc: 0.4424\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9939 - acc: 0.4692 - val_loss: 2.1861 - val_acc: 0.4433\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9923 - acc: 0.4687 - val_loss: 2.1797 - val_acc: 0.4422\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9902 - acc: 0.4719 - val_loss: 2.1872 - val_acc: 0.4454\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9925 - acc: 0.4713 - val_loss: 2.1939 - val_acc: 0.4406\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9914 - acc: 0.4708 - val_loss: 2.1889 - val_acc: 0.4432\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9882 - acc: 0.4727 - val_loss: 2.1938 - val_acc: 0.4425\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9903 - acc: 0.4700 - val_loss: 2.1910 - val_acc: 0.4439\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9815 - acc: 0.4748 - val_loss: 2.1963 - val_acc: 0.4409\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9800 - acc: 0.4725 - val_loss: 2.1876 - val_acc: 0.4445\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9836 - acc: 0.4737 - val_loss: 2.2149 - val_acc: 0.4395\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9995 - acc: 0.4684 - val_loss: 2.1785 - val_acc: 0.4462\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9914 - acc: 0.4701 - val_loss: 2.2083 - val_acc: 0.4443\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9921 - acc: 0.4701 - val_loss: 2.1796 - val_acc: 0.4462\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9909 - acc: 0.4705 - val_loss: 2.1956 - val_acc: 0.4421\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9806 - acc: 0.4715 - val_loss: 2.1924 - val_acc: 0.4442\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9869 - acc: 0.4727 - val_loss: 2.1943 - val_acc: 0.4413\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9761 - acc: 0.4737 - val_loss: 2.2129 - val_acc: 0.4412\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0170 - acc: 0.4646 - val_loss: 2.2270 - val_acc: 0.4356\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0048 - acc: 0.4662 - val_loss: 2.1992 - val_acc: 0.4423\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9750 - acc: 0.4742 - val_loss: 2.1850 - val_acc: 0.4436\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9701 - acc: 0.4761 - val_loss: 2.2122 - val_acc: 0.4419\n",
      "Epoch 79/100\n",
      "2s - loss: 1.9779 - acc: 0.4745 - val_loss: 2.1891 - val_acc: 0.4424\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9726 - acc: 0.4751 - val_loss: 2.2056 - val_acc: 0.4414\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9678 - acc: 0.4748 - val_loss: 2.1827 - val_acc: 0.4446\n",
      "Epoch 82/100\n",
      "2s - loss: 1.9595 - acc: 0.4769 - val_loss: 2.1886 - val_acc: 0.4446\n",
      "Epoch 83/100\n",
      "2s - loss: 1.9676 - acc: 0.4768 - val_loss: 2.1849 - val_acc: 0.4424\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9636 - acc: 0.4748 - val_loss: 2.2020 - val_acc: 0.4410\n",
      "Epoch 85/100\n",
      "2s - loss: 1.9632 - acc: 0.4743 - val_loss: 2.2211 - val_acc: 0.4375\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9722 - acc: 0.4751 - val_loss: 2.1939 - val_acc: 0.4429\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9612 - acc: 0.4766 - val_loss: 2.2591 - val_acc: 0.4314\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9989 - acc: 0.4709 - val_loss: 2.1970 - val_acc: 0.4416\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9551 - acc: 0.4782 - val_loss: 2.1953 - val_acc: 0.4436\n",
      "Epoch 90/100\n",
      "2s - loss: 1.9580 - acc: 0.4768 - val_loss: 2.2081 - val_acc: 0.4399\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9576 - acc: 0.4767 - val_loss: 2.1830 - val_acc: 0.4454\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9461 - acc: 0.4798 - val_loss: 2.2054 - val_acc: 0.4416\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9524 - acc: 0.4806 - val_loss: 2.1819 - val_acc: 0.4461\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9474 - acc: 0.4806 - val_loss: 2.2163 - val_acc: 0.4425\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9536 - acc: 0.4774 - val_loss: 2.1868 - val_acc: 0.4444\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9464 - acc: 0.4787 - val_loss: 2.2006 - val_acc: 0.4413\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9417 - acc: 0.4811 - val_loss: 2.1999 - val_acc: 0.4417\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9458 - acc: 0.4811 - val_loss: 2.2079 - val_acc: 0.4431\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9461 - acc: 0.4804 - val_loss: 2.2224 - val_acc: 0.4383\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9430 - acc: 0.4826 - val_loss: 2.1943 - val_acc: 0.4449\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2148 - acc: 0.4254 - val_loss: 2.1774 - val_acc: 0.4474\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1878 - acc: 0.4283 - val_loss: 2.1775 - val_acc: 0.4482\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1762 - acc: 0.4316 - val_loss: 2.1795 - val_acc: 0.4466\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1628 - acc: 0.4346 - val_loss: 2.1829 - val_acc: 0.4464\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1572 - acc: 0.4345 - val_loss: 2.1877 - val_acc: 0.4430\n",
      "Epoch 6/100\n",
      "2s - loss: 2.1516 - acc: 0.4353 - val_loss: 2.2048 - val_acc: 0.4426\n",
      "Epoch 7/100\n",
      "2s - loss: 2.1441 - acc: 0.4374 - val_loss: 2.2000 - val_acc: 0.4417\n",
      "Epoch 8/100\n",
      "2s - loss: 2.1376 - acc: 0.4401 - val_loss: 2.2234 - val_acc: 0.4387\n",
      "Epoch 9/100\n",
      "2s - loss: 2.1341 - acc: 0.4414 - val_loss: 2.2071 - val_acc: 0.4399\n",
      "Epoch 10/100\n",
      "2s - loss: 2.1263 - acc: 0.4432 - val_loss: 2.2129 - val_acc: 0.4384\n",
      "Epoch 11/100\n",
      "2s - loss: 2.1258 - acc: 0.4433 - val_loss: 2.2249 - val_acc: 0.4374\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1205 - acc: 0.4442 - val_loss: 2.2149 - val_acc: 0.4396\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1222 - acc: 0.4446 - val_loss: 2.2249 - val_acc: 0.4390\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1112 - acc: 0.4468 - val_loss: 2.2304 - val_acc: 0.4371\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1049 - acc: 0.4478 - val_loss: 2.2170 - val_acc: 0.4399\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1083 - acc: 0.4451 - val_loss: 2.2247 - val_acc: 0.4403\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1007 - acc: 0.4473 - val_loss: 2.2199 - val_acc: 0.4411\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0981 - acc: 0.4475 - val_loss: 2.2163 - val_acc: 0.4401\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0971 - acc: 0.4475 - val_loss: 2.2189 - val_acc: 0.4395\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0858 - acc: 0.4513 - val_loss: 2.2374 - val_acc: 0.4360\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0907 - acc: 0.4490 - val_loss: 2.2270 - val_acc: 0.4405\n",
      "Epoch 22/100\n",
      "2s - loss: 2.0923 - acc: 0.4492 - val_loss: 2.2426 - val_acc: 0.4372\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0954 - acc: 0.4480 - val_loss: 2.2246 - val_acc: 0.4399\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0845 - acc: 0.4517 - val_loss: 2.2182 - val_acc: 0.4393\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0786 - acc: 0.4546 - val_loss: 2.2230 - val_acc: 0.4412\n",
      "Epoch 26/100\n",
      "2s - loss: 2.0791 - acc: 0.4539 - val_loss: 2.2212 - val_acc: 0.4413\n",
      "Epoch 27/100\n",
      "2s - loss: 2.0699 - acc: 0.4551 - val_loss: 2.2131 - val_acc: 0.4403\n",
      "Epoch 28/100\n",
      "2s - loss: 2.0696 - acc: 0.4530 - val_loss: 2.2037 - val_acc: 0.4388\n",
      "Epoch 29/100\n",
      "2s - loss: 2.0652 - acc: 0.4563 - val_loss: 2.2156 - val_acc: 0.4409\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0636 - acc: 0.4558 - val_loss: 2.2087 - val_acc: 0.4394\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0608 - acc: 0.4560 - val_loss: 2.2115 - val_acc: 0.4400\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0590 - acc: 0.4577 - val_loss: 2.2168 - val_acc: 0.4438\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0575 - acc: 0.4554 - val_loss: 2.2032 - val_acc: 0.4400\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0548 - acc: 0.4567 - val_loss: 2.2052 - val_acc: 0.4410\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0582 - acc: 0.4549 - val_loss: 2.1967 - val_acc: 0.4435\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0678 - acc: 0.4550 - val_loss: 2.2128 - val_acc: 0.4389\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0548 - acc: 0.4565 - val_loss: 2.2062 - val_acc: 0.4419\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0539 - acc: 0.4567 - val_loss: 2.2212 - val_acc: 0.4389\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0525 - acc: 0.4573 - val_loss: 2.2044 - val_acc: 0.4425\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0483 - acc: 0.4605 - val_loss: 2.2233 - val_acc: 0.4386\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0503 - acc: 0.4587 - val_loss: 2.2313 - val_acc: 0.4407\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0497 - acc: 0.4587 - val_loss: 2.2031 - val_acc: 0.4423\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0330 - acc: 0.4613 - val_loss: 2.2542 - val_acc: 0.4359\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0455 - acc: 0.4589 - val_loss: 2.2039 - val_acc: 0.4400\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0409 - acc: 0.4608 - val_loss: 2.2039 - val_acc: 0.4425\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0407 - acc: 0.4592 - val_loss: 2.1955 - val_acc: 0.4423\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0341 - acc: 0.4628 - val_loss: 2.1796 - val_acc: 0.4463\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0350 - acc: 0.4622 - val_loss: 2.1863 - val_acc: 0.4429\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0284 - acc: 0.4622 - val_loss: 2.1821 - val_acc: 0.4456\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0279 - acc: 0.4636 - val_loss: 2.1999 - val_acc: 0.4419\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0481 - acc: 0.4604 - val_loss: 2.2121 - val_acc: 0.4418\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0307 - acc: 0.4613 - val_loss: 2.1962 - val_acc: 0.4431\n",
      "Epoch 53/100\n",
      "2s - loss: 2.0225 - acc: 0.4639 - val_loss: 2.2020 - val_acc: 0.4428\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0183 - acc: 0.4657 - val_loss: 2.1896 - val_acc: 0.4450\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0220 - acc: 0.4631 - val_loss: 2.1831 - val_acc: 0.4469\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0237 - acc: 0.4643 - val_loss: 2.2031 - val_acc: 0.4443\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0267 - acc: 0.4629 - val_loss: 2.1816 - val_acc: 0.4471\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0222 - acc: 0.4630 - val_loss: 2.1967 - val_acc: 0.4419\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0186 - acc: 0.4652 - val_loss: 2.1876 - val_acc: 0.4452\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0087 - acc: 0.4680 - val_loss: 2.1897 - val_acc: 0.4450\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0115 - acc: 0.4658 - val_loss: 2.1816 - val_acc: 0.4457\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0456 - acc: 0.4603 - val_loss: 2.2238 - val_acc: 0.4395\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0379 - acc: 0.4621 - val_loss: 2.1902 - val_acc: 0.4469\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0171 - acc: 0.4633 - val_loss: 2.1947 - val_acc: 0.4453\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0022 - acc: 0.4695 - val_loss: 2.1861 - val_acc: 0.4446\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9977 - acc: 0.4696 - val_loss: 2.1935 - val_acc: 0.4444\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0020 - acc: 0.4698 - val_loss: 2.2039 - val_acc: 0.4457\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9963 - acc: 0.4686 - val_loss: 2.1898 - val_acc: 0.4437\n",
      "Epoch 69/100\n",
      "2s - loss: 1.9967 - acc: 0.4704 - val_loss: 2.1901 - val_acc: 0.4499\n",
      "Epoch 70/100\n",
      "2s - loss: 2.0050 - acc: 0.4683 - val_loss: 2.1787 - val_acc: 0.4481\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9922 - acc: 0.4717 - val_loss: 2.1781 - val_acc: 0.4479\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9915 - acc: 0.4727 - val_loss: 2.1787 - val_acc: 0.4465\n",
      "Epoch 73/100\n",
      "2s - loss: 1.9899 - acc: 0.4719 - val_loss: 2.2052 - val_acc: 0.4437\n",
      "Epoch 74/100\n",
      "2s - loss: 1.9933 - acc: 0.4724 - val_loss: 2.2050 - val_acc: 0.4434\n",
      "Epoch 75/100\n",
      "2s - loss: 2.0236 - acc: 0.4664 - val_loss: 2.2005 - val_acc: 0.4414\n",
      "Epoch 76/100\n",
      "2s - loss: 2.0104 - acc: 0.4669 - val_loss: 2.1871 - val_acc: 0.4453\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9847 - acc: 0.4723 - val_loss: 2.1822 - val_acc: 0.4465\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9888 - acc: 0.4709 - val_loss: 2.1916 - val_acc: 0.4442\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9863 - acc: 0.4729 - val_loss: 2.1845 - val_acc: 0.4461\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9970 - acc: 0.4719 - val_loss: 2.2133 - val_acc: 0.4418\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9857 - acc: 0.4728 - val_loss: 2.1808 - val_acc: 0.4470\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9804 - acc: 0.4735 - val_loss: 2.1960 - val_acc: 0.4427\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9769 - acc: 0.4736 - val_loss: 2.1848 - val_acc: 0.4475\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9702 - acc: 0.4762 - val_loss: 2.1840 - val_acc: 0.4441\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9707 - acc: 0.4764 - val_loss: 2.1850 - val_acc: 0.4464\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9741 - acc: 0.4746 - val_loss: 2.1946 - val_acc: 0.4432\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9714 - acc: 0.4761 - val_loss: 2.1891 - val_acc: 0.4462\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9812 - acc: 0.4723 - val_loss: 2.2029 - val_acc: 0.4453\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9799 - acc: 0.4733 - val_loss: 2.1935 - val_acc: 0.4474\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9815 - acc: 0.4730 - val_loss: 2.2010 - val_acc: 0.4426\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9775 - acc: 0.4715 - val_loss: 2.1915 - val_acc: 0.4469\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9837 - acc: 0.4726 - val_loss: 2.2034 - val_acc: 0.4428\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9834 - acc: 0.4718 - val_loss: 2.2114 - val_acc: 0.4436\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9719 - acc: 0.4768 - val_loss: 2.1865 - val_acc: 0.4460\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9632 - acc: 0.4773 - val_loss: 2.1773 - val_acc: 0.4475\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9542 - acc: 0.4796 - val_loss: 2.1800 - val_acc: 0.4467\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9516 - acc: 0.4783 - val_loss: 2.1789 - val_acc: 0.4482\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9576 - acc: 0.4785 - val_loss: 2.2049 - val_acc: 0.4417\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9762 - acc: 0.4752 - val_loss: 2.1802 - val_acc: 0.4455\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9695 - acc: 0.4759 - val_loss: 2.2028 - val_acc: 0.4422\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2322 - acc: 0.4239 - val_loss: 2.2330 - val_acc: 0.4326\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2066 - acc: 0.4295 - val_loss: 2.2444 - val_acc: 0.4295\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1947 - acc: 0.4307 - val_loss: 2.2440 - val_acc: 0.4282\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1823 - acc: 0.4335 - val_loss: 2.2552 - val_acc: 0.4260\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1747 - acc: 0.4344 - val_loss: 2.2633 - val_acc: 0.4258\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1696 - acc: 0.4375 - val_loss: 2.2584 - val_acc: 0.4239\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1640 - acc: 0.4389 - val_loss: 2.2817 - val_acc: 0.4251\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1819 - acc: 0.4352 - val_loss: 2.2641 - val_acc: 0.4263\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1513 - acc: 0.4398 - val_loss: 2.2920 - val_acc: 0.4229\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1549 - acc: 0.4402 - val_loss: 2.2683 - val_acc: 0.4250\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1512 - acc: 0.4402 - val_loss: 2.3097 - val_acc: 0.4184\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1442 - acc: 0.4405 - val_loss: 2.2753 - val_acc: 0.4229\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1357 - acc: 0.4426 - val_loss: 2.2780 - val_acc: 0.4244\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1321 - acc: 0.4437 - val_loss: 2.2855 - val_acc: 0.4216\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1230 - acc: 0.4447 - val_loss: 2.2731 - val_acc: 0.4216\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1214 - acc: 0.4473 - val_loss: 2.2678 - val_acc: 0.4269\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1310 - acc: 0.4441 - val_loss: 2.2674 - val_acc: 0.4251\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1134 - acc: 0.4490 - val_loss: 2.2777 - val_acc: 0.4225\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1129 - acc: 0.4484 - val_loss: 2.2956 - val_acc: 0.4200\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1275 - acc: 0.4435 - val_loss: 2.2705 - val_acc: 0.4250\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1225 - acc: 0.4448 - val_loss: 2.3079 - val_acc: 0.4176\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1125 - acc: 0.4484 - val_loss: 2.2774 - val_acc: 0.4221\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1131 - acc: 0.4483 - val_loss: 2.3124 - val_acc: 0.4214\n",
      "Epoch 24/100\n",
      "1s - loss: 2.1022 - acc: 0.4490 - val_loss: 2.2731 - val_acc: 0.4220\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1038 - acc: 0.4488 - val_loss: 2.2979 - val_acc: 0.4197\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0932 - acc: 0.4528 - val_loss: 2.2819 - val_acc: 0.4217\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0859 - acc: 0.4538 - val_loss: 2.2891 - val_acc: 0.4176\n",
      "Epoch 28/100\n",
      "1s - loss: 2.1034 - acc: 0.4499 - val_loss: 2.3091 - val_acc: 0.4227\n",
      "Epoch 29/100\n",
      "1s - loss: 2.1217 - acc: 0.4450 - val_loss: 2.2794 - val_acc: 0.4237\n",
      "Epoch 30/100\n",
      "1s - loss: 2.1144 - acc: 0.4475 - val_loss: 2.3050 - val_acc: 0.4205\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0992 - acc: 0.4514 - val_loss: 2.2673 - val_acc: 0.4256\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0874 - acc: 0.4513 - val_loss: 2.2818 - val_acc: 0.4269\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0911 - acc: 0.4508 - val_loss: 2.2651 - val_acc: 0.4279\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0864 - acc: 0.4529 - val_loss: 2.2877 - val_acc: 0.4241\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0866 - acc: 0.4542 - val_loss: 2.2808 - val_acc: 0.4257\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0698 - acc: 0.4566 - val_loss: 2.2635 - val_acc: 0.4266\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0671 - acc: 0.4587 - val_loss: 2.2767 - val_acc: 0.4269\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0692 - acc: 0.4577 - val_loss: 2.2513 - val_acc: 0.4289\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0665 - acc: 0.4556 - val_loss: 2.2686 - val_acc: 0.4291\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0615 - acc: 0.4580 - val_loss: 2.2535 - val_acc: 0.4276\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0677 - acc: 0.4579 - val_loss: 2.2756 - val_acc: 0.4267\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0657 - acc: 0.4570 - val_loss: 2.2480 - val_acc: 0.4313\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0523 - acc: 0.4601 - val_loss: 2.2545 - val_acc: 0.4284\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0558 - acc: 0.4590 - val_loss: 2.2398 - val_acc: 0.4313\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0633 - acc: 0.4593 - val_loss: 2.2689 - val_acc: 0.4279\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0666 - acc: 0.4578 - val_loss: 2.2424 - val_acc: 0.4319\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0655 - acc: 0.4582 - val_loss: 2.2730 - val_acc: 0.4283\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0518 - acc: 0.4608 - val_loss: 2.2344 - val_acc: 0.4323\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0493 - acc: 0.4617 - val_loss: 2.2531 - val_acc: 0.4282\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0438 - acc: 0.4629 - val_loss: 2.2353 - val_acc: 0.4317\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0431 - acc: 0.4638 - val_loss: 2.2646 - val_acc: 0.4282\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0531 - acc: 0.4586 - val_loss: 2.3122 - val_acc: 0.4201\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0741 - acc: 0.4551 - val_loss: 2.2788 - val_acc: 0.4295\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0531 - acc: 0.4601 - val_loss: 2.2341 - val_acc: 0.4339\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0409 - acc: 0.4632 - val_loss: 2.2823 - val_acc: 0.4259\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0592 - acc: 0.4590 - val_loss: 2.2374 - val_acc: 0.4344\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0609 - acc: 0.4595 - val_loss: 2.2816 - val_acc: 0.4273\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0546 - acc: 0.4575 - val_loss: 2.2410 - val_acc: 0.4344\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0466 - acc: 0.4606 - val_loss: 2.2388 - val_acc: 0.4310\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0369 - acc: 0.4649 - val_loss: 2.2334 - val_acc: 0.4324\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0278 - acc: 0.4661 - val_loss: 2.2476 - val_acc: 0.4328\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0318 - acc: 0.4628 - val_loss: 2.2563 - val_acc: 0.4298\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0272 - acc: 0.4662 - val_loss: 2.2488 - val_acc: 0.4306\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0179 - acc: 0.4688 - val_loss: 2.2402 - val_acc: 0.4330\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0347 - acc: 0.4669 - val_loss: 2.2495 - val_acc: 0.4320\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0256 - acc: 0.4693 - val_loss: 2.2346 - val_acc: 0.4337\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0142 - acc: 0.4697 - val_loss: 2.2403 - val_acc: 0.4350\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0241 - acc: 0.4676 - val_loss: 2.2675 - val_acc: 0.4255\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0354 - acc: 0.4643 - val_loss: 2.2591 - val_acc: 0.4319\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0386 - acc: 0.4641 - val_loss: 2.2456 - val_acc: 0.4306\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0292 - acc: 0.4658 - val_loss: 2.2554 - val_acc: 0.4324\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0291 - acc: 0.4657 - val_loss: 2.2323 - val_acc: 0.4342\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0090 - acc: 0.4715 - val_loss: 2.2544 - val_acc: 0.4287\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0159 - acc: 0.4686 - val_loss: 2.2447 - val_acc: 0.4314\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0138 - acc: 0.4687 - val_loss: 2.2560 - val_acc: 0.4308\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0086 - acc: 0.4700 - val_loss: 2.2471 - val_acc: 0.4315\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0014 - acc: 0.4722 - val_loss: 2.2459 - val_acc: 0.4338\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0062 - acc: 0.4704 - val_loss: 2.2465 - val_acc: 0.4317\n",
      "Epoch 79/100\n",
      "1s - loss: 2.0079 - acc: 0.4702 - val_loss: 2.2676 - val_acc: 0.4320\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0032 - acc: 0.4723 - val_loss: 2.2414 - val_acc: 0.4328\n",
      "Epoch 81/100\n",
      "1s - loss: 2.0089 - acc: 0.4703 - val_loss: 2.2912 - val_acc: 0.4285\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0194 - acc: 0.4694 - val_loss: 2.2477 - val_acc: 0.4323\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9973 - acc: 0.4728 - val_loss: 2.2499 - val_acc: 0.4321\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9885 - acc: 0.4759 - val_loss: 2.2637 - val_acc: 0.4339\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0406 - acc: 0.4655 - val_loss: 2.2917 - val_acc: 0.4239\n",
      "Epoch 86/100\n",
      "1s - loss: 2.0322 - acc: 0.4656 - val_loss: 2.3250 - val_acc: 0.4251\n",
      "Epoch 87/100\n",
      "1s - loss: 2.0209 - acc: 0.4694 - val_loss: 2.2546 - val_acc: 0.4320\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9915 - acc: 0.4731 - val_loss: 2.2706 - val_acc: 0.4343\n",
      "Epoch 89/100\n",
      "1s - loss: 2.0122 - acc: 0.4715 - val_loss: 2.2598 - val_acc: 0.4290\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9883 - acc: 0.4759 - val_loss: 2.2358 - val_acc: 0.4368\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9797 - acc: 0.4765 - val_loss: 2.2521 - val_acc: 0.4297\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9893 - acc: 0.4739 - val_loss: 2.2377 - val_acc: 0.4358\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9897 - acc: 0.4728 - val_loss: 2.2654 - val_acc: 0.4283\n",
      "Epoch 94/100\n",
      "1s - loss: 2.0005 - acc: 0.4701 - val_loss: 2.2425 - val_acc: 0.4319\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9940 - acc: 0.4729 - val_loss: 2.2578 - val_acc: 0.4330\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9917 - acc: 0.4765 - val_loss: 2.2795 - val_acc: 0.4284\n",
      "Epoch 97/100\n",
      "1s - loss: 2.0153 - acc: 0.4672 - val_loss: 2.2566 - val_acc: 0.4319\n",
      "Epoch 98/100\n",
      "1s - loss: 2.0070 - acc: 0.4710 - val_loss: 2.2633 - val_acc: 0.4320\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9844 - acc: 0.4766 - val_loss: 2.2424 - val_acc: 0.4333\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9848 - acc: 0.4748 - val_loss: 2.2633 - val_acc: 0.4324\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2087 - acc: 0.4254 - val_loss: 2.2099 - val_acc: 0.4375\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1734 - acc: 0.4327 - val_loss: 2.2159 - val_acc: 0.4387\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1626 - acc: 0.4337 - val_loss: 2.2192 - val_acc: 0.4373\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1511 - acc: 0.4367 - val_loss: 2.2373 - val_acc: 0.4317\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1422 - acc: 0.4385 - val_loss: 2.2252 - val_acc: 0.4337\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1391 - acc: 0.4376 - val_loss: 2.2329 - val_acc: 0.4338\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1264 - acc: 0.4419 - val_loss: 2.2309 - val_acc: 0.4349\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1250 - acc: 0.4430 - val_loss: 2.2231 - val_acc: 0.4348\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1214 - acc: 0.4432 - val_loss: 2.2221 - val_acc: 0.4338\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1150 - acc: 0.4433 - val_loss: 2.2344 - val_acc: 0.4342\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1132 - acc: 0.4440 - val_loss: 2.2269 - val_acc: 0.4358\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1088 - acc: 0.4461 - val_loss: 2.2463 - val_acc: 0.4325\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1134 - acc: 0.4429 - val_loss: 2.2287 - val_acc: 0.4356\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0941 - acc: 0.4479 - val_loss: 2.2406 - val_acc: 0.4341\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1033 - acc: 0.4452 - val_loss: 2.2334 - val_acc: 0.4328\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0883 - acc: 0.4497 - val_loss: 2.2365 - val_acc: 0.4330\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0827 - acc: 0.4517 - val_loss: 2.2384 - val_acc: 0.4315\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0815 - acc: 0.4522 - val_loss: 2.2407 - val_acc: 0.4337\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0770 - acc: 0.4529 - val_loss: 2.2222 - val_acc: 0.4351\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0768 - acc: 0.4522 - val_loss: 2.2318 - val_acc: 0.4360\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0829 - acc: 0.4499 - val_loss: 2.2247 - val_acc: 0.4355\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0669 - acc: 0.4535 - val_loss: 2.2349 - val_acc: 0.4336\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0680 - acc: 0.4531 - val_loss: 2.2301 - val_acc: 0.4341\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0682 - acc: 0.4540 - val_loss: 2.2146 - val_acc: 0.4390\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0573 - acc: 0.4547 - val_loss: 2.2206 - val_acc: 0.4346\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0572 - acc: 0.4557 - val_loss: 2.2224 - val_acc: 0.4369\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0551 - acc: 0.4577 - val_loss: 2.2406 - val_acc: 0.4348\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0651 - acc: 0.4546 - val_loss: 2.2208 - val_acc: 0.4350\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0475 - acc: 0.4581 - val_loss: 2.2330 - val_acc: 0.4336\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0590 - acc: 0.4561 - val_loss: 2.2163 - val_acc: 0.4381\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0435 - acc: 0.4596 - val_loss: 2.2322 - val_acc: 0.4369\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0607 - acc: 0.4566 - val_loss: 2.2308 - val_acc: 0.4319\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0430 - acc: 0.4594 - val_loss: 2.2137 - val_acc: 0.4369\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0333 - acc: 0.4623 - val_loss: 2.2213 - val_acc: 0.4376\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0315 - acc: 0.4633 - val_loss: 2.2131 - val_acc: 0.4351\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0368 - acc: 0.4597 - val_loss: 2.2373 - val_acc: 0.4356\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0359 - acc: 0.4617 - val_loss: 2.2225 - val_acc: 0.4338\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0316 - acc: 0.4617 - val_loss: 2.2439 - val_acc: 0.4338\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0266 - acc: 0.4624 - val_loss: 2.2156 - val_acc: 0.4347\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0261 - acc: 0.4631 - val_loss: 2.2212 - val_acc: 0.4372\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0203 - acc: 0.4652 - val_loss: 2.2236 - val_acc: 0.4363\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0269 - acc: 0.4623 - val_loss: 2.2212 - val_acc: 0.4377\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0282 - acc: 0.4625 - val_loss: 2.2286 - val_acc: 0.4353\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0235 - acc: 0.4635 - val_loss: 2.2309 - val_acc: 0.4388\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0170 - acc: 0.4647 - val_loss: 2.2082 - val_acc: 0.4388\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0126 - acc: 0.4677 - val_loss: 2.2106 - val_acc: 0.4397\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0129 - acc: 0.4661 - val_loss: 2.2312 - val_acc: 0.4383\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0099 - acc: 0.4659 - val_loss: 2.2050 - val_acc: 0.4396\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0077 - acc: 0.4684 - val_loss: 2.2123 - val_acc: 0.4399\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0036 - acc: 0.4682 - val_loss: 2.2036 - val_acc: 0.4380\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0058 - acc: 0.4671 - val_loss: 2.2430 - val_acc: 0.4354\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0061 - acc: 0.4670 - val_loss: 2.2176 - val_acc: 0.4349\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0093 - acc: 0.4679 - val_loss: 2.2369 - val_acc: 0.4402\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0040 - acc: 0.4700 - val_loss: 2.2066 - val_acc: 0.4377\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9945 - acc: 0.4720 - val_loss: 2.2295 - val_acc: 0.4372\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0013 - acc: 0.4700 - val_loss: 2.2315 - val_acc: 0.4344\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0053 - acc: 0.4676 - val_loss: 2.2368 - val_acc: 0.4398\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0122 - acc: 0.4658 - val_loss: 2.2115 - val_acc: 0.4371\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9977 - acc: 0.4720 - val_loss: 2.2112 - val_acc: 0.4400\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9925 - acc: 0.4701 - val_loss: 2.2158 - val_acc: 0.4369\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9918 - acc: 0.4719 - val_loss: 2.2174 - val_acc: 0.4394\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9907 - acc: 0.4698 - val_loss: 2.2169 - val_acc: 0.4359\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9910 - acc: 0.4700 - val_loss: 2.2192 - val_acc: 0.4392\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9840 - acc: 0.4717 - val_loss: 2.2114 - val_acc: 0.4382\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9849 - acc: 0.4729 - val_loss: 2.2323 - val_acc: 0.4397\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0030 - acc: 0.4694 - val_loss: 2.2101 - val_acc: 0.4387\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9854 - acc: 0.4722 - val_loss: 2.2128 - val_acc: 0.4390\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9829 - acc: 0.4737 - val_loss: 2.1976 - val_acc: 0.4433\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9813 - acc: 0.4735 - val_loss: 2.2113 - val_acc: 0.4390\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9744 - acc: 0.4739 - val_loss: 2.2085 - val_acc: 0.4385\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9763 - acc: 0.4754 - val_loss: 2.2367 - val_acc: 0.4401\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9879 - acc: 0.4707 - val_loss: 2.2007 - val_acc: 0.4406\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9656 - acc: 0.4761 - val_loss: 2.2166 - val_acc: 0.4416\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9805 - acc: 0.4746 - val_loss: 2.2087 - val_acc: 0.4386\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9687 - acc: 0.4762 - val_loss: 2.2233 - val_acc: 0.4379\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9688 - acc: 0.4770 - val_loss: 2.2313 - val_acc: 0.4389\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9743 - acc: 0.4762 - val_loss: 2.2104 - val_acc: 0.4409\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9699 - acc: 0.4765 - val_loss: 2.2168 - val_acc: 0.4363\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9604 - acc: 0.4783 - val_loss: 2.2025 - val_acc: 0.4426\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9579 - acc: 0.4770 - val_loss: 2.2060 - val_acc: 0.4386\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9553 - acc: 0.4796 - val_loss: 2.2137 - val_acc: 0.4422\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9580 - acc: 0.4786 - val_loss: 2.2137 - val_acc: 0.4372\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9575 - acc: 0.4784 - val_loss: 2.2350 - val_acc: 0.4361\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9717 - acc: 0.4772 - val_loss: 2.2379 - val_acc: 0.4327\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9698 - acc: 0.4733 - val_loss: 2.2221 - val_acc: 0.4404\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9629 - acc: 0.4770 - val_loss: 2.2065 - val_acc: 0.4432\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9595 - acc: 0.4785 - val_loss: 2.2093 - val_acc: 0.4410\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9470 - acc: 0.4819 - val_loss: 2.2105 - val_acc: 0.4371\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9438 - acc: 0.4803 - val_loss: 2.2172 - val_acc: 0.4371\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9449 - acc: 0.4824 - val_loss: 2.2046 - val_acc: 0.4380\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9449 - acc: 0.4822 - val_loss: 2.2355 - val_acc: 0.4358\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9496 - acc: 0.4814 - val_loss: 2.2271 - val_acc: 0.4357\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9603 - acc: 0.4787 - val_loss: 2.2283 - val_acc: 0.4412\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9457 - acc: 0.4837 - val_loss: 2.2204 - val_acc: 0.4369\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9419 - acc: 0.4835 - val_loss: 2.2242 - val_acc: 0.4396\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9523 - acc: 0.4814 - val_loss: 2.2152 - val_acc: 0.4352\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9473 - acc: 0.4815 - val_loss: 2.2230 - val_acc: 0.4417\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9393 - acc: 0.4825 - val_loss: 2.2317 - val_acc: 0.4342\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9475 - acc: 0.4823 - val_loss: 2.2237 - val_acc: 0.4383\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9398 - acc: 0.4833 - val_loss: 2.2159 - val_acc: 0.4377\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1790 - acc: 0.4270 - val_loss: 2.1982 - val_acc: 0.4382\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1593 - acc: 0.4298 - val_loss: 2.2011 - val_acc: 0.4379\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1451 - acc: 0.4332 - val_loss: 2.2082 - val_acc: 0.4367\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1332 - acc: 0.4362 - val_loss: 2.2316 - val_acc: 0.4325\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1332 - acc: 0.4375 - val_loss: 2.2118 - val_acc: 0.4357\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1180 - acc: 0.4382 - val_loss: 2.2227 - val_acc: 0.4320\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1127 - acc: 0.4407 - val_loss: 2.2345 - val_acc: 0.4301\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1077 - acc: 0.4412 - val_loss: 2.2316 - val_acc: 0.4286\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0993 - acc: 0.4425 - val_loss: 2.2363 - val_acc: 0.4281\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0934 - acc: 0.4444 - val_loss: 2.2320 - val_acc: 0.4290\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0917 - acc: 0.4464 - val_loss: 2.2292 - val_acc: 0.4286\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0861 - acc: 0.4471 - val_loss: 2.2270 - val_acc: 0.4304\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0802 - acc: 0.4471 - val_loss: 2.2375 - val_acc: 0.4282\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0775 - acc: 0.4480 - val_loss: 2.2445 - val_acc: 0.4282\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0743 - acc: 0.4466 - val_loss: 2.2456 - val_acc: 0.4276\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0683 - acc: 0.4497 - val_loss: 2.2341 - val_acc: 0.4304\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0610 - acc: 0.4508 - val_loss: 2.2200 - val_acc: 0.4323\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0605 - acc: 0.4538 - val_loss: 2.2240 - val_acc: 0.4301\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0589 - acc: 0.4522 - val_loss: 2.2260 - val_acc: 0.4320\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0564 - acc: 0.4514 - val_loss: 2.2268 - val_acc: 0.4328\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0558 - acc: 0.4524 - val_loss: 2.2188 - val_acc: 0.4332\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0508 - acc: 0.4547 - val_loss: 2.2200 - val_acc: 0.4323\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0440 - acc: 0.4565 - val_loss: 2.2456 - val_acc: 0.4283\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0509 - acc: 0.4541 - val_loss: 2.2192 - val_acc: 0.4318\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0386 - acc: 0.4559 - val_loss: 2.2304 - val_acc: 0.4341\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0416 - acc: 0.4549 - val_loss: 2.2120 - val_acc: 0.4343\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0341 - acc: 0.4574 - val_loss: 2.2218 - val_acc: 0.4339\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0403 - acc: 0.4564 - val_loss: 2.2214 - val_acc: 0.4338\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0299 - acc: 0.4567 - val_loss: 2.2097 - val_acc: 0.4361\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0216 - acc: 0.4576 - val_loss: 2.2117 - val_acc: 0.4375\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0261 - acc: 0.4589 - val_loss: 2.2242 - val_acc: 0.4325\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0202 - acc: 0.4591 - val_loss: 2.2137 - val_acc: 0.4364\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0210 - acc: 0.4608 - val_loss: 2.2339 - val_acc: 0.4325\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0181 - acc: 0.4604 - val_loss: 2.2221 - val_acc: 0.4349\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0184 - acc: 0.4610 - val_loss: 2.2158 - val_acc: 0.4333\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0156 - acc: 0.4598 - val_loss: 2.2178 - val_acc: 0.4342\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0069 - acc: 0.4620 - val_loss: 2.2144 - val_acc: 0.4362\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0073 - acc: 0.4624 - val_loss: 2.2018 - val_acc: 0.4384\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0059 - acc: 0.4639 - val_loss: 2.2103 - val_acc: 0.4356\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0036 - acc: 0.4638 - val_loss: 2.2114 - val_acc: 0.4379\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0074 - acc: 0.4631 - val_loss: 2.2075 - val_acc: 0.4368\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0019 - acc: 0.4653 - val_loss: 2.2019 - val_acc: 0.4396\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0014 - acc: 0.4632 - val_loss: 2.2201 - val_acc: 0.4354\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0023 - acc: 0.4633 - val_loss: 2.1974 - val_acc: 0.4383\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9984 - acc: 0.4658 - val_loss: 2.2068 - val_acc: 0.4394\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9945 - acc: 0.4651 - val_loss: 2.2100 - val_acc: 0.4379\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9997 - acc: 0.4658 - val_loss: 2.1986 - val_acc: 0.4399\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9886 - acc: 0.4660 - val_loss: 2.2052 - val_acc: 0.4409\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9873 - acc: 0.4675 - val_loss: 2.1988 - val_acc: 0.4386\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9833 - acc: 0.4692 - val_loss: 2.2091 - val_acc: 0.4365\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9852 - acc: 0.4669 - val_loss: 2.2027 - val_acc: 0.4373\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9816 - acc: 0.4687 - val_loss: 2.1925 - val_acc: 0.4387\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9811 - acc: 0.4689 - val_loss: 2.1899 - val_acc: 0.4419\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9797 - acc: 0.4698 - val_loss: 2.1872 - val_acc: 0.4399\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9738 - acc: 0.4680 - val_loss: 2.2032 - val_acc: 0.4396\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9799 - acc: 0.4669 - val_loss: 2.2112 - val_acc: 0.4387\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9784 - acc: 0.4677 - val_loss: 2.2008 - val_acc: 0.4395\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9712 - acc: 0.4708 - val_loss: 2.1942 - val_acc: 0.4412\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9671 - acc: 0.4716 - val_loss: 2.1892 - val_acc: 0.4414\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9662 - acc: 0.4718 - val_loss: 2.1959 - val_acc: 0.4392\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9607 - acc: 0.4733 - val_loss: 2.1943 - val_acc: 0.4399\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9675 - acc: 0.4723 - val_loss: 2.2146 - val_acc: 0.4382\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9967 - acc: 0.4661 - val_loss: 2.2121 - val_acc: 0.4370\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9783 - acc: 0.4668 - val_loss: 2.1947 - val_acc: 0.4401\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9605 - acc: 0.4724 - val_loss: 2.2084 - val_acc: 0.4382\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9650 - acc: 0.4718 - val_loss: 2.2034 - val_acc: 0.4402\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9630 - acc: 0.4720 - val_loss: 2.2017 - val_acc: 0.4400\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9562 - acc: 0.4736 - val_loss: 2.2021 - val_acc: 0.4415\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9503 - acc: 0.4745 - val_loss: 2.1898 - val_acc: 0.4404\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9486 - acc: 0.4762 - val_loss: 2.1979 - val_acc: 0.4412\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9542 - acc: 0.4744 - val_loss: 2.2053 - val_acc: 0.4414\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9622 - acc: 0.4734 - val_loss: 2.1931 - val_acc: 0.4390\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9482 - acc: 0.4744 - val_loss: 2.2247 - val_acc: 0.4379\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9782 - acc: 0.4709 - val_loss: 2.2107 - val_acc: 0.4356\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9572 - acc: 0.4749 - val_loss: 2.2105 - val_acc: 0.4394\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9530 - acc: 0.4739 - val_loss: 2.2028 - val_acc: 0.4383\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9450 - acc: 0.4752 - val_loss: 2.2152 - val_acc: 0.4388\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9456 - acc: 0.4764 - val_loss: 2.1970 - val_acc: 0.4390\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9432 - acc: 0.4770 - val_loss: 2.1973 - val_acc: 0.4413\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9385 - acc: 0.4786 - val_loss: 2.2083 - val_acc: 0.4366\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9473 - acc: 0.4749 - val_loss: 2.2430 - val_acc: 0.4326\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9569 - acc: 0.4733 - val_loss: 2.2109 - val_acc: 0.4369\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9381 - acc: 0.4787 - val_loss: 2.2125 - val_acc: 0.4371\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9401 - acc: 0.4769 - val_loss: 2.1881 - val_acc: 0.4400\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9327 - acc: 0.4779 - val_loss: 2.2113 - val_acc: 0.4402\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9414 - acc: 0.4774 - val_loss: 2.1966 - val_acc: 0.4399\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9343 - acc: 0.4800 - val_loss: 2.1988 - val_acc: 0.4394\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9276 - acc: 0.4820 - val_loss: 2.2048 - val_acc: 0.4403\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9314 - acc: 0.4799 - val_loss: 2.2099 - val_acc: 0.4384\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9258 - acc: 0.4816 - val_loss: 2.1963 - val_acc: 0.4411\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9331 - acc: 0.4796 - val_loss: 2.2119 - val_acc: 0.4374\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9228 - acc: 0.4814 - val_loss: 2.1986 - val_acc: 0.4402\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9187 - acc: 0.4837 - val_loss: 2.2087 - val_acc: 0.4355\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9227 - acc: 0.4830 - val_loss: 2.2126 - val_acc: 0.4402\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9375 - acc: 0.4783 - val_loss: 2.2121 - val_acc: 0.4349\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9246 - acc: 0.4823 - val_loss: 2.2485 - val_acc: 0.4337\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9466 - acc: 0.4758 - val_loss: 2.2196 - val_acc: 0.4346\n",
      "Epoch 98/100\n",
      "2s - loss: 1.9369 - acc: 0.4788 - val_loss: 2.2135 - val_acc: 0.4372\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9258 - acc: 0.4819 - val_loss: 2.2168 - val_acc: 0.4367\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9130 - acc: 0.4833 - val_loss: 2.2051 - val_acc: 0.4389\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1755 - acc: 0.4315 - val_loss: 2.2369 - val_acc: 0.4306\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1539 - acc: 0.4369 - val_loss: 2.2465 - val_acc: 0.4278\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1410 - acc: 0.4401 - val_loss: 2.2461 - val_acc: 0.4276\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1277 - acc: 0.4429 - val_loss: 2.2491 - val_acc: 0.4262\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1240 - acc: 0.4422 - val_loss: 2.2654 - val_acc: 0.4233\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1160 - acc: 0.4452 - val_loss: 2.2676 - val_acc: 0.4252\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1389 - acc: 0.4408 - val_loss: 2.3039 - val_acc: 0.4209\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1166 - acc: 0.4447 - val_loss: 2.2643 - val_acc: 0.4227\n",
      "Epoch 9/100\n",
      "2s - loss: 2.0985 - acc: 0.4488 - val_loss: 2.2765 - val_acc: 0.4233\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0950 - acc: 0.4484 - val_loss: 2.2677 - val_acc: 0.4244\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0881 - acc: 0.4501 - val_loss: 2.2915 - val_acc: 0.4186\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0927 - acc: 0.4487 - val_loss: 2.2686 - val_acc: 0.4236\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0785 - acc: 0.4515 - val_loss: 2.2716 - val_acc: 0.4231\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0788 - acc: 0.4518 - val_loss: 2.2731 - val_acc: 0.4210\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0738 - acc: 0.4533 - val_loss: 2.2801 - val_acc: 0.4219\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0683 - acc: 0.4555 - val_loss: 2.2738 - val_acc: 0.4218\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0712 - acc: 0.4545 - val_loss: 2.2816 - val_acc: 0.4214\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0625 - acc: 0.4552 - val_loss: 2.2753 - val_acc: 0.4224\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0641 - acc: 0.4546 - val_loss: 2.2850 - val_acc: 0.4197\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0653 - acc: 0.4566 - val_loss: 2.2766 - val_acc: 0.4192\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0615 - acc: 0.4562 - val_loss: 2.2722 - val_acc: 0.4223\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0541 - acc: 0.4578 - val_loss: 2.2888 - val_acc: 0.4198\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0494 - acc: 0.4577 - val_loss: 2.2669 - val_acc: 0.4230\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0481 - acc: 0.4585 - val_loss: 2.2683 - val_acc: 0.4226\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0411 - acc: 0.4596 - val_loss: 2.2730 - val_acc: 0.4216\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0448 - acc: 0.4605 - val_loss: 2.2642 - val_acc: 0.4242\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0383 - acc: 0.4605 - val_loss: 2.2639 - val_acc: 0.4242\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0370 - acc: 0.4600 - val_loss: 2.2581 - val_acc: 0.4235\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0431 - acc: 0.4581 - val_loss: 2.2783 - val_acc: 0.4201\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0427 - acc: 0.4604 - val_loss: 2.2721 - val_acc: 0.4229\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0318 - acc: 0.4622 - val_loss: 2.2598 - val_acc: 0.4226\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0332 - acc: 0.4616 - val_loss: 2.2558 - val_acc: 0.4253\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0275 - acc: 0.4624 - val_loss: 2.2849 - val_acc: 0.4241\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0555 - acc: 0.4574 - val_loss: 2.2709 - val_acc: 0.4226\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0349 - acc: 0.4626 - val_loss: 2.2922 - val_acc: 0.4232\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0343 - acc: 0.4622 - val_loss: 2.2586 - val_acc: 0.4235\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0228 - acc: 0.4631 - val_loss: 2.2663 - val_acc: 0.4239\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0143 - acc: 0.4655 - val_loss: 2.2780 - val_acc: 0.4215\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0164 - acc: 0.4671 - val_loss: 2.2811 - val_acc: 0.4247\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0219 - acc: 0.4657 - val_loss: 2.2557 - val_acc: 0.4261\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0213 - acc: 0.4625 - val_loss: 2.2757 - val_acc: 0.4236\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0130 - acc: 0.4669 - val_loss: 2.2743 - val_acc: 0.4216\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0140 - acc: 0.4645 - val_loss: 2.2578 - val_acc: 0.4283\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0121 - acc: 0.4671 - val_loss: 2.2454 - val_acc: 0.4263\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0051 - acc: 0.4680 - val_loss: 2.2661 - val_acc: 0.4259\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0158 - acc: 0.4662 - val_loss: 2.2645 - val_acc: 0.4244\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0089 - acc: 0.4669 - val_loss: 2.2631 - val_acc: 0.4268\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0016 - acc: 0.4701 - val_loss: 2.2475 - val_acc: 0.4260\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9964 - acc: 0.4703 - val_loss: 2.2669 - val_acc: 0.4256\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9956 - acc: 0.4711 - val_loss: 2.2519 - val_acc: 0.4283\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9902 - acc: 0.4717 - val_loss: 2.2514 - val_acc: 0.4283\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9848 - acc: 0.4732 - val_loss: 2.2509 - val_acc: 0.4274\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9837 - acc: 0.4718 - val_loss: 2.2618 - val_acc: 0.4291\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9864 - acc: 0.4738 - val_loss: 2.2586 - val_acc: 0.4263\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9890 - acc: 0.4717 - val_loss: 2.2695 - val_acc: 0.4260\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9843 - acc: 0.4723 - val_loss: 2.2435 - val_acc: 0.4279\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9804 - acc: 0.4731 - val_loss: 2.2635 - val_acc: 0.4259\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9791 - acc: 0.4718 - val_loss: 2.2450 - val_acc: 0.4286\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9774 - acc: 0.4753 - val_loss: 2.2730 - val_acc: 0.4231\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9901 - acc: 0.4711 - val_loss: 2.2618 - val_acc: 0.4233\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9930 - acc: 0.4699 - val_loss: 2.2865 - val_acc: 0.4241\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0019 - acc: 0.4675 - val_loss: 2.2395 - val_acc: 0.4316\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0069 - acc: 0.4652 - val_loss: 2.2723 - val_acc: 0.4258\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9788 - acc: 0.4743 - val_loss: 2.2369 - val_acc: 0.4293\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9755 - acc: 0.4742 - val_loss: 2.2461 - val_acc: 0.4297\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9687 - acc: 0.4746 - val_loss: 2.2469 - val_acc: 0.4273\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9752 - acc: 0.4735 - val_loss: 2.2600 - val_acc: 0.4254\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9647 - acc: 0.4760 - val_loss: 2.2403 - val_acc: 0.4296\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9704 - acc: 0.4758 - val_loss: 2.2536 - val_acc: 0.4256\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9669 - acc: 0.4772 - val_loss: 2.2478 - val_acc: 0.4275\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9689 - acc: 0.4754 - val_loss: 2.2474 - val_acc: 0.4289\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9634 - acc: 0.4780 - val_loss: 2.2360 - val_acc: 0.4299\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9557 - acc: 0.4790 - val_loss: 2.2559 - val_acc: 0.4293\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9585 - acc: 0.4795 - val_loss: 2.2474 - val_acc: 0.4281\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9688 - acc: 0.4769 - val_loss: 2.2658 - val_acc: 0.4261\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9698 - acc: 0.4754 - val_loss: 2.2467 - val_acc: 0.4282\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9489 - acc: 0.4819 - val_loss: 2.2478 - val_acc: 0.4278\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9519 - acc: 0.4805 - val_loss: 2.2479 - val_acc: 0.4308\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9493 - acc: 0.4807 - val_loss: 2.2614 - val_acc: 0.4259\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9637 - acc: 0.4781 - val_loss: 2.2529 - val_acc: 0.4275\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9585 - acc: 0.4801 - val_loss: 2.2433 - val_acc: 0.4287\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9364 - acc: 0.4823 - val_loss: 2.2292 - val_acc: 0.4319\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9322 - acc: 0.4846 - val_loss: 2.2487 - val_acc: 0.4284\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9487 - acc: 0.4815 - val_loss: 2.2491 - val_acc: 0.4271\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9427 - acc: 0.4813 - val_loss: 2.2638 - val_acc: 0.4263\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9608 - acc: 0.4778 - val_loss: 2.2540 - val_acc: 0.4275\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9556 - acc: 0.4794 - val_loss: 2.2672 - val_acc: 0.4259\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9582 - acc: 0.4766 - val_loss: 2.2581 - val_acc: 0.4279\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9718 - acc: 0.4765 - val_loss: 2.2669 - val_acc: 0.4238\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9647 - acc: 0.4776 - val_loss: 2.2431 - val_acc: 0.4300\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9582 - acc: 0.4782 - val_loss: 2.2625 - val_acc: 0.4269\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9606 - acc: 0.4785 - val_loss: 2.2408 - val_acc: 0.4279\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9420 - acc: 0.4816 - val_loss: 2.2690 - val_acc: 0.4254\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9646 - acc: 0.4775 - val_loss: 2.2582 - val_acc: 0.4267\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9476 - acc: 0.4810 - val_loss: 2.2527 - val_acc: 0.4299\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9311 - acc: 0.4852 - val_loss: 2.2502 - val_acc: 0.4278\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9198 - acc: 0.4858 - val_loss: 2.2472 - val_acc: 0.4304\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9220 - acc: 0.4883 - val_loss: 2.2706 - val_acc: 0.4237\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9273 - acc: 0.4847 - val_loss: 2.2604 - val_acc: 0.4260\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9277 - acc: 0.4845 - val_loss: 2.2506 - val_acc: 0.4290\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1805 - acc: 0.4304 - val_loss: 2.1634 - val_acc: 0.4505\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1585 - acc: 0.4349 - val_loss: 2.1784 - val_acc: 0.4461\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1511 - acc: 0.4358 - val_loss: 2.1733 - val_acc: 0.4473\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1398 - acc: 0.4373 - val_loss: 2.1859 - val_acc: 0.4489\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1319 - acc: 0.4384 - val_loss: 2.1902 - val_acc: 0.4467\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1234 - acc: 0.4406 - val_loss: 2.1985 - val_acc: 0.4431\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1182 - acc: 0.4421 - val_loss: 2.1987 - val_acc: 0.4449\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1139 - acc: 0.4444 - val_loss: 2.2056 - val_acc: 0.4447\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1065 - acc: 0.4458 - val_loss: 2.2040 - val_acc: 0.4430\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1025 - acc: 0.4461 - val_loss: 2.2073 - val_acc: 0.4429\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0993 - acc: 0.4465 - val_loss: 2.2212 - val_acc: 0.4392\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0994 - acc: 0.4463 - val_loss: 2.2114 - val_acc: 0.4426\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0879 - acc: 0.4494 - val_loss: 2.2112 - val_acc: 0.4437\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0849 - acc: 0.4499 - val_loss: 2.2055 - val_acc: 0.4433\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0764 - acc: 0.4520 - val_loss: 2.2110 - val_acc: 0.4411\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0760 - acc: 0.4505 - val_loss: 2.2138 - val_acc: 0.4402\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0796 - acc: 0.4513 - val_loss: 2.2014 - val_acc: 0.4447\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0710 - acc: 0.4518 - val_loss: 2.2078 - val_acc: 0.4417\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0717 - acc: 0.4528 - val_loss: 2.2018 - val_acc: 0.4418\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0628 - acc: 0.4538 - val_loss: 2.2071 - val_acc: 0.4451\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0608 - acc: 0.4545 - val_loss: 2.2076 - val_acc: 0.4438\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0638 - acc: 0.4543 - val_loss: 2.2012 - val_acc: 0.4404\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0533 - acc: 0.4560 - val_loss: 2.1937 - val_acc: 0.4426\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0536 - acc: 0.4554 - val_loss: 2.1945 - val_acc: 0.4406\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0496 - acc: 0.4581 - val_loss: 2.2096 - val_acc: 0.4413\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0537 - acc: 0.4557 - val_loss: 2.1931 - val_acc: 0.4441\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0454 - acc: 0.4594 - val_loss: 2.2102 - val_acc: 0.4414\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0469 - acc: 0.4588 - val_loss: 2.1912 - val_acc: 0.4427\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0395 - acc: 0.4596 - val_loss: 2.1908 - val_acc: 0.4447\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0351 - acc: 0.4600 - val_loss: 2.1916 - val_acc: 0.4445\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0355 - acc: 0.4601 - val_loss: 2.1879 - val_acc: 0.4440\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0297 - acc: 0.4611 - val_loss: 2.1814 - val_acc: 0.4438\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0308 - acc: 0.4613 - val_loss: 2.1898 - val_acc: 0.4427\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0255 - acc: 0.4633 - val_loss: 2.1771 - val_acc: 0.4466\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0233 - acc: 0.4627 - val_loss: 2.1864 - val_acc: 0.4436\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0313 - acc: 0.4617 - val_loss: 2.2068 - val_acc: 0.4415\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0308 - acc: 0.4612 - val_loss: 2.1843 - val_acc: 0.4449\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0248 - acc: 0.4616 - val_loss: 2.1953 - val_acc: 0.4443\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0187 - acc: 0.4633 - val_loss: 2.1804 - val_acc: 0.4447\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0118 - acc: 0.4671 - val_loss: 2.1822 - val_acc: 0.4466\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0124 - acc: 0.4647 - val_loss: 2.1826 - val_acc: 0.4465\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0103 - acc: 0.4642 - val_loss: 2.1836 - val_acc: 0.4449\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0093 - acc: 0.4661 - val_loss: 2.1817 - val_acc: 0.4469\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0133 - acc: 0.4662 - val_loss: 2.1825 - val_acc: 0.4450\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0017 - acc: 0.4677 - val_loss: 2.1914 - val_acc: 0.4456\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0125 - acc: 0.4655 - val_loss: 2.2275 - val_acc: 0.4378\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0100 - acc: 0.4678 - val_loss: 2.1730 - val_acc: 0.4489\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0010 - acc: 0.4681 - val_loss: 2.1758 - val_acc: 0.4454\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9989 - acc: 0.4670 - val_loss: 2.1669 - val_acc: 0.4486\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9905 - acc: 0.4696 - val_loss: 2.1757 - val_acc: 0.4479\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9935 - acc: 0.4694 - val_loss: 2.2202 - val_acc: 0.4403\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0117 - acc: 0.4654 - val_loss: 2.1809 - val_acc: 0.4481\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9911 - acc: 0.4691 - val_loss: 2.1803 - val_acc: 0.4474\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9913 - acc: 0.4697 - val_loss: 2.1769 - val_acc: 0.4470\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9873 - acc: 0.4714 - val_loss: 2.1800 - val_acc: 0.4484\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9887 - acc: 0.4714 - val_loss: 2.1794 - val_acc: 0.4453\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9840 - acc: 0.4710 - val_loss: 2.1760 - val_acc: 0.4485\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9831 - acc: 0.4719 - val_loss: 2.1700 - val_acc: 0.4484\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9754 - acc: 0.4729 - val_loss: 2.1848 - val_acc: 0.4454\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9846 - acc: 0.4723 - val_loss: 2.1708 - val_acc: 0.4476\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9780 - acc: 0.4720 - val_loss: 2.1903 - val_acc: 0.4473\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0066 - acc: 0.4681 - val_loss: 2.1898 - val_acc: 0.4440\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9863 - acc: 0.4708 - val_loss: 2.1758 - val_acc: 0.4468\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9766 - acc: 0.4739 - val_loss: 2.1913 - val_acc: 0.4448\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9790 - acc: 0.4720 - val_loss: 2.1754 - val_acc: 0.4490\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9714 - acc: 0.4744 - val_loss: 2.1749 - val_acc: 0.4486\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9642 - acc: 0.4762 - val_loss: 2.1718 - val_acc: 0.4506\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9644 - acc: 0.4757 - val_loss: 2.1845 - val_acc: 0.4460\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9700 - acc: 0.4752 - val_loss: 2.1731 - val_acc: 0.4490\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9643 - acc: 0.4761 - val_loss: 2.1937 - val_acc: 0.4448\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9775 - acc: 0.4729 - val_loss: 2.1788 - val_acc: 0.4468\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9627 - acc: 0.4757 - val_loss: 2.1895 - val_acc: 0.4429\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9641 - acc: 0.4749 - val_loss: 2.1961 - val_acc: 0.4443\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9574 - acc: 0.4793 - val_loss: 2.1963 - val_acc: 0.4444\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9705 - acc: 0.4764 - val_loss: 2.1740 - val_acc: 0.4493\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9623 - acc: 0.4782 - val_loss: 2.1811 - val_acc: 0.4462\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9543 - acc: 0.4798 - val_loss: 2.1760 - val_acc: 0.4493\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9546 - acc: 0.4783 - val_loss: 2.1749 - val_acc: 0.4488\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9593 - acc: 0.4766 - val_loss: 2.1977 - val_acc: 0.4430\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9674 - acc: 0.4790 - val_loss: 2.1858 - val_acc: 0.4468\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9563 - acc: 0.4780 - val_loss: 2.1863 - val_acc: 0.4457\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9539 - acc: 0.4801 - val_loss: 2.1791 - val_acc: 0.4483\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9500 - acc: 0.4809 - val_loss: 2.1738 - val_acc: 0.4487\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9402 - acc: 0.4825 - val_loss: 2.1632 - val_acc: 0.4506\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9391 - acc: 0.4825 - val_loss: 2.2005 - val_acc: 0.4460\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9705 - acc: 0.4752 - val_loss: 2.1912 - val_acc: 0.4432\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9502 - acc: 0.4801 - val_loss: 2.2037 - val_acc: 0.4436\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9487 - acc: 0.4800 - val_loss: 2.1785 - val_acc: 0.4477\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9452 - acc: 0.4801 - val_loss: 2.1990 - val_acc: 0.4416\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9391 - acc: 0.4812 - val_loss: 2.1643 - val_acc: 0.4501\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9279 - acc: 0.4861 - val_loss: 2.1836 - val_acc: 0.4469\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9341 - acc: 0.4826 - val_loss: 2.1753 - val_acc: 0.4462\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9431 - acc: 0.4797 - val_loss: 2.1944 - val_acc: 0.4442\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9383 - acc: 0.4805 - val_loss: 2.2034 - val_acc: 0.4426\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9400 - acc: 0.4819 - val_loss: 2.1733 - val_acc: 0.4490\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9256 - acc: 0.4859 - val_loss: 2.1728 - val_acc: 0.4486\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9240 - acc: 0.4871 - val_loss: 2.1736 - val_acc: 0.4489\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9240 - acc: 0.4861 - val_loss: 2.1802 - val_acc: 0.4475\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9333 - acc: 0.4832 - val_loss: 2.1863 - val_acc: 0.4454\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9252 - acc: 0.4823 - val_loss: 2.2157 - val_acc: 0.4393\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2177 - acc: 0.4232 - val_loss: 2.1709 - val_acc: 0.4451\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1755 - acc: 0.4317 - val_loss: 2.1701 - val_acc: 0.4443\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1664 - acc: 0.4326 - val_loss: 2.1814 - val_acc: 0.4433\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1548 - acc: 0.4375 - val_loss: 2.1796 - val_acc: 0.4429\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1497 - acc: 0.4387 - val_loss: 2.1795 - val_acc: 0.4409\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1409 - acc: 0.4386 - val_loss: 2.1855 - val_acc: 0.4400\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1378 - acc: 0.4399 - val_loss: 2.2020 - val_acc: 0.4404\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1418 - acc: 0.4417 - val_loss: 2.1812 - val_acc: 0.4395\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1373 - acc: 0.4388 - val_loss: 2.2178 - val_acc: 0.4361\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1259 - acc: 0.4430 - val_loss: 2.1976 - val_acc: 0.4384\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1133 - acc: 0.4448 - val_loss: 2.1982 - val_acc: 0.4400\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1118 - acc: 0.4452 - val_loss: 2.2092 - val_acc: 0.4365\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1125 - acc: 0.4465 - val_loss: 2.1925 - val_acc: 0.4411\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1161 - acc: 0.4459 - val_loss: 2.2137 - val_acc: 0.4385\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1023 - acc: 0.4495 - val_loss: 2.1916 - val_acc: 0.4416\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0975 - acc: 0.4500 - val_loss: 2.1938 - val_acc: 0.4412\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0862 - acc: 0.4513 - val_loss: 2.1876 - val_acc: 0.4423\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0858 - acc: 0.4512 - val_loss: 2.1923 - val_acc: 0.4432\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0812 - acc: 0.4532 - val_loss: 2.1988 - val_acc: 0.4436\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1146 - acc: 0.4464 - val_loss: 2.1981 - val_acc: 0.4385\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0937 - acc: 0.4483 - val_loss: 2.2066 - val_acc: 0.4406\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0813 - acc: 0.4533 - val_loss: 2.1899 - val_acc: 0.4422\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0771 - acc: 0.4540 - val_loss: 2.1975 - val_acc: 0.4387\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0731 - acc: 0.4539 - val_loss: 2.1764 - val_acc: 0.4423\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0747 - acc: 0.4533 - val_loss: 2.1942 - val_acc: 0.4399\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0712 - acc: 0.4512 - val_loss: 2.1842 - val_acc: 0.4433\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0658 - acc: 0.4545 - val_loss: 2.1915 - val_acc: 0.4409\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0603 - acc: 0.4575 - val_loss: 2.2012 - val_acc: 0.4407\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0539 - acc: 0.4585 - val_loss: 2.1830 - val_acc: 0.4428\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0542 - acc: 0.4587 - val_loss: 2.2174 - val_acc: 0.4397\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0755 - acc: 0.4539 - val_loss: 2.1906 - val_acc: 0.4414\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0472 - acc: 0.4581 - val_loss: 2.1781 - val_acc: 0.4434\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0448 - acc: 0.4592 - val_loss: 2.1778 - val_acc: 0.4432\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0517 - acc: 0.4595 - val_loss: 2.1806 - val_acc: 0.4452\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0401 - acc: 0.4607 - val_loss: 2.1865 - val_acc: 0.4436\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0387 - acc: 0.4605 - val_loss: 2.1973 - val_acc: 0.4421\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0376 - acc: 0.4619 - val_loss: 2.1823 - val_acc: 0.4422\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0396 - acc: 0.4622 - val_loss: 2.1816 - val_acc: 0.4462\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0325 - acc: 0.4643 - val_loss: 2.1767 - val_acc: 0.4449\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0326 - acc: 0.4619 - val_loss: 2.1753 - val_acc: 0.4454\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0284 - acc: 0.4615 - val_loss: 2.1720 - val_acc: 0.4466\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0359 - acc: 0.4604 - val_loss: 2.1798 - val_acc: 0.4440\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0242 - acc: 0.4645 - val_loss: 2.1677 - val_acc: 0.4448\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0338 - acc: 0.4610 - val_loss: 2.1829 - val_acc: 0.4445\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0267 - acc: 0.4646 - val_loss: 2.1750 - val_acc: 0.4460\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0322 - acc: 0.4633 - val_loss: 2.1994 - val_acc: 0.4420\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0566 - acc: 0.4555 - val_loss: 2.1704 - val_acc: 0.4454\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0390 - acc: 0.4611 - val_loss: 2.1914 - val_acc: 0.4434\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0194 - acc: 0.4659 - val_loss: 2.1725 - val_acc: 0.4448\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0154 - acc: 0.4658 - val_loss: 2.1860 - val_acc: 0.4405\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0123 - acc: 0.4659 - val_loss: 2.1791 - val_acc: 0.4434\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0206 - acc: 0.4638 - val_loss: 2.1820 - val_acc: 0.4432\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0091 - acc: 0.4668 - val_loss: 2.1674 - val_acc: 0.4446\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0052 - acc: 0.4668 - val_loss: 2.1851 - val_acc: 0.4440\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0134 - acc: 0.4654 - val_loss: 2.1855 - val_acc: 0.4444\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0305 - acc: 0.4622 - val_loss: 2.1730 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0055 - acc: 0.4689 - val_loss: 2.1731 - val_acc: 0.4434\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0232 - acc: 0.4669 - val_loss: 2.1872 - val_acc: 0.4422\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9955 - acc: 0.4709 - val_loss: 2.1760 - val_acc: 0.4417\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9989 - acc: 0.4700 - val_loss: 2.1764 - val_acc: 0.4455\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9971 - acc: 0.4710 - val_loss: 2.1858 - val_acc: 0.4439\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0088 - acc: 0.4677 - val_loss: 2.1993 - val_acc: 0.4426\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0085 - acc: 0.4668 - val_loss: 2.1989 - val_acc: 0.4379\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9992 - acc: 0.4705 - val_loss: 2.1806 - val_acc: 0.4450\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9940 - acc: 0.4704 - val_loss: 2.1777 - val_acc: 0.4428\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9852 - acc: 0.4728 - val_loss: 2.1897 - val_acc: 0.4441\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0110 - acc: 0.4689 - val_loss: 2.1929 - val_acc: 0.4431\n",
      "Epoch 68/100\n",
      "2s - loss: 1.9956 - acc: 0.4692 - val_loss: 2.1847 - val_acc: 0.4439\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9988 - acc: 0.4705 - val_loss: 2.1973 - val_acc: 0.4406\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0072 - acc: 0.4672 - val_loss: 2.1790 - val_acc: 0.4443\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9786 - acc: 0.4751 - val_loss: 2.1804 - val_acc: 0.4420\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9766 - acc: 0.4755 - val_loss: 2.1656 - val_acc: 0.4429\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9733 - acc: 0.4760 - val_loss: 2.1719 - val_acc: 0.4458\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9780 - acc: 0.4754 - val_loss: 2.2214 - val_acc: 0.4378\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9917 - acc: 0.4709 - val_loss: 2.1793 - val_acc: 0.4461\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9741 - acc: 0.4747 - val_loss: 2.1895 - val_acc: 0.4418\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9751 - acc: 0.4754 - val_loss: 2.1910 - val_acc: 0.4460\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9858 - acc: 0.4721 - val_loss: 2.1873 - val_acc: 0.4439\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9821 - acc: 0.4736 - val_loss: 2.1776 - val_acc: 0.4462\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9671 - acc: 0.4785 - val_loss: 2.1754 - val_acc: 0.4465\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9644 - acc: 0.4775 - val_loss: 2.1855 - val_acc: 0.4448\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9783 - acc: 0.4747 - val_loss: 2.2016 - val_acc: 0.4401\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9826 - acc: 0.4726 - val_loss: 2.1767 - val_acc: 0.4453\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9716 - acc: 0.4771 - val_loss: 2.1704 - val_acc: 0.4451\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9794 - acc: 0.4747 - val_loss: 2.1812 - val_acc: 0.4422\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9608 - acc: 0.4772 - val_loss: 2.1691 - val_acc: 0.4447\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9649 - acc: 0.4772 - val_loss: 2.1841 - val_acc: 0.4429\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9869 - acc: 0.4736 - val_loss: 2.1722 - val_acc: 0.4470\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9577 - acc: 0.4789 - val_loss: 2.1831 - val_acc: 0.4432\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9595 - acc: 0.4792 - val_loss: 2.1718 - val_acc: 0.4446\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9460 - acc: 0.4815 - val_loss: 2.1716 - val_acc: 0.4447\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9485 - acc: 0.4817 - val_loss: 2.1751 - val_acc: 0.4455\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9612 - acc: 0.4772 - val_loss: 2.1891 - val_acc: 0.4413\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9546 - acc: 0.4804 - val_loss: 2.1817 - val_acc: 0.4440\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9571 - acc: 0.4794 - val_loss: 2.1963 - val_acc: 0.4407\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9567 - acc: 0.4780 - val_loss: 2.1768 - val_acc: 0.4441\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9548 - acc: 0.4784 - val_loss: 2.1922 - val_acc: 0.4428\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9555 - acc: 0.4806 - val_loss: 2.1717 - val_acc: 0.4436\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9514 - acc: 0.4812 - val_loss: 2.1776 - val_acc: 0.4454\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9472 - acc: 0.4818 - val_loss: 2.1819 - val_acc: 0.4431\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2057 - acc: 0.4251 - val_loss: 2.1896 - val_acc: 0.4423\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1825 - acc: 0.4290 - val_loss: 2.1911 - val_acc: 0.4416\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1698 - acc: 0.4317 - val_loss: 2.1981 - val_acc: 0.4405\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1561 - acc: 0.4356 - val_loss: 2.2030 - val_acc: 0.4375\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1479 - acc: 0.4364 - val_loss: 2.2007 - val_acc: 0.4366\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1403 - acc: 0.4376 - val_loss: 2.1993 - val_acc: 0.4399\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1384 - acc: 0.4379 - val_loss: 2.2081 - val_acc: 0.4347\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1335 - acc: 0.4403 - val_loss: 2.2108 - val_acc: 0.4345\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1260 - acc: 0.4405 - val_loss: 2.2102 - val_acc: 0.4342\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1212 - acc: 0.4430 - val_loss: 2.2146 - val_acc: 0.4335\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1150 - acc: 0.4432 - val_loss: 2.2090 - val_acc: 0.4359\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1208 - acc: 0.4416 - val_loss: 2.2064 - val_acc: 0.4353\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1171 - acc: 0.4441 - val_loss: 2.2144 - val_acc: 0.4366\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1154 - acc: 0.4444 - val_loss: 2.2109 - val_acc: 0.4368\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1059 - acc: 0.4452 - val_loss: 2.2126 - val_acc: 0.4350\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1013 - acc: 0.4463 - val_loss: 2.2256 - val_acc: 0.4352\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1029 - acc: 0.4463 - val_loss: 2.2130 - val_acc: 0.4362\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0899 - acc: 0.4496 - val_loss: 2.2098 - val_acc: 0.4354\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0888 - acc: 0.4486 - val_loss: 2.2104 - val_acc: 0.4376\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0855 - acc: 0.4508 - val_loss: 2.2101 - val_acc: 0.4360\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0793 - acc: 0.4501 - val_loss: 2.2228 - val_acc: 0.4315\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0825 - acc: 0.4502 - val_loss: 2.2189 - val_acc: 0.4334\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0806 - acc: 0.4514 - val_loss: 2.2122 - val_acc: 0.4361\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0761 - acc: 0.4524 - val_loss: 2.2098 - val_acc: 0.4380\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0741 - acc: 0.4518 - val_loss: 2.2107 - val_acc: 0.4375\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0771 - acc: 0.4533 - val_loss: 2.2000 - val_acc: 0.4379\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0692 - acc: 0.4540 - val_loss: 2.2031 - val_acc: 0.4381\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0591 - acc: 0.4556 - val_loss: 2.2265 - val_acc: 0.4330\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0650 - acc: 0.4553 - val_loss: 2.1990 - val_acc: 0.4387\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0585 - acc: 0.4555 - val_loss: 2.1929 - val_acc: 0.4407\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0556 - acc: 0.4576 - val_loss: 2.2081 - val_acc: 0.4370\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0574 - acc: 0.4573 - val_loss: 2.1997 - val_acc: 0.4381\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0596 - acc: 0.4563 - val_loss: 2.1978 - val_acc: 0.4399\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0494 - acc: 0.4588 - val_loss: 2.2208 - val_acc: 0.4366\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0598 - acc: 0.4558 - val_loss: 2.1978 - val_acc: 0.4380\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0430 - acc: 0.4587 - val_loss: 2.2171 - val_acc: 0.4369\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0583 - acc: 0.4569 - val_loss: 2.2190 - val_acc: 0.4359\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0476 - acc: 0.4593 - val_loss: 2.2194 - val_acc: 0.4345\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0431 - acc: 0.4605 - val_loss: 2.2024 - val_acc: 0.4403\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0386 - acc: 0.4594 - val_loss: 2.2052 - val_acc: 0.4383\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0347 - acc: 0.4612 - val_loss: 2.2099 - val_acc: 0.4384\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0344 - acc: 0.4619 - val_loss: 2.2009 - val_acc: 0.4405\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0306 - acc: 0.4609 - val_loss: 2.1976 - val_acc: 0.4405\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0294 - acc: 0.4632 - val_loss: 2.2105 - val_acc: 0.4394\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0482 - acc: 0.4605 - val_loss: 2.1955 - val_acc: 0.4396\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0280 - acc: 0.4622 - val_loss: 2.2004 - val_acc: 0.4406\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0255 - acc: 0.4636 - val_loss: 2.2012 - val_acc: 0.4399\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0224 - acc: 0.4631 - val_loss: 2.1888 - val_acc: 0.4437\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0171 - acc: 0.4661 - val_loss: 2.1884 - val_acc: 0.4427\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0180 - acc: 0.4651 - val_loss: 2.2007 - val_acc: 0.4391\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0175 - acc: 0.4646 - val_loss: 2.2030 - val_acc: 0.4395\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0201 - acc: 0.4626 - val_loss: 2.1928 - val_acc: 0.4420\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0170 - acc: 0.4664 - val_loss: 2.1872 - val_acc: 0.4424\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0057 - acc: 0.4668 - val_loss: 2.1885 - val_acc: 0.4414\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0091 - acc: 0.4674 - val_loss: 2.2008 - val_acc: 0.4384\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0236 - acc: 0.4639 - val_loss: 2.1959 - val_acc: 0.4414\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0114 - acc: 0.4671 - val_loss: 2.1964 - val_acc: 0.4400\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0087 - acc: 0.4668 - val_loss: 2.1865 - val_acc: 0.4425\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0055 - acc: 0.4674 - val_loss: 2.2033 - val_acc: 0.4408\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0225 - acc: 0.4644 - val_loss: 2.2105 - val_acc: 0.4377\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0060 - acc: 0.4673 - val_loss: 2.1970 - val_acc: 0.4386\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0127 - acc: 0.4660 - val_loss: 2.2029 - val_acc: 0.4366\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0144 - acc: 0.4663 - val_loss: 2.2194 - val_acc: 0.4365\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0166 - acc: 0.4650 - val_loss: 2.1927 - val_acc: 0.4389\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0113 - acc: 0.4658 - val_loss: 2.1961 - val_acc: 0.4414\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9990 - acc: 0.4692 - val_loss: 2.1892 - val_acc: 0.4414\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0068 - acc: 0.4668 - val_loss: 2.2388 - val_acc: 0.4333\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0100 - acc: 0.4683 - val_loss: 2.2075 - val_acc: 0.4377\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0035 - acc: 0.4682 - val_loss: 2.2012 - val_acc: 0.4413\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9861 - acc: 0.4713 - val_loss: 2.1886 - val_acc: 0.4398\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9813 - acc: 0.4735 - val_loss: 2.2101 - val_acc: 0.4417\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0007 - acc: 0.4691 - val_loss: 2.2019 - val_acc: 0.4378\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9885 - acc: 0.4735 - val_loss: 2.1993 - val_acc: 0.4396\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9812 - acc: 0.4724 - val_loss: 2.1906 - val_acc: 0.4404\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9791 - acc: 0.4726 - val_loss: 2.1830 - val_acc: 0.4421\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9809 - acc: 0.4746 - val_loss: 2.1920 - val_acc: 0.4411\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9841 - acc: 0.4714 - val_loss: 2.2033 - val_acc: 0.4404\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9896 - acc: 0.4699 - val_loss: 2.1865 - val_acc: 0.4404\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9796 - acc: 0.4721 - val_loss: 2.2123 - val_acc: 0.4357\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9852 - acc: 0.4731 - val_loss: 2.2112 - val_acc: 0.4367\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9907 - acc: 0.4722 - val_loss: 2.2029 - val_acc: 0.4404\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9718 - acc: 0.4763 - val_loss: 2.2053 - val_acc: 0.4384\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9778 - acc: 0.4740 - val_loss: 2.2034 - val_acc: 0.4391\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9696 - acc: 0.4773 - val_loss: 2.1997 - val_acc: 0.4390\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9723 - acc: 0.4748 - val_loss: 2.2079 - val_acc: 0.4394\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9768 - acc: 0.4748 - val_loss: 2.1953 - val_acc: 0.4394\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9727 - acc: 0.4750 - val_loss: 2.1937 - val_acc: 0.4423\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9573 - acc: 0.4810 - val_loss: 2.1942 - val_acc: 0.4400\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9618 - acc: 0.4785 - val_loss: 2.2102 - val_acc: 0.4383\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9942 - acc: 0.4690 - val_loss: 2.1899 - val_acc: 0.4402\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9659 - acc: 0.4768 - val_loss: 2.2103 - val_acc: 0.4415\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9667 - acc: 0.4764 - val_loss: 2.1879 - val_acc: 0.4386\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9640 - acc: 0.4758 - val_loss: 2.2200 - val_acc: 0.4397\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9971 - acc: 0.4678 - val_loss: 2.2165 - val_acc: 0.4364\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9856 - acc: 0.4714 - val_loss: 2.2074 - val_acc: 0.4395\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9606 - acc: 0.4793 - val_loss: 2.1871 - val_acc: 0.4420\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9612 - acc: 0.4793 - val_loss: 2.1851 - val_acc: 0.4437\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9481 - acc: 0.4810 - val_loss: 2.1864 - val_acc: 0.4426\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9553 - acc: 0.4789 - val_loss: 2.1904 - val_acc: 0.4391\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9561 - acc: 0.4777 - val_loss: 2.1953 - val_acc: 0.4390\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1908 - acc: 0.4270 - val_loss: 2.2075 - val_acc: 0.4403\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1686 - acc: 0.4306 - val_loss: 2.2141 - val_acc: 0.4406\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1564 - acc: 0.4332 - val_loss: 2.2101 - val_acc: 0.4378\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1475 - acc: 0.4338 - val_loss: 2.2063 - val_acc: 0.4391\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1372 - acc: 0.4368 - val_loss: 2.2130 - val_acc: 0.4378\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1271 - acc: 0.4378 - val_loss: 2.2171 - val_acc: 0.4359\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1221 - acc: 0.4412 - val_loss: 2.2134 - val_acc: 0.4375\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1171 - acc: 0.4419 - val_loss: 2.2146 - val_acc: 0.4349\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1110 - acc: 0.4411 - val_loss: 2.2206 - val_acc: 0.4362\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1084 - acc: 0.4454 - val_loss: 2.2249 - val_acc: 0.4357\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1035 - acc: 0.4440 - val_loss: 2.2337 - val_acc: 0.4331\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0975 - acc: 0.4451 - val_loss: 2.2248 - val_acc: 0.4343\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0938 - acc: 0.4478 - val_loss: 2.2333 - val_acc: 0.4335\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0927 - acc: 0.4468 - val_loss: 2.2308 - val_acc: 0.4342\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0833 - acc: 0.4475 - val_loss: 2.2226 - val_acc: 0.4348\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0806 - acc: 0.4499 - val_loss: 2.2335 - val_acc: 0.4342\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0974 - acc: 0.4461 - val_loss: 2.2403 - val_acc: 0.4297\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0770 - acc: 0.4486 - val_loss: 2.2247 - val_acc: 0.4336\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0759 - acc: 0.4494 - val_loss: 2.2172 - val_acc: 0.4359\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0674 - acc: 0.4512 - val_loss: 2.2219 - val_acc: 0.4363\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0608 - acc: 0.4539 - val_loss: 2.2138 - val_acc: 0.4357\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0620 - acc: 0.4537 - val_loss: 2.2174 - val_acc: 0.4361\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0583 - acc: 0.4528 - val_loss: 2.2140 - val_acc: 0.4345\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0550 - acc: 0.4535 - val_loss: 2.2377 - val_acc: 0.4311\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0593 - acc: 0.4547 - val_loss: 2.2261 - val_acc: 0.4325\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0493 - acc: 0.4582 - val_loss: 2.2173 - val_acc: 0.4354\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0515 - acc: 0.4549 - val_loss: 2.2169 - val_acc: 0.4386\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0555 - acc: 0.4568 - val_loss: 2.2194 - val_acc: 0.4341\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0455 - acc: 0.4555 - val_loss: 2.2164 - val_acc: 0.4360\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0419 - acc: 0.4574 - val_loss: 2.2105 - val_acc: 0.4370\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0386 - acc: 0.4574 - val_loss: 2.2129 - val_acc: 0.4359\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0366 - acc: 0.4565 - val_loss: 2.2200 - val_acc: 0.4345\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0315 - acc: 0.4586 - val_loss: 2.2158 - val_acc: 0.4374\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0388 - acc: 0.4578 - val_loss: 2.2146 - val_acc: 0.4385\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0366 - acc: 0.4595 - val_loss: 2.2263 - val_acc: 0.4375\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0368 - acc: 0.4578 - val_loss: 2.2312 - val_acc: 0.4323\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0325 - acc: 0.4587 - val_loss: 2.2222 - val_acc: 0.4353\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0309 - acc: 0.4597 - val_loss: 2.2178 - val_acc: 0.4375\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0285 - acc: 0.4611 - val_loss: 2.2044 - val_acc: 0.4387\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0199 - acc: 0.4613 - val_loss: 2.2199 - val_acc: 0.4331\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0184 - acc: 0.4636 - val_loss: 2.2155 - val_acc: 0.4373\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0190 - acc: 0.4629 - val_loss: 2.2121 - val_acc: 0.4389\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0152 - acc: 0.4634 - val_loss: 2.2221 - val_acc: 0.4373\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0172 - acc: 0.4616 - val_loss: 2.2057 - val_acc: 0.4389\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0108 - acc: 0.4650 - val_loss: 2.2195 - val_acc: 0.4349\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0185 - acc: 0.4641 - val_loss: 2.2121 - val_acc: 0.4371\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0050 - acc: 0.4659 - val_loss: 2.2067 - val_acc: 0.4373\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0094 - acc: 0.4644 - val_loss: 2.2031 - val_acc: 0.4404\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9994 - acc: 0.4667 - val_loss: 2.2026 - val_acc: 0.4401\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9954 - acc: 0.4667 - val_loss: 2.2082 - val_acc: 0.4365\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9963 - acc: 0.4663 - val_loss: 2.2040 - val_acc: 0.4397\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9979 - acc: 0.4680 - val_loss: 2.2006 - val_acc: 0.4391\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0013 - acc: 0.4678 - val_loss: 2.2298 - val_acc: 0.4370\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0042 - acc: 0.4642 - val_loss: 2.3059 - val_acc: 0.4229\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0216 - acc: 0.4630 - val_loss: 2.2083 - val_acc: 0.4418\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9958 - acc: 0.4678 - val_loss: 2.2193 - val_acc: 0.4361\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9901 - acc: 0.4693 - val_loss: 2.2075 - val_acc: 0.4404\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9948 - acc: 0.4699 - val_loss: 2.2318 - val_acc: 0.4343\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9951 - acc: 0.4677 - val_loss: 2.2147 - val_acc: 0.4388\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9907 - acc: 0.4688 - val_loss: 2.2257 - val_acc: 0.4354\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9915 - acc: 0.4684 - val_loss: 2.2127 - val_acc: 0.4381\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9863 - acc: 0.4705 - val_loss: 2.2318 - val_acc: 0.4348\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9885 - acc: 0.4683 - val_loss: 2.2046 - val_acc: 0.4407\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9847 - acc: 0.4698 - val_loss: 2.2501 - val_acc: 0.4323\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9842 - acc: 0.4703 - val_loss: 2.2180 - val_acc: 0.4359\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9849 - acc: 0.4695 - val_loss: 2.2461 - val_acc: 0.4310\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9806 - acc: 0.4719 - val_loss: 2.2047 - val_acc: 0.4408\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9749 - acc: 0.4737 - val_loss: 2.2185 - val_acc: 0.4370\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9684 - acc: 0.4742 - val_loss: 2.2050 - val_acc: 0.4395\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9643 - acc: 0.4750 - val_loss: 2.2500 - val_acc: 0.4311\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9781 - acc: 0.4711 - val_loss: 2.2095 - val_acc: 0.4367\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9733 - acc: 0.4724 - val_loss: 2.2229 - val_acc: 0.4351\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9786 - acc: 0.4730 - val_loss: 2.2140 - val_acc: 0.4385\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9778 - acc: 0.4731 - val_loss: 2.2059 - val_acc: 0.4402\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9752 - acc: 0.4719 - val_loss: 2.2127 - val_acc: 0.4370\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9642 - acc: 0.4754 - val_loss: 2.2103 - val_acc: 0.4402\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9679 - acc: 0.4733 - val_loss: 2.2145 - val_acc: 0.4388\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9757 - acc: 0.4729 - val_loss: 2.2062 - val_acc: 0.4399\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9568 - acc: 0.4762 - val_loss: 2.2153 - val_acc: 0.4373\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9613 - acc: 0.4750 - val_loss: 2.2096 - val_acc: 0.4390\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9561 - acc: 0.4791 - val_loss: 2.2290 - val_acc: 0.4349\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9567 - acc: 0.4769 - val_loss: 2.2086 - val_acc: 0.4402\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9553 - acc: 0.4768 - val_loss: 2.2157 - val_acc: 0.4362\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9572 - acc: 0.4752 - val_loss: 2.2242 - val_acc: 0.4353\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9660 - acc: 0.4751 - val_loss: 2.2286 - val_acc: 0.4326\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9490 - acc: 0.4796 - val_loss: 2.2119 - val_acc: 0.4379\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9577 - acc: 0.4761 - val_loss: 2.2333 - val_acc: 0.4358\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9546 - acc: 0.4760 - val_loss: 2.2411 - val_acc: 0.4326\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9548 - acc: 0.4764 - val_loss: 2.2186 - val_acc: 0.4371\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9492 - acc: 0.4790 - val_loss: 2.2363 - val_acc: 0.4345\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9435 - acc: 0.4793 - val_loss: 2.2152 - val_acc: 0.4342\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9365 - acc: 0.4821 - val_loss: 2.2701 - val_acc: 0.4297\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9469 - acc: 0.4777 - val_loss: 2.2128 - val_acc: 0.4392\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9480 - acc: 0.4782 - val_loss: 2.2244 - val_acc: 0.4357\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9533 - acc: 0.4774 - val_loss: 2.2178 - val_acc: 0.4375\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9364 - acc: 0.4817 - val_loss: 2.2027 - val_acc: 0.4399\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9332 - acc: 0.4823 - val_loss: 2.2104 - val_acc: 0.4382\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9333 - acc: 0.4796 - val_loss: 2.2509 - val_acc: 0.4308\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9519 - acc: 0.4788 - val_loss: 2.2322 - val_acc: 0.4337\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9403 - acc: 0.4800 - val_loss: 2.2151 - val_acc: 0.4369\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1941 - acc: 0.4214 - val_loss: 2.2022 - val_acc: 0.4410\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1937 - acc: 0.4229 - val_loss: 2.2063 - val_acc: 0.4378\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1682 - acc: 0.4269 - val_loss: 2.1960 - val_acc: 0.4416\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1460 - acc: 0.4294 - val_loss: 2.1956 - val_acc: 0.4406\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1388 - acc: 0.4329 - val_loss: 2.2135 - val_acc: 0.4349\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1299 - acc: 0.4329 - val_loss: 2.2179 - val_acc: 0.4347\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1268 - acc: 0.4347 - val_loss: 2.2601 - val_acc: 0.4291\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1263 - acc: 0.4350 - val_loss: 2.2193 - val_acc: 0.4350\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1131 - acc: 0.4365 - val_loss: 2.2312 - val_acc: 0.4364\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1125 - acc: 0.4388 - val_loss: 2.2281 - val_acc: 0.4343\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1101 - acc: 0.4381 - val_loss: 2.2303 - val_acc: 0.4352\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1040 - acc: 0.4402 - val_loss: 2.2273 - val_acc: 0.4348\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0973 - acc: 0.4409 - val_loss: 2.2226 - val_acc: 0.4339\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0912 - acc: 0.4427 - val_loss: 2.2197 - val_acc: 0.4373\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0917 - acc: 0.4431 - val_loss: 2.2358 - val_acc: 0.4346\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0890 - acc: 0.4431 - val_loss: 2.2257 - val_acc: 0.4362\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0848 - acc: 0.4427 - val_loss: 2.2189 - val_acc: 0.4381\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0811 - acc: 0.4432 - val_loss: 2.2104 - val_acc: 0.4388\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0732 - acc: 0.4458 - val_loss: 2.2192 - val_acc: 0.4376\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0679 - acc: 0.4475 - val_loss: 2.2301 - val_acc: 0.4330\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0688 - acc: 0.4470 - val_loss: 2.2111 - val_acc: 0.4374\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0672 - acc: 0.4483 - val_loss: 2.2068 - val_acc: 0.4383\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0641 - acc: 0.4467 - val_loss: 2.2041 - val_acc: 0.4401\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0586 - acc: 0.4487 - val_loss: 2.2327 - val_acc: 0.4352\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0579 - acc: 0.4476 - val_loss: 2.2065 - val_acc: 0.4395\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0545 - acc: 0.4502 - val_loss: 2.2053 - val_acc: 0.4386\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0542 - acc: 0.4504 - val_loss: 2.1953 - val_acc: 0.4406\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0492 - acc: 0.4513 - val_loss: 2.2021 - val_acc: 0.4406\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0461 - acc: 0.4523 - val_loss: 2.2028 - val_acc: 0.4384\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0427 - acc: 0.4531 - val_loss: 2.2402 - val_acc: 0.4322\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0490 - acc: 0.4516 - val_loss: 2.1947 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0368 - acc: 0.4543 - val_loss: 2.2170 - val_acc: 0.4368\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0387 - acc: 0.4542 - val_loss: 2.2159 - val_acc: 0.4393\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0431 - acc: 0.4534 - val_loss: 2.2098 - val_acc: 0.4378\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0323 - acc: 0.4543 - val_loss: 2.1954 - val_acc: 0.4406\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0277 - acc: 0.4564 - val_loss: 2.2030 - val_acc: 0.4403\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0286 - acc: 0.4560 - val_loss: 2.1970 - val_acc: 0.4395\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0264 - acc: 0.4574 - val_loss: 2.2068 - val_acc: 0.4384\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0218 - acc: 0.4581 - val_loss: 2.1876 - val_acc: 0.4427\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0210 - acc: 0.4583 - val_loss: 2.1996 - val_acc: 0.4400\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0212 - acc: 0.4580 - val_loss: 2.1967 - val_acc: 0.4403\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0164 - acc: 0.4603 - val_loss: 2.1979 - val_acc: 0.4413\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0196 - acc: 0.4576 - val_loss: 2.1891 - val_acc: 0.4440\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0149 - acc: 0.4602 - val_loss: 2.2106 - val_acc: 0.4373\n",
      "Epoch 45/100\n",
      "2s - loss: 2.0166 - acc: 0.4599 - val_loss: 2.1902 - val_acc: 0.4435\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0084 - acc: 0.4606 - val_loss: 2.1868 - val_acc: 0.4427\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0116 - acc: 0.4599 - val_loss: 2.1921 - val_acc: 0.4423\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0089 - acc: 0.4624 - val_loss: 2.2150 - val_acc: 0.4352\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0127 - acc: 0.4597 - val_loss: 2.2106 - val_acc: 0.4402\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0072 - acc: 0.4611 - val_loss: 2.1856 - val_acc: 0.4409\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9986 - acc: 0.4616 - val_loss: 2.1847 - val_acc: 0.4434\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9939 - acc: 0.4655 - val_loss: 2.1970 - val_acc: 0.4431\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9992 - acc: 0.4635 - val_loss: 2.1815 - val_acc: 0.4447\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9922 - acc: 0.4644 - val_loss: 2.1840 - val_acc: 0.4448\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9888 - acc: 0.4661 - val_loss: 2.1767 - val_acc: 0.4443\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9825 - acc: 0.4676 - val_loss: 2.1910 - val_acc: 0.4418\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9858 - acc: 0.4662 - val_loss: 2.1742 - val_acc: 0.4459\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9851 - acc: 0.4667 - val_loss: 2.2027 - val_acc: 0.4391\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9953 - acc: 0.4640 - val_loss: 2.2098 - val_acc: 0.4405\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0018 - acc: 0.4637 - val_loss: 2.1871 - val_acc: 0.4449\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9875 - acc: 0.4673 - val_loss: 2.1809 - val_acc: 0.4439\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9803 - acc: 0.4687 - val_loss: 2.1854 - val_acc: 0.4440\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9832 - acc: 0.4675 - val_loss: 2.1790 - val_acc: 0.4440\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9800 - acc: 0.4679 - val_loss: 2.2157 - val_acc: 0.4378\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9863 - acc: 0.4656 - val_loss: 2.2104 - val_acc: 0.4408\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9918 - acc: 0.4634 - val_loss: 2.2006 - val_acc: 0.4430\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9816 - acc: 0.4674 - val_loss: 2.2123 - val_acc: 0.4398\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9831 - acc: 0.4670 - val_loss: 2.1842 - val_acc: 0.4428\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9712 - acc: 0.4690 - val_loss: 2.1944 - val_acc: 0.4430\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9722 - acc: 0.4687 - val_loss: 2.1955 - val_acc: 0.4432\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9833 - acc: 0.4679 - val_loss: 2.1842 - val_acc: 0.4447\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9685 - acc: 0.4706 - val_loss: 2.1824 - val_acc: 0.4421\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9689 - acc: 0.4707 - val_loss: 2.1827 - val_acc: 0.4442\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9702 - acc: 0.4705 - val_loss: 2.2027 - val_acc: 0.4429\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9662 - acc: 0.4704 - val_loss: 2.1874 - val_acc: 0.4442\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9614 - acc: 0.4712 - val_loss: 2.1895 - val_acc: 0.4448\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9589 - acc: 0.4739 - val_loss: 2.1814 - val_acc: 0.4445\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9543 - acc: 0.4732 - val_loss: 2.1975 - val_acc: 0.4434\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9606 - acc: 0.4709 - val_loss: 2.1901 - val_acc: 0.4436\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9567 - acc: 0.4739 - val_loss: 2.1850 - val_acc: 0.4448\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9596 - acc: 0.4722 - val_loss: 2.2023 - val_acc: 0.4413\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9575 - acc: 0.4724 - val_loss: 2.1882 - val_acc: 0.4464\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9665 - acc: 0.4710 - val_loss: 2.2128 - val_acc: 0.4395\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9659 - acc: 0.4702 - val_loss: 2.1811 - val_acc: 0.4450\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9548 - acc: 0.4743 - val_loss: 2.1920 - val_acc: 0.4420\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9514 - acc: 0.4739 - val_loss: 2.1810 - val_acc: 0.4443\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9503 - acc: 0.4742 - val_loss: 2.1845 - val_acc: 0.4440\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9422 - acc: 0.4777 - val_loss: 2.1911 - val_acc: 0.4445\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9446 - acc: 0.4770 - val_loss: 2.1863 - val_acc: 0.4437\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9415 - acc: 0.4779 - val_loss: 2.1806 - val_acc: 0.4458\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9364 - acc: 0.4788 - val_loss: 2.2217 - val_acc: 0.4363\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9539 - acc: 0.4754 - val_loss: 2.1828 - val_acc: 0.4442\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9389 - acc: 0.4777 - val_loss: 2.2026 - val_acc: 0.4387\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9422 - acc: 0.4780 - val_loss: 2.1876 - val_acc: 0.4434\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9390 - acc: 0.4760 - val_loss: 2.1929 - val_acc: 0.4423\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9353 - acc: 0.4775 - val_loss: 2.1835 - val_acc: 0.4446\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9389 - acc: 0.4777 - val_loss: 2.1895 - val_acc: 0.4432\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9327 - acc: 0.4798 - val_loss: 2.1905 - val_acc: 0.4439\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9249 - acc: 0.4810 - val_loss: 2.1827 - val_acc: 0.4438\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9246 - acc: 0.4806 - val_loss: 2.1832 - val_acc: 0.4427\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1783 - acc: 0.4311 - val_loss: 2.1955 - val_acc: 0.4418\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1588 - acc: 0.4343 - val_loss: 2.1911 - val_acc: 0.4420\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1461 - acc: 0.4371 - val_loss: 2.2120 - val_acc: 0.4404\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1467 - acc: 0.4379 - val_loss: 2.2150 - val_acc: 0.4404\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1306 - acc: 0.4403 - val_loss: 2.2104 - val_acc: 0.4399\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1219 - acc: 0.4428 - val_loss: 2.2207 - val_acc: 0.4391\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1144 - acc: 0.4451 - val_loss: 2.2222 - val_acc: 0.4383\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1099 - acc: 0.4451 - val_loss: 2.2190 - val_acc: 0.4364\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1113 - acc: 0.4454 - val_loss: 2.2133 - val_acc: 0.4365\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1048 - acc: 0.4457 - val_loss: 2.2365 - val_acc: 0.4359\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0951 - acc: 0.4479 - val_loss: 2.2329 - val_acc: 0.4353\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0916 - acc: 0.4475 - val_loss: 2.2298 - val_acc: 0.4350\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0913 - acc: 0.4494 - val_loss: 2.2241 - val_acc: 0.4372\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0861 - acc: 0.4489 - val_loss: 2.2276 - val_acc: 0.4392\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0910 - acc: 0.4469 - val_loss: 2.2188 - val_acc: 0.4373\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0736 - acc: 0.4518 - val_loss: 2.2318 - val_acc: 0.4358\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0689 - acc: 0.4553 - val_loss: 2.2222 - val_acc: 0.4369\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0696 - acc: 0.4532 - val_loss: 2.2293 - val_acc: 0.4367\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0634 - acc: 0.4540 - val_loss: 2.2151 - val_acc: 0.4381\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0598 - acc: 0.4537 - val_loss: 2.2078 - val_acc: 0.4373\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0628 - acc: 0.4546 - val_loss: 2.2282 - val_acc: 0.4367\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0533 - acc: 0.4541 - val_loss: 2.2268 - val_acc: 0.4361\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0513 - acc: 0.4564 - val_loss: 2.2303 - val_acc: 0.4347\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0498 - acc: 0.4563 - val_loss: 2.2113 - val_acc: 0.4379\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0580 - acc: 0.4551 - val_loss: 2.2289 - val_acc: 0.4367\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0508 - acc: 0.4570 - val_loss: 2.2260 - val_acc: 0.4352\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0432 - acc: 0.4588 - val_loss: 2.2427 - val_acc: 0.4321\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0598 - acc: 0.4558 - val_loss: 2.2305 - val_acc: 0.4359\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0362 - acc: 0.4609 - val_loss: 2.2252 - val_acc: 0.4374\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0338 - acc: 0.4596 - val_loss: 2.2206 - val_acc: 0.4358\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0311 - acc: 0.4599 - val_loss: 2.2350 - val_acc: 0.4347\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0373 - acc: 0.4590 - val_loss: 2.2173 - val_acc: 0.4383\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0257 - acc: 0.4607 - val_loss: 2.2363 - val_acc: 0.4358\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0294 - acc: 0.4619 - val_loss: 2.2055 - val_acc: 0.4344\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0277 - acc: 0.4610 - val_loss: 2.2249 - val_acc: 0.4375\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0229 - acc: 0.4629 - val_loss: 2.2001 - val_acc: 0.4379\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0198 - acc: 0.4648 - val_loss: 2.2490 - val_acc: 0.4338\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0265 - acc: 0.4623 - val_loss: 2.2204 - val_acc: 0.4360\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0172 - acc: 0.4632 - val_loss: 2.2112 - val_acc: 0.4352\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0199 - acc: 0.4634 - val_loss: 2.2120 - val_acc: 0.4383\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0073 - acc: 0.4668 - val_loss: 2.2052 - val_acc: 0.4372\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0099 - acc: 0.4648 - val_loss: 2.2066 - val_acc: 0.4384\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0134 - acc: 0.4646 - val_loss: 2.2040 - val_acc: 0.4371\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9983 - acc: 0.4679 - val_loss: 2.2096 - val_acc: 0.4366\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9956 - acc: 0.4696 - val_loss: 2.2126 - val_acc: 0.4370\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9995 - acc: 0.4679 - val_loss: 2.2089 - val_acc: 0.4369\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9985 - acc: 0.4698 - val_loss: 2.2178 - val_acc: 0.4360\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0007 - acc: 0.4679 - val_loss: 2.2025 - val_acc: 0.4381\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0020 - acc: 0.4668 - val_loss: 2.2315 - val_acc: 0.4327\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0087 - acc: 0.4654 - val_loss: 2.2094 - val_acc: 0.4377\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9906 - acc: 0.4696 - val_loss: 2.2217 - val_acc: 0.4343\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9908 - acc: 0.4714 - val_loss: 2.2006 - val_acc: 0.4372\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9946 - acc: 0.4702 - val_loss: 2.2368 - val_acc: 0.4343\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9960 - acc: 0.4686 - val_loss: 2.1938 - val_acc: 0.4367\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9838 - acc: 0.4725 - val_loss: 2.2230 - val_acc: 0.4380\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9807 - acc: 0.4733 - val_loss: 2.1933 - val_acc: 0.4393\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9824 - acc: 0.4722 - val_loss: 2.2409 - val_acc: 0.4377\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9892 - acc: 0.4708 - val_loss: 2.2074 - val_acc: 0.4380\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9850 - acc: 0.4700 - val_loss: 2.2025 - val_acc: 0.4385\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9775 - acc: 0.4717 - val_loss: 2.2121 - val_acc: 0.4379\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9703 - acc: 0.4749 - val_loss: 2.2131 - val_acc: 0.4351\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9803 - acc: 0.4734 - val_loss: 2.2124 - val_acc: 0.4357\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9727 - acc: 0.4751 - val_loss: 2.1972 - val_acc: 0.4403\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9626 - acc: 0.4747 - val_loss: 2.2163 - val_acc: 0.4367\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9649 - acc: 0.4765 - val_loss: 2.1927 - val_acc: 0.4415\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9656 - acc: 0.4766 - val_loss: 2.2226 - val_acc: 0.4340\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9667 - acc: 0.4760 - val_loss: 2.1993 - val_acc: 0.4389\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9601 - acc: 0.4759 - val_loss: 2.2022 - val_acc: 0.4388\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9588 - acc: 0.4748 - val_loss: 2.1949 - val_acc: 0.4419\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9591 - acc: 0.4773 - val_loss: 2.2134 - val_acc: 0.4381\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9651 - acc: 0.4776 - val_loss: 2.1947 - val_acc: 0.4375\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9586 - acc: 0.4773 - val_loss: 2.2168 - val_acc: 0.4392\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9614 - acc: 0.4766 - val_loss: 2.1950 - val_acc: 0.4388\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9587 - acc: 0.4775 - val_loss: 2.2298 - val_acc: 0.4339\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9596 - acc: 0.4755 - val_loss: 2.1948 - val_acc: 0.4383\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9552 - acc: 0.4772 - val_loss: 2.2150 - val_acc: 0.4397\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9630 - acc: 0.4780 - val_loss: 2.1998 - val_acc: 0.4392\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9590 - acc: 0.4760 - val_loss: 2.2155 - val_acc: 0.4371\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9545 - acc: 0.4793 - val_loss: 2.1956 - val_acc: 0.4386\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9499 - acc: 0.4791 - val_loss: 2.2260 - val_acc: 0.4353\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9516 - acc: 0.4773 - val_loss: 2.2105 - val_acc: 0.4353\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9443 - acc: 0.4799 - val_loss: 2.2270 - val_acc: 0.4360\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9476 - acc: 0.4809 - val_loss: 2.1987 - val_acc: 0.4384\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9403 - acc: 0.4806 - val_loss: 2.2133 - val_acc: 0.4375\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9427 - acc: 0.4806 - val_loss: 2.2077 - val_acc: 0.4368\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9348 - acc: 0.4830 - val_loss: 2.2028 - val_acc: 0.4384\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9428 - acc: 0.4804 - val_loss: 2.2011 - val_acc: 0.4403\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9292 - acc: 0.4846 - val_loss: 2.2007 - val_acc: 0.4396\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9345 - acc: 0.4836 - val_loss: 2.2369 - val_acc: 0.4346\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9410 - acc: 0.4820 - val_loss: 2.2084 - val_acc: 0.4398\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9339 - acc: 0.4828 - val_loss: 2.2060 - val_acc: 0.4412\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9260 - acc: 0.4852 - val_loss: 2.2011 - val_acc: 0.4396\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9277 - acc: 0.4861 - val_loss: 2.2232 - val_acc: 0.4359\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9328 - acc: 0.4816 - val_loss: 2.2093 - val_acc: 0.4404\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9297 - acc: 0.4848 - val_loss: 2.2096 - val_acc: 0.4385\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9220 - acc: 0.4852 - val_loss: 2.1946 - val_acc: 0.4412\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9198 - acc: 0.4866 - val_loss: 2.2061 - val_acc: 0.4385\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9154 - acc: 0.4863 - val_loss: 2.1983 - val_acc: 0.4390\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9188 - acc: 0.4861 - val_loss: 2.2194 - val_acc: 0.4354\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9275 - acc: 0.4835 - val_loss: 2.2174 - val_acc: 0.4359\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2111 - acc: 0.4223 - val_loss: 2.1630 - val_acc: 0.4440\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1833 - acc: 0.4287 - val_loss: 2.1722 - val_acc: 0.4443\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1773 - acc: 0.4287 - val_loss: 2.1690 - val_acc: 0.4431\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1598 - acc: 0.4325 - val_loss: 2.1822 - val_acc: 0.4416\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1524 - acc: 0.4357 - val_loss: 2.1803 - val_acc: 0.4407\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1444 - acc: 0.4362 - val_loss: 2.1783 - val_acc: 0.4431\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1352 - acc: 0.4374 - val_loss: 2.1883 - val_acc: 0.4400\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1377 - acc: 0.4369 - val_loss: 2.1846 - val_acc: 0.4418\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1228 - acc: 0.4392 - val_loss: 2.1798 - val_acc: 0.4421\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1214 - acc: 0.4410 - val_loss: 2.1822 - val_acc: 0.4407\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1185 - acc: 0.4420 - val_loss: 2.1850 - val_acc: 0.4419\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1120 - acc: 0.4424 - val_loss: 2.1857 - val_acc: 0.4394\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1046 - acc: 0.4463 - val_loss: 2.2022 - val_acc: 0.4389\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1035 - acc: 0.4455 - val_loss: 2.1955 - val_acc: 0.4411\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1021 - acc: 0.4446 - val_loss: 2.1881 - val_acc: 0.4409\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0958 - acc: 0.4466 - val_loss: 2.1911 - val_acc: 0.4391\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0914 - acc: 0.4476 - val_loss: 2.1881 - val_acc: 0.4410\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0905 - acc: 0.4478 - val_loss: 2.1927 - val_acc: 0.4420\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0859 - acc: 0.4484 - val_loss: 2.1916 - val_acc: 0.4392\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0829 - acc: 0.4502 - val_loss: 2.2001 - val_acc: 0.4396\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0810 - acc: 0.4500 - val_loss: 2.2002 - val_acc: 0.4377\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0749 - acc: 0.4504 - val_loss: 2.1989 - val_acc: 0.4409\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0729 - acc: 0.4513 - val_loss: 2.1895 - val_acc: 0.4404\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0686 - acc: 0.4545 - val_loss: 2.1913 - val_acc: 0.4404\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0685 - acc: 0.4536 - val_loss: 2.1872 - val_acc: 0.4396\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0715 - acc: 0.4517 - val_loss: 2.1959 - val_acc: 0.4405\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0648 - acc: 0.4507 - val_loss: 2.1850 - val_acc: 0.4414\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0609 - acc: 0.4525 - val_loss: 2.1863 - val_acc: 0.4429\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0570 - acc: 0.4540 - val_loss: 2.1837 - val_acc: 0.4399\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0543 - acc: 0.4545 - val_loss: 2.1841 - val_acc: 0.4423\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0523 - acc: 0.4557 - val_loss: 2.1790 - val_acc: 0.4414\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0484 - acc: 0.4587 - val_loss: 2.1960 - val_acc: 0.4411\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0464 - acc: 0.4570 - val_loss: 2.1974 - val_acc: 0.4396\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0449 - acc: 0.4566 - val_loss: 2.1809 - val_acc: 0.4417\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0439 - acc: 0.4573 - val_loss: 2.1944 - val_acc: 0.4419\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0444 - acc: 0.4587 - val_loss: 2.1795 - val_acc: 0.4425\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0456 - acc: 0.4580 - val_loss: 2.1944 - val_acc: 0.4428\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0368 - acc: 0.4590 - val_loss: 2.1740 - val_acc: 0.4441\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0326 - acc: 0.4597 - val_loss: 2.1803 - val_acc: 0.4437\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0347 - acc: 0.4595 - val_loss: 2.1850 - val_acc: 0.4425\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0272 - acc: 0.4621 - val_loss: 2.1741 - val_acc: 0.4427\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0252 - acc: 0.4617 - val_loss: 2.1792 - val_acc: 0.4439\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0243 - acc: 0.4622 - val_loss: 2.1746 - val_acc: 0.4449\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0226 - acc: 0.4619 - val_loss: 2.1961 - val_acc: 0.4402\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0262 - acc: 0.4620 - val_loss: 2.1805 - val_acc: 0.4448\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0245 - acc: 0.4627 - val_loss: 2.1884 - val_acc: 0.4421\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0239 - acc: 0.4624 - val_loss: 2.1923 - val_acc: 0.4412\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0290 - acc: 0.4598 - val_loss: 2.1941 - val_acc: 0.4421\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0371 - acc: 0.4611 - val_loss: 2.1858 - val_acc: 0.4419\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0152 - acc: 0.4630 - val_loss: 2.1797 - val_acc: 0.4438\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0175 - acc: 0.4638 - val_loss: 2.1777 - val_acc: 0.4454\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0138 - acc: 0.4638 - val_loss: 2.1818 - val_acc: 0.4442\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0102 - acc: 0.4671 - val_loss: 2.1784 - val_acc: 0.4449\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0074 - acc: 0.4661 - val_loss: 2.1787 - val_acc: 0.4429\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0087 - acc: 0.4675 - val_loss: 2.1769 - val_acc: 0.4438\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0047 - acc: 0.4668 - val_loss: 2.1693 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9981 - acc: 0.4684 - val_loss: 2.1664 - val_acc: 0.4466\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9980 - acc: 0.4669 - val_loss: 2.1790 - val_acc: 0.4430\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0013 - acc: 0.4676 - val_loss: 2.1738 - val_acc: 0.4450\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0096 - acc: 0.4655 - val_loss: 2.1747 - val_acc: 0.4445\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9966 - acc: 0.4675 - val_loss: 2.1697 - val_acc: 0.4461\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9918 - acc: 0.4697 - val_loss: 2.1785 - val_acc: 0.4407\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9907 - acc: 0.4697 - val_loss: 2.1747 - val_acc: 0.4454\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9955 - acc: 0.4685 - val_loss: 2.1897 - val_acc: 0.4419\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0049 - acc: 0.4654 - val_loss: 2.1762 - val_acc: 0.4436\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0016 - acc: 0.4679 - val_loss: 2.1962 - val_acc: 0.4417\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9948 - acc: 0.4676 - val_loss: 2.1838 - val_acc: 0.4418\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9955 - acc: 0.4686 - val_loss: 2.1816 - val_acc: 0.4422\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9839 - acc: 0.4696 - val_loss: 2.1757 - val_acc: 0.4428\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9848 - acc: 0.4699 - val_loss: 2.1938 - val_acc: 0.4405\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9832 - acc: 0.4704 - val_loss: 2.1799 - val_acc: 0.4428\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9827 - acc: 0.4696 - val_loss: 2.1788 - val_acc: 0.4412\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9830 - acc: 0.4710 - val_loss: 2.1787 - val_acc: 0.4414\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9772 - acc: 0.4740 - val_loss: 2.1921 - val_acc: 0.4423\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9823 - acc: 0.4712 - val_loss: 2.2044 - val_acc: 0.4409\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9865 - acc: 0.4717 - val_loss: 2.1791 - val_acc: 0.4457\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9816 - acc: 0.4708 - val_loss: 2.1985 - val_acc: 0.4403\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9849 - acc: 0.4699 - val_loss: 2.1902 - val_acc: 0.4425\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9798 - acc: 0.4716 - val_loss: 2.1874 - val_acc: 0.4410\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9734 - acc: 0.4730 - val_loss: 2.1785 - val_acc: 0.4430\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9631 - acc: 0.4750 - val_loss: 2.1823 - val_acc: 0.4427\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9584 - acc: 0.4779 - val_loss: 2.1739 - val_acc: 0.4462\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9572 - acc: 0.4776 - val_loss: 2.1745 - val_acc: 0.4449\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9611 - acc: 0.4771 - val_loss: 2.1894 - val_acc: 0.4431\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9763 - acc: 0.4739 - val_loss: 2.1941 - val_acc: 0.4419\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9822 - acc: 0.4711 - val_loss: 2.1923 - val_acc: 0.4402\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9730 - acc: 0.4733 - val_loss: 2.1995 - val_acc: 0.4419\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9622 - acc: 0.4771 - val_loss: 2.1822 - val_acc: 0.4413\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9609 - acc: 0.4745 - val_loss: 2.1967 - val_acc: 0.4408\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9601 - acc: 0.4771 - val_loss: 2.1837 - val_acc: 0.4434\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9555 - acc: 0.4783 - val_loss: 2.1894 - val_acc: 0.4409\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9566 - acc: 0.4771 - val_loss: 2.1925 - val_acc: 0.4406\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9654 - acc: 0.4743 - val_loss: 2.1937 - val_acc: 0.4386\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9578 - acc: 0.4769 - val_loss: 2.1890 - val_acc: 0.4419\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9617 - acc: 0.4771 - val_loss: 2.1941 - val_acc: 0.4402\n",
      "Epoch 96/100\n",
      "2s - loss: 1.9568 - acc: 0.4766 - val_loss: 2.1874 - val_acc: 0.4421\n",
      "Epoch 97/100\n",
      "2s - loss: 1.9528 - acc: 0.4792 - val_loss: 2.1784 - val_acc: 0.4426\n",
      "Epoch 98/100\n",
      "2s - loss: 1.9443 - acc: 0.4797 - val_loss: 2.1827 - val_acc: 0.4414\n",
      "Epoch 99/100\n",
      "2s - loss: 1.9523 - acc: 0.4783 - val_loss: 2.1999 - val_acc: 0.4388\n",
      "Epoch 100/100\n",
      "2s - loss: 1.9538 - acc: 0.4777 - val_loss: 2.1921 - val_acc: 0.4400\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1947 - acc: 0.4285 - val_loss: 2.2056 - val_acc: 0.4417\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1757 - acc: 0.4306 - val_loss: 2.2072 - val_acc: 0.4423\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1662 - acc: 0.4340 - val_loss: 2.2014 - val_acc: 0.4417\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1537 - acc: 0.4346 - val_loss: 2.2163 - val_acc: 0.4359\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1479 - acc: 0.4378 - val_loss: 2.2254 - val_acc: 0.4360\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1424 - acc: 0.4382 - val_loss: 2.2244 - val_acc: 0.4352\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1327 - acc: 0.4378 - val_loss: 2.2310 - val_acc: 0.4355\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1247 - acc: 0.4407 - val_loss: 2.2268 - val_acc: 0.4365\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1185 - acc: 0.4414 - val_loss: 2.2367 - val_acc: 0.4358\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1178 - acc: 0.4424 - val_loss: 2.2298 - val_acc: 0.4337\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1153 - acc: 0.4443 - val_loss: 2.2262 - val_acc: 0.4365\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1068 - acc: 0.4459 - val_loss: 2.2257 - val_acc: 0.4361\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1019 - acc: 0.4460 - val_loss: 2.2301 - val_acc: 0.4333\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0974 - acc: 0.4467 - val_loss: 2.2414 - val_acc: 0.4328\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0940 - acc: 0.4492 - val_loss: 2.2290 - val_acc: 0.4340\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0945 - acc: 0.4478 - val_loss: 2.2319 - val_acc: 0.4332\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0877 - acc: 0.4504 - val_loss: 2.2193 - val_acc: 0.4346\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0835 - acc: 0.4485 - val_loss: 2.2292 - val_acc: 0.4347\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0791 - acc: 0.4529 - val_loss: 2.2610 - val_acc: 0.4300\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0780 - acc: 0.4515 - val_loss: 2.2253 - val_acc: 0.4349\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0723 - acc: 0.4532 - val_loss: 2.2554 - val_acc: 0.4280\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0707 - acc: 0.4517 - val_loss: 2.2280 - val_acc: 0.4340\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0897 - acc: 0.4501 - val_loss: 2.2523 - val_acc: 0.4314\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0730 - acc: 0.4522 - val_loss: 2.2261 - val_acc: 0.4350\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0698 - acc: 0.4533 - val_loss: 2.2290 - val_acc: 0.4353\n",
      "Epoch 26/100\n",
      "2s - loss: 2.0558 - acc: 0.4581 - val_loss: 2.2280 - val_acc: 0.4342\n",
      "Epoch 27/100\n",
      "2s - loss: 2.0549 - acc: 0.4561 - val_loss: 2.2217 - val_acc: 0.4339\n",
      "Epoch 28/100\n",
      "2s - loss: 2.0531 - acc: 0.4585 - val_loss: 2.2308 - val_acc: 0.4318\n",
      "Epoch 29/100\n",
      "2s - loss: 2.0555 - acc: 0.4568 - val_loss: 2.2239 - val_acc: 0.4319\n",
      "Epoch 30/100\n",
      "2s - loss: 2.0484 - acc: 0.4583 - val_loss: 2.2454 - val_acc: 0.4317\n",
      "Epoch 31/100\n",
      "2s - loss: 2.0495 - acc: 0.4584 - val_loss: 2.2403 - val_acc: 0.4309\n",
      "Epoch 32/100\n",
      "2s - loss: 2.0531 - acc: 0.4584 - val_loss: 2.2309 - val_acc: 0.4346\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0448 - acc: 0.4592 - val_loss: 2.2258 - val_acc: 0.4316\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0404 - acc: 0.4603 - val_loss: 2.2248 - val_acc: 0.4358\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0527 - acc: 0.4576 - val_loss: 2.2228 - val_acc: 0.4324\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0355 - acc: 0.4611 - val_loss: 2.2287 - val_acc: 0.4324\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0360 - acc: 0.4597 - val_loss: 2.2225 - val_acc: 0.4336\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0305 - acc: 0.4634 - val_loss: 2.2306 - val_acc: 0.4364\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0299 - acc: 0.4627 - val_loss: 2.2228 - val_acc: 0.4352\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0250 - acc: 0.4621 - val_loss: 2.2204 - val_acc: 0.4353\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0273 - acc: 0.4635 - val_loss: 2.2627 - val_acc: 0.4292\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0371 - acc: 0.4601 - val_loss: 2.2271 - val_acc: 0.4367\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0338 - acc: 0.4601 - val_loss: 2.2697 - val_acc: 0.4337\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0357 - acc: 0.4597 - val_loss: 2.2183 - val_acc: 0.4367\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0363 - acc: 0.4609 - val_loss: 2.2200 - val_acc: 0.4359\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0138 - acc: 0.4662 - val_loss: 2.2270 - val_acc: 0.4341\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0423 - acc: 0.4585 - val_loss: 2.2139 - val_acc: 0.4393\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0214 - acc: 0.4626 - val_loss: 2.2128 - val_acc: 0.4398\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0133 - acc: 0.4663 - val_loss: 2.2302 - val_acc: 0.4363\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0169 - acc: 0.4643 - val_loss: 2.2201 - val_acc: 0.4354\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0128 - acc: 0.4686 - val_loss: 2.2792 - val_acc: 0.4312\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0297 - acc: 0.4615 - val_loss: 2.2251 - val_acc: 0.4358\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9999 - acc: 0.4679 - val_loss: 2.2021 - val_acc: 0.4389\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9969 - acc: 0.4694 - val_loss: 2.2102 - val_acc: 0.4377\n",
      "Epoch 55/100\n",
      "2s - loss: 1.9979 - acc: 0.4701 - val_loss: 2.2262 - val_acc: 0.4365\n",
      "Epoch 56/100\n",
      "2s - loss: 1.9985 - acc: 0.4689 - val_loss: 2.2098 - val_acc: 0.4390\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9899 - acc: 0.4712 - val_loss: 2.2011 - val_acc: 0.4405\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9928 - acc: 0.4694 - val_loss: 2.2371 - val_acc: 0.4347\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9921 - acc: 0.4690 - val_loss: 2.2496 - val_acc: 0.4348\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0262 - acc: 0.4625 - val_loss: 2.2463 - val_acc: 0.4332\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0035 - acc: 0.4659 - val_loss: 2.2173 - val_acc: 0.4375\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9984 - acc: 0.4689 - val_loss: 2.2365 - val_acc: 0.4376\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0010 - acc: 0.4686 - val_loss: 2.2192 - val_acc: 0.4373\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9899 - acc: 0.4731 - val_loss: 2.2224 - val_acc: 0.4364\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9919 - acc: 0.4684 - val_loss: 2.2336 - val_acc: 0.4379\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0145 - acc: 0.4674 - val_loss: 2.2283 - val_acc: 0.4368\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9906 - acc: 0.4716 - val_loss: 2.2363 - val_acc: 0.4371\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9912 - acc: 0.4706 - val_loss: 2.2355 - val_acc: 0.4378\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9848 - acc: 0.4722 - val_loss: 2.2241 - val_acc: 0.4371\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9812 - acc: 0.4713 - val_loss: 2.2232 - val_acc: 0.4370\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9730 - acc: 0.4748 - val_loss: 2.2227 - val_acc: 0.4351\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9743 - acc: 0.4738 - val_loss: 2.2291 - val_acc: 0.4358\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9726 - acc: 0.4752 - val_loss: 2.2297 - val_acc: 0.4347\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9783 - acc: 0.4748 - val_loss: 2.2257 - val_acc: 0.4379\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9772 - acc: 0.4752 - val_loss: 2.2279 - val_acc: 0.4373\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9762 - acc: 0.4756 - val_loss: 2.2328 - val_acc: 0.4372\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9742 - acc: 0.4760 - val_loss: 2.2229 - val_acc: 0.4383\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9691 - acc: 0.4775 - val_loss: 2.2270 - val_acc: 0.4402\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9691 - acc: 0.4747 - val_loss: 2.2290 - val_acc: 0.4372\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9681 - acc: 0.4777 - val_loss: 2.2313 - val_acc: 0.4379\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9690 - acc: 0.4757 - val_loss: 2.2241 - val_acc: 0.4398\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9686 - acc: 0.4766 - val_loss: 2.2229 - val_acc: 0.4412\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9597 - acc: 0.4772 - val_loss: 2.2123 - val_acc: 0.4382\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9589 - acc: 0.4782 - val_loss: 2.2061 - val_acc: 0.4425\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9549 - acc: 0.4784 - val_loss: 2.2170 - val_acc: 0.4381\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9512 - acc: 0.4810 - val_loss: 2.2111 - val_acc: 0.4406\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9484 - acc: 0.4828 - val_loss: 2.2159 - val_acc: 0.4395\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9507 - acc: 0.4809 - val_loss: 2.2281 - val_acc: 0.4390\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9447 - acc: 0.4820 - val_loss: 2.2119 - val_acc: 0.4387\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9536 - acc: 0.4803 - val_loss: 2.2262 - val_acc: 0.4377\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9559 - acc: 0.4810 - val_loss: 2.2089 - val_acc: 0.4412\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9539 - acc: 0.4804 - val_loss: 2.2334 - val_acc: 0.4390\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9594 - acc: 0.4784 - val_loss: 2.2140 - val_acc: 0.4397\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9557 - acc: 0.4797 - val_loss: 2.2346 - val_acc: 0.4387\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9531 - acc: 0.4806 - val_loss: 2.2054 - val_acc: 0.4422\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9522 - acc: 0.4805 - val_loss: 2.2413 - val_acc: 0.4389\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9630 - acc: 0.4781 - val_loss: 2.2165 - val_acc: 0.4405\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9490 - acc: 0.4811 - val_loss: 2.2153 - val_acc: 0.4408\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9395 - acc: 0.4820 - val_loss: 2.2102 - val_acc: 0.4435\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9411 - acc: 0.4825 - val_loss: 2.2248 - val_acc: 0.4381\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2186 - acc: 0.4217 - val_loss: 2.1649 - val_acc: 0.4463\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1984 - acc: 0.4252 - val_loss: 2.1670 - val_acc: 0.4472\n",
      "Epoch 3/100\n",
      "1s - loss: 2.2044 - acc: 0.4259 - val_loss: 2.1708 - val_acc: 0.4452\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1778 - acc: 0.4319 - val_loss: 2.1673 - val_acc: 0.4450\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1684 - acc: 0.4302 - val_loss: 2.1602 - val_acc: 0.4459\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1615 - acc: 0.4346 - val_loss: 2.1662 - val_acc: 0.4450\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1590 - acc: 0.4328 - val_loss: 2.1690 - val_acc: 0.4439\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1592 - acc: 0.4348 - val_loss: 2.1845 - val_acc: 0.4428\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1496 - acc: 0.4358 - val_loss: 2.1846 - val_acc: 0.4433\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1521 - acc: 0.4376 - val_loss: 2.1690 - val_acc: 0.4463\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1367 - acc: 0.4394 - val_loss: 2.1741 - val_acc: 0.4462\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1383 - acc: 0.4386 - val_loss: 2.1700 - val_acc: 0.4448\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1235 - acc: 0.4438 - val_loss: 2.1712 - val_acc: 0.4459\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1200 - acc: 0.4423 - val_loss: 2.1704 - val_acc: 0.4461\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1198 - acc: 0.4413 - val_loss: 2.1715 - val_acc: 0.4442\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1169 - acc: 0.4431 - val_loss: 2.1783 - val_acc: 0.4430\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1239 - acc: 0.4413 - val_loss: 2.1654 - val_acc: 0.4474\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1102 - acc: 0.4457 - val_loss: 2.1764 - val_acc: 0.4421\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1173 - acc: 0.4416 - val_loss: 2.1737 - val_acc: 0.4446\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1241 - acc: 0.4427 - val_loss: 2.1772 - val_acc: 0.4440\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1016 - acc: 0.4469 - val_loss: 2.1827 - val_acc: 0.4427\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1161 - acc: 0.4437 - val_loss: 2.1707 - val_acc: 0.4448\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0951 - acc: 0.4469 - val_loss: 2.1728 - val_acc: 0.4443\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0881 - acc: 0.4489 - val_loss: 2.1655 - val_acc: 0.4472\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0866 - acc: 0.4492 - val_loss: 2.1640 - val_acc: 0.4483\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0824 - acc: 0.4512 - val_loss: 2.1614 - val_acc: 0.4486\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0856 - acc: 0.4489 - val_loss: 2.1662 - val_acc: 0.4459\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0870 - acc: 0.4510 - val_loss: 2.1727 - val_acc: 0.4453\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0842 - acc: 0.4498 - val_loss: 2.1707 - val_acc: 0.4430\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0766 - acc: 0.4519 - val_loss: 2.1682 - val_acc: 0.4445\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0851 - acc: 0.4510 - val_loss: 2.1621 - val_acc: 0.4464\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0751 - acc: 0.4527 - val_loss: 2.1666 - val_acc: 0.4475\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0746 - acc: 0.4525 - val_loss: 2.1666 - val_acc: 0.4456\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0714 - acc: 0.4525 - val_loss: 2.1631 - val_acc: 0.4456\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0716 - acc: 0.4517 - val_loss: 2.1600 - val_acc: 0.4448\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0665 - acc: 0.4538 - val_loss: 2.1793 - val_acc: 0.4429\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0709 - acc: 0.4511 - val_loss: 2.1662 - val_acc: 0.4470\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0759 - acc: 0.4535 - val_loss: 2.1660 - val_acc: 0.4436\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0583 - acc: 0.4567 - val_loss: 2.1615 - val_acc: 0.4457\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0534 - acc: 0.4576 - val_loss: 2.2093 - val_acc: 0.4401\n",
      "Epoch 41/100\n",
      "1s - loss: 2.1122 - acc: 0.4459 - val_loss: 2.2349 - val_acc: 0.4333\n",
      "Epoch 42/100\n",
      "1s - loss: 2.1135 - acc: 0.4433 - val_loss: 2.2048 - val_acc: 0.4376\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0617 - acc: 0.4544 - val_loss: 2.1753 - val_acc: 0.4425\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0495 - acc: 0.4574 - val_loss: 2.1664 - val_acc: 0.4443\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0419 - acc: 0.4595 - val_loss: 2.1623 - val_acc: 0.4446\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0409 - acc: 0.4583 - val_loss: 2.1820 - val_acc: 0.4406\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0461 - acc: 0.4577 - val_loss: 2.1689 - val_acc: 0.4432\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0456 - acc: 0.4592 - val_loss: 2.1662 - val_acc: 0.4410\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0340 - acc: 0.4620 - val_loss: 2.1604 - val_acc: 0.4436\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0303 - acc: 0.4621 - val_loss: 2.1650 - val_acc: 0.4452\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0359 - acc: 0.4614 - val_loss: 2.1706 - val_acc: 0.4422\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0377 - acc: 0.4609 - val_loss: 2.1766 - val_acc: 0.4435\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0327 - acc: 0.4628 - val_loss: 2.1730 - val_acc: 0.4440\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0279 - acc: 0.4639 - val_loss: 2.1557 - val_acc: 0.4464\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0232 - acc: 0.4631 - val_loss: 2.1621 - val_acc: 0.4458\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0216 - acc: 0.4646 - val_loss: 2.1623 - val_acc: 0.4459\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0233 - acc: 0.4639 - val_loss: 2.1714 - val_acc: 0.4448\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0257 - acc: 0.4628 - val_loss: 2.1658 - val_acc: 0.4462\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0254 - acc: 0.4629 - val_loss: 2.1839 - val_acc: 0.4419\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0331 - acc: 0.4637 - val_loss: 2.1725 - val_acc: 0.4450\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0172 - acc: 0.4629 - val_loss: 2.1603 - val_acc: 0.4459\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0139 - acc: 0.4660 - val_loss: 2.1857 - val_acc: 0.4420\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0226 - acc: 0.4635 - val_loss: 2.1734 - val_acc: 0.4450\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0310 - acc: 0.4610 - val_loss: 2.2105 - val_acc: 0.4382\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0577 - acc: 0.4554 - val_loss: 2.1664 - val_acc: 0.4470\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0219 - acc: 0.4632 - val_loss: 2.1835 - val_acc: 0.4396\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0325 - acc: 0.4610 - val_loss: 2.1640 - val_acc: 0.4472\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0204 - acc: 0.4642 - val_loss: 2.1717 - val_acc: 0.4451\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0118 - acc: 0.4680 - val_loss: 2.1646 - val_acc: 0.4463\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0207 - acc: 0.4649 - val_loss: 2.1829 - val_acc: 0.4408\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0360 - acc: 0.4612 - val_loss: 2.1714 - val_acc: 0.4448\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0223 - acc: 0.4628 - val_loss: 2.1699 - val_acc: 0.4453\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0043 - acc: 0.4658 - val_loss: 2.1808 - val_acc: 0.4430\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0030 - acc: 0.4694 - val_loss: 2.1622 - val_acc: 0.4442\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9994 - acc: 0.4708 - val_loss: 2.1826 - val_acc: 0.4420\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9957 - acc: 0.4688 - val_loss: 2.1628 - val_acc: 0.4437\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9947 - acc: 0.4705 - val_loss: 2.1774 - val_acc: 0.4424\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9960 - acc: 0.4702 - val_loss: 2.1745 - val_acc: 0.4436\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9930 - acc: 0.4689 - val_loss: 2.1752 - val_acc: 0.4426\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9925 - acc: 0.4718 - val_loss: 2.1912 - val_acc: 0.4400\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9880 - acc: 0.4712 - val_loss: 2.1672 - val_acc: 0.4444\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9944 - acc: 0.4695 - val_loss: 2.1960 - val_acc: 0.4401\n",
      "Epoch 83/100\n",
      "1s - loss: 2.0101 - acc: 0.4676 - val_loss: 2.1970 - val_acc: 0.4412\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0022 - acc: 0.4690 - val_loss: 2.1777 - val_acc: 0.4451\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9905 - acc: 0.4723 - val_loss: 2.1680 - val_acc: 0.4459\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9851 - acc: 0.4723 - val_loss: 2.1642 - val_acc: 0.4465\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9815 - acc: 0.4742 - val_loss: 2.1829 - val_acc: 0.4433\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9963 - acc: 0.4669 - val_loss: 2.1631 - val_acc: 0.4469\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9873 - acc: 0.4708 - val_loss: 2.1758 - val_acc: 0.4425\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9861 - acc: 0.4731 - val_loss: 2.1674 - val_acc: 0.4444\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9878 - acc: 0.4730 - val_loss: 2.1927 - val_acc: 0.4408\n",
      "Epoch 92/100\n",
      "1s - loss: 2.0169 - acc: 0.4662 - val_loss: 2.1857 - val_acc: 0.4418\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9846 - acc: 0.4716 - val_loss: 2.1829 - val_acc: 0.4427\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9745 - acc: 0.4758 - val_loss: 2.1796 - val_acc: 0.4425\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9868 - acc: 0.4722 - val_loss: 2.1819 - val_acc: 0.4386\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9741 - acc: 0.4760 - val_loss: 2.1693 - val_acc: 0.4453\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9738 - acc: 0.4778 - val_loss: 2.1927 - val_acc: 0.4397\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9702 - acc: 0.4763 - val_loss: 2.1625 - val_acc: 0.4465\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9651 - acc: 0.4763 - val_loss: 2.1814 - val_acc: 0.4432\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9675 - acc: 0.4763 - val_loss: 2.1748 - val_acc: 0.4450\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1901 - acc: 0.4292 - val_loss: 2.1767 - val_acc: 0.4468\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1693 - acc: 0.4330 - val_loss: 2.1726 - val_acc: 0.4458\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1554 - acc: 0.4335 - val_loss: 2.1766 - val_acc: 0.4447\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1435 - acc: 0.4368 - val_loss: 2.1774 - val_acc: 0.4443\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1355 - acc: 0.4382 - val_loss: 2.1830 - val_acc: 0.4405\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1258 - acc: 0.4401 - val_loss: 2.1936 - val_acc: 0.4386\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1280 - acc: 0.4405 - val_loss: 2.1897 - val_acc: 0.4398\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1186 - acc: 0.4437 - val_loss: 2.1868 - val_acc: 0.4430\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1138 - acc: 0.4435 - val_loss: 2.1896 - val_acc: 0.4405\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1036 - acc: 0.4436 - val_loss: 2.1874 - val_acc: 0.4401\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1049 - acc: 0.4453 - val_loss: 2.1989 - val_acc: 0.4417\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0957 - acc: 0.4451 - val_loss: 2.2069 - val_acc: 0.4372\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0952 - acc: 0.4477 - val_loss: 2.2040 - val_acc: 0.4386\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0900 - acc: 0.4462 - val_loss: 2.1940 - val_acc: 0.4423\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0997 - acc: 0.4460 - val_loss: 2.1886 - val_acc: 0.4420\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0838 - acc: 0.4475 - val_loss: 2.2041 - val_acc: 0.4379\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0780 - acc: 0.4496 - val_loss: 2.1929 - val_acc: 0.4402\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0738 - acc: 0.4517 - val_loss: 2.1993 - val_acc: 0.4389\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0762 - acc: 0.4512 - val_loss: 2.1945 - val_acc: 0.4423\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0686 - acc: 0.4524 - val_loss: 2.1959 - val_acc: 0.4407\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0683 - acc: 0.4536 - val_loss: 2.2117 - val_acc: 0.4406\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0880 - acc: 0.4491 - val_loss: 2.2228 - val_acc: 0.4359\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0681 - acc: 0.4511 - val_loss: 2.1954 - val_acc: 0.4410\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0510 - acc: 0.4553 - val_loss: 2.1995 - val_acc: 0.4414\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0506 - acc: 0.4560 - val_loss: 2.1958 - val_acc: 0.4402\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0532 - acc: 0.4541 - val_loss: 2.2047 - val_acc: 0.4391\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0485 - acc: 0.4557 - val_loss: 2.2148 - val_acc: 0.4392\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0457 - acc: 0.4571 - val_loss: 2.1932 - val_acc: 0.4422\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0425 - acc: 0.4577 - val_loss: 2.2207 - val_acc: 0.4411\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0623 - acc: 0.4522 - val_loss: 2.1977 - val_acc: 0.4395\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0421 - acc: 0.4570 - val_loss: 2.2115 - val_acc: 0.4381\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0357 - acc: 0.4592 - val_loss: 2.1916 - val_acc: 0.4410\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0328 - acc: 0.4586 - val_loss: 2.2072 - val_acc: 0.4430\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0334 - acc: 0.4599 - val_loss: 2.1996 - val_acc: 0.4408\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0284 - acc: 0.4599 - val_loss: 2.2140 - val_acc: 0.4394\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0294 - acc: 0.4607 - val_loss: 2.1880 - val_acc: 0.4429\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0197 - acc: 0.4632 - val_loss: 2.1930 - val_acc: 0.4400\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0163 - acc: 0.4628 - val_loss: 2.2007 - val_acc: 0.4412\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0227 - acc: 0.4612 - val_loss: 2.1910 - val_acc: 0.4422\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0191 - acc: 0.4636 - val_loss: 2.2091 - val_acc: 0.4390\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0159 - acc: 0.4635 - val_loss: 2.1913 - val_acc: 0.4413\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0161 - acc: 0.4655 - val_loss: 2.1894 - val_acc: 0.4427\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0093 - acc: 0.4652 - val_loss: 2.1904 - val_acc: 0.4420\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0102 - acc: 0.4645 - val_loss: 2.2011 - val_acc: 0.4411\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0195 - acc: 0.4641 - val_loss: 2.2113 - val_acc: 0.4408\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0202 - acc: 0.4619 - val_loss: 2.2063 - val_acc: 0.4406\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0149 - acc: 0.4640 - val_loss: 2.2348 - val_acc: 0.4401\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0484 - acc: 0.4581 - val_loss: 2.2277 - val_acc: 0.4360\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0326 - acc: 0.4602 - val_loss: 2.2203 - val_acc: 0.4399\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0014 - acc: 0.4679 - val_loss: 2.1861 - val_acc: 0.4410\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0030 - acc: 0.4652 - val_loss: 2.2188 - val_acc: 0.4393\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9999 - acc: 0.4680 - val_loss: 2.1938 - val_acc: 0.4402\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9946 - acc: 0.4695 - val_loss: 2.2153 - val_acc: 0.4396\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0008 - acc: 0.4673 - val_loss: 2.1787 - val_acc: 0.4432\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9949 - acc: 0.4685 - val_loss: 2.2028 - val_acc: 0.4406\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9973 - acc: 0.4685 - val_loss: 2.1847 - val_acc: 0.4441\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9936 - acc: 0.4690 - val_loss: 2.1960 - val_acc: 0.4408\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9853 - acc: 0.4695 - val_loss: 2.1875 - val_acc: 0.4409\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9910 - acc: 0.4695 - val_loss: 2.2089 - val_acc: 0.4416\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9863 - acc: 0.4701 - val_loss: 2.2007 - val_acc: 0.4407\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9916 - acc: 0.4700 - val_loss: 2.2050 - val_acc: 0.4395\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9846 - acc: 0.4717 - val_loss: 2.2028 - val_acc: 0.4399\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9749 - acc: 0.4728 - val_loss: 2.1994 - val_acc: 0.4407\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9766 - acc: 0.4708 - val_loss: 2.2121 - val_acc: 0.4368\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9851 - acc: 0.4712 - val_loss: 2.1977 - val_acc: 0.4383\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9865 - acc: 0.4703 - val_loss: 2.2036 - val_acc: 0.4419\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9784 - acc: 0.4721 - val_loss: 2.2030 - val_acc: 0.4401\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9745 - acc: 0.4739 - val_loss: 2.2037 - val_acc: 0.4399\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9800 - acc: 0.4725 - val_loss: 2.1881 - val_acc: 0.4436\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9700 - acc: 0.4723 - val_loss: 2.1952 - val_acc: 0.4407\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9667 - acc: 0.4745 - val_loss: 2.2003 - val_acc: 0.4395\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9814 - acc: 0.4720 - val_loss: 2.2386 - val_acc: 0.4382\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0154 - acc: 0.4636 - val_loss: 2.2246 - val_acc: 0.4335\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9838 - acc: 0.4722 - val_loss: 2.1992 - val_acc: 0.4425\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9606 - acc: 0.4755 - val_loss: 2.1997 - val_acc: 0.4404\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9561 - acc: 0.4788 - val_loss: 2.2032 - val_acc: 0.4420\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9611 - acc: 0.4754 - val_loss: 2.1880 - val_acc: 0.4430\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9550 - acc: 0.4765 - val_loss: 2.2057 - val_acc: 0.4410\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9569 - acc: 0.4768 - val_loss: 2.2068 - val_acc: 0.4403\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9624 - acc: 0.4749 - val_loss: 2.2381 - val_acc: 0.4364\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9675 - acc: 0.4759 - val_loss: 2.2208 - val_acc: 0.4395\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9600 - acc: 0.4745 - val_loss: 2.2175 - val_acc: 0.4426\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9707 - acc: 0.4746 - val_loss: 2.2009 - val_acc: 0.4393\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9560 - acc: 0.4780 - val_loss: 2.2032 - val_acc: 0.4412\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9457 - acc: 0.4802 - val_loss: 2.1871 - val_acc: 0.4420\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9445 - acc: 0.4805 - val_loss: 2.2016 - val_acc: 0.4422\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9511 - acc: 0.4781 - val_loss: 2.1810 - val_acc: 0.4412\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9382 - acc: 0.4800 - val_loss: 2.1889 - val_acc: 0.4432\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9373 - acc: 0.4811 - val_loss: 2.1894 - val_acc: 0.4434\n",
      "Epoch 90/100\n",
      "2s - loss: 1.9345 - acc: 0.4819 - val_loss: 2.2192 - val_acc: 0.4388\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9511 - acc: 0.4793 - val_loss: 2.2366 - val_acc: 0.4393\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9519 - acc: 0.4790 - val_loss: 2.2222 - val_acc: 0.4383\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9482 - acc: 0.4786 - val_loss: 2.2076 - val_acc: 0.4406\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9332 - acc: 0.4817 - val_loss: 2.2006 - val_acc: 0.4430\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9353 - acc: 0.4808 - val_loss: 2.2098 - val_acc: 0.4390\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9342 - acc: 0.4831 - val_loss: 2.2105 - val_acc: 0.4409\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9331 - acc: 0.4829 - val_loss: 2.1920 - val_acc: 0.4412\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9330 - acc: 0.4822 - val_loss: 2.2164 - val_acc: 0.4408\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9429 - acc: 0.4795 - val_loss: 2.2241 - val_acc: 0.4394\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9328 - acc: 0.4828 - val_loss: 2.2076 - val_acc: 0.4407\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2124 - acc: 0.4206 - val_loss: 2.1526 - val_acc: 0.4461\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1844 - acc: 0.4291 - val_loss: 2.1673 - val_acc: 0.4445\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1730 - acc: 0.4299 - val_loss: 2.1585 - val_acc: 0.4445\n",
      "Epoch 4/100\n",
      "2s - loss: 2.1628 - acc: 0.4313 - val_loss: 2.1663 - val_acc: 0.4445\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1542 - acc: 0.4337 - val_loss: 2.1706 - val_acc: 0.4434\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1499 - acc: 0.4341 - val_loss: 2.1769 - val_acc: 0.4433\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1425 - acc: 0.4350 - val_loss: 2.1733 - val_acc: 0.4422\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1344 - acc: 0.4374 - val_loss: 2.1795 - val_acc: 0.4389\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1349 - acc: 0.4375 - val_loss: 2.1867 - val_acc: 0.4379\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1255 - acc: 0.4404 - val_loss: 2.1814 - val_acc: 0.4392\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1187 - acc: 0.4404 - val_loss: 2.1913 - val_acc: 0.4359\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1202 - acc: 0.4414 - val_loss: 2.1837 - val_acc: 0.4381\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1153 - acc: 0.4410 - val_loss: 2.1868 - val_acc: 0.4378\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1080 - acc: 0.4432 - val_loss: 2.1861 - val_acc: 0.4369\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1053 - acc: 0.4440 - val_loss: 2.1814 - val_acc: 0.4382\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1000 - acc: 0.4441 - val_loss: 2.1828 - val_acc: 0.4381\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0967 - acc: 0.4435 - val_loss: 2.1726 - val_acc: 0.4412\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0928 - acc: 0.4460 - val_loss: 2.1768 - val_acc: 0.4417\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0886 - acc: 0.4464 - val_loss: 2.1800 - val_acc: 0.4373\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0900 - acc: 0.4479 - val_loss: 2.1912 - val_acc: 0.4384\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0849 - acc: 0.4485 - val_loss: 2.1733 - val_acc: 0.4429\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0828 - acc: 0.4475 - val_loss: 2.1834 - val_acc: 0.4408\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0791 - acc: 0.4497 - val_loss: 2.1751 - val_acc: 0.4411\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0736 - acc: 0.4513 - val_loss: 2.1751 - val_acc: 0.4416\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0678 - acc: 0.4517 - val_loss: 2.1844 - val_acc: 0.4382\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0713 - acc: 0.4520 - val_loss: 2.1763 - val_acc: 0.4405\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0690 - acc: 0.4510 - val_loss: 2.1782 - val_acc: 0.4393\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0669 - acc: 0.4505 - val_loss: 2.1808 - val_acc: 0.4424\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0627 - acc: 0.4519 - val_loss: 2.1871 - val_acc: 0.4385\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0591 - acc: 0.4539 - val_loss: 2.2004 - val_acc: 0.4401\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0652 - acc: 0.4528 - val_loss: 2.1850 - val_acc: 0.4417\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0707 - acc: 0.4518 - val_loss: 2.1878 - val_acc: 0.4411\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0608 - acc: 0.4512 - val_loss: 2.1877 - val_acc: 0.4397\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0534 - acc: 0.4540 - val_loss: 2.1841 - val_acc: 0.4425\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0545 - acc: 0.4548 - val_loss: 2.1937 - val_acc: 0.4382\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0519 - acc: 0.4568 - val_loss: 2.1750 - val_acc: 0.4405\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0471 - acc: 0.4582 - val_loss: 2.1805 - val_acc: 0.4404\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0471 - acc: 0.4551 - val_loss: 2.1919 - val_acc: 0.4429\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0367 - acc: 0.4586 - val_loss: 2.1711 - val_acc: 0.4443\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0393 - acc: 0.4581 - val_loss: 2.1769 - val_acc: 0.4420\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0375 - acc: 0.4602 - val_loss: 2.1974 - val_acc: 0.4370\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0464 - acc: 0.4573 - val_loss: 2.1783 - val_acc: 0.4412\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0339 - acc: 0.4583 - val_loss: 2.1851 - val_acc: 0.4391\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0382 - acc: 0.4577 - val_loss: 2.1826 - val_acc: 0.4414\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0281 - acc: 0.4619 - val_loss: 2.1776 - val_acc: 0.4415\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0284 - acc: 0.4610 - val_loss: 2.1790 - val_acc: 0.4406\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0263 - acc: 0.4625 - val_loss: 2.1863 - val_acc: 0.4388\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0248 - acc: 0.4623 - val_loss: 2.1996 - val_acc: 0.4372\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0309 - acc: 0.4605 - val_loss: 2.1831 - val_acc: 0.4412\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0296 - acc: 0.4593 - val_loss: 2.1889 - val_acc: 0.4398\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0173 - acc: 0.4621 - val_loss: 2.1793 - val_acc: 0.4413\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0195 - acc: 0.4622 - val_loss: 2.1903 - val_acc: 0.4402\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0235 - acc: 0.4627 - val_loss: 2.1683 - val_acc: 0.4437\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0113 - acc: 0.4651 - val_loss: 2.1760 - val_acc: 0.4419\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0066 - acc: 0.4659 - val_loss: 2.1724 - val_acc: 0.4433\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0001 - acc: 0.4677 - val_loss: 2.1794 - val_acc: 0.4426\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0033 - acc: 0.4674 - val_loss: 2.1697 - val_acc: 0.4426\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0071 - acc: 0.4670 - val_loss: 2.1854 - val_acc: 0.4415\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0047 - acc: 0.4651 - val_loss: 2.1740 - val_acc: 0.4428\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0057 - acc: 0.4648 - val_loss: 2.1831 - val_acc: 0.4386\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9998 - acc: 0.4663 - val_loss: 2.1623 - val_acc: 0.4440\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9901 - acc: 0.4684 - val_loss: 2.1756 - val_acc: 0.4408\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0061 - acc: 0.4652 - val_loss: 2.1632 - val_acc: 0.4453\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0135 - acc: 0.4652 - val_loss: 2.1861 - val_acc: 0.4390\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9970 - acc: 0.4693 - val_loss: 2.1766 - val_acc: 0.4446\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9941 - acc: 0.4692 - val_loss: 2.1790 - val_acc: 0.4402\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9860 - acc: 0.4713 - val_loss: 2.2163 - val_acc: 0.4408\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0654 - acc: 0.4559 - val_loss: 2.2597 - val_acc: 0.4229\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0665 - acc: 0.4548 - val_loss: 2.2399 - val_acc: 0.4283\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0402 - acc: 0.4585 - val_loss: 2.2199 - val_acc: 0.4325\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0258 - acc: 0.4605 - val_loss: 2.2117 - val_acc: 0.4345\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0235 - acc: 0.4627 - val_loss: 2.2405 - val_acc: 0.4347\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0106 - acc: 0.4644 - val_loss: 2.1965 - val_acc: 0.4385\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9881 - acc: 0.4715 - val_loss: 2.1925 - val_acc: 0.4411\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9878 - acc: 0.4694 - val_loss: 2.1967 - val_acc: 0.4424\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9863 - acc: 0.4699 - val_loss: 2.1968 - val_acc: 0.4383\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9891 - acc: 0.4696 - val_loss: 2.1736 - val_acc: 0.4442\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9850 - acc: 0.4707 - val_loss: 2.1901 - val_acc: 0.4411\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9765 - acc: 0.4717 - val_loss: 2.1725 - val_acc: 0.4432\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9718 - acc: 0.4746 - val_loss: 2.1909 - val_acc: 0.4397\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9756 - acc: 0.4748 - val_loss: 2.1875 - val_acc: 0.4417\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9798 - acc: 0.4716 - val_loss: 2.1963 - val_acc: 0.4383\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9814 - acc: 0.4707 - val_loss: 2.1738 - val_acc: 0.4444\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9696 - acc: 0.4736 - val_loss: 2.1845 - val_acc: 0.4396\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9705 - acc: 0.4752 - val_loss: 2.1815 - val_acc: 0.4410\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9698 - acc: 0.4742 - val_loss: 2.1820 - val_acc: 0.4412\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9630 - acc: 0.4755 - val_loss: 2.1788 - val_acc: 0.4424\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9782 - acc: 0.4731 - val_loss: 2.2060 - val_acc: 0.4377\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9693 - acc: 0.4749 - val_loss: 2.1860 - val_acc: 0.4438\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9620 - acc: 0.4761 - val_loss: 2.1971 - val_acc: 0.4373\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9709 - acc: 0.4730 - val_loss: 2.1834 - val_acc: 0.4421\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9592 - acc: 0.4774 - val_loss: 2.1817 - val_acc: 0.4384\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9606 - acc: 0.4766 - val_loss: 2.1776 - val_acc: 0.4411\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9620 - acc: 0.4752 - val_loss: 2.2077 - val_acc: 0.4381\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9744 - acc: 0.4735 - val_loss: 2.2320 - val_acc: 0.4348\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9685 - acc: 0.4733 - val_loss: 2.2176 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9750 - acc: 0.4730 - val_loss: 2.2425 - val_acc: 0.4339\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9655 - acc: 0.4746 - val_loss: 2.1954 - val_acc: 0.4419\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9618 - acc: 0.4752 - val_loss: 2.2366 - val_acc: 0.4346\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9474 - acc: 0.4793 - val_loss: 2.1787 - val_acc: 0.4435\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1917 - acc: 0.4290 - val_loss: 2.2123 - val_acc: 0.4446\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1754 - acc: 0.4313 - val_loss: 2.1931 - val_acc: 0.4466\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1622 - acc: 0.4353 - val_loss: 2.2131 - val_acc: 0.4411\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1542 - acc: 0.4358 - val_loss: 2.1879 - val_acc: 0.4460\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1418 - acc: 0.4380 - val_loss: 2.2093 - val_acc: 0.4431\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1359 - acc: 0.4378 - val_loss: 2.2408 - val_acc: 0.4387\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1317 - acc: 0.4405 - val_loss: 2.2245 - val_acc: 0.4400\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1237 - acc: 0.4414 - val_loss: 2.2343 - val_acc: 0.4382\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1177 - acc: 0.4415 - val_loss: 2.2244 - val_acc: 0.4384\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1119 - acc: 0.4448 - val_loss: 2.2440 - val_acc: 0.4370\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1060 - acc: 0.4448 - val_loss: 2.2321 - val_acc: 0.4387\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1035 - acc: 0.4449 - val_loss: 2.2454 - val_acc: 0.4384\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1013 - acc: 0.4442 - val_loss: 2.2275 - val_acc: 0.4409\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0943 - acc: 0.4466 - val_loss: 2.2177 - val_acc: 0.4416\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0874 - acc: 0.4489 - val_loss: 2.2193 - val_acc: 0.4402\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0858 - acc: 0.4479 - val_loss: 2.2340 - val_acc: 0.4382\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0839 - acc: 0.4507 - val_loss: 2.2201 - val_acc: 0.4394\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0793 - acc: 0.4518 - val_loss: 2.2319 - val_acc: 0.4378\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0779 - acc: 0.4516 - val_loss: 2.2229 - val_acc: 0.4409\n",
      "Epoch 20/100\n",
      "2s - loss: 2.0708 - acc: 0.4535 - val_loss: 2.2178 - val_acc: 0.4410\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0709 - acc: 0.4525 - val_loss: 2.2148 - val_acc: 0.4409\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0699 - acc: 0.4538 - val_loss: 2.2112 - val_acc: 0.4410\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0672 - acc: 0.4522 - val_loss: 2.2069 - val_acc: 0.4441\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0648 - acc: 0.4543 - val_loss: 2.2140 - val_acc: 0.4396\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0599 - acc: 0.4552 - val_loss: 2.2163 - val_acc: 0.4376\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0700 - acc: 0.4532 - val_loss: 2.2025 - val_acc: 0.4407\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0581 - acc: 0.4546 - val_loss: 2.1998 - val_acc: 0.4427\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0554 - acc: 0.4557 - val_loss: 2.2120 - val_acc: 0.4415\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0552 - acc: 0.4555 - val_loss: 2.1947 - val_acc: 0.4437\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0559 - acc: 0.4555 - val_loss: 2.2015 - val_acc: 0.4426\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0493 - acc: 0.4565 - val_loss: 2.2009 - val_acc: 0.4437\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0715 - acc: 0.4523 - val_loss: 2.2146 - val_acc: 0.4414\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0448 - acc: 0.4575 - val_loss: 2.1853 - val_acc: 0.4436\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0349 - acc: 0.4585 - val_loss: 2.1883 - val_acc: 0.4449\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0370 - acc: 0.4606 - val_loss: 2.1867 - val_acc: 0.4420\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0288 - acc: 0.4620 - val_loss: 2.1896 - val_acc: 0.4455\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0325 - acc: 0.4583 - val_loss: 2.1934 - val_acc: 0.4440\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0238 - acc: 0.4631 - val_loss: 2.1971 - val_acc: 0.4431\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0287 - acc: 0.4620 - val_loss: 2.2026 - val_acc: 0.4438\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0423 - acc: 0.4584 - val_loss: 2.1986 - val_acc: 0.4436\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0231 - acc: 0.4650 - val_loss: 2.1905 - val_acc: 0.4443\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0191 - acc: 0.4660 - val_loss: 2.1862 - val_acc: 0.4432\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0233 - acc: 0.4636 - val_loss: 2.1908 - val_acc: 0.4449\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0290 - acc: 0.4635 - val_loss: 2.1853 - val_acc: 0.4427\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0220 - acc: 0.4638 - val_loss: 2.1875 - val_acc: 0.4423\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0208 - acc: 0.4635 - val_loss: 2.1895 - val_acc: 0.4447\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0191 - acc: 0.4643 - val_loss: 2.1895 - val_acc: 0.4425\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0159 - acc: 0.4647 - val_loss: 2.1854 - val_acc: 0.4449\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0131 - acc: 0.4644 - val_loss: 2.1955 - val_acc: 0.4409\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0078 - acc: 0.4663 - val_loss: 2.1929 - val_acc: 0.4447\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0030 - acc: 0.4687 - val_loss: 2.1853 - val_acc: 0.4409\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0016 - acc: 0.4668 - val_loss: 2.1711 - val_acc: 0.4464\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9975 - acc: 0.4676 - val_loss: 2.1916 - val_acc: 0.4416\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9998 - acc: 0.4673 - val_loss: 2.1771 - val_acc: 0.4433\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9978 - acc: 0.4694 - val_loss: 2.1830 - val_acc: 0.4442\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9938 - acc: 0.4692 - val_loss: 2.1730 - val_acc: 0.4461\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9978 - acc: 0.4695 - val_loss: 2.1968 - val_acc: 0.4432\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0015 - acc: 0.4695 - val_loss: 2.1886 - val_acc: 0.4415\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0003 - acc: 0.4671 - val_loss: 2.1905 - val_acc: 0.4462\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9945 - acc: 0.4690 - val_loss: 2.1742 - val_acc: 0.4455\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9847 - acc: 0.4721 - val_loss: 2.1985 - val_acc: 0.4428\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9865 - acc: 0.4708 - val_loss: 2.1738 - val_acc: 0.4462\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9782 - acc: 0.4738 - val_loss: 2.1802 - val_acc: 0.4465\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9792 - acc: 0.4727 - val_loss: 2.1748 - val_acc: 0.4470\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9822 - acc: 0.4734 - val_loss: 2.1842 - val_acc: 0.4456\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9959 - acc: 0.4701 - val_loss: 2.1855 - val_acc: 0.4432\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9929 - acc: 0.4701 - val_loss: 2.1788 - val_acc: 0.4470\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9780 - acc: 0.4737 - val_loss: 2.1797 - val_acc: 0.4456\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9759 - acc: 0.4744 - val_loss: 2.1762 - val_acc: 0.4478\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9736 - acc: 0.4729 - val_loss: 2.1746 - val_acc: 0.4446\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9698 - acc: 0.4765 - val_loss: 2.1841 - val_acc: 0.4431\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9706 - acc: 0.4751 - val_loss: 2.1779 - val_acc: 0.4462\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9770 - acc: 0.4746 - val_loss: 2.1773 - val_acc: 0.4461\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9803 - acc: 0.4724 - val_loss: 2.1780 - val_acc: 0.4448\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9743 - acc: 0.4736 - val_loss: 2.1801 - val_acc: 0.4462\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9670 - acc: 0.4751 - val_loss: 2.1807 - val_acc: 0.4434\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9677 - acc: 0.4760 - val_loss: 2.2173 - val_acc: 0.4369\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9750 - acc: 0.4745 - val_loss: 2.1794 - val_acc: 0.4451\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9745 - acc: 0.4741 - val_loss: 2.1960 - val_acc: 0.4434\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9720 - acc: 0.4736 - val_loss: 2.1767 - val_acc: 0.4461\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9606 - acc: 0.4787 - val_loss: 2.1942 - val_acc: 0.4453\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9579 - acc: 0.4769 - val_loss: 2.1821 - val_acc: 0.4450\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9567 - acc: 0.4787 - val_loss: 2.1756 - val_acc: 0.4461\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9585 - acc: 0.4786 - val_loss: 2.1853 - val_acc: 0.4435\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9583 - acc: 0.4784 - val_loss: 2.1755 - val_acc: 0.4455\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9632 - acc: 0.4760 - val_loss: 2.2081 - val_acc: 0.4410\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9580 - acc: 0.4768 - val_loss: 2.1821 - val_acc: 0.4426\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9561 - acc: 0.4788 - val_loss: 2.2051 - val_acc: 0.4391\n",
      "Epoch 89/100\n",
      "2s - loss: 1.9460 - acc: 0.4804 - val_loss: 2.1856 - val_acc: 0.4447\n",
      "Epoch 90/100\n",
      "2s - loss: 1.9425 - acc: 0.4817 - val_loss: 2.1890 - val_acc: 0.4422\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9399 - acc: 0.4823 - val_loss: 2.1752 - val_acc: 0.4453\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9412 - acc: 0.4814 - val_loss: 2.2367 - val_acc: 0.4357\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9574 - acc: 0.4786 - val_loss: 2.1875 - val_acc: 0.4443\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9516 - acc: 0.4787 - val_loss: 2.2153 - val_acc: 0.4389\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9613 - acc: 0.4770 - val_loss: 2.1818 - val_acc: 0.4433\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9438 - acc: 0.4820 - val_loss: 2.2227 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9705 - acc: 0.4765 - val_loss: 2.1897 - val_acc: 0.4409\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9445 - acc: 0.4820 - val_loss: 2.1839 - val_acc: 0.4448\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9382 - acc: 0.4831 - val_loss: 2.1946 - val_acc: 0.4436\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9495 - acc: 0.4810 - val_loss: 2.1920 - val_acc: 0.4447\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1937 - acc: 0.4273 - val_loss: 2.2128 - val_acc: 0.4388\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1852 - acc: 0.4292 - val_loss: 2.1984 - val_acc: 0.4395\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1594 - acc: 0.4329 - val_loss: 2.2010 - val_acc: 0.4389\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1489 - acc: 0.4346 - val_loss: 2.1929 - val_acc: 0.4383\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1398 - acc: 0.4355 - val_loss: 2.2037 - val_acc: 0.4371\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1394 - acc: 0.4389 - val_loss: 2.2099 - val_acc: 0.4372\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1262 - acc: 0.4395 - val_loss: 2.2128 - val_acc: 0.4360\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1224 - acc: 0.4401 - val_loss: 2.2105 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1148 - acc: 0.4425 - val_loss: 2.2128 - val_acc: 0.4370\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1104 - acc: 0.4428 - val_loss: 2.2019 - val_acc: 0.4384\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1024 - acc: 0.4447 - val_loss: 2.2108 - val_acc: 0.4364\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0988 - acc: 0.4437 - val_loss: 2.2123 - val_acc: 0.4388\n",
      "Epoch 13/100\n",
      "2s - loss: 2.0963 - acc: 0.4458 - val_loss: 2.2511 - val_acc: 0.4373\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1261 - acc: 0.4392 - val_loss: 2.2368 - val_acc: 0.4352\n",
      "Epoch 15/100\n",
      "2s - loss: 2.1029 - acc: 0.4448 - val_loss: 2.2142 - val_acc: 0.4383\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0854 - acc: 0.4486 - val_loss: 2.2174 - val_acc: 0.4371\n",
      "Epoch 17/100\n",
      "2s - loss: 2.0780 - acc: 0.4501 - val_loss: 2.2321 - val_acc: 0.4369\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0772 - acc: 0.4495 - val_loss: 2.2122 - val_acc: 0.4392\n",
      "Epoch 19/100\n",
      "2s - loss: 2.0759 - acc: 0.4499 - val_loss: 2.2204 - val_acc: 0.4380\n",
      "Epoch 20/100\n",
      "2s - loss: 2.0681 - acc: 0.4520 - val_loss: 2.2277 - val_acc: 0.4358\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0703 - acc: 0.4517 - val_loss: 2.2064 - val_acc: 0.4403\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0674 - acc: 0.4514 - val_loss: 2.2176 - val_acc: 0.4352\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0627 - acc: 0.4541 - val_loss: 2.2146 - val_acc: 0.4376\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0560 - acc: 0.4538 - val_loss: 2.2088 - val_acc: 0.4388\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0563 - acc: 0.4537 - val_loss: 2.2440 - val_acc: 0.4360\n",
      "Epoch 26/100\n",
      "2s - loss: 2.0533 - acc: 0.4547 - val_loss: 2.2232 - val_acc: 0.4410\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0735 - acc: 0.4506 - val_loss: 2.2204 - val_acc: 0.4382\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0493 - acc: 0.4561 - val_loss: 2.2103 - val_acc: 0.4409\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0468 - acc: 0.4564 - val_loss: 2.2032 - val_acc: 0.4436\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0446 - acc: 0.4545 - val_loss: 2.2215 - val_acc: 0.4369\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0433 - acc: 0.4563 - val_loss: 2.2075 - val_acc: 0.4412\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0372 - acc: 0.4587 - val_loss: 2.2038 - val_acc: 0.4396\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0377 - acc: 0.4569 - val_loss: 2.2166 - val_acc: 0.4421\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0508 - acc: 0.4552 - val_loss: 2.1989 - val_acc: 0.4413\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0354 - acc: 0.4580 - val_loss: 2.2060 - val_acc: 0.4411\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0316 - acc: 0.4586 - val_loss: 2.2129 - val_acc: 0.4393\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0259 - acc: 0.4616 - val_loss: 2.1994 - val_acc: 0.4402\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0297 - acc: 0.4598 - val_loss: 2.2173 - val_acc: 0.4409\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0333 - acc: 0.4585 - val_loss: 2.1968 - val_acc: 0.4413\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0391 - acc: 0.4582 - val_loss: 2.2450 - val_acc: 0.4387\n",
      "Epoch 41/100\n",
      "2s - loss: 2.0442 - acc: 0.4575 - val_loss: 2.2068 - val_acc: 0.4406\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0237 - acc: 0.4608 - val_loss: 2.2370 - val_acc: 0.4371\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0226 - acc: 0.4616 - val_loss: 2.1967 - val_acc: 0.4444\n",
      "Epoch 44/100\n",
      "2s - loss: 2.0124 - acc: 0.4647 - val_loss: 2.2021 - val_acc: 0.4422\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0096 - acc: 0.4638 - val_loss: 2.1861 - val_acc: 0.4419\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0106 - acc: 0.4638 - val_loss: 2.2199 - val_acc: 0.4395\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0083 - acc: 0.4642 - val_loss: 2.2302 - val_acc: 0.4374\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0214 - acc: 0.4627 - val_loss: 2.2119 - val_acc: 0.4396\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0136 - acc: 0.4635 - val_loss: 2.1972 - val_acc: 0.4421\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0163 - acc: 0.4614 - val_loss: 2.2105 - val_acc: 0.4418\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0013 - acc: 0.4671 - val_loss: 2.1893 - val_acc: 0.4412\n",
      "Epoch 52/100\n",
      "2s - loss: 2.0018 - acc: 0.4660 - val_loss: 2.2295 - val_acc: 0.4419\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0161 - acc: 0.4642 - val_loss: 2.1822 - val_acc: 0.4437\n",
      "Epoch 54/100\n",
      "2s - loss: 1.9937 - acc: 0.4670 - val_loss: 2.2016 - val_acc: 0.4423\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9898 - acc: 0.4681 - val_loss: 2.2045 - val_acc: 0.4384\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0035 - acc: 0.4658 - val_loss: 2.2091 - val_acc: 0.4408\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0020 - acc: 0.4668 - val_loss: 2.1836 - val_acc: 0.4432\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9892 - acc: 0.4700 - val_loss: 2.2078 - val_acc: 0.4438\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9947 - acc: 0.4659 - val_loss: 2.2049 - val_acc: 0.4407\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9896 - acc: 0.4707 - val_loss: 2.2068 - val_acc: 0.4425\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9890 - acc: 0.4680 - val_loss: 2.2075 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9903 - acc: 0.4687 - val_loss: 2.2086 - val_acc: 0.4398\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9850 - acc: 0.4695 - val_loss: 2.1802 - val_acc: 0.4462\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9761 - acc: 0.4724 - val_loss: 2.1848 - val_acc: 0.4451\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9756 - acc: 0.4722 - val_loss: 2.1936 - val_acc: 0.4434\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9808 - acc: 0.4724 - val_loss: 2.1919 - val_acc: 0.4446\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9752 - acc: 0.4720 - val_loss: 2.1885 - val_acc: 0.4440\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9786 - acc: 0.4713 - val_loss: 2.1880 - val_acc: 0.4444\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9686 - acc: 0.4749 - val_loss: 2.1861 - val_acc: 0.4455\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9667 - acc: 0.4728 - val_loss: 2.1943 - val_acc: 0.4457\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9716 - acc: 0.4729 - val_loss: 2.2411 - val_acc: 0.4380\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9853 - acc: 0.4707 - val_loss: 2.2022 - val_acc: 0.4424\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9756 - acc: 0.4717 - val_loss: 2.2428 - val_acc: 0.4375\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9839 - acc: 0.4688 - val_loss: 2.1978 - val_acc: 0.4451\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9751 - acc: 0.4725 - val_loss: 2.1904 - val_acc: 0.4449\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9689 - acc: 0.4716 - val_loss: 2.2368 - val_acc: 0.4420\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0070 - acc: 0.4669 - val_loss: 2.2021 - val_acc: 0.4409\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9703 - acc: 0.4745 - val_loss: 2.2138 - val_acc: 0.4425\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9622 - acc: 0.4773 - val_loss: 2.1834 - val_acc: 0.4450\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9665 - acc: 0.4730 - val_loss: 2.2224 - val_acc: 0.4379\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9739 - acc: 0.4716 - val_loss: 2.2006 - val_acc: 0.4420\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9653 - acc: 0.4744 - val_loss: 2.2175 - val_acc: 0.4395\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9540 - acc: 0.4746 - val_loss: 2.1933 - val_acc: 0.4430\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9585 - acc: 0.4766 - val_loss: 2.2293 - val_acc: 0.4392\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9706 - acc: 0.4735 - val_loss: 2.1878 - val_acc: 0.4430\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9506 - acc: 0.4764 - val_loss: 2.2250 - val_acc: 0.4389\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9627 - acc: 0.4760 - val_loss: 2.1948 - val_acc: 0.4432\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9463 - acc: 0.4776 - val_loss: 2.2030 - val_acc: 0.4422\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9478 - acc: 0.4789 - val_loss: 2.1842 - val_acc: 0.4454\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9510 - acc: 0.4773 - val_loss: 2.2419 - val_acc: 0.4403\n",
      "Epoch 91/100\n",
      "2s - loss: 2.0059 - acc: 0.4659 - val_loss: 2.2587 - val_acc: 0.4321\n",
      "Epoch 92/100\n",
      "2s - loss: 1.9900 - acc: 0.4687 - val_loss: 2.2311 - val_acc: 0.4375\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9491 - acc: 0.4783 - val_loss: 2.1986 - val_acc: 0.4422\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9301 - acc: 0.4826 - val_loss: 2.2000 - val_acc: 0.4431\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9350 - acc: 0.4804 - val_loss: 2.1991 - val_acc: 0.4454\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9311 - acc: 0.4819 - val_loss: 2.2029 - val_acc: 0.4437\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9362 - acc: 0.4804 - val_loss: 2.2070 - val_acc: 0.4404\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9419 - acc: 0.4793 - val_loss: 2.2079 - val_acc: 0.4393\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9453 - acc: 0.4767 - val_loss: 2.1900 - val_acc: 0.4438\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9356 - acc: 0.4821 - val_loss: 2.2052 - val_acc: 0.4424\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1807 - acc: 0.4317 - val_loss: 2.1957 - val_acc: 0.4407\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1600 - acc: 0.4338 - val_loss: 2.2046 - val_acc: 0.4440\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1520 - acc: 0.4368 - val_loss: 2.2039 - val_acc: 0.4428\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1402 - acc: 0.4379 - val_loss: 2.2135 - val_acc: 0.4387\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1309 - acc: 0.4383 - val_loss: 2.2079 - val_acc: 0.4403\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1264 - acc: 0.4406 - val_loss: 2.2206 - val_acc: 0.4368\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1195 - acc: 0.4401 - val_loss: 2.2071 - val_acc: 0.4399\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1116 - acc: 0.4437 - val_loss: 2.2211 - val_acc: 0.4381\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1042 - acc: 0.4449 - val_loss: 2.2126 - val_acc: 0.4384\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1016 - acc: 0.4448 - val_loss: 2.2176 - val_acc: 0.4401\n",
      "Epoch 11/100\n",
      "2s - loss: 2.1045 - acc: 0.4447 - val_loss: 2.2128 - val_acc: 0.4369\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0891 - acc: 0.4482 - val_loss: 2.2027 - val_acc: 0.4405\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0862 - acc: 0.4479 - val_loss: 2.2113 - val_acc: 0.4358\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0822 - acc: 0.4507 - val_loss: 2.2084 - val_acc: 0.4399\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0780 - acc: 0.4505 - val_loss: 2.2128 - val_acc: 0.4383\n",
      "Epoch 16/100\n",
      "2s - loss: 2.0734 - acc: 0.4524 - val_loss: 2.2052 - val_acc: 0.4386\n",
      "Epoch 17/100\n",
      "2s - loss: 2.0722 - acc: 0.4537 - val_loss: 2.2270 - val_acc: 0.4369\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0778 - acc: 0.4502 - val_loss: 2.2122 - val_acc: 0.4362\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0645 - acc: 0.4531 - val_loss: 2.2212 - val_acc: 0.4368\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0614 - acc: 0.4534 - val_loss: 2.2107 - val_acc: 0.4372\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0591 - acc: 0.4562 - val_loss: 2.2089 - val_acc: 0.4375\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0584 - acc: 0.4542 - val_loss: 2.2021 - val_acc: 0.4403\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0488 - acc: 0.4582 - val_loss: 2.2189 - val_acc: 0.4369\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0517 - acc: 0.4577 - val_loss: 2.2189 - val_acc: 0.4382\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0516 - acc: 0.4568 - val_loss: 2.2336 - val_acc: 0.4350\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0560 - acc: 0.4561 - val_loss: 2.2101 - val_acc: 0.4376\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0414 - acc: 0.4592 - val_loss: 2.2161 - val_acc: 0.4359\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0390 - acc: 0.4593 - val_loss: 2.2219 - val_acc: 0.4359\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0388 - acc: 0.4580 - val_loss: 2.2276 - val_acc: 0.4314\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0378 - acc: 0.4571 - val_loss: 2.1982 - val_acc: 0.4424\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0374 - acc: 0.4574 - val_loss: 2.2389 - val_acc: 0.4351\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0580 - acc: 0.4571 - val_loss: 2.2137 - val_acc: 0.4417\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0308 - acc: 0.4638 - val_loss: 2.2386 - val_acc: 0.4364\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0432 - acc: 0.4590 - val_loss: 2.2036 - val_acc: 0.4386\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0239 - acc: 0.4606 - val_loss: 2.2084 - val_acc: 0.4390\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0215 - acc: 0.4641 - val_loss: 2.2073 - val_acc: 0.4429\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0175 - acc: 0.4661 - val_loss: 2.1983 - val_acc: 0.4410\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0160 - acc: 0.4643 - val_loss: 2.2047 - val_acc: 0.4403\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0104 - acc: 0.4664 - val_loss: 2.2043 - val_acc: 0.4399\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0135 - acc: 0.4666 - val_loss: 2.2206 - val_acc: 0.4399\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0153 - acc: 0.4649 - val_loss: 2.1988 - val_acc: 0.4414\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0023 - acc: 0.4696 - val_loss: 2.2399 - val_acc: 0.4363\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0197 - acc: 0.4624 - val_loss: 2.2034 - val_acc: 0.4410\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0020 - acc: 0.4684 - val_loss: 2.2574 - val_acc: 0.4369\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0643 - acc: 0.4581 - val_loss: 2.2509 - val_acc: 0.4297\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0522 - acc: 0.4582 - val_loss: 2.2343 - val_acc: 0.4319\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0216 - acc: 0.4646 - val_loss: 2.2133 - val_acc: 0.4400\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9986 - acc: 0.4685 - val_loss: 2.2143 - val_acc: 0.4410\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9939 - acc: 0.4672 - val_loss: 2.1975 - val_acc: 0.4453\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9980 - acc: 0.4686 - val_loss: 2.2175 - val_acc: 0.4373\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9943 - acc: 0.4708 - val_loss: 2.1964 - val_acc: 0.4438\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9903 - acc: 0.4677 - val_loss: 2.2107 - val_acc: 0.4406\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9986 - acc: 0.4678 - val_loss: 2.2180 - val_acc: 0.4434\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0193 - acc: 0.4619 - val_loss: 2.2210 - val_acc: 0.4360\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9964 - acc: 0.4680 - val_loss: 2.1943 - val_acc: 0.4421\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9904 - acc: 0.4704 - val_loss: 2.2294 - val_acc: 0.4379\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9860 - acc: 0.4712 - val_loss: 2.2110 - val_acc: 0.4414\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9938 - acc: 0.4690 - val_loss: 2.2238 - val_acc: 0.4392\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9858 - acc: 0.4715 - val_loss: 2.2416 - val_acc: 0.4358\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9973 - acc: 0.4692 - val_loss: 2.2290 - val_acc: 0.4351\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9767 - acc: 0.4722 - val_loss: 2.2118 - val_acc: 0.4405\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9916 - acc: 0.4682 - val_loss: 2.2158 - val_acc: 0.4396\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9749 - acc: 0.4720 - val_loss: 2.1985 - val_acc: 0.4429\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9729 - acc: 0.4730 - val_loss: 2.2356 - val_acc: 0.4386\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9974 - acc: 0.4682 - val_loss: 2.2236 - val_acc: 0.4383\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9766 - acc: 0.4726 - val_loss: 2.2153 - val_acc: 0.4432\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9768 - acc: 0.4736 - val_loss: 2.2008 - val_acc: 0.4416\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9694 - acc: 0.4739 - val_loss: 2.2008 - val_acc: 0.4435\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9639 - acc: 0.4768 - val_loss: 2.2039 - val_acc: 0.4422\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9691 - acc: 0.4749 - val_loss: 2.2166 - val_acc: 0.4418\n",
      "Epoch 71/100\n",
      "2s - loss: 1.9650 - acc: 0.4769 - val_loss: 2.1967 - val_acc: 0.4440\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9589 - acc: 0.4783 - val_loss: 2.2230 - val_acc: 0.4373\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9603 - acc: 0.4761 - val_loss: 2.2062 - val_acc: 0.4401\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9642 - acc: 0.4757 - val_loss: 2.2024 - val_acc: 0.4423\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9502 - acc: 0.4802 - val_loss: 2.1989 - val_acc: 0.4454\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9520 - acc: 0.4783 - val_loss: 2.2138 - val_acc: 0.4397\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9586 - acc: 0.4770 - val_loss: 2.2216 - val_acc: 0.4412\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9636 - acc: 0.4759 - val_loss: 2.2119 - val_acc: 0.4436\n",
      "Epoch 79/100\n",
      "2s - loss: 1.9583 - acc: 0.4781 - val_loss: 2.2279 - val_acc: 0.4402\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9540 - acc: 0.4778 - val_loss: 2.2018 - val_acc: 0.4467\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9443 - acc: 0.4808 - val_loss: 2.1949 - val_acc: 0.4423\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9443 - acc: 0.4805 - val_loss: 2.2156 - val_acc: 0.4437\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9463 - acc: 0.4798 - val_loss: 2.2145 - val_acc: 0.4414\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9486 - acc: 0.4790 - val_loss: 2.2593 - val_acc: 0.4404\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9995 - acc: 0.4702 - val_loss: 2.2427 - val_acc: 0.4338\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9852 - acc: 0.4724 - val_loss: 2.2214 - val_acc: 0.4393\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9548 - acc: 0.4793 - val_loss: 2.2042 - val_acc: 0.4459\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9565 - acc: 0.4773 - val_loss: 2.2158 - val_acc: 0.4431\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9482 - acc: 0.4828 - val_loss: 2.1921 - val_acc: 0.4470\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9395 - acc: 0.4819 - val_loss: 2.2087 - val_acc: 0.4406\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9433 - acc: 0.4819 - val_loss: 2.2186 - val_acc: 0.4421\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9406 - acc: 0.4813 - val_loss: 2.2153 - val_acc: 0.4430\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9527 - acc: 0.4796 - val_loss: 2.1967 - val_acc: 0.4423\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9282 - acc: 0.4842 - val_loss: 2.2062 - val_acc: 0.4433\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9257 - acc: 0.4864 - val_loss: 2.2031 - val_acc: 0.4418\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9312 - acc: 0.4841 - val_loss: 2.2266 - val_acc: 0.4391\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9443 - acc: 0.4820 - val_loss: 2.2051 - val_acc: 0.4439\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9390 - acc: 0.4817 - val_loss: 2.2277 - val_acc: 0.4422\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9302 - acc: 0.4855 - val_loss: 2.2026 - val_acc: 0.4432\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9268 - acc: 0.4845 - val_loss: 2.2231 - val_acc: 0.4413\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1803 - acc: 0.4299 - val_loss: 2.1959 - val_acc: 0.4391\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1544 - acc: 0.4335 - val_loss: 2.1990 - val_acc: 0.4378\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1441 - acc: 0.4368 - val_loss: 2.2045 - val_acc: 0.4370\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1348 - acc: 0.4376 - val_loss: 2.2191 - val_acc: 0.4353\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1282 - acc: 0.4390 - val_loss: 2.2076 - val_acc: 0.4364\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1209 - acc: 0.4395 - val_loss: 2.2083 - val_acc: 0.4355\n",
      "Epoch 7/100\n",
      "2s - loss: 2.1126 - acc: 0.4429 - val_loss: 2.2076 - val_acc: 0.4377\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1058 - acc: 0.4429 - val_loss: 2.2063 - val_acc: 0.4358\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1013 - acc: 0.4442 - val_loss: 2.2210 - val_acc: 0.4334\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0962 - acc: 0.4441 - val_loss: 2.2214 - val_acc: 0.4335\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0946 - acc: 0.4456 - val_loss: 2.2237 - val_acc: 0.4358\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0901 - acc: 0.4463 - val_loss: 2.2111 - val_acc: 0.4333\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0802 - acc: 0.4479 - val_loss: 2.2131 - val_acc: 0.4355\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0783 - acc: 0.4503 - val_loss: 2.2157 - val_acc: 0.4351\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0731 - acc: 0.4495 - val_loss: 2.2282 - val_acc: 0.4324\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0707 - acc: 0.4504 - val_loss: 2.2248 - val_acc: 0.4346\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0696 - acc: 0.4498 - val_loss: 2.2344 - val_acc: 0.4341\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0642 - acc: 0.4526 - val_loss: 2.2230 - val_acc: 0.4352\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0669 - acc: 0.4517 - val_loss: 2.2331 - val_acc: 0.4338\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0575 - acc: 0.4535 - val_loss: 2.2096 - val_acc: 0.4362\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0557 - acc: 0.4545 - val_loss: 2.2189 - val_acc: 0.4347\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0523 - acc: 0.4561 - val_loss: 2.2085 - val_acc: 0.4345\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0522 - acc: 0.4545 - val_loss: 2.2139 - val_acc: 0.4351\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0468 - acc: 0.4551 - val_loss: 2.2230 - val_acc: 0.4343\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0448 - acc: 0.4557 - val_loss: 2.2194 - val_acc: 0.4332\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0397 - acc: 0.4568 - val_loss: 2.2261 - val_acc: 0.4328\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0394 - acc: 0.4575 - val_loss: 2.2214 - val_acc: 0.4332\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0382 - acc: 0.4585 - val_loss: 2.2093 - val_acc: 0.4372\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0272 - acc: 0.4608 - val_loss: 2.2263 - val_acc: 0.4340\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0285 - acc: 0.4598 - val_loss: 2.2133 - val_acc: 0.4362\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0258 - acc: 0.4605 - val_loss: 2.2306 - val_acc: 0.4340\n",
      "Epoch 32/100\n",
      "2s - loss: 2.0316 - acc: 0.4599 - val_loss: 2.2229 - val_acc: 0.4337\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0264 - acc: 0.4626 - val_loss: 2.2105 - val_acc: 0.4342\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0241 - acc: 0.4612 - val_loss: 2.2070 - val_acc: 0.4340\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0173 - acc: 0.4620 - val_loss: 2.2013 - val_acc: 0.4382\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0139 - acc: 0.4621 - val_loss: 2.2137 - val_acc: 0.4345\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0164 - acc: 0.4603 - val_loss: 2.2073 - val_acc: 0.4340\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0120 - acc: 0.4656 - val_loss: 2.2177 - val_acc: 0.4346\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0111 - acc: 0.4642 - val_loss: 2.2132 - val_acc: 0.4362\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0077 - acc: 0.4666 - val_loss: 2.2057 - val_acc: 0.4360\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0059 - acc: 0.4630 - val_loss: 2.2074 - val_acc: 0.4360\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0020 - acc: 0.4644 - val_loss: 2.2095 - val_acc: 0.4349\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9995 - acc: 0.4654 - val_loss: 2.1988 - val_acc: 0.4342\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9952 - acc: 0.4648 - val_loss: 2.2172 - val_acc: 0.4338\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9999 - acc: 0.4654 - val_loss: 2.2011 - val_acc: 0.4349\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9900 - acc: 0.4670 - val_loss: 2.2133 - val_acc: 0.4378\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9888 - acc: 0.4689 - val_loss: 2.2089 - val_acc: 0.4358\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9864 - acc: 0.4685 - val_loss: 2.1958 - val_acc: 0.4386\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9889 - acc: 0.4683 - val_loss: 2.2204 - val_acc: 0.4353\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9953 - acc: 0.4675 - val_loss: 2.2008 - val_acc: 0.4371\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9958 - acc: 0.4679 - val_loss: 2.2116 - val_acc: 0.4333\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9865 - acc: 0.4697 - val_loss: 2.1978 - val_acc: 0.4357\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9838 - acc: 0.4702 - val_loss: 2.2160 - val_acc: 0.4356\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9875 - acc: 0.4708 - val_loss: 2.2071 - val_acc: 0.4339\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9905 - acc: 0.4670 - val_loss: 2.2166 - val_acc: 0.4358\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9778 - acc: 0.4713 - val_loss: 2.2090 - val_acc: 0.4316\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9859 - acc: 0.4691 - val_loss: 2.2221 - val_acc: 0.4328\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9841 - acc: 0.4687 - val_loss: 2.1954 - val_acc: 0.4355\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9732 - acc: 0.4722 - val_loss: 2.2046 - val_acc: 0.4369\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9675 - acc: 0.4732 - val_loss: 2.1988 - val_acc: 0.4369\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9670 - acc: 0.4742 - val_loss: 2.2076 - val_acc: 0.4338\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9668 - acc: 0.4723 - val_loss: 2.2007 - val_acc: 0.4353\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9611 - acc: 0.4737 - val_loss: 2.1930 - val_acc: 0.4364\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9604 - acc: 0.4766 - val_loss: 2.2063 - val_acc: 0.4332\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9620 - acc: 0.4752 - val_loss: 2.1974 - val_acc: 0.4375\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9613 - acc: 0.4730 - val_loss: 2.2079 - val_acc: 0.4332\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9641 - acc: 0.4747 - val_loss: 2.1982 - val_acc: 0.4364\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9655 - acc: 0.4728 - val_loss: 2.2175 - val_acc: 0.4326\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9598 - acc: 0.4752 - val_loss: 2.1931 - val_acc: 0.4371\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9573 - acc: 0.4756 - val_loss: 2.2129 - val_acc: 0.4372\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9628 - acc: 0.4752 - val_loss: 2.1993 - val_acc: 0.4371\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9558 - acc: 0.4746 - val_loss: 2.2059 - val_acc: 0.4359\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9507 - acc: 0.4773 - val_loss: 2.2081 - val_acc: 0.4345\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9554 - acc: 0.4781 - val_loss: 2.2176 - val_acc: 0.4343\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9501 - acc: 0.4780 - val_loss: 2.2111 - val_acc: 0.4337\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9477 - acc: 0.4784 - val_loss: 2.2053 - val_acc: 0.4364\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9420 - acc: 0.4781 - val_loss: 2.2007 - val_acc: 0.4364\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9431 - acc: 0.4777 - val_loss: 2.1938 - val_acc: 0.4375\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9453 - acc: 0.4793 - val_loss: 2.2073 - val_acc: 0.4341\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9425 - acc: 0.4784 - val_loss: 2.1973 - val_acc: 0.4365\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9406 - acc: 0.4813 - val_loss: 2.2180 - val_acc: 0.4326\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9376 - acc: 0.4779 - val_loss: 2.1913 - val_acc: 0.4360\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9327 - acc: 0.4821 - val_loss: 2.1995 - val_acc: 0.4368\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9333 - acc: 0.4817 - val_loss: 2.1947 - val_acc: 0.4356\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9372 - acc: 0.4801 - val_loss: 2.2098 - val_acc: 0.4356\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9357 - acc: 0.4800 - val_loss: 2.2074 - val_acc: 0.4333\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9351 - acc: 0.4807 - val_loss: 2.2202 - val_acc: 0.4321\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9370 - acc: 0.4799 - val_loss: 2.2148 - val_acc: 0.4328\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9412 - acc: 0.4785 - val_loss: 2.2226 - val_acc: 0.4330\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9357 - acc: 0.4814 - val_loss: 2.2123 - val_acc: 0.4333\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9349 - acc: 0.4825 - val_loss: 2.2254 - val_acc: 0.4309\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9372 - acc: 0.4799 - val_loss: 2.2018 - val_acc: 0.4340\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9258 - acc: 0.4835 - val_loss: 2.2180 - val_acc: 0.4341\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9225 - acc: 0.4835 - val_loss: 2.2079 - val_acc: 0.4330\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9257 - acc: 0.4840 - val_loss: 2.2287 - val_acc: 0.4324\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9339 - acc: 0.4827 - val_loss: 2.2278 - val_acc: 0.4305\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9265 - acc: 0.4810 - val_loss: 2.2374 - val_acc: 0.4329\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9526 - acc: 0.4778 - val_loss: 2.2730 - val_acc: 0.4217\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9342 - acc: 0.4814 - val_loss: 2.2157 - val_acc: 0.4335\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9178 - acc: 0.4868 - val_loss: 2.2191 - val_acc: 0.4319\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2113 - acc: 0.4257 - val_loss: 2.2672 - val_acc: 0.4259\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1763 - acc: 0.4310 - val_loss: 2.2759 - val_acc: 0.4262\n",
      "Epoch 3/100\n",
      "1s - loss: 2.2031 - acc: 0.4271 - val_loss: 2.3360 - val_acc: 0.4144\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1909 - acc: 0.4297 - val_loss: 2.3038 - val_acc: 0.4199\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1619 - acc: 0.4333 - val_loss: 2.2800 - val_acc: 0.4252\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1384 - acc: 0.4386 - val_loss: 2.2811 - val_acc: 0.4238\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1321 - acc: 0.4396 - val_loss: 2.2732 - val_acc: 0.4232\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1282 - acc: 0.4401 - val_loss: 2.2888 - val_acc: 0.4230\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1257 - acc: 0.4396 - val_loss: 2.2784 - val_acc: 0.4235\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1175 - acc: 0.4426 - val_loss: 2.2779 - val_acc: 0.4226\n",
      "Epoch 11/100\n",
      "2s - loss: 2.1074 - acc: 0.4459 - val_loss: 2.2879 - val_acc: 0.4212\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1029 - acc: 0.4466 - val_loss: 2.2794 - val_acc: 0.4236\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1030 - acc: 0.4470 - val_loss: 2.2886 - val_acc: 0.4220\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0980 - acc: 0.4475 - val_loss: 2.2911 - val_acc: 0.4198\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0954 - acc: 0.4482 - val_loss: 2.2878 - val_acc: 0.4205\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0888 - acc: 0.4477 - val_loss: 2.2840 - val_acc: 0.4236\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0906 - acc: 0.4487 - val_loss: 2.3033 - val_acc: 0.4195\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0808 - acc: 0.4508 - val_loss: 2.2876 - val_acc: 0.4205\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0845 - acc: 0.4509 - val_loss: 2.2770 - val_acc: 0.4233\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0756 - acc: 0.4518 - val_loss: 2.2929 - val_acc: 0.4218\n",
      "Epoch 21/100\n",
      "2s - loss: 2.0804 - acc: 0.4522 - val_loss: 2.3058 - val_acc: 0.4211\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0848 - acc: 0.4516 - val_loss: 2.2878 - val_acc: 0.4213\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0684 - acc: 0.4539 - val_loss: 2.2816 - val_acc: 0.4219\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0661 - acc: 0.4544 - val_loss: 2.2749 - val_acc: 0.4219\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0612 - acc: 0.4543 - val_loss: 2.2771 - val_acc: 0.4245\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0560 - acc: 0.4562 - val_loss: 2.2794 - val_acc: 0.4239\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0569 - acc: 0.4571 - val_loss: 2.2813 - val_acc: 0.4234\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0600 - acc: 0.4545 - val_loss: 2.2828 - val_acc: 0.4229\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0488 - acc: 0.4582 - val_loss: 2.2754 - val_acc: 0.4224\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0470 - acc: 0.4597 - val_loss: 2.2823 - val_acc: 0.4221\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0567 - acc: 0.4566 - val_loss: 2.2713 - val_acc: 0.4255\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0416 - acc: 0.4583 - val_loss: 2.2683 - val_acc: 0.4243\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0410 - acc: 0.4600 - val_loss: 2.2831 - val_acc: 0.4237\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0411 - acc: 0.4586 - val_loss: 2.2761 - val_acc: 0.4241\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0436 - acc: 0.4584 - val_loss: 2.3132 - val_acc: 0.4202\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0498 - acc: 0.4571 - val_loss: 2.3245 - val_acc: 0.4181\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0615 - acc: 0.4546 - val_loss: 2.2998 - val_acc: 0.4212\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0509 - acc: 0.4584 - val_loss: 2.2851 - val_acc: 0.4243\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0373 - acc: 0.4612 - val_loss: 2.2834 - val_acc: 0.4232\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0307 - acc: 0.4621 - val_loss: 2.2659 - val_acc: 0.4240\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0288 - acc: 0.4637 - val_loss: 2.2959 - val_acc: 0.4224\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0363 - acc: 0.4615 - val_loss: 2.2621 - val_acc: 0.4249\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0277 - acc: 0.4637 - val_loss: 2.2845 - val_acc: 0.4216\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0245 - acc: 0.4631 - val_loss: 2.2841 - val_acc: 0.4235\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0280 - acc: 0.4623 - val_loss: 2.2680 - val_acc: 0.4266\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0191 - acc: 0.4638 - val_loss: 2.2615 - val_acc: 0.4245\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0125 - acc: 0.4669 - val_loss: 2.2752 - val_acc: 0.4255\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0140 - acc: 0.4657 - val_loss: 2.2844 - val_acc: 0.4252\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0149 - acc: 0.4639 - val_loss: 2.2948 - val_acc: 0.4235\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0197 - acc: 0.4657 - val_loss: 2.2723 - val_acc: 0.4255\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0081 - acc: 0.4671 - val_loss: 2.2919 - val_acc: 0.4255\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0147 - acc: 0.4653 - val_loss: 2.2610 - val_acc: 0.4277\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0004 - acc: 0.4702 - val_loss: 2.2667 - val_acc: 0.4286\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9959 - acc: 0.4706 - val_loss: 2.2547 - val_acc: 0.4278\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9938 - acc: 0.4695 - val_loss: 2.2693 - val_acc: 0.4272\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9940 - acc: 0.4694 - val_loss: 2.2738 - val_acc: 0.4247\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0023 - acc: 0.4682 - val_loss: 2.2672 - val_acc: 0.4261\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0001 - acc: 0.4674 - val_loss: 2.2745 - val_acc: 0.4253\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0071 - acc: 0.4673 - val_loss: 2.2674 - val_acc: 0.4269\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9883 - acc: 0.4713 - val_loss: 2.2632 - val_acc: 0.4289\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9849 - acc: 0.4718 - val_loss: 2.2813 - val_acc: 0.4265\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0000 - acc: 0.4672 - val_loss: 2.2783 - val_acc: 0.4249\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0032 - acc: 0.4683 - val_loss: 2.2887 - val_acc: 0.4260\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9938 - acc: 0.4706 - val_loss: 2.2829 - val_acc: 0.4266\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9916 - acc: 0.4699 - val_loss: 2.2710 - val_acc: 0.4275\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9931 - acc: 0.4727 - val_loss: 2.2625 - val_acc: 0.4270\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9772 - acc: 0.4743 - val_loss: 2.2613 - val_acc: 0.4261\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9754 - acc: 0.4750 - val_loss: 2.2637 - val_acc: 0.4270\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9773 - acc: 0.4743 - val_loss: 2.2787 - val_acc: 0.4242\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9816 - acc: 0.4730 - val_loss: 2.2583 - val_acc: 0.4267\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9857 - acc: 0.4711 - val_loss: 2.2831 - val_acc: 0.4252\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9873 - acc: 0.4709 - val_loss: 2.2598 - val_acc: 0.4277\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9862 - acc: 0.4710 - val_loss: 2.2736 - val_acc: 0.4244\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9817 - acc: 0.4737 - val_loss: 2.2655 - val_acc: 0.4263\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9882 - acc: 0.4703 - val_loss: 2.2674 - val_acc: 0.4258\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9848 - acc: 0.4721 - val_loss: 2.2708 - val_acc: 0.4275\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9901 - acc: 0.4690 - val_loss: 2.2998 - val_acc: 0.4211\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9770 - acc: 0.4751 - val_loss: 2.2716 - val_acc: 0.4257\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9825 - acc: 0.4718 - val_loss: 2.2701 - val_acc: 0.4229\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9633 - acc: 0.4780 - val_loss: 2.2864 - val_acc: 0.4236\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9753 - acc: 0.4740 - val_loss: 2.2726 - val_acc: 0.4244\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9694 - acc: 0.4762 - val_loss: 2.2719 - val_acc: 0.4256\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9655 - acc: 0.4763 - val_loss: 2.2994 - val_acc: 0.4228\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9769 - acc: 0.4723 - val_loss: 2.2730 - val_acc: 0.4261\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9687 - acc: 0.4741 - val_loss: 2.3000 - val_acc: 0.4245\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9620 - acc: 0.4773 - val_loss: 2.2688 - val_acc: 0.4273\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9711 - acc: 0.4755 - val_loss: 2.2754 - val_acc: 0.4246\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9516 - acc: 0.4790 - val_loss: 2.2589 - val_acc: 0.4283\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9483 - acc: 0.4794 - val_loss: 2.2739 - val_acc: 0.4251\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9597 - acc: 0.4787 - val_loss: 2.2672 - val_acc: 0.4273\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9523 - acc: 0.4790 - val_loss: 2.2711 - val_acc: 0.4272\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9587 - acc: 0.4775 - val_loss: 2.2853 - val_acc: 0.4236\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9555 - acc: 0.4771 - val_loss: 2.2719 - val_acc: 0.4262\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9542 - acc: 0.4786 - val_loss: 2.2705 - val_acc: 0.4272\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9475 - acc: 0.4814 - val_loss: 2.2650 - val_acc: 0.4270\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9347 - acc: 0.4829 - val_loss: 2.2615 - val_acc: 0.4295\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9343 - acc: 0.4838 - val_loss: 2.2731 - val_acc: 0.4264\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9384 - acc: 0.4846 - val_loss: 2.2662 - val_acc: 0.4280\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9382 - acc: 0.4831 - val_loss: 2.2909 - val_acc: 0.4253\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9667 - acc: 0.4779 - val_loss: 2.2749 - val_acc: 0.4270\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1791 - acc: 0.4311 - val_loss: 2.2262 - val_acc: 0.4313\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1557 - acc: 0.4361 - val_loss: 2.2199 - val_acc: 0.4342\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1446 - acc: 0.4388 - val_loss: 2.2195 - val_acc: 0.4337\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1322 - acc: 0.4409 - val_loss: 2.2209 - val_acc: 0.4320\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1287 - acc: 0.4408 - val_loss: 2.2264 - val_acc: 0.4305\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1200 - acc: 0.4414 - val_loss: 2.2269 - val_acc: 0.4302\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1113 - acc: 0.4449 - val_loss: 2.2327 - val_acc: 0.4315\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1090 - acc: 0.4432 - val_loss: 2.2298 - val_acc: 0.4320\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1049 - acc: 0.4455 - val_loss: 2.2414 - val_acc: 0.4322\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0967 - acc: 0.4471 - val_loss: 2.2372 - val_acc: 0.4302\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0927 - acc: 0.4479 - val_loss: 2.2409 - val_acc: 0.4300\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0887 - acc: 0.4481 - val_loss: 2.2450 - val_acc: 0.4290\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0879 - acc: 0.4480 - val_loss: 2.2434 - val_acc: 0.4297\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0795 - acc: 0.4513 - val_loss: 2.2488 - val_acc: 0.4288\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0748 - acc: 0.4492 - val_loss: 2.2520 - val_acc: 0.4277\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0700 - acc: 0.4512 - val_loss: 2.2572 - val_acc: 0.4267\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0689 - acc: 0.4528 - val_loss: 2.2503 - val_acc: 0.4280\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0677 - acc: 0.4548 - val_loss: 2.2426 - val_acc: 0.4299\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0630 - acc: 0.4537 - val_loss: 2.2476 - val_acc: 0.4259\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0571 - acc: 0.4554 - val_loss: 2.2466 - val_acc: 0.4283\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0540 - acc: 0.4572 - val_loss: 2.2470 - val_acc: 0.4260\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0522 - acc: 0.4567 - val_loss: 2.2486 - val_acc: 0.4278\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0485 - acc: 0.4570 - val_loss: 2.2645 - val_acc: 0.4237\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0523 - acc: 0.4564 - val_loss: 2.2457 - val_acc: 0.4280\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0472 - acc: 0.4568 - val_loss: 2.2462 - val_acc: 0.4282\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0395 - acc: 0.4596 - val_loss: 2.2523 - val_acc: 0.4301\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0402 - acc: 0.4583 - val_loss: 2.2509 - val_acc: 0.4283\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0409 - acc: 0.4586 - val_loss: 2.2323 - val_acc: 0.4292\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0328 - acc: 0.4613 - val_loss: 2.2379 - val_acc: 0.4318\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0304 - acc: 0.4624 - val_loss: 2.2462 - val_acc: 0.4313\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0270 - acc: 0.4625 - val_loss: 2.2512 - val_acc: 0.4319\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0251 - acc: 0.4640 - val_loss: 2.2431 - val_acc: 0.4303\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0218 - acc: 0.4635 - val_loss: 2.2520 - val_acc: 0.4299\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0196 - acc: 0.4633 - val_loss: 2.2392 - val_acc: 0.4272\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0179 - acc: 0.4638 - val_loss: 2.2345 - val_acc: 0.4310\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0168 - acc: 0.4640 - val_loss: 2.2447 - val_acc: 0.4297\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0176 - acc: 0.4655 - val_loss: 2.2366 - val_acc: 0.4335\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0192 - acc: 0.4643 - val_loss: 2.2292 - val_acc: 0.4295\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0083 - acc: 0.4668 - val_loss: 2.2541 - val_acc: 0.4303\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0135 - acc: 0.4652 - val_loss: 2.2266 - val_acc: 0.4323\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0039 - acc: 0.4686 - val_loss: 2.2238 - val_acc: 0.4312\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0083 - acc: 0.4651 - val_loss: 2.2373 - val_acc: 0.4295\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0036 - acc: 0.4669 - val_loss: 2.2221 - val_acc: 0.4339\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9992 - acc: 0.4668 - val_loss: 2.2361 - val_acc: 0.4320\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0014 - acc: 0.4695 - val_loss: 2.2253 - val_acc: 0.4323\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9983 - acc: 0.4694 - val_loss: 2.2370 - val_acc: 0.4324\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9936 - acc: 0.4699 - val_loss: 2.2386 - val_acc: 0.4305\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0129 - acc: 0.4662 - val_loss: 2.2393 - val_acc: 0.4314\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9880 - acc: 0.4726 - val_loss: 2.2254 - val_acc: 0.4328\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9848 - acc: 0.4711 - val_loss: 2.2250 - val_acc: 0.4332\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9828 - acc: 0.4728 - val_loss: 2.2282 - val_acc: 0.4327\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9870 - acc: 0.4720 - val_loss: 2.2372 - val_acc: 0.4302\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9813 - acc: 0.4707 - val_loss: 2.2302 - val_acc: 0.4340\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9785 - acc: 0.4755 - val_loss: 2.2209 - val_acc: 0.4343\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9798 - acc: 0.4733 - val_loss: 2.2256 - val_acc: 0.4348\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9814 - acc: 0.4712 - val_loss: 2.2255 - val_acc: 0.4343\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9823 - acc: 0.4713 - val_loss: 2.2601 - val_acc: 0.4291\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9930 - acc: 0.4700 - val_loss: 2.2305 - val_acc: 0.4336\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9865 - acc: 0.4712 - val_loss: 2.2383 - val_acc: 0.4310\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9757 - acc: 0.4731 - val_loss: 2.2248 - val_acc: 0.4327\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9707 - acc: 0.4753 - val_loss: 2.2366 - val_acc: 0.4349\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9650 - acc: 0.4755 - val_loss: 2.2187 - val_acc: 0.4349\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9640 - acc: 0.4761 - val_loss: 2.2294 - val_acc: 0.4336\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9651 - acc: 0.4759 - val_loss: 2.2195 - val_acc: 0.4329\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9624 - acc: 0.4749 - val_loss: 2.2333 - val_acc: 0.4345\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9663 - acc: 0.4740 - val_loss: 2.2343 - val_acc: 0.4340\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9737 - acc: 0.4743 - val_loss: 2.2259 - val_acc: 0.4342\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9658 - acc: 0.4754 - val_loss: 2.2193 - val_acc: 0.4341\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9628 - acc: 0.4783 - val_loss: 2.2209 - val_acc: 0.4349\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9645 - acc: 0.4760 - val_loss: 2.2228 - val_acc: 0.4339\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9584 - acc: 0.4759 - val_loss: 2.2307 - val_acc: 0.4317\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9541 - acc: 0.4782 - val_loss: 2.2088 - val_acc: 0.4354\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9505 - acc: 0.4809 - val_loss: 2.2149 - val_acc: 0.4342\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9517 - acc: 0.4784 - val_loss: 2.2292 - val_acc: 0.4336\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9574 - acc: 0.4779 - val_loss: 2.2211 - val_acc: 0.4358\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9509 - acc: 0.4780 - val_loss: 2.2130 - val_acc: 0.4347\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9417 - acc: 0.4833 - val_loss: 2.2087 - val_acc: 0.4357\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9431 - acc: 0.4806 - val_loss: 2.2162 - val_acc: 0.4349\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9404 - acc: 0.4806 - val_loss: 2.2133 - val_acc: 0.4356\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9368 - acc: 0.4819 - val_loss: 2.2223 - val_acc: 0.4350\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9504 - acc: 0.4787 - val_loss: 2.2361 - val_acc: 0.4321\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9461 - acc: 0.4801 - val_loss: 2.2205 - val_acc: 0.4355\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9397 - acc: 0.4822 - val_loss: 2.2344 - val_acc: 0.4347\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9351 - acc: 0.4822 - val_loss: 2.2097 - val_acc: 0.4365\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9296 - acc: 0.4850 - val_loss: 2.2672 - val_acc: 0.4293\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9344 - acc: 0.4850 - val_loss: 2.2105 - val_acc: 0.4364\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9320 - acc: 0.4826 - val_loss: 2.2195 - val_acc: 0.4348\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9286 - acc: 0.4831 - val_loss: 2.2227 - val_acc: 0.4374\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9382 - acc: 0.4819 - val_loss: 2.2375 - val_acc: 0.4319\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9544 - acc: 0.4779 - val_loss: 2.2307 - val_acc: 0.4358\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9468 - acc: 0.4797 - val_loss: 2.2296 - val_acc: 0.4343\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9313 - acc: 0.4845 - val_loss: 2.2453 - val_acc: 0.4336\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9423 - acc: 0.4817 - val_loss: 2.2457 - val_acc: 0.4337\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9414 - acc: 0.4811 - val_loss: 2.2632 - val_acc: 0.4285\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9301 - acc: 0.4850 - val_loss: 2.2239 - val_acc: 0.4357\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9208 - acc: 0.4856 - val_loss: 2.2558 - val_acc: 0.4314\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9263 - acc: 0.4850 - val_loss: 2.2290 - val_acc: 0.4355\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9269 - acc: 0.4838 - val_loss: 2.2283 - val_acc: 0.4332\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9175 - acc: 0.4876 - val_loss: 2.2212 - val_acc: 0.4353\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9147 - acc: 0.4891 - val_loss: 2.2313 - val_acc: 0.4332\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2063 - acc: 0.4263 - val_loss: 2.1467 - val_acc: 0.4512\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1849 - acc: 0.4290 - val_loss: 2.1665 - val_acc: 0.4490\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1780 - acc: 0.4319 - val_loss: 2.1616 - val_acc: 0.4483\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1602 - acc: 0.4332 - val_loss: 2.1557 - val_acc: 0.4505\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1655 - acc: 0.4338 - val_loss: 2.1555 - val_acc: 0.4488\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1469 - acc: 0.4389 - val_loss: 2.1591 - val_acc: 0.4505\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1401 - acc: 0.4389 - val_loss: 2.1585 - val_acc: 0.4492\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1441 - acc: 0.4400 - val_loss: 2.1654 - val_acc: 0.4494\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1299 - acc: 0.4421 - val_loss: 2.1607 - val_acc: 0.4490\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1257 - acc: 0.4420 - val_loss: 2.1743 - val_acc: 0.4475\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1202 - acc: 0.4441 - val_loss: 2.1609 - val_acc: 0.4496\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1139 - acc: 0.4454 - val_loss: 2.1581 - val_acc: 0.4497\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1089 - acc: 0.4450 - val_loss: 2.1579 - val_acc: 0.4489\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1051 - acc: 0.4456 - val_loss: 2.1684 - val_acc: 0.4473\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1102 - acc: 0.4468 - val_loss: 2.1570 - val_acc: 0.4499\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0982 - acc: 0.4467 - val_loss: 2.1638 - val_acc: 0.4474\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0973 - acc: 0.4485 - val_loss: 2.1809 - val_acc: 0.4455\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0948 - acc: 0.4482 - val_loss: 2.1662 - val_acc: 0.4479\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0944 - acc: 0.4501 - val_loss: 2.1594 - val_acc: 0.4515\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0828 - acc: 0.4491 - val_loss: 2.1624 - val_acc: 0.4477\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0871 - acc: 0.4498 - val_loss: 2.1588 - val_acc: 0.4520\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0787 - acc: 0.4508 - val_loss: 2.1591 - val_acc: 0.4494\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0830 - acc: 0.4528 - val_loss: 2.1583 - val_acc: 0.4505\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0813 - acc: 0.4497 - val_loss: 2.1791 - val_acc: 0.4455\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0721 - acc: 0.4538 - val_loss: 2.1555 - val_acc: 0.4496\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0686 - acc: 0.4560 - val_loss: 2.1609 - val_acc: 0.4501\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0677 - acc: 0.4541 - val_loss: 2.1552 - val_acc: 0.4516\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0692 - acc: 0.4545 - val_loss: 2.1728 - val_acc: 0.4457\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0667 - acc: 0.4546 - val_loss: 2.1557 - val_acc: 0.4511\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0554 - acc: 0.4579 - val_loss: 2.1618 - val_acc: 0.4461\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0562 - acc: 0.4565 - val_loss: 2.1528 - val_acc: 0.4495\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0526 - acc: 0.4583 - val_loss: 2.1583 - val_acc: 0.4490\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0483 - acc: 0.4600 - val_loss: 2.1555 - val_acc: 0.4494\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0465 - acc: 0.4589 - val_loss: 2.1727 - val_acc: 0.4469\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0562 - acc: 0.4569 - val_loss: 2.1550 - val_acc: 0.4495\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0452 - acc: 0.4591 - val_loss: 2.1608 - val_acc: 0.4488\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0444 - acc: 0.4597 - val_loss: 2.1924 - val_acc: 0.4398\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0561 - acc: 0.4561 - val_loss: 2.1814 - val_acc: 0.4439\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0405 - acc: 0.4606 - val_loss: 2.1584 - val_acc: 0.4518\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0338 - acc: 0.4614 - val_loss: 2.1634 - val_acc: 0.4491\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0383 - acc: 0.4606 - val_loss: 2.1516 - val_acc: 0.4487\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0419 - acc: 0.4603 - val_loss: 2.1598 - val_acc: 0.4485\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0333 - acc: 0.4627 - val_loss: 2.1609 - val_acc: 0.4499\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0433 - acc: 0.4607 - val_loss: 2.1626 - val_acc: 0.4465\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0353 - acc: 0.4616 - val_loss: 2.1491 - val_acc: 0.4515\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0316 - acc: 0.4622 - val_loss: 2.1778 - val_acc: 0.4470\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0560 - acc: 0.4580 - val_loss: 2.1765 - val_acc: 0.4439\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0391 - acc: 0.4609 - val_loss: 2.1698 - val_acc: 0.4470\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0200 - acc: 0.4660 - val_loss: 2.1460 - val_acc: 0.4510\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0194 - acc: 0.4645 - val_loss: 2.1738 - val_acc: 0.4448\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0226 - acc: 0.4633 - val_loss: 2.1465 - val_acc: 0.4514\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0170 - acc: 0.4665 - val_loss: 2.1587 - val_acc: 0.4478\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0136 - acc: 0.4666 - val_loss: 2.1404 - val_acc: 0.4530\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0067 - acc: 0.4678 - val_loss: 2.1606 - val_acc: 0.4483\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0148 - acc: 0.4650 - val_loss: 2.1428 - val_acc: 0.4528\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0097 - acc: 0.4656 - val_loss: 2.1527 - val_acc: 0.4479\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0095 - acc: 0.4651 - val_loss: 2.1472 - val_acc: 0.4538\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0145 - acc: 0.4662 - val_loss: 2.1534 - val_acc: 0.4472\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0044 - acc: 0.4679 - val_loss: 2.1533 - val_acc: 0.4475\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0026 - acc: 0.4682 - val_loss: 2.1503 - val_acc: 0.4483\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9952 - acc: 0.4718 - val_loss: 2.1605 - val_acc: 0.4477\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9941 - acc: 0.4719 - val_loss: 2.1600 - val_acc: 0.4470\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0010 - acc: 0.4687 - val_loss: 2.1854 - val_acc: 0.4457\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0214 - acc: 0.4654 - val_loss: 2.1673 - val_acc: 0.4466\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0041 - acc: 0.4711 - val_loss: 2.1551 - val_acc: 0.4491\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9877 - acc: 0.4719 - val_loss: 2.1562 - val_acc: 0.4478\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0009 - acc: 0.4684 - val_loss: 2.1607 - val_acc: 0.4491\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9896 - acc: 0.4720 - val_loss: 2.1906 - val_acc: 0.4447\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0009 - acc: 0.4707 - val_loss: 2.1606 - val_acc: 0.4458\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0003 - acc: 0.4707 - val_loss: 2.1613 - val_acc: 0.4479\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9969 - acc: 0.4704 - val_loss: 2.1526 - val_acc: 0.4492\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9858 - acc: 0.4729 - val_loss: 2.1620 - val_acc: 0.4476\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9853 - acc: 0.4725 - val_loss: 2.1723 - val_acc: 0.4471\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9830 - acc: 0.4737 - val_loss: 2.1462 - val_acc: 0.4504\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9728 - acc: 0.4744 - val_loss: 2.1656 - val_acc: 0.4478\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9830 - acc: 0.4744 - val_loss: 2.1548 - val_acc: 0.4506\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9788 - acc: 0.4745 - val_loss: 2.1733 - val_acc: 0.4462\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9844 - acc: 0.4744 - val_loss: 2.1598 - val_acc: 0.4471\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9847 - acc: 0.4717 - val_loss: 2.1614 - val_acc: 0.4484\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9852 - acc: 0.4730 - val_loss: 2.1518 - val_acc: 0.4491\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9820 - acc: 0.4722 - val_loss: 2.2021 - val_acc: 0.4392\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9831 - acc: 0.4733 - val_loss: 2.1518 - val_acc: 0.4481\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9647 - acc: 0.4779 - val_loss: 2.1642 - val_acc: 0.4465\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9645 - acc: 0.4788 - val_loss: 2.1564 - val_acc: 0.4483\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9630 - acc: 0.4799 - val_loss: 2.1530 - val_acc: 0.4500\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9549 - acc: 0.4799 - val_loss: 2.1565 - val_acc: 0.4478\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9562 - acc: 0.4793 - val_loss: 2.1422 - val_acc: 0.4524\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9613 - acc: 0.4794 - val_loss: 2.1537 - val_acc: 0.4492\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9525 - acc: 0.4809 - val_loss: 2.1426 - val_acc: 0.4509\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9559 - acc: 0.4779 - val_loss: 2.1582 - val_acc: 0.4487\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9629 - acc: 0.4785 - val_loss: 2.1471 - val_acc: 0.4528\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9702 - acc: 0.4744 - val_loss: 2.1612 - val_acc: 0.4456\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9651 - acc: 0.4790 - val_loss: 2.1515 - val_acc: 0.4495\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9640 - acc: 0.4783 - val_loss: 2.1630 - val_acc: 0.4484\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9493 - acc: 0.4816 - val_loss: 2.1483 - val_acc: 0.4489\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9494 - acc: 0.4814 - val_loss: 2.1934 - val_acc: 0.4437\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9640 - acc: 0.4783 - val_loss: 2.1565 - val_acc: 0.4481\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9481 - acc: 0.4828 - val_loss: 2.1636 - val_acc: 0.4484\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9507 - acc: 0.4806 - val_loss: 2.2028 - val_acc: 0.4413\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9584 - acc: 0.4808 - val_loss: 2.1817 - val_acc: 0.4450\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1721 - acc: 0.4271 - val_loss: 2.2065 - val_acc: 0.4438\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1477 - acc: 0.4324 - val_loss: 2.2079 - val_acc: 0.4437\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1324 - acc: 0.4366 - val_loss: 2.2139 - val_acc: 0.4414\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1234 - acc: 0.4365 - val_loss: 2.2169 - val_acc: 0.4394\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1144 - acc: 0.4392 - val_loss: 2.2292 - val_acc: 0.4365\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1077 - acc: 0.4401 - val_loss: 2.2299 - val_acc: 0.4375\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0988 - acc: 0.4421 - val_loss: 2.2511 - val_acc: 0.4313\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0968 - acc: 0.4441 - val_loss: 2.2487 - val_acc: 0.4328\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0866 - acc: 0.4439 - val_loss: 2.2422 - val_acc: 0.4348\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0880 - acc: 0.4427 - val_loss: 2.2473 - val_acc: 0.4334\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0809 - acc: 0.4455 - val_loss: 2.2770 - val_acc: 0.4297\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0766 - acc: 0.4474 - val_loss: 2.2478 - val_acc: 0.4329\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0699 - acc: 0.4471 - val_loss: 2.2317 - val_acc: 0.4353\n",
      "Epoch 14/100\n",
      "2s - loss: 2.0672 - acc: 0.4493 - val_loss: 2.2553 - val_acc: 0.4314\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0629 - acc: 0.4489 - val_loss: 2.2482 - val_acc: 0.4343\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0638 - acc: 0.4509 - val_loss: 2.2364 - val_acc: 0.4370\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0548 - acc: 0.4510 - val_loss: 2.2399 - val_acc: 0.4365\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0591 - acc: 0.4508 - val_loss: 2.2440 - val_acc: 0.4339\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0483 - acc: 0.4518 - val_loss: 2.2560 - val_acc: 0.4327\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0435 - acc: 0.4538 - val_loss: 2.2546 - val_acc: 0.4325\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0439 - acc: 0.4535 - val_loss: 2.2468 - val_acc: 0.4328\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0408 - acc: 0.4547 - val_loss: 2.2395 - val_acc: 0.4353\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0343 - acc: 0.4576 - val_loss: 2.2391 - val_acc: 0.4363\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0319 - acc: 0.4570 - val_loss: 2.2516 - val_acc: 0.4316\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0329 - acc: 0.4570 - val_loss: 2.2290 - val_acc: 0.4394\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0242 - acc: 0.4569 - val_loss: 2.2356 - val_acc: 0.4365\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0242 - acc: 0.4581 - val_loss: 2.2599 - val_acc: 0.4318\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0255 - acc: 0.4582 - val_loss: 2.2271 - val_acc: 0.4392\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0252 - acc: 0.4571 - val_loss: 2.2720 - val_acc: 0.4309\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0227 - acc: 0.4591 - val_loss: 2.2195 - val_acc: 0.4413\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0197 - acc: 0.4588 - val_loss: 2.2365 - val_acc: 0.4351\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0099 - acc: 0.4604 - val_loss: 2.2308 - val_acc: 0.4384\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0117 - acc: 0.4616 - val_loss: 2.2262 - val_acc: 0.4367\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0107 - acc: 0.4636 - val_loss: 2.2292 - val_acc: 0.4390\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0123 - acc: 0.4623 - val_loss: 2.2132 - val_acc: 0.4400\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0049 - acc: 0.4628 - val_loss: 2.2286 - val_acc: 0.4398\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0033 - acc: 0.4629 - val_loss: 2.2286 - val_acc: 0.4370\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9990 - acc: 0.4637 - val_loss: 2.2242 - val_acc: 0.4420\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0006 - acc: 0.4651 - val_loss: 2.2268 - val_acc: 0.4374\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9998 - acc: 0.4647 - val_loss: 2.2132 - val_acc: 0.4414\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9947 - acc: 0.4653 - val_loss: 2.2135 - val_acc: 0.4416\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9917 - acc: 0.4648 - val_loss: 2.2630 - val_acc: 0.4358\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9960 - acc: 0.4653 - val_loss: 2.2048 - val_acc: 0.4418\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9870 - acc: 0.4670 - val_loss: 2.2065 - val_acc: 0.4429\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9801 - acc: 0.4699 - val_loss: 2.2186 - val_acc: 0.4381\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9799 - acc: 0.4685 - val_loss: 2.2176 - val_acc: 0.4394\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9802 - acc: 0.4682 - val_loss: 2.2151 - val_acc: 0.4396\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9763 - acc: 0.4691 - val_loss: 2.2398 - val_acc: 0.4355\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9794 - acc: 0.4690 - val_loss: 2.2087 - val_acc: 0.4405\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9747 - acc: 0.4677 - val_loss: 2.2130 - val_acc: 0.4410\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9741 - acc: 0.4698 - val_loss: 2.2068 - val_acc: 0.4399\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9686 - acc: 0.4706 - val_loss: 2.2048 - val_acc: 0.4422\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9645 - acc: 0.4717 - val_loss: 2.2305 - val_acc: 0.4365\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9724 - acc: 0.4700 - val_loss: 2.2249 - val_acc: 0.4436\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9766 - acc: 0.4697 - val_loss: 2.2648 - val_acc: 0.4349\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9794 - acc: 0.4687 - val_loss: 2.2251 - val_acc: 0.4430\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9759 - acc: 0.4712 - val_loss: 2.2391 - val_acc: 0.4320\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9769 - acc: 0.4687 - val_loss: 2.2222 - val_acc: 0.4422\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9676 - acc: 0.4726 - val_loss: 2.2369 - val_acc: 0.4366\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9689 - acc: 0.4707 - val_loss: 2.2166 - val_acc: 0.4434\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9658 - acc: 0.4722 - val_loss: 2.2166 - val_acc: 0.4400\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9593 - acc: 0.4721 - val_loss: 2.2226 - val_acc: 0.4399\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9716 - acc: 0.4706 - val_loss: 2.2629 - val_acc: 0.4296\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9664 - acc: 0.4716 - val_loss: 2.2063 - val_acc: 0.4435\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9533 - acc: 0.4742 - val_loss: 2.2146 - val_acc: 0.4421\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9487 - acc: 0.4758 - val_loss: 2.2045 - val_acc: 0.4449\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9481 - acc: 0.4767 - val_loss: 2.2070 - val_acc: 0.4419\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9473 - acc: 0.4760 - val_loss: 2.2154 - val_acc: 0.4443\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9515 - acc: 0.4759 - val_loss: 2.2420 - val_acc: 0.4372\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9558 - acc: 0.4733 - val_loss: 2.2288 - val_acc: 0.4433\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9511 - acc: 0.4753 - val_loss: 2.2296 - val_acc: 0.4384\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9500 - acc: 0.4739 - val_loss: 2.2203 - val_acc: 0.4426\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9455 - acc: 0.4773 - val_loss: 2.2303 - val_acc: 0.4360\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9508 - acc: 0.4749 - val_loss: 2.2247 - val_acc: 0.4406\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9452 - acc: 0.4782 - val_loss: 2.2252 - val_acc: 0.4392\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9332 - acc: 0.4801 - val_loss: 2.2167 - val_acc: 0.4436\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9381 - acc: 0.4778 - val_loss: 2.2079 - val_acc: 0.4437\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9301 - acc: 0.4813 - val_loss: 2.2094 - val_acc: 0.4430\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9273 - acc: 0.4804 - val_loss: 2.2038 - val_acc: 0.4411\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9241 - acc: 0.4808 - val_loss: 2.2038 - val_acc: 0.4424\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9239 - acc: 0.4805 - val_loss: 2.2007 - val_acc: 0.4433\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9179 - acc: 0.4822 - val_loss: 2.2128 - val_acc: 0.4428\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9223 - acc: 0.4806 - val_loss: 2.2286 - val_acc: 0.4382\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9355 - acc: 0.4792 - val_loss: 2.2356 - val_acc: 0.4412\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9364 - acc: 0.4786 - val_loss: 2.2335 - val_acc: 0.4371\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9328 - acc: 0.4811 - val_loss: 2.2224 - val_acc: 0.4427\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9240 - acc: 0.4806 - val_loss: 2.2319 - val_acc: 0.4368\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9320 - acc: 0.4791 - val_loss: 2.2168 - val_acc: 0.4450\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9211 - acc: 0.4811 - val_loss: 2.2419 - val_acc: 0.4341\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9241 - acc: 0.4823 - val_loss: 2.2192 - val_acc: 0.4438\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9132 - acc: 0.4852 - val_loss: 2.2120 - val_acc: 0.4438\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9196 - acc: 0.4818 - val_loss: 2.2178 - val_acc: 0.4419\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9196 - acc: 0.4827 - val_loss: 2.2179 - val_acc: 0.4425\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9237 - acc: 0.4822 - val_loss: 2.2491 - val_acc: 0.4346\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9169 - acc: 0.4819 - val_loss: 2.2145 - val_acc: 0.4422\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9117 - acc: 0.4866 - val_loss: 2.2340 - val_acc: 0.4404\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9164 - acc: 0.4854 - val_loss: 2.2199 - val_acc: 0.4412\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9096 - acc: 0.4828 - val_loss: 2.2295 - val_acc: 0.4358\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9078 - acc: 0.4828 - val_loss: 2.2141 - val_acc: 0.4440\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9057 - acc: 0.4867 - val_loss: 2.2224 - val_acc: 0.4384\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1924 - acc: 0.4273 - val_loss: 2.2361 - val_acc: 0.4375\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1745 - acc: 0.4309 - val_loss: 2.2354 - val_acc: 0.4376\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1532 - acc: 0.4354 - val_loss: 2.2389 - val_acc: 0.4353\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1450 - acc: 0.4365 - val_loss: 2.2392 - val_acc: 0.4374\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1416 - acc: 0.4356 - val_loss: 2.2379 - val_acc: 0.4363\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1297 - acc: 0.4387 - val_loss: 2.2630 - val_acc: 0.4317\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1319 - acc: 0.4400 - val_loss: 2.2688 - val_acc: 0.4321\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1272 - acc: 0.4400 - val_loss: 2.2491 - val_acc: 0.4323\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1169 - acc: 0.4433 - val_loss: 2.2597 - val_acc: 0.4284\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1158 - acc: 0.4423 - val_loss: 2.2634 - val_acc: 0.4305\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1043 - acc: 0.4430 - val_loss: 2.2552 - val_acc: 0.4305\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0987 - acc: 0.4458 - val_loss: 2.2708 - val_acc: 0.4313\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1003 - acc: 0.4452 - val_loss: 2.2566 - val_acc: 0.4322\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0885 - acc: 0.4482 - val_loss: 2.2539 - val_acc: 0.4306\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0840 - acc: 0.4503 - val_loss: 2.2656 - val_acc: 0.4297\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0850 - acc: 0.4484 - val_loss: 2.2496 - val_acc: 0.4330\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0802 - acc: 0.4497 - val_loss: 2.2480 - val_acc: 0.4296\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0715 - acc: 0.4505 - val_loss: 2.2557 - val_acc: 0.4309\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0763 - acc: 0.4497 - val_loss: 2.2663 - val_acc: 0.4279\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0805 - acc: 0.4491 - val_loss: 2.2478 - val_acc: 0.4310\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0656 - acc: 0.4533 - val_loss: 2.2585 - val_acc: 0.4323\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0691 - acc: 0.4526 - val_loss: 2.2488 - val_acc: 0.4334\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0567 - acc: 0.4552 - val_loss: 2.2541 - val_acc: 0.4318\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0636 - acc: 0.4522 - val_loss: 2.2546 - val_acc: 0.4302\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0730 - acc: 0.4509 - val_loss: 2.2570 - val_acc: 0.4315\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0591 - acc: 0.4543 - val_loss: 2.2547 - val_acc: 0.4308\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0510 - acc: 0.4575 - val_loss: 2.2627 - val_acc: 0.4302\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0475 - acc: 0.4572 - val_loss: 2.2477 - val_acc: 0.4308\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0423 - acc: 0.4592 - val_loss: 2.2570 - val_acc: 0.4305\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0421 - acc: 0.4591 - val_loss: 2.2409 - val_acc: 0.4328\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0424 - acc: 0.4601 - val_loss: 2.2671 - val_acc: 0.4296\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0411 - acc: 0.4576 - val_loss: 2.2557 - val_acc: 0.4296\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0447 - acc: 0.4580 - val_loss: 2.2504 - val_acc: 0.4307\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0319 - acc: 0.4600 - val_loss: 2.2381 - val_acc: 0.4339\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0408 - acc: 0.4590 - val_loss: 2.2777 - val_acc: 0.4280\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0488 - acc: 0.4579 - val_loss: 2.2479 - val_acc: 0.4309\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0256 - acc: 0.4635 - val_loss: 2.2428 - val_acc: 0.4333\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0225 - acc: 0.4614 - val_loss: 2.2435 - val_acc: 0.4310\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0258 - acc: 0.4630 - val_loss: 2.2439 - val_acc: 0.4302\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0274 - acc: 0.4603 - val_loss: 2.2429 - val_acc: 0.4338\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0218 - acc: 0.4636 - val_loss: 2.2453 - val_acc: 0.4320\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0249 - acc: 0.4610 - val_loss: 2.2384 - val_acc: 0.4346\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0327 - acc: 0.4601 - val_loss: 2.2449 - val_acc: 0.4339\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0111 - acc: 0.4649 - val_loss: 2.2350 - val_acc: 0.4348\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0096 - acc: 0.4648 - val_loss: 2.2552 - val_acc: 0.4312\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0276 - acc: 0.4602 - val_loss: 2.2442 - val_acc: 0.4355\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0204 - acc: 0.4630 - val_loss: 2.2476 - val_acc: 0.4319\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0056 - acc: 0.4671 - val_loss: 2.2516 - val_acc: 0.4308\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0191 - acc: 0.4632 - val_loss: 2.2686 - val_acc: 0.4303\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0239 - acc: 0.4629 - val_loss: 2.2398 - val_acc: 0.4330\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9999 - acc: 0.4685 - val_loss: 2.2379 - val_acc: 0.4340\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0011 - acc: 0.4682 - val_loss: 2.2394 - val_acc: 0.4347\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0014 - acc: 0.4657 - val_loss: 2.2394 - val_acc: 0.4315\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9981 - acc: 0.4680 - val_loss: 2.2451 - val_acc: 0.4326\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0033 - acc: 0.4671 - val_loss: 2.2423 - val_acc: 0.4308\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9931 - acc: 0.4689 - val_loss: 2.2395 - val_acc: 0.4332\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9953 - acc: 0.4706 - val_loss: 2.2382 - val_acc: 0.4342\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9886 - acc: 0.4717 - val_loss: 2.2527 - val_acc: 0.4326\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9993 - acc: 0.4696 - val_loss: 2.2516 - val_acc: 0.4337\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9876 - acc: 0.4706 - val_loss: 2.2635 - val_acc: 0.4291\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9937 - acc: 0.4694 - val_loss: 2.2563 - val_acc: 0.4346\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9978 - acc: 0.4674 - val_loss: 2.2470 - val_acc: 0.4328\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0053 - acc: 0.4668 - val_loss: 2.2521 - val_acc: 0.4307\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9852 - acc: 0.4711 - val_loss: 2.2497 - val_acc: 0.4297\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9864 - acc: 0.4707 - val_loss: 2.2356 - val_acc: 0.4314\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9786 - acc: 0.4730 - val_loss: 2.2344 - val_acc: 0.4353\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9778 - acc: 0.4745 - val_loss: 2.2349 - val_acc: 0.4323\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9670 - acc: 0.4740 - val_loss: 2.2331 - val_acc: 0.4341\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9637 - acc: 0.4764 - val_loss: 2.2235 - val_acc: 0.4350\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9619 - acc: 0.4755 - val_loss: 2.2438 - val_acc: 0.4339\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9722 - acc: 0.4750 - val_loss: 2.2290 - val_acc: 0.4348\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9629 - acc: 0.4770 - val_loss: 2.2523 - val_acc: 0.4317\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9678 - acc: 0.4738 - val_loss: 2.2242 - val_acc: 0.4367\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9697 - acc: 0.4745 - val_loss: 2.2631 - val_acc: 0.4287\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9998 - acc: 0.4683 - val_loss: 2.2342 - val_acc: 0.4358\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9732 - acc: 0.4744 - val_loss: 2.2463 - val_acc: 0.4327\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9762 - acc: 0.4732 - val_loss: 2.2395 - val_acc: 0.4341\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9675 - acc: 0.4760 - val_loss: 2.2477 - val_acc: 0.4315\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9618 - acc: 0.4762 - val_loss: 2.2309 - val_acc: 0.4355\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9600 - acc: 0.4767 - val_loss: 2.2449 - val_acc: 0.4326\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9619 - acc: 0.4757 - val_loss: 2.2381 - val_acc: 0.4329\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9740 - acc: 0.4729 - val_loss: 2.2681 - val_acc: 0.4294\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9834 - acc: 0.4723 - val_loss: 2.2386 - val_acc: 0.4330\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9612 - acc: 0.4759 - val_loss: 2.2636 - val_acc: 0.4291\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9682 - acc: 0.4749 - val_loss: 2.2307 - val_acc: 0.4357\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9558 - acc: 0.4780 - val_loss: 2.2534 - val_acc: 0.4308\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9603 - acc: 0.4781 - val_loss: 2.2494 - val_acc: 0.4322\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9601 - acc: 0.4765 - val_loss: 2.2460 - val_acc: 0.4332\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9509 - acc: 0.4786 - val_loss: 2.2378 - val_acc: 0.4344\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9505 - acc: 0.4787 - val_loss: 2.2569 - val_acc: 0.4311\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9507 - acc: 0.4798 - val_loss: 2.2356 - val_acc: 0.4335\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9489 - acc: 0.4789 - val_loss: 2.2538 - val_acc: 0.4316\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9480 - acc: 0.4783 - val_loss: 2.2493 - val_acc: 0.4354\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9495 - acc: 0.4784 - val_loss: 2.2632 - val_acc: 0.4304\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9504 - acc: 0.4789 - val_loss: 2.2313 - val_acc: 0.4357\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9378 - acc: 0.4814 - val_loss: 2.2447 - val_acc: 0.4330\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9352 - acc: 0.4830 - val_loss: 2.2438 - val_acc: 0.4320\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9392 - acc: 0.4835 - val_loss: 2.2645 - val_acc: 0.4305\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9446 - acc: 0.4811 - val_loss: 2.3035 - val_acc: 0.4222\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9605 - acc: 0.4766 - val_loss: 2.2893 - val_acc: 0.4305\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1876 - acc: 0.4277 - val_loss: 2.1937 - val_acc: 0.4490\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1472 - acc: 0.4371 - val_loss: 2.2036 - val_acc: 0.4442\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1369 - acc: 0.4384 - val_loss: 2.2016 - val_acc: 0.4461\n",
      "Epoch 4/100\n",
      "2s - loss: 2.1278 - acc: 0.4403 - val_loss: 2.2183 - val_acc: 0.4450\n",
      "Epoch 5/100\n",
      "2s - loss: 2.1193 - acc: 0.4423 - val_loss: 2.2105 - val_acc: 0.4448\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1119 - acc: 0.4439 - val_loss: 2.2079 - val_acc: 0.4431\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1061 - acc: 0.4439 - val_loss: 2.2174 - val_acc: 0.4441\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1127 - acc: 0.4438 - val_loss: 2.2095 - val_acc: 0.4453\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0929 - acc: 0.4463 - val_loss: 2.2149 - val_acc: 0.4435\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0887 - acc: 0.4473 - val_loss: 2.2150 - val_acc: 0.4443\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0866 - acc: 0.4488 - val_loss: 2.2118 - val_acc: 0.4430\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0806 - acc: 0.4505 - val_loss: 2.2157 - val_acc: 0.4430\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0794 - acc: 0.4508 - val_loss: 2.2260 - val_acc: 0.4419\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0747 - acc: 0.4512 - val_loss: 2.2380 - val_acc: 0.4403\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0768 - acc: 0.4505 - val_loss: 2.2108 - val_acc: 0.4431\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0633 - acc: 0.4532 - val_loss: 2.2076 - val_acc: 0.4451\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0578 - acc: 0.4558 - val_loss: 2.2167 - val_acc: 0.4437\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0580 - acc: 0.4546 - val_loss: 2.2106 - val_acc: 0.4436\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0536 - acc: 0.4548 - val_loss: 2.2156 - val_acc: 0.4432\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0484 - acc: 0.4577 - val_loss: 2.2199 - val_acc: 0.4443\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0625 - acc: 0.4542 - val_loss: 2.2103 - val_acc: 0.4438\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0454 - acc: 0.4591 - val_loss: 2.2154 - val_acc: 0.4410\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0378 - acc: 0.4594 - val_loss: 2.2039 - val_acc: 0.4454\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0422 - acc: 0.4576 - val_loss: 2.2239 - val_acc: 0.4421\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0426 - acc: 0.4580 - val_loss: 2.2036 - val_acc: 0.4462\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0363 - acc: 0.4572 - val_loss: 2.2049 - val_acc: 0.4448\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0267 - acc: 0.4592 - val_loss: 2.2108 - val_acc: 0.4421\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0317 - acc: 0.4597 - val_loss: 2.2043 - val_acc: 0.4445\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0317 - acc: 0.4615 - val_loss: 2.2178 - val_acc: 0.4408\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0300 - acc: 0.4622 - val_loss: 2.2112 - val_acc: 0.4421\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0216 - acc: 0.4612 - val_loss: 2.2036 - val_acc: 0.4436\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0173 - acc: 0.4630 - val_loss: 2.2148 - val_acc: 0.4428\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0154 - acc: 0.4633 - val_loss: 2.2125 - val_acc: 0.4425\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0220 - acc: 0.4625 - val_loss: 2.2108 - val_acc: 0.4434\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0157 - acc: 0.4634 - val_loss: 2.2089 - val_acc: 0.4438\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0142 - acc: 0.4635 - val_loss: 2.2038 - val_acc: 0.4433\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0086 - acc: 0.4659 - val_loss: 2.2086 - val_acc: 0.4429\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0075 - acc: 0.4659 - val_loss: 2.1990 - val_acc: 0.4442\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0007 - acc: 0.4669 - val_loss: 2.2074 - val_acc: 0.4435\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0001 - acc: 0.4666 - val_loss: 2.2029 - val_acc: 0.4442\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9978 - acc: 0.4680 - val_loss: 2.2056 - val_acc: 0.4447\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9968 - acc: 0.4694 - val_loss: 2.2027 - val_acc: 0.4451\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9970 - acc: 0.4655 - val_loss: 2.2097 - val_acc: 0.4437\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0006 - acc: 0.4657 - val_loss: 2.2036 - val_acc: 0.4430\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9911 - acc: 0.4692 - val_loss: 2.2113 - val_acc: 0.4422\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9888 - acc: 0.4684 - val_loss: 2.2398 - val_acc: 0.4411\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0378 - acc: 0.4600 - val_loss: 2.2513 - val_acc: 0.4331\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0264 - acc: 0.4610 - val_loss: 2.2247 - val_acc: 0.4402\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9951 - acc: 0.4694 - val_loss: 2.2058 - val_acc: 0.4425\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9831 - acc: 0.4703 - val_loss: 2.1939 - val_acc: 0.4458\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9805 - acc: 0.4720 - val_loss: 2.2060 - val_acc: 0.4435\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9803 - acc: 0.4694 - val_loss: 2.2005 - val_acc: 0.4457\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9887 - acc: 0.4693 - val_loss: 2.2163 - val_acc: 0.4410\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9753 - acc: 0.4717 - val_loss: 2.1947 - val_acc: 0.4465\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9735 - acc: 0.4739 - val_loss: 2.2054 - val_acc: 0.4419\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9702 - acc: 0.4739 - val_loss: 2.1909 - val_acc: 0.4458\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9629 - acc: 0.4738 - val_loss: 2.1943 - val_acc: 0.4431\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9652 - acc: 0.4735 - val_loss: 2.2041 - val_acc: 0.4443\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9736 - acc: 0.4722 - val_loss: 2.2060 - val_acc: 0.4406\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9699 - acc: 0.4729 - val_loss: 2.1996 - val_acc: 0.4435\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9621 - acc: 0.4747 - val_loss: 2.1981 - val_acc: 0.4437\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9680 - acc: 0.4726 - val_loss: 2.2138 - val_acc: 0.4454\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9929 - acc: 0.4708 - val_loss: 2.2276 - val_acc: 0.4391\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9687 - acc: 0.4749 - val_loss: 2.2028 - val_acc: 0.4463\n",
      "Epoch 65/100\n",
      "2s - loss: 1.9692 - acc: 0.4733 - val_loss: 2.2275 - val_acc: 0.4397\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9799 - acc: 0.4713 - val_loss: 2.2010 - val_acc: 0.4455\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9568 - acc: 0.4758 - val_loss: 2.1992 - val_acc: 0.4430\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9469 - acc: 0.4788 - val_loss: 2.1940 - val_acc: 0.4451\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9510 - acc: 0.4785 - val_loss: 2.1947 - val_acc: 0.4438\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9420 - acc: 0.4795 - val_loss: 2.2023 - val_acc: 0.4443\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9423 - acc: 0.4804 - val_loss: 2.1973 - val_acc: 0.4443\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9403 - acc: 0.4789 - val_loss: 2.2261 - val_acc: 0.4409\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9484 - acc: 0.4782 - val_loss: 2.2361 - val_acc: 0.4427\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0036 - acc: 0.4673 - val_loss: 2.2722 - val_acc: 0.4299\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9938 - acc: 0.4706 - val_loss: 2.2360 - val_acc: 0.4374\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9687 - acc: 0.4752 - val_loss: 2.2192 - val_acc: 0.4440\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9537 - acc: 0.4747 - val_loss: 2.2250 - val_acc: 0.4418\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9530 - acc: 0.4760 - val_loss: 2.2202 - val_acc: 0.4420\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9527 - acc: 0.4765 - val_loss: 2.1984 - val_acc: 0.4450\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9359 - acc: 0.4818 - val_loss: 2.2159 - val_acc: 0.4439\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9414 - acc: 0.4801 - val_loss: 2.1987 - val_acc: 0.4446\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9358 - acc: 0.4804 - val_loss: 2.2243 - val_acc: 0.4426\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9359 - acc: 0.4796 - val_loss: 2.2086 - val_acc: 0.4446\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9372 - acc: 0.4794 - val_loss: 2.2379 - val_acc: 0.4399\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9361 - acc: 0.4793 - val_loss: 2.2112 - val_acc: 0.4440\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9304 - acc: 0.4831 - val_loss: 2.2094 - val_acc: 0.4428\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9255 - acc: 0.4829 - val_loss: 2.2034 - val_acc: 0.4431\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9313 - acc: 0.4822 - val_loss: 2.2135 - val_acc: 0.4436\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9232 - acc: 0.4836 - val_loss: 2.2006 - val_acc: 0.4458\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9263 - acc: 0.4842 - val_loss: 2.2654 - val_acc: 0.4362\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9424 - acc: 0.4796 - val_loss: 2.2131 - val_acc: 0.4454\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9261 - acc: 0.4836 - val_loss: 2.2372 - val_acc: 0.4400\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9239 - acc: 0.4835 - val_loss: 2.2102 - val_acc: 0.4437\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9259 - acc: 0.4829 - val_loss: 2.2337 - val_acc: 0.4383\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9202 - acc: 0.4847 - val_loss: 2.2063 - val_acc: 0.4432\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9244 - acc: 0.4841 - val_loss: 2.2300 - val_acc: 0.4390\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9162 - acc: 0.4857 - val_loss: 2.2336 - val_acc: 0.4438\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9535 - acc: 0.4782 - val_loss: 2.2523 - val_acc: 0.4331\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9357 - acc: 0.4820 - val_loss: 2.2097 - val_acc: 0.4410\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9174 - acc: 0.4875 - val_loss: 2.2089 - val_acc: 0.4432\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2176 - acc: 0.4249 - val_loss: 2.2291 - val_acc: 0.4391\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1915 - acc: 0.4293 - val_loss: 2.2392 - val_acc: 0.4370\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1780 - acc: 0.4299 - val_loss: 2.2391 - val_acc: 0.4376\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1691 - acc: 0.4344 - val_loss: 2.2493 - val_acc: 0.4364\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1614 - acc: 0.4344 - val_loss: 2.2401 - val_acc: 0.4367\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1523 - acc: 0.4373 - val_loss: 2.2519 - val_acc: 0.4332\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1473 - acc: 0.4363 - val_loss: 2.2515 - val_acc: 0.4366\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1423 - acc: 0.4404 - val_loss: 2.2465 - val_acc: 0.4330\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1338 - acc: 0.4410 - val_loss: 2.2595 - val_acc: 0.4319\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1374 - acc: 0.4392 - val_loss: 2.2543 - val_acc: 0.4314\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1326 - acc: 0.4422 - val_loss: 2.2513 - val_acc: 0.4335\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1256 - acc: 0.4421 - val_loss: 2.2599 - val_acc: 0.4324\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1202 - acc: 0.4444 - val_loss: 2.2520 - val_acc: 0.4326\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1165 - acc: 0.4447 - val_loss: 2.2504 - val_acc: 0.4339\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1135 - acc: 0.4449 - val_loss: 2.2499 - val_acc: 0.4324\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1052 - acc: 0.4466 - val_loss: 2.2563 - val_acc: 0.4314\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1020 - acc: 0.4473 - val_loss: 2.2558 - val_acc: 0.4302\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1034 - acc: 0.4481 - val_loss: 2.2562 - val_acc: 0.4347\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1252 - acc: 0.4418 - val_loss: 2.2619 - val_acc: 0.4300\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1047 - acc: 0.4480 - val_loss: 2.2471 - val_acc: 0.4345\n",
      "Epoch 21/100\n",
      "1s - loss: 2.1015 - acc: 0.4486 - val_loss: 2.3052 - val_acc: 0.4312\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1486 - acc: 0.4390 - val_loss: 2.3152 - val_acc: 0.4203\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1311 - acc: 0.4424 - val_loss: 2.2899 - val_acc: 0.4260\n",
      "Epoch 24/100\n",
      "1s - loss: 2.1011 - acc: 0.4477 - val_loss: 2.2549 - val_acc: 0.4339\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0827 - acc: 0.4540 - val_loss: 2.2371 - val_acc: 0.4370\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0720 - acc: 0.4546 - val_loss: 2.2539 - val_acc: 0.4319\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0760 - acc: 0.4544 - val_loss: 2.2385 - val_acc: 0.4349\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0665 - acc: 0.4553 - val_loss: 2.2610 - val_acc: 0.4326\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0762 - acc: 0.4530 - val_loss: 2.2539 - val_acc: 0.4321\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0661 - acc: 0.4560 - val_loss: 2.2505 - val_acc: 0.4341\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0692 - acc: 0.4549 - val_loss: 2.2466 - val_acc: 0.4353\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0708 - acc: 0.4536 - val_loss: 2.2416 - val_acc: 0.4348\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0606 - acc: 0.4564 - val_loss: 2.2414 - val_acc: 0.4365\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0692 - acc: 0.4557 - val_loss: 2.2798 - val_acc: 0.4305\n",
      "Epoch 35/100\n",
      "2s - loss: 2.0614 - acc: 0.4564 - val_loss: 2.2467 - val_acc: 0.4347\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0557 - acc: 0.4564 - val_loss: 2.2543 - val_acc: 0.4338\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0466 - acc: 0.4619 - val_loss: 2.2396 - val_acc: 0.4360\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0451 - acc: 0.4593 - val_loss: 2.2568 - val_acc: 0.4338\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0473 - acc: 0.4605 - val_loss: 2.2465 - val_acc: 0.4336\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0421 - acc: 0.4614 - val_loss: 2.2426 - val_acc: 0.4361\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0440 - acc: 0.4585 - val_loss: 2.2587 - val_acc: 0.4315\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0498 - acc: 0.4584 - val_loss: 2.2443 - val_acc: 0.4355\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0547 - acc: 0.4584 - val_loss: 2.2482 - val_acc: 0.4359\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0368 - acc: 0.4610 - val_loss: 2.2489 - val_acc: 0.4359\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0428 - acc: 0.4617 - val_loss: 2.2731 - val_acc: 0.4332\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0683 - acc: 0.4538 - val_loss: 2.2786 - val_acc: 0.4282\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0565 - acc: 0.4591 - val_loss: 2.2958 - val_acc: 0.4278\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0470 - acc: 0.4584 - val_loss: 2.2603 - val_acc: 0.4341\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0512 - acc: 0.4576 - val_loss: 2.2497 - val_acc: 0.4373\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0340 - acc: 0.4632 - val_loss: 2.2308 - val_acc: 0.4381\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0248 - acc: 0.4637 - val_loss: 2.2411 - val_acc: 0.4396\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0283 - acc: 0.4648 - val_loss: 2.2237 - val_acc: 0.4405\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0200 - acc: 0.4645 - val_loss: 2.2553 - val_acc: 0.4357\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0359 - acc: 0.4624 - val_loss: 2.2349 - val_acc: 0.4346\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0304 - acc: 0.4628 - val_loss: 2.2448 - val_acc: 0.4349\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0210 - acc: 0.4675 - val_loss: 2.2331 - val_acc: 0.4370\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0212 - acc: 0.4645 - val_loss: 2.2795 - val_acc: 0.4338\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0355 - acc: 0.4633 - val_loss: 2.2406 - val_acc: 0.4367\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0151 - acc: 0.4667 - val_loss: 2.2456 - val_acc: 0.4358\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0145 - acc: 0.4663 - val_loss: 2.2401 - val_acc: 0.4375\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0080 - acc: 0.4690 - val_loss: 2.2356 - val_acc: 0.4349\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0043 - acc: 0.4706 - val_loss: 2.2315 - val_acc: 0.4370\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0061 - acc: 0.4683 - val_loss: 2.2438 - val_acc: 0.4356\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0051 - acc: 0.4692 - val_loss: 2.2270 - val_acc: 0.4374\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0010 - acc: 0.4707 - val_loss: 2.2453 - val_acc: 0.4356\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9983 - acc: 0.4703 - val_loss: 2.2769 - val_acc: 0.4286\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0033 - acc: 0.4692 - val_loss: 2.2392 - val_acc: 0.4340\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9930 - acc: 0.4710 - val_loss: 2.2919 - val_acc: 0.4265\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0068 - acc: 0.4666 - val_loss: 2.2477 - val_acc: 0.4334\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0018 - acc: 0.4705 - val_loss: 2.2545 - val_acc: 0.4350\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9893 - acc: 0.4735 - val_loss: 2.2345 - val_acc: 0.4367\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9944 - acc: 0.4731 - val_loss: 2.2372 - val_acc: 0.4390\n",
      "Epoch 73/100\n",
      "2s - loss: 1.9948 - acc: 0.4700 - val_loss: 2.2630 - val_acc: 0.4311\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0177 - acc: 0.4654 - val_loss: 2.2512 - val_acc: 0.4348\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9974 - acc: 0.4703 - val_loss: 2.2462 - val_acc: 0.4352\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9918 - acc: 0.4709 - val_loss: 2.2563 - val_acc: 0.4355\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0072 - acc: 0.4705 - val_loss: 2.2426 - val_acc: 0.4380\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9845 - acc: 0.4733 - val_loss: 2.2615 - val_acc: 0.4333\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9970 - acc: 0.4706 - val_loss: 2.2530 - val_acc: 0.4343\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9795 - acc: 0.4747 - val_loss: 2.2688 - val_acc: 0.4332\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9822 - acc: 0.4761 - val_loss: 2.2447 - val_acc: 0.4364\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9809 - acc: 0.4754 - val_loss: 2.2564 - val_acc: 0.4347\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9962 - acc: 0.4701 - val_loss: 2.2503 - val_acc: 0.4368\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9725 - acc: 0.4758 - val_loss: 2.2265 - val_acc: 0.4387\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9639 - acc: 0.4793 - val_loss: 2.2517 - val_acc: 0.4366\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9716 - acc: 0.4781 - val_loss: 2.2241 - val_acc: 0.4392\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9706 - acc: 0.4771 - val_loss: 2.2521 - val_acc: 0.4351\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9784 - acc: 0.4750 - val_loss: 2.2400 - val_acc: 0.4356\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9714 - acc: 0.4771 - val_loss: 2.2588 - val_acc: 0.4341\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9772 - acc: 0.4747 - val_loss: 2.2449 - val_acc: 0.4347\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9763 - acc: 0.4768 - val_loss: 2.2757 - val_acc: 0.4332\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9867 - acc: 0.4734 - val_loss: 2.2391 - val_acc: 0.4374\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9683 - acc: 0.4772 - val_loss: 2.2742 - val_acc: 0.4320\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9717 - acc: 0.4768 - val_loss: 2.2426 - val_acc: 0.4354\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9842 - acc: 0.4736 - val_loss: 2.2685 - val_acc: 0.4333\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9570 - acc: 0.4800 - val_loss: 2.2399 - val_acc: 0.4367\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9499 - acc: 0.4819 - val_loss: 2.2439 - val_acc: 0.4341\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9477 - acc: 0.4823 - val_loss: 2.2387 - val_acc: 0.4360\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9564 - acc: 0.4805 - val_loss: 2.3056 - val_acc: 0.4290\n",
      "Epoch 100/100\n",
      "1s - loss: 2.0062 - acc: 0.4687 - val_loss: 2.2798 - val_acc: 0.4304\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2261 - acc: 0.4200 - val_loss: 2.2385 - val_acc: 0.4343\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1788 - acc: 0.4314 - val_loss: 2.2327 - val_acc: 0.4360\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1634 - acc: 0.4329 - val_loss: 2.2265 - val_acc: 0.4371\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1518 - acc: 0.4370 - val_loss: 2.2390 - val_acc: 0.4365\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1455 - acc: 0.4387 - val_loss: 2.2420 - val_acc: 0.4337\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1353 - acc: 0.4399 - val_loss: 2.2496 - val_acc: 0.4360\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1353 - acc: 0.4394 - val_loss: 2.2526 - val_acc: 0.4344\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1206 - acc: 0.4426 - val_loss: 2.2503 - val_acc: 0.4337\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1167 - acc: 0.4424 - val_loss: 2.2457 - val_acc: 0.4344\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1104 - acc: 0.4435 - val_loss: 2.2732 - val_acc: 0.4295\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1086 - acc: 0.4436 - val_loss: 2.2579 - val_acc: 0.4328\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1053 - acc: 0.4438 - val_loss: 2.2525 - val_acc: 0.4328\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0988 - acc: 0.4449 - val_loss: 2.2633 - val_acc: 0.4340\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0978 - acc: 0.4454 - val_loss: 2.2528 - val_acc: 0.4337\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0929 - acc: 0.4462 - val_loss: 2.2518 - val_acc: 0.4327\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0942 - acc: 0.4477 - val_loss: 2.2547 - val_acc: 0.4343\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0883 - acc: 0.4493 - val_loss: 2.2705 - val_acc: 0.4332\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0911 - acc: 0.4497 - val_loss: 2.2601 - val_acc: 0.4329\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0818 - acc: 0.4509 - val_loss: 2.2703 - val_acc: 0.4312\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0825 - acc: 0.4511 - val_loss: 2.2787 - val_acc: 0.4286\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0803 - acc: 0.4507 - val_loss: 2.2591 - val_acc: 0.4319\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0740 - acc: 0.4517 - val_loss: 2.2602 - val_acc: 0.4343\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0767 - acc: 0.4512 - val_loss: 2.2516 - val_acc: 0.4356\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0731 - acc: 0.4524 - val_loss: 2.2473 - val_acc: 0.4351\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0595 - acc: 0.4539 - val_loss: 2.2463 - val_acc: 0.4345\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0564 - acc: 0.4562 - val_loss: 2.2525 - val_acc: 0.4345\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0558 - acc: 0.4559 - val_loss: 2.2515 - val_acc: 0.4328\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0547 - acc: 0.4556 - val_loss: 2.2466 - val_acc: 0.4352\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0600 - acc: 0.4557 - val_loss: 2.2565 - val_acc: 0.4325\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0770 - acc: 0.4507 - val_loss: 2.2515 - val_acc: 0.4341\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0496 - acc: 0.4569 - val_loss: 2.2364 - val_acc: 0.4345\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0382 - acc: 0.4599 - val_loss: 2.2523 - val_acc: 0.4343\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0371 - acc: 0.4615 - val_loss: 2.2338 - val_acc: 0.4372\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0398 - acc: 0.4599 - val_loss: 2.2501 - val_acc: 0.4334\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0325 - acc: 0.4593 - val_loss: 2.2300 - val_acc: 0.4370\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0304 - acc: 0.4611 - val_loss: 2.2389 - val_acc: 0.4360\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0284 - acc: 0.4629 - val_loss: 2.2354 - val_acc: 0.4375\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0258 - acc: 0.4630 - val_loss: 2.2413 - val_acc: 0.4354\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0265 - acc: 0.4614 - val_loss: 2.2795 - val_acc: 0.4289\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0380 - acc: 0.4607 - val_loss: 2.2337 - val_acc: 0.4365\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0331 - acc: 0.4586 - val_loss: 2.2389 - val_acc: 0.4385\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0384 - acc: 0.4594 - val_loss: 2.2634 - val_acc: 0.4322\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0365 - acc: 0.4591 - val_loss: 2.2382 - val_acc: 0.4343\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0261 - acc: 0.4633 - val_loss: 2.2378 - val_acc: 0.4355\n",
      "Epoch 45/100\n",
      "2s - loss: 2.0291 - acc: 0.4611 - val_loss: 2.2490 - val_acc: 0.4347\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0282 - acc: 0.4605 - val_loss: 2.2406 - val_acc: 0.4354\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0213 - acc: 0.4640 - val_loss: 2.2376 - val_acc: 0.4347\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0136 - acc: 0.4665 - val_loss: 2.2353 - val_acc: 0.4357\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0167 - acc: 0.4653 - val_loss: 2.2564 - val_acc: 0.4330\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0160 - acc: 0.4644 - val_loss: 2.2511 - val_acc: 0.4334\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0055 - acc: 0.4677 - val_loss: 2.2363 - val_acc: 0.4331\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0044 - acc: 0.4673 - val_loss: 2.2333 - val_acc: 0.4378\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0148 - acc: 0.4647 - val_loss: 2.2555 - val_acc: 0.4337\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0137 - acc: 0.4663 - val_loss: 2.2405 - val_acc: 0.4357\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0048 - acc: 0.4687 - val_loss: 2.2563 - val_acc: 0.4319\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0072 - acc: 0.4670 - val_loss: 2.2465 - val_acc: 0.4349\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0037 - acc: 0.4686 - val_loss: 2.2494 - val_acc: 0.4328\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9991 - acc: 0.4679 - val_loss: 2.2411 - val_acc: 0.4374\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0006 - acc: 0.4687 - val_loss: 2.2346 - val_acc: 0.4366\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9909 - acc: 0.4701 - val_loss: 2.2358 - val_acc: 0.4354\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9864 - acc: 0.4689 - val_loss: 2.2345 - val_acc: 0.4343\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9894 - acc: 0.4711 - val_loss: 2.2233 - val_acc: 0.4384\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9801 - acc: 0.4732 - val_loss: 2.2435 - val_acc: 0.4350\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9934 - acc: 0.4714 - val_loss: 2.2385 - val_acc: 0.4356\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9973 - acc: 0.4686 - val_loss: 2.2890 - val_acc: 0.4312\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9873 - acc: 0.4722 - val_loss: 2.2349 - val_acc: 0.4365\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9905 - acc: 0.4723 - val_loss: 2.2556 - val_acc: 0.4336\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9905 - acc: 0.4704 - val_loss: 2.2447 - val_acc: 0.4350\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9868 - acc: 0.4695 - val_loss: 2.2452 - val_acc: 0.4351\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9877 - acc: 0.4706 - val_loss: 2.2425 - val_acc: 0.4354\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9961 - acc: 0.4694 - val_loss: 2.2499 - val_acc: 0.4355\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9864 - acc: 0.4711 - val_loss: 2.2287 - val_acc: 0.4362\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9792 - acc: 0.4729 - val_loss: 2.2300 - val_acc: 0.4367\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9752 - acc: 0.4738 - val_loss: 2.2430 - val_acc: 0.4361\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9909 - acc: 0.4721 - val_loss: 2.2303 - val_acc: 0.4376\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9775 - acc: 0.4727 - val_loss: 2.2287 - val_acc: 0.4370\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9614 - acc: 0.4788 - val_loss: 2.2273 - val_acc: 0.4378\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9625 - acc: 0.4767 - val_loss: 2.2426 - val_acc: 0.4352\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9586 - acc: 0.4785 - val_loss: 2.2324 - val_acc: 0.4369\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9581 - acc: 0.4782 - val_loss: 2.2419 - val_acc: 0.4360\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9598 - acc: 0.4777 - val_loss: 2.2303 - val_acc: 0.4368\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9628 - acc: 0.4764 - val_loss: 2.3024 - val_acc: 0.4274\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9758 - acc: 0.4730 - val_loss: 2.2401 - val_acc: 0.4371\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9727 - acc: 0.4750 - val_loss: 2.2681 - val_acc: 0.4315\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9638 - acc: 0.4754 - val_loss: 2.2428 - val_acc: 0.4343\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9652 - acc: 0.4774 - val_loss: 2.2860 - val_acc: 0.4287\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9713 - acc: 0.4762 - val_loss: 2.2409 - val_acc: 0.4363\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9636 - acc: 0.4774 - val_loss: 2.2325 - val_acc: 0.4370\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9481 - acc: 0.4809 - val_loss: 2.2557 - val_acc: 0.4317\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9539 - acc: 0.4791 - val_loss: 2.2287 - val_acc: 0.4349\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9457 - acc: 0.4800 - val_loss: 2.2315 - val_acc: 0.4348\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9459 - acc: 0.4831 - val_loss: 2.2434 - val_acc: 0.4352\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9637 - acc: 0.4766 - val_loss: 2.2322 - val_acc: 0.4348\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9425 - acc: 0.4806 - val_loss: 2.2411 - val_acc: 0.4358\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9559 - acc: 0.4795 - val_loss: 2.2432 - val_acc: 0.4355\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9460 - acc: 0.4813 - val_loss: 2.2525 - val_acc: 0.4332\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9508 - acc: 0.4793 - val_loss: 2.2470 - val_acc: 0.4356\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9476 - acc: 0.4798 - val_loss: 2.2292 - val_acc: 0.4360\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9425 - acc: 0.4816 - val_loss: 2.2366 - val_acc: 0.4339\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9365 - acc: 0.4850 - val_loss: 2.2526 - val_acc: 0.4332\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1841 - acc: 0.4288 - val_loss: 2.2426 - val_acc: 0.4348\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1695 - acc: 0.4304 - val_loss: 2.2201 - val_acc: 0.4355\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1493 - acc: 0.4353 - val_loss: 2.2167 - val_acc: 0.4384\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1379 - acc: 0.4372 - val_loss: 2.2179 - val_acc: 0.4378\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1322 - acc: 0.4391 - val_loss: 2.2193 - val_acc: 0.4369\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1238 - acc: 0.4403 - val_loss: 2.2580 - val_acc: 0.4309\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1250 - acc: 0.4411 - val_loss: 2.2290 - val_acc: 0.4348\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1141 - acc: 0.4424 - val_loss: 2.2236 - val_acc: 0.4381\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1056 - acc: 0.4448 - val_loss: 2.2277 - val_acc: 0.4365\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0998 - acc: 0.4444 - val_loss: 2.2531 - val_acc: 0.4325\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0971 - acc: 0.4448 - val_loss: 2.2297 - val_acc: 0.4357\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0929 - acc: 0.4457 - val_loss: 2.2268 - val_acc: 0.4347\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0882 - acc: 0.4468 - val_loss: 2.2202 - val_acc: 0.4368\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0880 - acc: 0.4501 - val_loss: 2.2296 - val_acc: 0.4378\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0795 - acc: 0.4492 - val_loss: 2.2304 - val_acc: 0.4360\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0823 - acc: 0.4499 - val_loss: 2.2350 - val_acc: 0.4358\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0710 - acc: 0.4506 - val_loss: 2.2294 - val_acc: 0.4354\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0697 - acc: 0.4509 - val_loss: 2.2320 - val_acc: 0.4363\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0658 - acc: 0.4516 - val_loss: 2.2301 - val_acc: 0.4365\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0574 - acc: 0.4529 - val_loss: 2.2329 - val_acc: 0.4365\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0582 - acc: 0.4549 - val_loss: 2.2355 - val_acc: 0.4362\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0925 - acc: 0.4499 - val_loss: 2.2533 - val_acc: 0.4315\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0734 - acc: 0.4517 - val_loss: 2.2452 - val_acc: 0.4346\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0509 - acc: 0.4557 - val_loss: 2.2352 - val_acc: 0.4349\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0491 - acc: 0.4551 - val_loss: 2.2281 - val_acc: 0.4354\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0431 - acc: 0.4582 - val_loss: 2.2317 - val_acc: 0.4363\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0398 - acc: 0.4580 - val_loss: 2.2334 - val_acc: 0.4354\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0410 - acc: 0.4599 - val_loss: 2.2246 - val_acc: 0.4379\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0430 - acc: 0.4573 - val_loss: 2.2344 - val_acc: 0.4373\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0598 - acc: 0.4551 - val_loss: 2.2355 - val_acc: 0.4357\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0365 - acc: 0.4594 - val_loss: 2.2221 - val_acc: 0.4393\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0271 - acc: 0.4609 - val_loss: 2.2240 - val_acc: 0.4363\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0308 - acc: 0.4606 - val_loss: 2.2243 - val_acc: 0.4379\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0259 - acc: 0.4614 - val_loss: 2.2220 - val_acc: 0.4366\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0202 - acc: 0.4633 - val_loss: 2.2188 - val_acc: 0.4376\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0166 - acc: 0.4654 - val_loss: 2.2183 - val_acc: 0.4402\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0209 - acc: 0.4647 - val_loss: 2.2215 - val_acc: 0.4385\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0142 - acc: 0.4670 - val_loss: 2.2272 - val_acc: 0.4370\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0105 - acc: 0.4662 - val_loss: 2.2169 - val_acc: 0.4399\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0123 - acc: 0.4641 - val_loss: 2.2235 - val_acc: 0.4393\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0068 - acc: 0.4653 - val_loss: 2.2265 - val_acc: 0.4360\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0099 - acc: 0.4654 - val_loss: 2.2250 - val_acc: 0.4378\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0077 - acc: 0.4662 - val_loss: 2.2527 - val_acc: 0.4314\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0139 - acc: 0.4651 - val_loss: 2.2164 - val_acc: 0.4399\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0120 - acc: 0.4659 - val_loss: 2.2418 - val_acc: 0.4339\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0146 - acc: 0.4643 - val_loss: 2.2360 - val_acc: 0.4356\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0135 - acc: 0.4631 - val_loss: 2.2329 - val_acc: 0.4362\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0016 - acc: 0.4672 - val_loss: 2.2206 - val_acc: 0.4399\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9980 - acc: 0.4684 - val_loss: 2.2216 - val_acc: 0.4371\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9974 - acc: 0.4679 - val_loss: 2.2149 - val_acc: 0.4391\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0000 - acc: 0.4655 - val_loss: 2.2438 - val_acc: 0.4349\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9944 - acc: 0.4679 - val_loss: 2.2181 - val_acc: 0.4412\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9936 - acc: 0.4706 - val_loss: 2.2305 - val_acc: 0.4366\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0037 - acc: 0.4671 - val_loss: 2.2229 - val_acc: 0.4395\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9911 - acc: 0.4704 - val_loss: 2.2264 - val_acc: 0.4370\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9878 - acc: 0.4720 - val_loss: 2.2154 - val_acc: 0.4382\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9823 - acc: 0.4722 - val_loss: 2.2259 - val_acc: 0.4371\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9801 - acc: 0.4725 - val_loss: 2.2173 - val_acc: 0.4369\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9818 - acc: 0.4715 - val_loss: 2.2518 - val_acc: 0.4353\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9820 - acc: 0.4719 - val_loss: 2.2167 - val_acc: 0.4417\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9726 - acc: 0.4736 - val_loss: 2.2376 - val_acc: 0.4358\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9827 - acc: 0.4714 - val_loss: 2.2674 - val_acc: 0.4328\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9880 - acc: 0.4703 - val_loss: 2.2397 - val_acc: 0.4375\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9775 - acc: 0.4733 - val_loss: 2.2449 - val_acc: 0.4363\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9719 - acc: 0.4745 - val_loss: 2.2173 - val_acc: 0.4410\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9650 - acc: 0.4759 - val_loss: 2.2395 - val_acc: 0.4362\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9720 - acc: 0.4735 - val_loss: 2.2157 - val_acc: 0.4399\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9738 - acc: 0.4737 - val_loss: 2.2518 - val_acc: 0.4342\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9765 - acc: 0.4724 - val_loss: 2.2119 - val_acc: 0.4441\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9591 - acc: 0.4771 - val_loss: 2.2401 - val_acc: 0.4338\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9645 - acc: 0.4753 - val_loss: 2.2624 - val_acc: 0.4354\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0041 - acc: 0.4666 - val_loss: 2.2663 - val_acc: 0.4332\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9741 - acc: 0.4740 - val_loss: 2.2429 - val_acc: 0.4342\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9782 - acc: 0.4728 - val_loss: 2.2441 - val_acc: 0.4411\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9663 - acc: 0.4768 - val_loss: 2.2396 - val_acc: 0.4362\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9626 - acc: 0.4774 - val_loss: 2.2230 - val_acc: 0.4374\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9540 - acc: 0.4778 - val_loss: 2.2218 - val_acc: 0.4373\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9501 - acc: 0.4800 - val_loss: 2.2225 - val_acc: 0.4366\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9461 - acc: 0.4796 - val_loss: 2.2129 - val_acc: 0.4420\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9469 - acc: 0.4817 - val_loss: 2.2327 - val_acc: 0.4376\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9502 - acc: 0.4787 - val_loss: 2.2526 - val_acc: 0.4383\n",
      "Epoch 82/100\n",
      "1s - loss: 2.0227 - acc: 0.4652 - val_loss: 2.2952 - val_acc: 0.4226\n",
      "Epoch 83/100\n",
      "1s - loss: 2.0170 - acc: 0.4654 - val_loss: 2.2718 - val_acc: 0.4310\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9824 - acc: 0.4734 - val_loss: 2.2721 - val_acc: 0.4336\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9607 - acc: 0.4757 - val_loss: 2.2413 - val_acc: 0.4376\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9515 - acc: 0.4783 - val_loss: 2.2360 - val_acc: 0.4355\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9495 - acc: 0.4795 - val_loss: 2.2369 - val_acc: 0.4392\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9513 - acc: 0.4810 - val_loss: 2.2375 - val_acc: 0.4372\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9361 - acc: 0.4821 - val_loss: 2.2222 - val_acc: 0.4416\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9391 - acc: 0.4830 - val_loss: 2.2217 - val_acc: 0.4363\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9261 - acc: 0.4873 - val_loss: 2.2287 - val_acc: 0.4366\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9328 - acc: 0.4839 - val_loss: 2.2452 - val_acc: 0.4345\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9394 - acc: 0.4838 - val_loss: 2.2375 - val_acc: 0.4370\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9259 - acc: 0.4853 - val_loss: 2.2238 - val_acc: 0.4388\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9221 - acc: 0.4861 - val_loss: 2.2322 - val_acc: 0.4374\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9317 - acc: 0.4862 - val_loss: 2.2387 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9309 - acc: 0.4846 - val_loss: 2.2727 - val_acc: 0.4318\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9339 - acc: 0.4840 - val_loss: 2.2506 - val_acc: 0.4337\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9375 - acc: 0.4801 - val_loss: 2.2407 - val_acc: 0.4367\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9299 - acc: 0.4834 - val_loss: 2.2308 - val_acc: 0.4382\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1695 - acc: 0.4316 - val_loss: 2.1607 - val_acc: 0.4512\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1459 - acc: 0.4343 - val_loss: 2.1664 - val_acc: 0.4489\n",
      "Epoch 3/100\n",
      "2s - loss: 2.1339 - acc: 0.4387 - val_loss: 2.1970 - val_acc: 0.4477\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1525 - acc: 0.4334 - val_loss: 2.1982 - val_acc: 0.4420\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1316 - acc: 0.4374 - val_loss: 2.1904 - val_acc: 0.4450\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1107 - acc: 0.4418 - val_loss: 2.1836 - val_acc: 0.4454\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1051 - acc: 0.4422 - val_loss: 2.1863 - val_acc: 0.4434\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0986 - acc: 0.4435 - val_loss: 2.1818 - val_acc: 0.4436\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0892 - acc: 0.4470 - val_loss: 2.2381 - val_acc: 0.4414\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1343 - acc: 0.4403 - val_loss: 2.2335 - val_acc: 0.4352\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1193 - acc: 0.4403 - val_loss: 2.2189 - val_acc: 0.4366\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0885 - acc: 0.4460 - val_loss: 2.1977 - val_acc: 0.4408\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0753 - acc: 0.4502 - val_loss: 2.1971 - val_acc: 0.4395\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0714 - acc: 0.4515 - val_loss: 2.2038 - val_acc: 0.4379\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0653 - acc: 0.4517 - val_loss: 2.1995 - val_acc: 0.4402\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0615 - acc: 0.4525 - val_loss: 2.2029 - val_acc: 0.4388\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0619 - acc: 0.4520 - val_loss: 2.2054 - val_acc: 0.4406\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0578 - acc: 0.4546 - val_loss: 2.1845 - val_acc: 0.4407\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0520 - acc: 0.4541 - val_loss: 2.2102 - val_acc: 0.4425\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0649 - acc: 0.4530 - val_loss: 2.2037 - val_acc: 0.4402\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0488 - acc: 0.4554 - val_loss: 2.1941 - val_acc: 0.4409\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0407 - acc: 0.4573 - val_loss: 2.2070 - val_acc: 0.4397\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0386 - acc: 0.4588 - val_loss: 2.1811 - val_acc: 0.4442\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0334 - acc: 0.4591 - val_loss: 2.1739 - val_acc: 0.4424\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0340 - acc: 0.4571 - val_loss: 2.1831 - val_acc: 0.4416\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0303 - acc: 0.4579 - val_loss: 2.1924 - val_acc: 0.4399\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0290 - acc: 0.4608 - val_loss: 2.1868 - val_acc: 0.4437\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0313 - acc: 0.4599 - val_loss: 2.1902 - val_acc: 0.4420\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0284 - acc: 0.4582 - val_loss: 2.1789 - val_acc: 0.4450\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0219 - acc: 0.4605 - val_loss: 2.1871 - val_acc: 0.4397\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0159 - acc: 0.4640 - val_loss: 2.1854 - val_acc: 0.4436\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0138 - acc: 0.4618 - val_loss: 2.1801 - val_acc: 0.4443\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0140 - acc: 0.4641 - val_loss: 2.1679 - val_acc: 0.4459\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0107 - acc: 0.4653 - val_loss: 2.1780 - val_acc: 0.4454\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0088 - acc: 0.4638 - val_loss: 2.1703 - val_acc: 0.4472\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0039 - acc: 0.4665 - val_loss: 2.1679 - val_acc: 0.4479\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9993 - acc: 0.4656 - val_loss: 2.1766 - val_acc: 0.4460\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9974 - acc: 0.4680 - val_loss: 2.1671 - val_acc: 0.4481\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9965 - acc: 0.4659 - val_loss: 2.1668 - val_acc: 0.4473\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9930 - acc: 0.4689 - val_loss: 2.1691 - val_acc: 0.4467\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9915 - acc: 0.4673 - val_loss: 2.1646 - val_acc: 0.4483\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9923 - acc: 0.4687 - val_loss: 2.1725 - val_acc: 0.4482\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0039 - acc: 0.4640 - val_loss: 2.1831 - val_acc: 0.4408\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9920 - acc: 0.4699 - val_loss: 2.1762 - val_acc: 0.4487\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9893 - acc: 0.4692 - val_loss: 2.1666 - val_acc: 0.4461\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9848 - acc: 0.4688 - val_loss: 2.1653 - val_acc: 0.4485\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9866 - acc: 0.4697 - val_loss: 2.1930 - val_acc: 0.4418\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0015 - acc: 0.4663 - val_loss: 2.1753 - val_acc: 0.4460\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9834 - acc: 0.4686 - val_loss: 2.1748 - val_acc: 0.4457\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9782 - acc: 0.4727 - val_loss: 2.1739 - val_acc: 0.4493\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9806 - acc: 0.4716 - val_loss: 2.1669 - val_acc: 0.4457\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9797 - acc: 0.4705 - val_loss: 2.1674 - val_acc: 0.4476\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9782 - acc: 0.4719 - val_loss: 2.1760 - val_acc: 0.4432\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9833 - acc: 0.4706 - val_loss: 2.1730 - val_acc: 0.4451\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9775 - acc: 0.4702 - val_loss: 2.1648 - val_acc: 0.4481\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9716 - acc: 0.4743 - val_loss: 2.1761 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9643 - acc: 0.4737 - val_loss: 2.1593 - val_acc: 0.4484\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9655 - acc: 0.4743 - val_loss: 2.1747 - val_acc: 0.4435\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9679 - acc: 0.4730 - val_loss: 2.1687 - val_acc: 0.4447\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9570 - acc: 0.4774 - val_loss: 2.1601 - val_acc: 0.4482\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9576 - acc: 0.4750 - val_loss: 2.1699 - val_acc: 0.4475\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9524 - acc: 0.4779 - val_loss: 2.1588 - val_acc: 0.4459\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9544 - acc: 0.4777 - val_loss: 2.1814 - val_acc: 0.4460\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9624 - acc: 0.4754 - val_loss: 2.1873 - val_acc: 0.4417\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9659 - acc: 0.4753 - val_loss: 2.1749 - val_acc: 0.4459\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9610 - acc: 0.4749 - val_loss: 2.1834 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9593 - acc: 0.4753 - val_loss: 2.1718 - val_acc: 0.4474\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9599 - acc: 0.4749 - val_loss: 2.1691 - val_acc: 0.4460\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9556 - acc: 0.4766 - val_loss: 2.1626 - val_acc: 0.4500\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9423 - acc: 0.4797 - val_loss: 2.1733 - val_acc: 0.4411\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9451 - acc: 0.4784 - val_loss: 2.1621 - val_acc: 0.4486\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9419 - acc: 0.4818 - val_loss: 2.1771 - val_acc: 0.4450\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9404 - acc: 0.4787 - val_loss: 2.1628 - val_acc: 0.4502\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9330 - acc: 0.4815 - val_loss: 2.1631 - val_acc: 0.4464\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9378 - acc: 0.4809 - val_loss: 2.1735 - val_acc: 0.4477\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9432 - acc: 0.4810 - val_loss: 2.1699 - val_acc: 0.4422\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9375 - acc: 0.4800 - val_loss: 2.1840 - val_acc: 0.4448\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9510 - acc: 0.4752 - val_loss: 2.1697 - val_acc: 0.4447\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9435 - acc: 0.4794 - val_loss: 2.1760 - val_acc: 0.4474\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9372 - acc: 0.4797 - val_loss: 2.1653 - val_acc: 0.4479\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9335 - acc: 0.4838 - val_loss: 2.1696 - val_acc: 0.4456\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9314 - acc: 0.4828 - val_loss: 2.1664 - val_acc: 0.4459\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9325 - acc: 0.4820 - val_loss: 2.1760 - val_acc: 0.4435\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9201 - acc: 0.4830 - val_loss: 2.1717 - val_acc: 0.4471\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9352 - acc: 0.4827 - val_loss: 2.2106 - val_acc: 0.4388\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9431 - acc: 0.4803 - val_loss: 2.1847 - val_acc: 0.4429\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9270 - acc: 0.4830 - val_loss: 2.1858 - val_acc: 0.4419\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9247 - acc: 0.4836 - val_loss: 2.1616 - val_acc: 0.4468\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9197 - acc: 0.4845 - val_loss: 2.1828 - val_acc: 0.4427\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9223 - acc: 0.4838 - val_loss: 2.1658 - val_acc: 0.4478\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9255 - acc: 0.4831 - val_loss: 2.2153 - val_acc: 0.4410\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9592 - acc: 0.4775 - val_loss: 2.1900 - val_acc: 0.4442\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9457 - acc: 0.4780 - val_loss: 2.2011 - val_acc: 0.4430\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9284 - acc: 0.4828 - val_loss: 2.1750 - val_acc: 0.4459\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9145 - acc: 0.4855 - val_loss: 2.1854 - val_acc: 0.4424\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9149 - acc: 0.4868 - val_loss: 2.1666 - val_acc: 0.4477\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9113 - acc: 0.4859 - val_loss: 2.1706 - val_acc: 0.4437\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9073 - acc: 0.4889 - val_loss: 2.1674 - val_acc: 0.4481\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9050 - acc: 0.4888 - val_loss: 2.1902 - val_acc: 0.4422\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9043 - acc: 0.4891 - val_loss: 2.1650 - val_acc: 0.4457\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1733 - acc: 0.4289 - val_loss: 2.1888 - val_acc: 0.4422\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1540 - acc: 0.4324 - val_loss: 2.1894 - val_acc: 0.4395\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1379 - acc: 0.4357 - val_loss: 2.2043 - val_acc: 0.4389\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1386 - acc: 0.4342 - val_loss: 2.2060 - val_acc: 0.4375\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1223 - acc: 0.4395 - val_loss: 2.2061 - val_acc: 0.4387\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1111 - acc: 0.4417 - val_loss: 2.2037 - val_acc: 0.4388\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1100 - acc: 0.4389 - val_loss: 2.2012 - val_acc: 0.4415\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1078 - acc: 0.4406 - val_loss: 2.1974 - val_acc: 0.4404\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0950 - acc: 0.4440 - val_loss: 2.2049 - val_acc: 0.4380\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0884 - acc: 0.4446 - val_loss: 2.2114 - val_acc: 0.4397\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0928 - acc: 0.4436 - val_loss: 2.2119 - val_acc: 0.4393\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0834 - acc: 0.4458 - val_loss: 2.2122 - val_acc: 0.4387\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0811 - acc: 0.4444 - val_loss: 2.2099 - val_acc: 0.4363\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0719 - acc: 0.4492 - val_loss: 2.2159 - val_acc: 0.4365\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0734 - acc: 0.4493 - val_loss: 2.2138 - val_acc: 0.4392\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0677 - acc: 0.4493 - val_loss: 2.2173 - val_acc: 0.4376\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0698 - acc: 0.4491 - val_loss: 2.2208 - val_acc: 0.4378\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0873 - acc: 0.4466 - val_loss: 2.2076 - val_acc: 0.4367\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0718 - acc: 0.4495 - val_loss: 2.2285 - val_acc: 0.4348\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0624 - acc: 0.4502 - val_loss: 2.2220 - val_acc: 0.4376\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0563 - acc: 0.4527 - val_loss: 2.2106 - val_acc: 0.4395\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0485 - acc: 0.4536 - val_loss: 2.2091 - val_acc: 0.4398\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0453 - acc: 0.4546 - val_loss: 2.2231 - val_acc: 0.4349\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0477 - acc: 0.4539 - val_loss: 2.2171 - val_acc: 0.4378\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0618 - acc: 0.4499 - val_loss: 2.2322 - val_acc: 0.4369\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0540 - acc: 0.4537 - val_loss: 2.2068 - val_acc: 0.4399\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0336 - acc: 0.4575 - val_loss: 2.1965 - val_acc: 0.4430\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0358 - acc: 0.4562 - val_loss: 2.2021 - val_acc: 0.4381\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0294 - acc: 0.4579 - val_loss: 2.2048 - val_acc: 0.4387\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0353 - acc: 0.4579 - val_loss: 2.2042 - val_acc: 0.4408\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0349 - acc: 0.4567 - val_loss: 2.2051 - val_acc: 0.4423\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0403 - acc: 0.4580 - val_loss: 2.1992 - val_acc: 0.4426\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0228 - acc: 0.4600 - val_loss: 2.1929 - val_acc: 0.4424\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0177 - acc: 0.4606 - val_loss: 2.1990 - val_acc: 0.4419\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0249 - acc: 0.4591 - val_loss: 2.2164 - val_acc: 0.4378\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0175 - acc: 0.4616 - val_loss: 2.1947 - val_acc: 0.4410\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0146 - acc: 0.4633 - val_loss: 2.1879 - val_acc: 0.4423\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0080 - acc: 0.4618 - val_loss: 2.1909 - val_acc: 0.4457\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0167 - acc: 0.4615 - val_loss: 2.1922 - val_acc: 0.4412\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0030 - acc: 0.4649 - val_loss: 2.1997 - val_acc: 0.4421\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0265 - acc: 0.4613 - val_loss: 2.2054 - val_acc: 0.4407\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0233 - acc: 0.4609 - val_loss: 2.2205 - val_acc: 0.4369\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0207 - acc: 0.4579 - val_loss: 2.1971 - val_acc: 0.4419\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0015 - acc: 0.4631 - val_loss: 2.1996 - val_acc: 0.4397\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9926 - acc: 0.4669 - val_loss: 2.1996 - val_acc: 0.4420\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9955 - acc: 0.4654 - val_loss: 2.1942 - val_acc: 0.4439\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0002 - acc: 0.4657 - val_loss: 2.1785 - val_acc: 0.4470\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9852 - acc: 0.4694 - val_loss: 2.1879 - val_acc: 0.4436\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9927 - acc: 0.4676 - val_loss: 2.2033 - val_acc: 0.4439\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0183 - acc: 0.4613 - val_loss: 2.2005 - val_acc: 0.4435\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9934 - acc: 0.4650 - val_loss: 2.2087 - val_acc: 0.4383\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0094 - acc: 0.4611 - val_loss: 2.1943 - val_acc: 0.4472\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9981 - acc: 0.4655 - val_loss: 2.2061 - val_acc: 0.4450\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0149 - acc: 0.4624 - val_loss: 2.1987 - val_acc: 0.4444\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0048 - acc: 0.4639 - val_loss: 2.2073 - val_acc: 0.4417\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9870 - acc: 0.4690 - val_loss: 2.1910 - val_acc: 0.4472\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0002 - acc: 0.4649 - val_loss: 2.2074 - val_acc: 0.4430\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9838 - acc: 0.4676 - val_loss: 2.1844 - val_acc: 0.4464\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9783 - acc: 0.4706 - val_loss: 2.1981 - val_acc: 0.4427\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9698 - acc: 0.4711 - val_loss: 2.1926 - val_acc: 0.4444\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9658 - acc: 0.4713 - val_loss: 2.1936 - val_acc: 0.4449\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9673 - acc: 0.4719 - val_loss: 2.2121 - val_acc: 0.4406\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9696 - acc: 0.4717 - val_loss: 2.1985 - val_acc: 0.4444\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9778 - acc: 0.4703 - val_loss: 2.2392 - val_acc: 0.4378\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9684 - acc: 0.4730 - val_loss: 2.2039 - val_acc: 0.4452\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9820 - acc: 0.4680 - val_loss: 2.2169 - val_acc: 0.4390\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9840 - acc: 0.4676 - val_loss: 2.2145 - val_acc: 0.4431\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9867 - acc: 0.4657 - val_loss: 2.2024 - val_acc: 0.4444\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9685 - acc: 0.4720 - val_loss: 2.1954 - val_acc: 0.4463\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9584 - acc: 0.4757 - val_loss: 2.1937 - val_acc: 0.4468\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9450 - acc: 0.4779 - val_loss: 2.1939 - val_acc: 0.4442\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9443 - acc: 0.4754 - val_loss: 2.2016 - val_acc: 0.4418\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9582 - acc: 0.4720 - val_loss: 2.1880 - val_acc: 0.4457\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9495 - acc: 0.4750 - val_loss: 2.1952 - val_acc: 0.4440\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9566 - acc: 0.4756 - val_loss: 2.2024 - val_acc: 0.4453\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9696 - acc: 0.4720 - val_loss: 2.2101 - val_acc: 0.4422\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9485 - acc: 0.4760 - val_loss: 2.2014 - val_acc: 0.4433\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9582 - acc: 0.4748 - val_loss: 2.2153 - val_acc: 0.4433\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9773 - acc: 0.4691 - val_loss: 2.2134 - val_acc: 0.4413\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9505 - acc: 0.4773 - val_loss: 2.2110 - val_acc: 0.4445\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9645 - acc: 0.4735 - val_loss: 2.2269 - val_acc: 0.4395\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9538 - acc: 0.4754 - val_loss: 2.2036 - val_acc: 0.4433\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9637 - acc: 0.4728 - val_loss: 2.2089 - val_acc: 0.4445\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9387 - acc: 0.4795 - val_loss: 2.1906 - val_acc: 0.4458\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9417 - acc: 0.4786 - val_loss: 2.1899 - val_acc: 0.4450\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9254 - acc: 0.4795 - val_loss: 2.1901 - val_acc: 0.4430\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9253 - acc: 0.4826 - val_loss: 2.1994 - val_acc: 0.4431\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9261 - acc: 0.4820 - val_loss: 2.1957 - val_acc: 0.4457\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9268 - acc: 0.4814 - val_loss: 2.2099 - val_acc: 0.4414\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9301 - acc: 0.4803 - val_loss: 2.2139 - val_acc: 0.4433\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9672 - acc: 0.4714 - val_loss: 2.2380 - val_acc: 0.4376\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9505 - acc: 0.4754 - val_loss: 2.2117 - val_acc: 0.4404\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9449 - acc: 0.4765 - val_loss: 2.2028 - val_acc: 0.4431\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9325 - acc: 0.4793 - val_loss: 2.1960 - val_acc: 0.4437\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9297 - acc: 0.4798 - val_loss: 2.2139 - val_acc: 0.4411\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9220 - acc: 0.4822 - val_loss: 2.2038 - val_acc: 0.4421\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9333 - acc: 0.4788 - val_loss: 2.2029 - val_acc: 0.4461\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9449 - acc: 0.4766 - val_loss: 2.2207 - val_acc: 0.4367\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9292 - acc: 0.4804 - val_loss: 2.2013 - val_acc: 0.4446\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9341 - acc: 0.4793 - val_loss: 2.2127 - val_acc: 0.4400\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2168 - acc: 0.4234 - val_loss: 2.2369 - val_acc: 0.4319\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1762 - acc: 0.4318 - val_loss: 2.2336 - val_acc: 0.4326\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1658 - acc: 0.4324 - val_loss: 2.2463 - val_acc: 0.4291\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1541 - acc: 0.4346 - val_loss: 2.2426 - val_acc: 0.4313\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1424 - acc: 0.4383 - val_loss: 2.2509 - val_acc: 0.4307\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1392 - acc: 0.4375 - val_loss: 2.2593 - val_acc: 0.4284\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1355 - acc: 0.4384 - val_loss: 2.2523 - val_acc: 0.4289\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1256 - acc: 0.4417 - val_loss: 2.2726 - val_acc: 0.4281\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1388 - acc: 0.4394 - val_loss: 2.2630 - val_acc: 0.4273\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1215 - acc: 0.4415 - val_loss: 2.2569 - val_acc: 0.4299\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1121 - acc: 0.4447 - val_loss: 2.2586 - val_acc: 0.4284\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1081 - acc: 0.4425 - val_loss: 2.2552 - val_acc: 0.4307\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0982 - acc: 0.4483 - val_loss: 2.2487 - val_acc: 0.4286\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0972 - acc: 0.4457 - val_loss: 2.2519 - val_acc: 0.4310\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0958 - acc: 0.4465 - val_loss: 2.2615 - val_acc: 0.4280\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1015 - acc: 0.4465 - val_loss: 2.2481 - val_acc: 0.4306\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0849 - acc: 0.4490 - val_loss: 2.2449 - val_acc: 0.4307\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0863 - acc: 0.4495 - val_loss: 2.2460 - val_acc: 0.4323\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0784 - acc: 0.4511 - val_loss: 2.2475 - val_acc: 0.4294\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0780 - acc: 0.4506 - val_loss: 2.2562 - val_acc: 0.4301\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0830 - acc: 0.4500 - val_loss: 2.2478 - val_acc: 0.4295\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0718 - acc: 0.4527 - val_loss: 2.2384 - val_acc: 0.4313\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0677 - acc: 0.4538 - val_loss: 2.2610 - val_acc: 0.4303\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0718 - acc: 0.4527 - val_loss: 2.2546 - val_acc: 0.4267\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0629 - acc: 0.4536 - val_loss: 2.2589 - val_acc: 0.4282\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0612 - acc: 0.4544 - val_loss: 2.2495 - val_acc: 0.4260\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0618 - acc: 0.4558 - val_loss: 2.2318 - val_acc: 0.4328\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0512 - acc: 0.4568 - val_loss: 2.2525 - val_acc: 0.4279\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0564 - acc: 0.4554 - val_loss: 2.2423 - val_acc: 0.4293\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0490 - acc: 0.4568 - val_loss: 2.2383 - val_acc: 0.4309\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0455 - acc: 0.4569 - val_loss: 2.2376 - val_acc: 0.4293\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0457 - acc: 0.4587 - val_loss: 2.2603 - val_acc: 0.4268\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0516 - acc: 0.4577 - val_loss: 2.2423 - val_acc: 0.4311\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0464 - acc: 0.4559 - val_loss: 2.2480 - val_acc: 0.4270\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0419 - acc: 0.4582 - val_loss: 2.2409 - val_acc: 0.4307\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0411 - acc: 0.4598 - val_loss: 2.2424 - val_acc: 0.4308\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0358 - acc: 0.4599 - val_loss: 2.2282 - val_acc: 0.4308\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0352 - acc: 0.4587 - val_loss: 2.2413 - val_acc: 0.4292\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0286 - acc: 0.4630 - val_loss: 2.2320 - val_acc: 0.4328\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0283 - acc: 0.4607 - val_loss: 2.2528 - val_acc: 0.4292\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0304 - acc: 0.4617 - val_loss: 2.2353 - val_acc: 0.4318\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0259 - acc: 0.4626 - val_loss: 2.2537 - val_acc: 0.4297\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0248 - acc: 0.4618 - val_loss: 2.2525 - val_acc: 0.4319\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0265 - acc: 0.4629 - val_loss: 2.2357 - val_acc: 0.4297\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0157 - acc: 0.4642 - val_loss: 2.2281 - val_acc: 0.4315\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0150 - acc: 0.4636 - val_loss: 2.2628 - val_acc: 0.4246\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0390 - acc: 0.4597 - val_loss: 2.2442 - val_acc: 0.4312\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0163 - acc: 0.4634 - val_loss: 2.2389 - val_acc: 0.4293\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0201 - acc: 0.4619 - val_loss: 2.2728 - val_acc: 0.4275\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0210 - acc: 0.4636 - val_loss: 2.2417 - val_acc: 0.4314\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0111 - acc: 0.4663 - val_loss: 2.2327 - val_acc: 0.4314\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0054 - acc: 0.4680 - val_loss: 2.2422 - val_acc: 0.4301\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0051 - acc: 0.4660 - val_loss: 2.2343 - val_acc: 0.4323\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0018 - acc: 0.4685 - val_loss: 2.2253 - val_acc: 0.4305\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9984 - acc: 0.4687 - val_loss: 2.2360 - val_acc: 0.4328\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0010 - acc: 0.4686 - val_loss: 2.2420 - val_acc: 0.4287\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9994 - acc: 0.4673 - val_loss: 2.2455 - val_acc: 0.4316\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9913 - acc: 0.4707 - val_loss: 2.2411 - val_acc: 0.4282\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9899 - acc: 0.4700 - val_loss: 2.2477 - val_acc: 0.4309\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0015 - acc: 0.4675 - val_loss: 2.2710 - val_acc: 0.4228\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0058 - acc: 0.4665 - val_loss: 2.2562 - val_acc: 0.4297\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0105 - acc: 0.4658 - val_loss: 2.2567 - val_acc: 0.4260\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0037 - acc: 0.4656 - val_loss: 2.2379 - val_acc: 0.4323\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9828 - acc: 0.4717 - val_loss: 2.2402 - val_acc: 0.4302\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9848 - acc: 0.4724 - val_loss: 2.2410 - val_acc: 0.4291\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9791 - acc: 0.4716 - val_loss: 2.2377 - val_acc: 0.4294\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9773 - acc: 0.4723 - val_loss: 2.2351 - val_acc: 0.4320\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9730 - acc: 0.4740 - val_loss: 2.2466 - val_acc: 0.4279\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9777 - acc: 0.4724 - val_loss: 2.2582 - val_acc: 0.4312\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9916 - acc: 0.4707 - val_loss: 2.2704 - val_acc: 0.4215\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9793 - acc: 0.4720 - val_loss: 2.2440 - val_acc: 0.4313\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9701 - acc: 0.4750 - val_loss: 2.2395 - val_acc: 0.4281\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9773 - acc: 0.4740 - val_loss: 2.2429 - val_acc: 0.4331\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9676 - acc: 0.4740 - val_loss: 2.2248 - val_acc: 0.4319\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9677 - acc: 0.4764 - val_loss: 2.2481 - val_acc: 0.4297\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9749 - acc: 0.4754 - val_loss: 2.2301 - val_acc: 0.4311\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9794 - acc: 0.4736 - val_loss: 2.2424 - val_acc: 0.4279\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9653 - acc: 0.4743 - val_loss: 2.2433 - val_acc: 0.4293\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9599 - acc: 0.4774 - val_loss: 2.2438 - val_acc: 0.4283\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9623 - acc: 0.4777 - val_loss: 2.2344 - val_acc: 0.4308\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9588 - acc: 0.4766 - val_loss: 2.2451 - val_acc: 0.4278\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9617 - acc: 0.4771 - val_loss: 2.2619 - val_acc: 0.4283\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9660 - acc: 0.4751 - val_loss: 2.2812 - val_acc: 0.4219\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9789 - acc: 0.4731 - val_loss: 2.3095 - val_acc: 0.4220\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9854 - acc: 0.4717 - val_loss: 2.2414 - val_acc: 0.4267\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9582 - acc: 0.4771 - val_loss: 2.2669 - val_acc: 0.4262\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9654 - acc: 0.4769 - val_loss: 2.2433 - val_acc: 0.4267\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9573 - acc: 0.4761 - val_loss: 2.2521 - val_acc: 0.4289\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9535 - acc: 0.4781 - val_loss: 2.2350 - val_acc: 0.4298\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9475 - acc: 0.4802 - val_loss: 2.2496 - val_acc: 0.4297\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9490 - acc: 0.4806 - val_loss: 2.2371 - val_acc: 0.4278\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9543 - acc: 0.4787 - val_loss: 2.2657 - val_acc: 0.4293\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9514 - acc: 0.4776 - val_loss: 2.2324 - val_acc: 0.4307\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9468 - acc: 0.4793 - val_loss: 2.2585 - val_acc: 0.4259\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9473 - acc: 0.4805 - val_loss: 2.2379 - val_acc: 0.4302\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9442 - acc: 0.4817 - val_loss: 2.2692 - val_acc: 0.4253\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9601 - acc: 0.4773 - val_loss: 2.2508 - val_acc: 0.4286\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9410 - acc: 0.4818 - val_loss: 2.2748 - val_acc: 0.4222\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9406 - acc: 0.4802 - val_loss: 2.2503 - val_acc: 0.4295\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9388 - acc: 0.4824 - val_loss: 2.2574 - val_acc: 0.4256\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1946 - acc: 0.4283 - val_loss: 2.1600 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1712 - acc: 0.4319 - val_loss: 2.1701 - val_acc: 0.4457\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1653 - acc: 0.4344 - val_loss: 2.1715 - val_acc: 0.4459\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1509 - acc: 0.4355 - val_loss: 2.1561 - val_acc: 0.4458\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1450 - acc: 0.4385 - val_loss: 2.1510 - val_acc: 0.4489\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1387 - acc: 0.4386 - val_loss: 2.1578 - val_acc: 0.4466\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1288 - acc: 0.4413 - val_loss: 2.1550 - val_acc: 0.4490\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1229 - acc: 0.4417 - val_loss: 2.1790 - val_acc: 0.4418\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1222 - acc: 0.4403 - val_loss: 2.1725 - val_acc: 0.4459\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1118 - acc: 0.4433 - val_loss: 2.1613 - val_acc: 0.4466\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1222 - acc: 0.4405 - val_loss: 2.1632 - val_acc: 0.4469\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1075 - acc: 0.4446 - val_loss: 2.1734 - val_acc: 0.4455\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1021 - acc: 0.4449 - val_loss: 2.1664 - val_acc: 0.4471\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1017 - acc: 0.4454 - val_loss: 2.1736 - val_acc: 0.4454\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1034 - acc: 0.4465 - val_loss: 2.1741 - val_acc: 0.4441\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0913 - acc: 0.4482 - val_loss: 2.1681 - val_acc: 0.4475\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0841 - acc: 0.4508 - val_loss: 2.1652 - val_acc: 0.4463\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0825 - acc: 0.4492 - val_loss: 2.1647 - val_acc: 0.4465\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0784 - acc: 0.4497 - val_loss: 2.1840 - val_acc: 0.4455\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0975 - acc: 0.4458 - val_loss: 2.1658 - val_acc: 0.4467\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0706 - acc: 0.4515 - val_loss: 2.1771 - val_acc: 0.4443\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0699 - acc: 0.4524 - val_loss: 2.1650 - val_acc: 0.4449\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0675 - acc: 0.4536 - val_loss: 2.1703 - val_acc: 0.4472\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0683 - acc: 0.4526 - val_loss: 2.1571 - val_acc: 0.4482\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0682 - acc: 0.4517 - val_loss: 2.1733 - val_acc: 0.4454\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0624 - acc: 0.4558 - val_loss: 2.1655 - val_acc: 0.4479\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0565 - acc: 0.4568 - val_loss: 2.1705 - val_acc: 0.4476\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0556 - acc: 0.4550 - val_loss: 2.1580 - val_acc: 0.4489\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0493 - acc: 0.4573 - val_loss: 2.1511 - val_acc: 0.4498\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0479 - acc: 0.4586 - val_loss: 2.1702 - val_acc: 0.4458\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0600 - acc: 0.4553 - val_loss: 2.1630 - val_acc: 0.4488\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0415 - acc: 0.4588 - val_loss: 2.1562 - val_acc: 0.4499\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0384 - acc: 0.4588 - val_loss: 2.1485 - val_acc: 0.4513\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0387 - acc: 0.4609 - val_loss: 2.1519 - val_acc: 0.4486\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0357 - acc: 0.4591 - val_loss: 2.1537 - val_acc: 0.4488\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0277 - acc: 0.4608 - val_loss: 2.1575 - val_acc: 0.4498\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0510 - acc: 0.4572 - val_loss: 2.1580 - val_acc: 0.4490\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0411 - acc: 0.4579 - val_loss: 2.1486 - val_acc: 0.4500\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0295 - acc: 0.4624 - val_loss: 2.1575 - val_acc: 0.4492\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0200 - acc: 0.4618 - val_loss: 2.1496 - val_acc: 0.4497\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0194 - acc: 0.4630 - val_loss: 2.1473 - val_acc: 0.4507\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0238 - acc: 0.4627 - val_loss: 2.1560 - val_acc: 0.4491\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0255 - acc: 0.4624 - val_loss: 2.1542 - val_acc: 0.4497\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0192 - acc: 0.4642 - val_loss: 2.1522 - val_acc: 0.4504\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0284 - acc: 0.4612 - val_loss: 2.1583 - val_acc: 0.4489\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0270 - acc: 0.4601 - val_loss: 2.1575 - val_acc: 0.4490\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0489 - acc: 0.4567 - val_loss: 2.1494 - val_acc: 0.4490\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0147 - acc: 0.4661 - val_loss: 2.1420 - val_acc: 0.4519\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0106 - acc: 0.4633 - val_loss: 2.1453 - val_acc: 0.4499\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0082 - acc: 0.4656 - val_loss: 2.1501 - val_acc: 0.4498\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0280 - acc: 0.4619 - val_loss: 2.1704 - val_acc: 0.4461\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0224 - acc: 0.4602 - val_loss: 2.1558 - val_acc: 0.4483\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0072 - acc: 0.4667 - val_loss: 2.1479 - val_acc: 0.4496\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9975 - acc: 0.4687 - val_loss: 2.1522 - val_acc: 0.4498\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0033 - acc: 0.4678 - val_loss: 2.1525 - val_acc: 0.4470\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9918 - acc: 0.4699 - val_loss: 2.1370 - val_acc: 0.4502\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9899 - acc: 0.4712 - val_loss: 2.1536 - val_acc: 0.4482\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9972 - acc: 0.4692 - val_loss: 2.1430 - val_acc: 0.4503\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9992 - acc: 0.4689 - val_loss: 2.1691 - val_acc: 0.4451\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0024 - acc: 0.4676 - val_loss: 2.1503 - val_acc: 0.4500\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9942 - acc: 0.4690 - val_loss: 2.1500 - val_acc: 0.4508\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9927 - acc: 0.4691 - val_loss: 2.1768 - val_acc: 0.4437\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0029 - acc: 0.4660 - val_loss: 2.1519 - val_acc: 0.4488\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9893 - acc: 0.4685 - val_loss: 2.1453 - val_acc: 0.4474\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9828 - acc: 0.4722 - val_loss: 2.1400 - val_acc: 0.4517\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9821 - acc: 0.4709 - val_loss: 2.1616 - val_acc: 0.4475\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9982 - acc: 0.4691 - val_loss: 2.1688 - val_acc: 0.4463\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9868 - acc: 0.4721 - val_loss: 2.1729 - val_acc: 0.4454\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9782 - acc: 0.4742 - val_loss: 2.1670 - val_acc: 0.4463\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9896 - acc: 0.4707 - val_loss: 2.1833 - val_acc: 0.4437\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9825 - acc: 0.4701 - val_loss: 2.1626 - val_acc: 0.4470\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9770 - acc: 0.4713 - val_loss: 2.1739 - val_acc: 0.4460\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9695 - acc: 0.4725 - val_loss: 2.1496 - val_acc: 0.4503\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9807 - acc: 0.4723 - val_loss: 2.1451 - val_acc: 0.4496\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9653 - acc: 0.4755 - val_loss: 2.1445 - val_acc: 0.4523\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9673 - acc: 0.4767 - val_loss: 2.1632 - val_acc: 0.4486\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9899 - acc: 0.4674 - val_loss: 2.1551 - val_acc: 0.4490\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9740 - acc: 0.4716 - val_loss: 2.1545 - val_acc: 0.4479\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9865 - acc: 0.4717 - val_loss: 2.2017 - val_acc: 0.4385\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0140 - acc: 0.4657 - val_loss: 2.1728 - val_acc: 0.4461\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9856 - acc: 0.4723 - val_loss: 2.1725 - val_acc: 0.4451\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9744 - acc: 0.4738 - val_loss: 2.1527 - val_acc: 0.4512\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9654 - acc: 0.4768 - val_loss: 2.1584 - val_acc: 0.4497\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9636 - acc: 0.4757 - val_loss: 2.1486 - val_acc: 0.4517\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9530 - acc: 0.4780 - val_loss: 2.1595 - val_acc: 0.4489\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9561 - acc: 0.4777 - val_loss: 2.1583 - val_acc: 0.4508\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9594 - acc: 0.4765 - val_loss: 2.1717 - val_acc: 0.4481\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9679 - acc: 0.4748 - val_loss: 2.1524 - val_acc: 0.4512\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9516 - acc: 0.4794 - val_loss: 2.1633 - val_acc: 0.4478\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9794 - acc: 0.4732 - val_loss: 2.2049 - val_acc: 0.4430\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9867 - acc: 0.4702 - val_loss: 2.1818 - val_acc: 0.4462\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9890 - acc: 0.4696 - val_loss: 2.1548 - val_acc: 0.4499\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9368 - acc: 0.4824 - val_loss: 2.1611 - val_acc: 0.4488\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9505 - acc: 0.4790 - val_loss: 2.1570 - val_acc: 0.4468\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9386 - acc: 0.4826 - val_loss: 2.1600 - val_acc: 0.4493\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9443 - acc: 0.4799 - val_loss: 2.1878 - val_acc: 0.4473\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9465 - acc: 0.4810 - val_loss: 2.1614 - val_acc: 0.4485\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9372 - acc: 0.4822 - val_loss: 2.1819 - val_acc: 0.4440\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9395 - acc: 0.4825 - val_loss: 2.1599 - val_acc: 0.4501\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9432 - acc: 0.4788 - val_loss: 2.1694 - val_acc: 0.4468\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2307 - acc: 0.4200 - val_loss: 2.2112 - val_acc: 0.4400\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1915 - acc: 0.4277 - val_loss: 2.2164 - val_acc: 0.4384\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1825 - acc: 0.4263 - val_loss: 2.2308 - val_acc: 0.4355\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1715 - acc: 0.4296 - val_loss: 2.2355 - val_acc: 0.4359\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1673 - acc: 0.4302 - val_loss: 2.2289 - val_acc: 0.4357\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1538 - acc: 0.4336 - val_loss: 2.2346 - val_acc: 0.4351\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1467 - acc: 0.4354 - val_loss: 2.2431 - val_acc: 0.4344\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1447 - acc: 0.4365 - val_loss: 2.2441 - val_acc: 0.4331\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1352 - acc: 0.4367 - val_loss: 2.2366 - val_acc: 0.4370\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1336 - acc: 0.4387 - val_loss: 2.2494 - val_acc: 0.4356\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1246 - acc: 0.4387 - val_loss: 2.2423 - val_acc: 0.4344\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1219 - acc: 0.4396 - val_loss: 2.2443 - val_acc: 0.4341\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1182 - acc: 0.4408 - val_loss: 2.2516 - val_acc: 0.4306\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1147 - acc: 0.4402 - val_loss: 2.2623 - val_acc: 0.4286\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1122 - acc: 0.4432 - val_loss: 2.2644 - val_acc: 0.4308\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1071 - acc: 0.4449 - val_loss: 2.2471 - val_acc: 0.4333\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1069 - acc: 0.4437 - val_loss: 2.2505 - val_acc: 0.4351\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1036 - acc: 0.4451 - val_loss: 2.2513 - val_acc: 0.4323\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0971 - acc: 0.4450 - val_loss: 2.2562 - val_acc: 0.4334\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0889 - acc: 0.4464 - val_loss: 2.2548 - val_acc: 0.4318\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0920 - acc: 0.4462 - val_loss: 2.2622 - val_acc: 0.4313\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0916 - acc: 0.4474 - val_loss: 2.2518 - val_acc: 0.4332\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0857 - acc: 0.4472 - val_loss: 2.2568 - val_acc: 0.4323\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0799 - acc: 0.4478 - val_loss: 2.2411 - val_acc: 0.4341\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0766 - acc: 0.4491 - val_loss: 2.2495 - val_acc: 0.4326\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0723 - acc: 0.4509 - val_loss: 2.2469 - val_acc: 0.4330\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0707 - acc: 0.4503 - val_loss: 2.2615 - val_acc: 0.4320\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0729 - acc: 0.4494 - val_loss: 2.2541 - val_acc: 0.4305\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0669 - acc: 0.4517 - val_loss: 2.2492 - val_acc: 0.4337\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0664 - acc: 0.4511 - val_loss: 2.2423 - val_acc: 0.4327\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0589 - acc: 0.4534 - val_loss: 2.2352 - val_acc: 0.4367\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0565 - acc: 0.4535 - val_loss: 2.2536 - val_acc: 0.4341\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0557 - acc: 0.4526 - val_loss: 2.2478 - val_acc: 0.4320\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0570 - acc: 0.4542 - val_loss: 2.2399 - val_acc: 0.4348\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0508 - acc: 0.4545 - val_loss: 2.2502 - val_acc: 0.4311\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0467 - acc: 0.4553 - val_loss: 2.2394 - val_acc: 0.4375\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0511 - acc: 0.4561 - val_loss: 2.2435 - val_acc: 0.4332\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0458 - acc: 0.4567 - val_loss: 2.2484 - val_acc: 0.4365\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0525 - acc: 0.4540 - val_loss: 2.2527 - val_acc: 0.4333\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0486 - acc: 0.4567 - val_loss: 2.2435 - val_acc: 0.4377\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0408 - acc: 0.4564 - val_loss: 2.2612 - val_acc: 0.4322\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0424 - acc: 0.4579 - val_loss: 2.2462 - val_acc: 0.4340\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0429 - acc: 0.4580 - val_loss: 2.2502 - val_acc: 0.4341\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0411 - acc: 0.4590 - val_loss: 2.2323 - val_acc: 0.4402\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0321 - acc: 0.4597 - val_loss: 2.2306 - val_acc: 0.4392\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0256 - acc: 0.4609 - val_loss: 2.2262 - val_acc: 0.4403\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0233 - acc: 0.4590 - val_loss: 2.2352 - val_acc: 0.4346\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0254 - acc: 0.4617 - val_loss: 2.2342 - val_acc: 0.4370\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0266 - acc: 0.4619 - val_loss: 2.2300 - val_acc: 0.4369\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0216 - acc: 0.4627 - val_loss: 2.2234 - val_acc: 0.4393\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0274 - acc: 0.4599 - val_loss: 2.2449 - val_acc: 0.4323\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0186 - acc: 0.4601 - val_loss: 2.2301 - val_acc: 0.4370\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0208 - acc: 0.4633 - val_loss: 2.2575 - val_acc: 0.4328\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0244 - acc: 0.4608 - val_loss: 2.2299 - val_acc: 0.4382\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0150 - acc: 0.4634 - val_loss: 2.2675 - val_acc: 0.4364\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0388 - acc: 0.4587 - val_loss: 2.2374 - val_acc: 0.4368\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0179 - acc: 0.4624 - val_loss: 2.2378 - val_acc: 0.4399\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0169 - acc: 0.4629 - val_loss: 2.2483 - val_acc: 0.4332\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0132 - acc: 0.4648 - val_loss: 2.2305 - val_acc: 0.4408\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0111 - acc: 0.4658 - val_loss: 2.2304 - val_acc: 0.4372\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0089 - acc: 0.4657 - val_loss: 2.2263 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0017 - acc: 0.4663 - val_loss: 2.2276 - val_acc: 0.4369\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0073 - acc: 0.4640 - val_loss: 2.2300 - val_acc: 0.4409\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9993 - acc: 0.4672 - val_loss: 2.2299 - val_acc: 0.4392\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9970 - acc: 0.4690 - val_loss: 2.2419 - val_acc: 0.4384\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0251 - acc: 0.4635 - val_loss: 2.2556 - val_acc: 0.4307\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0023 - acc: 0.4670 - val_loss: 2.2230 - val_acc: 0.4406\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9919 - acc: 0.4684 - val_loss: 2.2484 - val_acc: 0.4344\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9953 - acc: 0.4689 - val_loss: 2.2206 - val_acc: 0.4410\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9893 - acc: 0.4678 - val_loss: 2.2331 - val_acc: 0.4358\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9933 - acc: 0.4686 - val_loss: 2.2225 - val_acc: 0.4408\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9884 - acc: 0.4685 - val_loss: 2.2361 - val_acc: 0.4382\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9843 - acc: 0.4705 - val_loss: 2.2182 - val_acc: 0.4422\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9831 - acc: 0.4704 - val_loss: 2.2237 - val_acc: 0.4408\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9793 - acc: 0.4699 - val_loss: 2.2147 - val_acc: 0.4416\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9803 - acc: 0.4704 - val_loss: 2.2367 - val_acc: 0.4383\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9721 - acc: 0.4713 - val_loss: 2.2199 - val_acc: 0.4400\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9764 - acc: 0.4712 - val_loss: 2.2440 - val_acc: 0.4362\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9756 - acc: 0.4717 - val_loss: 2.2322 - val_acc: 0.4379\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9765 - acc: 0.4722 - val_loss: 2.2376 - val_acc: 0.4383\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9744 - acc: 0.4726 - val_loss: 2.2239 - val_acc: 0.4380\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9811 - acc: 0.4698 - val_loss: 2.2511 - val_acc: 0.4396\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9879 - acc: 0.4680 - val_loss: 2.2431 - val_acc: 0.4360\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9799 - acc: 0.4724 - val_loss: 2.2351 - val_acc: 0.4392\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9734 - acc: 0.4727 - val_loss: 2.2254 - val_acc: 0.4390\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9702 - acc: 0.4761 - val_loss: 2.2388 - val_acc: 0.4410\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9752 - acc: 0.4740 - val_loss: 2.2476 - val_acc: 0.4368\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9801 - acc: 0.4712 - val_loss: 2.2416 - val_acc: 0.4393\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9738 - acc: 0.4736 - val_loss: 2.2237 - val_acc: 0.4403\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9587 - acc: 0.4763 - val_loss: 2.2310 - val_acc: 0.4406\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9672 - acc: 0.4752 - val_loss: 2.2272 - val_acc: 0.4402\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9604 - acc: 0.4763 - val_loss: 2.2397 - val_acc: 0.4397\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9658 - acc: 0.4745 - val_loss: 2.2391 - val_acc: 0.4400\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9595 - acc: 0.4763 - val_loss: 2.2438 - val_acc: 0.4391\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9617 - acc: 0.4749 - val_loss: 2.2302 - val_acc: 0.4384\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9652 - acc: 0.4745 - val_loss: 2.2363 - val_acc: 0.4385\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9597 - acc: 0.4751 - val_loss: 2.2190 - val_acc: 0.4401\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9403 - acc: 0.4816 - val_loss: 2.2187 - val_acc: 0.4444\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9432 - acc: 0.4805 - val_loss: 2.2134 - val_acc: 0.4407\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9410 - acc: 0.4812 - val_loss: 2.2142 - val_acc: 0.4421\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2230 - acc: 0.4220 - val_loss: 2.1873 - val_acc: 0.4425\n",
      "Epoch 2/100\n",
      "1s - loss: 2.2021 - acc: 0.4246 - val_loss: 2.1957 - val_acc: 0.4400\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1902 - acc: 0.4276 - val_loss: 2.2090 - val_acc: 0.4374\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1817 - acc: 0.4291 - val_loss: 2.2153 - val_acc: 0.4371\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1720 - acc: 0.4305 - val_loss: 2.2193 - val_acc: 0.4359\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1643 - acc: 0.4324 - val_loss: 2.2244 - val_acc: 0.4361\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1547 - acc: 0.4355 - val_loss: 2.2222 - val_acc: 0.4337\n",
      "Epoch 8/100\n",
      "2s - loss: 2.1508 - acc: 0.4339 - val_loss: 2.2143 - val_acc: 0.4363\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1475 - acc: 0.4366 - val_loss: 2.2256 - val_acc: 0.4353\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1460 - acc: 0.4351 - val_loss: 2.2292 - val_acc: 0.4329\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1343 - acc: 0.4368 - val_loss: 2.2144 - val_acc: 0.4362\n",
      "Epoch 12/100\n",
      "2s - loss: 2.1311 - acc: 0.4390 - val_loss: 2.2179 - val_acc: 0.4356\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1277 - acc: 0.4397 - val_loss: 2.2222 - val_acc: 0.4341\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1229 - acc: 0.4416 - val_loss: 2.2226 - val_acc: 0.4342\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1267 - acc: 0.4415 - val_loss: 2.2395 - val_acc: 0.4334\n",
      "Epoch 16/100\n",
      "2s - loss: 2.1302 - acc: 0.4400 - val_loss: 2.2197 - val_acc: 0.4376\n",
      "Epoch 17/100\n",
      "2s - loss: 2.1118 - acc: 0.4422 - val_loss: 2.2279 - val_acc: 0.4327\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1118 - acc: 0.4432 - val_loss: 2.2212 - val_acc: 0.4356\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1043 - acc: 0.4446 - val_loss: 2.2157 - val_acc: 0.4370\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1082 - acc: 0.4439 - val_loss: 2.2177 - val_acc: 0.4376\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0973 - acc: 0.4470 - val_loss: 2.2109 - val_acc: 0.4405\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0937 - acc: 0.4460 - val_loss: 2.2120 - val_acc: 0.4369\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0970 - acc: 0.4484 - val_loss: 2.2223 - val_acc: 0.4354\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0916 - acc: 0.4489 - val_loss: 2.2182 - val_acc: 0.4382\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0881 - acc: 0.4477 - val_loss: 2.2094 - val_acc: 0.4395\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0897 - acc: 0.4452 - val_loss: 2.2062 - val_acc: 0.4405\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0793 - acc: 0.4506 - val_loss: 2.2075 - val_acc: 0.4384\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0781 - acc: 0.4512 - val_loss: 2.2114 - val_acc: 0.4372\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0793 - acc: 0.4523 - val_loss: 2.2096 - val_acc: 0.4388\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0721 - acc: 0.4507 - val_loss: 2.2102 - val_acc: 0.4379\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0743 - acc: 0.4527 - val_loss: 2.1999 - val_acc: 0.4395\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0684 - acc: 0.4530 - val_loss: 2.2051 - val_acc: 0.4413\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0778 - acc: 0.4491 - val_loss: 2.2092 - val_acc: 0.4377\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0741 - acc: 0.4517 - val_loss: 2.2110 - val_acc: 0.4361\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0673 - acc: 0.4526 - val_loss: 2.2104 - val_acc: 0.4376\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0695 - acc: 0.4526 - val_loss: 2.2154 - val_acc: 0.4379\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0629 - acc: 0.4539 - val_loss: 2.2050 - val_acc: 0.4376\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0673 - acc: 0.4553 - val_loss: 2.2104 - val_acc: 0.4397\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0581 - acc: 0.4551 - val_loss: 2.2056 - val_acc: 0.4372\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0584 - acc: 0.4540 - val_loss: 2.2320 - val_acc: 0.4368\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0760 - acc: 0.4516 - val_loss: 2.2201 - val_acc: 0.4366\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0673 - acc: 0.4521 - val_loss: 2.2057 - val_acc: 0.4383\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0474 - acc: 0.4568 - val_loss: 2.2175 - val_acc: 0.4371\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0525 - acc: 0.4550 - val_loss: 2.2111 - val_acc: 0.4394\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0538 - acc: 0.4573 - val_loss: 2.2101 - val_acc: 0.4370\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0554 - acc: 0.4549 - val_loss: 2.2227 - val_acc: 0.4378\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0490 - acc: 0.4577 - val_loss: 2.2121 - val_acc: 0.4394\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0502 - acc: 0.4573 - val_loss: 2.2094 - val_acc: 0.4406\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0328 - acc: 0.4619 - val_loss: 2.1974 - val_acc: 0.4404\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0346 - acc: 0.4600 - val_loss: 2.2032 - val_acc: 0.4410\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0272 - acc: 0.4623 - val_loss: 2.2005 - val_acc: 0.4400\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0253 - acc: 0.4614 - val_loss: 2.1984 - val_acc: 0.4401\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0217 - acc: 0.4619 - val_loss: 2.1991 - val_acc: 0.4413\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0287 - acc: 0.4621 - val_loss: 2.2051 - val_acc: 0.4397\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0249 - acc: 0.4634 - val_loss: 2.2014 - val_acc: 0.4416\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0300 - acc: 0.4616 - val_loss: 2.2111 - val_acc: 0.4378\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0245 - acc: 0.4620 - val_loss: 2.2044 - val_acc: 0.4416\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0210 - acc: 0.4642 - val_loss: 2.2028 - val_acc: 0.4398\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0205 - acc: 0.4647 - val_loss: 2.1969 - val_acc: 0.4408\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0269 - acc: 0.4624 - val_loss: 2.2240 - val_acc: 0.4382\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0293 - acc: 0.4633 - val_loss: 2.2024 - val_acc: 0.4398\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0275 - acc: 0.4614 - val_loss: 2.2475 - val_acc: 0.4355\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0369 - acc: 0.4586 - val_loss: 2.2030 - val_acc: 0.4371\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0203 - acc: 0.4635 - val_loss: 2.2399 - val_acc: 0.4362\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0320 - acc: 0.4610 - val_loss: 2.2045 - val_acc: 0.4396\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0201 - acc: 0.4641 - val_loss: 2.2255 - val_acc: 0.4372\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0287 - acc: 0.4614 - val_loss: 2.2051 - val_acc: 0.4372\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0192 - acc: 0.4619 - val_loss: 2.2272 - val_acc: 0.4363\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0159 - acc: 0.4653 - val_loss: 2.2074 - val_acc: 0.4402\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0081 - acc: 0.4640 - val_loss: 2.2069 - val_acc: 0.4406\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0070 - acc: 0.4659 - val_loss: 2.2307 - val_acc: 0.4358\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9998 - acc: 0.4685 - val_loss: 2.2222 - val_acc: 0.4360\n",
      "Epoch 73/100\n",
      "1s - loss: 2.0058 - acc: 0.4682 - val_loss: 2.2135 - val_acc: 0.4378\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9999 - acc: 0.4686 - val_loss: 2.2206 - val_acc: 0.4375\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0054 - acc: 0.4673 - val_loss: 2.1981 - val_acc: 0.4406\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9864 - acc: 0.4701 - val_loss: 2.2083 - val_acc: 0.4393\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9870 - acc: 0.4709 - val_loss: 2.2108 - val_acc: 0.4386\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9857 - acc: 0.4721 - val_loss: 2.2087 - val_acc: 0.4414\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9926 - acc: 0.4702 - val_loss: 2.2039 - val_acc: 0.4394\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9897 - acc: 0.4710 - val_loss: 2.2094 - val_acc: 0.4409\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9968 - acc: 0.4690 - val_loss: 2.2133 - val_acc: 0.4387\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9908 - acc: 0.4692 - val_loss: 2.2279 - val_acc: 0.4386\n",
      "Epoch 83/100\n",
      "1s - loss: 2.0030 - acc: 0.4687 - val_loss: 2.2265 - val_acc: 0.4357\n",
      "Epoch 84/100\n",
      "1s - loss: 2.0044 - acc: 0.4659 - val_loss: 2.2168 - val_acc: 0.4386\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9939 - acc: 0.4698 - val_loss: 2.2189 - val_acc: 0.4375\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9940 - acc: 0.4693 - val_loss: 2.2118 - val_acc: 0.4398\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9887 - acc: 0.4700 - val_loss: 2.2040 - val_acc: 0.4388\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9755 - acc: 0.4734 - val_loss: 2.2174 - val_acc: 0.4389\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9807 - acc: 0.4729 - val_loss: 2.2071 - val_acc: 0.4398\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9734 - acc: 0.4756 - val_loss: 2.2111 - val_acc: 0.4400\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9826 - acc: 0.4731 - val_loss: 2.2399 - val_acc: 0.4328\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9931 - acc: 0.4695 - val_loss: 2.2286 - val_acc: 0.4390\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9820 - acc: 0.4705 - val_loss: 2.2095 - val_acc: 0.4384\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9817 - acc: 0.4720 - val_loss: 2.2236 - val_acc: 0.4391\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9793 - acc: 0.4722 - val_loss: 2.2117 - val_acc: 0.4386\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9690 - acc: 0.4749 - val_loss: 2.2130 - val_acc: 0.4409\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9749 - acc: 0.4745 - val_loss: 2.2141 - val_acc: 0.4387\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9662 - acc: 0.4766 - val_loss: 2.2113 - val_acc: 0.4395\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9588 - acc: 0.4785 - val_loss: 2.2010 - val_acc: 0.4405\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9532 - acc: 0.4798 - val_loss: 2.2020 - val_acc: 0.4385\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1959 - acc: 0.4267 - val_loss: 2.2185 - val_acc: 0.4387\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1756 - acc: 0.4312 - val_loss: 2.2172 - val_acc: 0.4419\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1629 - acc: 0.4329 - val_loss: 2.2241 - val_acc: 0.4396\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1517 - acc: 0.4343 - val_loss: 2.2517 - val_acc: 0.4337\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1469 - acc: 0.4365 - val_loss: 2.2367 - val_acc: 0.4366\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1355 - acc: 0.4398 - val_loss: 2.2386 - val_acc: 0.4360\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1329 - acc: 0.4389 - val_loss: 2.2445 - val_acc: 0.4345\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1249 - acc: 0.4384 - val_loss: 2.2440 - val_acc: 0.4396\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1220 - acc: 0.4397 - val_loss: 2.2586 - val_acc: 0.4308\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1179 - acc: 0.4417 - val_loss: 2.2444 - val_acc: 0.4351\n",
      "Epoch 11/100\n",
      "2s - loss: 2.1093 - acc: 0.4427 - val_loss: 2.2405 - val_acc: 0.4365\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1018 - acc: 0.4447 - val_loss: 2.2455 - val_acc: 0.4358\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1076 - acc: 0.4430 - val_loss: 2.2443 - val_acc: 0.4336\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0976 - acc: 0.4456 - val_loss: 2.2480 - val_acc: 0.4351\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0916 - acc: 0.4473 - val_loss: 2.2519 - val_acc: 0.4351\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0976 - acc: 0.4461 - val_loss: 2.2489 - val_acc: 0.4331\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0868 - acc: 0.4472 - val_loss: 2.2418 - val_acc: 0.4385\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0844 - acc: 0.4472 - val_loss: 2.2381 - val_acc: 0.4382\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0778 - acc: 0.4516 - val_loss: 2.2489 - val_acc: 0.4347\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0779 - acc: 0.4517 - val_loss: 2.2430 - val_acc: 0.4360\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0832 - acc: 0.4498 - val_loss: 2.2408 - val_acc: 0.4363\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0664 - acc: 0.4530 - val_loss: 2.2578 - val_acc: 0.4367\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0659 - acc: 0.4525 - val_loss: 2.2362 - val_acc: 0.4372\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0647 - acc: 0.4552 - val_loss: 2.2490 - val_acc: 0.4387\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0628 - acc: 0.4521 - val_loss: 2.2493 - val_acc: 0.4360\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0528 - acc: 0.4555 - val_loss: 2.2440 - val_acc: 0.4380\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0650 - acc: 0.4542 - val_loss: 2.2493 - val_acc: 0.4354\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0538 - acc: 0.4538 - val_loss: 2.2528 - val_acc: 0.4362\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0517 - acc: 0.4554 - val_loss: 2.2478 - val_acc: 0.4374\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0470 - acc: 0.4561 - val_loss: 2.2520 - val_acc: 0.4369\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0472 - acc: 0.4570 - val_loss: 2.2569 - val_acc: 0.4360\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0473 - acc: 0.4559 - val_loss: 2.2391 - val_acc: 0.4378\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0395 - acc: 0.4599 - val_loss: 2.2468 - val_acc: 0.4364\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0419 - acc: 0.4575 - val_loss: 2.2482 - val_acc: 0.4361\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0349 - acc: 0.4579 - val_loss: 2.2324 - val_acc: 0.4402\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0281 - acc: 0.4601 - val_loss: 2.2257 - val_acc: 0.4405\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0263 - acc: 0.4611 - val_loss: 2.2278 - val_acc: 0.4384\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0333 - acc: 0.4602 - val_loss: 2.2281 - val_acc: 0.4396\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0236 - acc: 0.4598 - val_loss: 2.2381 - val_acc: 0.4380\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0234 - acc: 0.4599 - val_loss: 2.2260 - val_acc: 0.4409\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0191 - acc: 0.4636 - val_loss: 2.2296 - val_acc: 0.4403\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0199 - acc: 0.4615 - val_loss: 2.2576 - val_acc: 0.4360\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0310 - acc: 0.4616 - val_loss: 2.2504 - val_acc: 0.4358\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0192 - acc: 0.4604 - val_loss: 2.2291 - val_acc: 0.4389\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0171 - acc: 0.4647 - val_loss: 2.2443 - val_acc: 0.4373\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0115 - acc: 0.4622 - val_loss: 2.2334 - val_acc: 0.4387\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0153 - acc: 0.4636 - val_loss: 2.2483 - val_acc: 0.4371\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0137 - acc: 0.4634 - val_loss: 2.2490 - val_acc: 0.4384\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0181 - acc: 0.4640 - val_loss: 2.2475 - val_acc: 0.4366\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0115 - acc: 0.4659 - val_loss: 2.2281 - val_acc: 0.4389\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0050 - acc: 0.4647 - val_loss: 2.2398 - val_acc: 0.4382\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0169 - acc: 0.4646 - val_loss: 2.2425 - val_acc: 0.4371\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0078 - acc: 0.4637 - val_loss: 2.2303 - val_acc: 0.4395\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9980 - acc: 0.4670 - val_loss: 2.2419 - val_acc: 0.4371\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0062 - acc: 0.4660 - val_loss: 2.2560 - val_acc: 0.4349\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0033 - acc: 0.4679 - val_loss: 2.2543 - val_acc: 0.4338\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9993 - acc: 0.4676 - val_loss: 2.2608 - val_acc: 0.4340\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0227 - acc: 0.4622 - val_loss: 2.2474 - val_acc: 0.4358\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9966 - acc: 0.4682 - val_loss: 2.2509 - val_acc: 0.4351\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9948 - acc: 0.4694 - val_loss: 2.2471 - val_acc: 0.4392\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0047 - acc: 0.4672 - val_loss: 2.2400 - val_acc: 0.4355\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9864 - acc: 0.4695 - val_loss: 2.2398 - val_acc: 0.4371\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0027 - acc: 0.4685 - val_loss: 2.2605 - val_acc: 0.4348\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0000 - acc: 0.4666 - val_loss: 2.2471 - val_acc: 0.4361\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9961 - acc: 0.4679 - val_loss: 2.2518 - val_acc: 0.4367\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9808 - acc: 0.4718 - val_loss: 2.2218 - val_acc: 0.4400\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9747 - acc: 0.4743 - val_loss: 2.2416 - val_acc: 0.4388\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9730 - acc: 0.4742 - val_loss: 2.2287 - val_acc: 0.4399\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9787 - acc: 0.4704 - val_loss: 2.2238 - val_acc: 0.4388\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9647 - acc: 0.4751 - val_loss: 2.2289 - val_acc: 0.4414\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9675 - acc: 0.4736 - val_loss: 2.2405 - val_acc: 0.4371\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9745 - acc: 0.4723 - val_loss: 2.2333 - val_acc: 0.4387\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9790 - acc: 0.4725 - val_loss: 2.2438 - val_acc: 0.4376\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9833 - acc: 0.4687 - val_loss: 2.2403 - val_acc: 0.4390\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9822 - acc: 0.4714 - val_loss: 2.2564 - val_acc: 0.4377\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9773 - acc: 0.4741 - val_loss: 2.2358 - val_acc: 0.4382\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9643 - acc: 0.4756 - val_loss: 2.2371 - val_acc: 0.4368\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9602 - acc: 0.4766 - val_loss: 2.2311 - val_acc: 0.4400\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9712 - acc: 0.4727 - val_loss: 2.2491 - val_acc: 0.4381\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9636 - acc: 0.4763 - val_loss: 2.2870 - val_acc: 0.4307\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9912 - acc: 0.4690 - val_loss: 2.2321 - val_acc: 0.4409\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9601 - acc: 0.4760 - val_loss: 2.2415 - val_acc: 0.4386\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9562 - acc: 0.4781 - val_loss: 2.2489 - val_acc: 0.4368\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9634 - acc: 0.4741 - val_loss: 2.2586 - val_acc: 0.4364\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9595 - acc: 0.4759 - val_loss: 2.2526 - val_acc: 0.4368\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9558 - acc: 0.4795 - val_loss: 2.2811 - val_acc: 0.4303\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9673 - acc: 0.4732 - val_loss: 2.2576 - val_acc: 0.4337\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9876 - acc: 0.4739 - val_loss: 2.2893 - val_acc: 0.4280\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9746 - acc: 0.4733 - val_loss: 2.2509 - val_acc: 0.4355\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9497 - acc: 0.4787 - val_loss: 2.2611 - val_acc: 0.4325\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9518 - acc: 0.4795 - val_loss: 2.2481 - val_acc: 0.4334\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9497 - acc: 0.4780 - val_loss: 2.2798 - val_acc: 0.4298\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9524 - acc: 0.4760 - val_loss: 2.2230 - val_acc: 0.4380\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9324 - acc: 0.4842 - val_loss: 2.2309 - val_acc: 0.4416\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9354 - acc: 0.4830 - val_loss: 2.2368 - val_acc: 0.4387\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9297 - acc: 0.4842 - val_loss: 2.2405 - val_acc: 0.4379\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9399 - acc: 0.4824 - val_loss: 2.2353 - val_acc: 0.4386\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9481 - acc: 0.4772 - val_loss: 2.2854 - val_acc: 0.4299\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9577 - acc: 0.4765 - val_loss: 2.2332 - val_acc: 0.4372\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9416 - acc: 0.4805 - val_loss: 2.2624 - val_acc: 0.4314\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1869 - acc: 0.4309 - val_loss: 2.1835 - val_acc: 0.4426\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1624 - acc: 0.4339 - val_loss: 2.1821 - val_acc: 0.4408\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1493 - acc: 0.4350 - val_loss: 2.1868 - val_acc: 0.4417\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1377 - acc: 0.4396 - val_loss: 2.1982 - val_acc: 0.4401\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1291 - acc: 0.4402 - val_loss: 2.1953 - val_acc: 0.4415\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1215 - acc: 0.4412 - val_loss: 2.1956 - val_acc: 0.4422\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1144 - acc: 0.4426 - val_loss: 2.2004 - val_acc: 0.4401\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1059 - acc: 0.4450 - val_loss: 2.2219 - val_acc: 0.4370\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1065 - acc: 0.4443 - val_loss: 2.2090 - val_acc: 0.4369\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0977 - acc: 0.4455 - val_loss: 2.2105 - val_acc: 0.4390\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0976 - acc: 0.4446 - val_loss: 2.2134 - val_acc: 0.4386\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0894 - acc: 0.4466 - val_loss: 2.2041 - val_acc: 0.4410\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0872 - acc: 0.4490 - val_loss: 2.2487 - val_acc: 0.4345\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0823 - acc: 0.4499 - val_loss: 2.2182 - val_acc: 0.4376\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0797 - acc: 0.4515 - val_loss: 2.2222 - val_acc: 0.4370\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0808 - acc: 0.4504 - val_loss: 2.2048 - val_acc: 0.4393\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0656 - acc: 0.4550 - val_loss: 2.2193 - val_acc: 0.4364\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0667 - acc: 0.4546 - val_loss: 2.1906 - val_acc: 0.4409\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0619 - acc: 0.4542 - val_loss: 2.2064 - val_acc: 0.4363\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0607 - acc: 0.4535 - val_loss: 2.2046 - val_acc: 0.4374\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0597 - acc: 0.4559 - val_loss: 2.2133 - val_acc: 0.4368\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0760 - acc: 0.4522 - val_loss: 2.2097 - val_acc: 0.4376\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0525 - acc: 0.4578 - val_loss: 2.2009 - val_acc: 0.4409\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0453 - acc: 0.4572 - val_loss: 2.1960 - val_acc: 0.4402\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0429 - acc: 0.4603 - val_loss: 2.1989 - val_acc: 0.4408\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0417 - acc: 0.4596 - val_loss: 2.2040 - val_acc: 0.4410\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0449 - acc: 0.4571 - val_loss: 2.2042 - val_acc: 0.4375\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0405 - acc: 0.4596 - val_loss: 2.2066 - val_acc: 0.4387\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0321 - acc: 0.4633 - val_loss: 2.1921 - val_acc: 0.4416\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0302 - acc: 0.4634 - val_loss: 2.2029 - val_acc: 0.4386\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0314 - acc: 0.4594 - val_loss: 2.1979 - val_acc: 0.4409\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0343 - acc: 0.4628 - val_loss: 2.2100 - val_acc: 0.4392\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0319 - acc: 0.4616 - val_loss: 2.1850 - val_acc: 0.4428\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0199 - acc: 0.4653 - val_loss: 2.2004 - val_acc: 0.4402\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0210 - acc: 0.4650 - val_loss: 2.2054 - val_acc: 0.4399\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0255 - acc: 0.4638 - val_loss: 2.2045 - val_acc: 0.4411\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0162 - acc: 0.4645 - val_loss: 2.2010 - val_acc: 0.4383\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0141 - acc: 0.4639 - val_loss: 2.1894 - val_acc: 0.4405\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0074 - acc: 0.4667 - val_loss: 2.2029 - val_acc: 0.4411\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0035 - acc: 0.4684 - val_loss: 2.1843 - val_acc: 0.4453\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0101 - acc: 0.4669 - val_loss: 2.1880 - val_acc: 0.4440\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0026 - acc: 0.4670 - val_loss: 2.1872 - val_acc: 0.4435\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0125 - acc: 0.4648 - val_loss: 2.1985 - val_acc: 0.4394\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0037 - acc: 0.4654 - val_loss: 2.1891 - val_acc: 0.4440\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0154 - acc: 0.4674 - val_loss: 2.1974 - val_acc: 0.4402\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0067 - acc: 0.4671 - val_loss: 2.2403 - val_acc: 0.4360\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0207 - acc: 0.4652 - val_loss: 2.2012 - val_acc: 0.4407\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9979 - acc: 0.4673 - val_loss: 2.2188 - val_acc: 0.4406\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9984 - acc: 0.4691 - val_loss: 2.1927 - val_acc: 0.4423\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9968 - acc: 0.4689 - val_loss: 2.2029 - val_acc: 0.4427\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9932 - acc: 0.4700 - val_loss: 2.1948 - val_acc: 0.4419\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9941 - acc: 0.4694 - val_loss: 2.2196 - val_acc: 0.4377\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9955 - acc: 0.4719 - val_loss: 2.2018 - val_acc: 0.4441\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9920 - acc: 0.4704 - val_loss: 2.1971 - val_acc: 0.4424\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9836 - acc: 0.4743 - val_loss: 2.1859 - val_acc: 0.4473\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9886 - acc: 0.4699 - val_loss: 2.1708 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9717 - acc: 0.4770 - val_loss: 2.1818 - val_acc: 0.4460\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9708 - acc: 0.4748 - val_loss: 2.1762 - val_acc: 0.4442\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9758 - acc: 0.4724 - val_loss: 2.1993 - val_acc: 0.4429\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9820 - acc: 0.4718 - val_loss: 2.1882 - val_acc: 0.4414\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9768 - acc: 0.4745 - val_loss: 2.1795 - val_acc: 0.4455\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9797 - acc: 0.4738 - val_loss: 2.1811 - val_acc: 0.4453\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9690 - acc: 0.4739 - val_loss: 2.2142 - val_acc: 0.4411\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9726 - acc: 0.4755 - val_loss: 2.1861 - val_acc: 0.4403\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9679 - acc: 0.4768 - val_loss: 2.1922 - val_acc: 0.4438\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9643 - acc: 0.4767 - val_loss: 2.1994 - val_acc: 0.4411\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9744 - acc: 0.4739 - val_loss: 2.2159 - val_acc: 0.4381\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9692 - acc: 0.4735 - val_loss: 2.2004 - val_acc: 0.4433\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9659 - acc: 0.4763 - val_loss: 2.2009 - val_acc: 0.4425\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9681 - acc: 0.4769 - val_loss: 2.1854 - val_acc: 0.4426\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9643 - acc: 0.4746 - val_loss: 2.1949 - val_acc: 0.4413\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9612 - acc: 0.4777 - val_loss: 2.1875 - val_acc: 0.4456\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9799 - acc: 0.4735 - val_loss: 2.2174 - val_acc: 0.4387\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9614 - acc: 0.4774 - val_loss: 2.1836 - val_acc: 0.4429\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9576 - acc: 0.4773 - val_loss: 2.2001 - val_acc: 0.4438\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9596 - acc: 0.4774 - val_loss: 2.1786 - val_acc: 0.4426\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9497 - acc: 0.4795 - val_loss: 2.2130 - val_acc: 0.4417\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9506 - acc: 0.4798 - val_loss: 2.1911 - val_acc: 0.4404\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9581 - acc: 0.4788 - val_loss: 2.1942 - val_acc: 0.4454\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9518 - acc: 0.4792 - val_loss: 2.1973 - val_acc: 0.4424\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9504 - acc: 0.4802 - val_loss: 2.1842 - val_acc: 0.4449\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9529 - acc: 0.4787 - val_loss: 2.1929 - val_acc: 0.4422\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9442 - acc: 0.4828 - val_loss: 2.1690 - val_acc: 0.4465\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9352 - acc: 0.4832 - val_loss: 2.1860 - val_acc: 0.4424\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9359 - acc: 0.4834 - val_loss: 2.1698 - val_acc: 0.4435\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9346 - acc: 0.4839 - val_loss: 2.1914 - val_acc: 0.4433\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9430 - acc: 0.4823 - val_loss: 2.1866 - val_acc: 0.4442\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9413 - acc: 0.4807 - val_loss: 2.2041 - val_acc: 0.4417\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9438 - acc: 0.4825 - val_loss: 2.1851 - val_acc: 0.4430\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9450 - acc: 0.4833 - val_loss: 2.2003 - val_acc: 0.4420\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9458 - acc: 0.4814 - val_loss: 2.1824 - val_acc: 0.4457\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9353 - acc: 0.4822 - val_loss: 2.1930 - val_acc: 0.4422\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9316 - acc: 0.4827 - val_loss: 2.1844 - val_acc: 0.4453\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9311 - acc: 0.4853 - val_loss: 2.1794 - val_acc: 0.4450\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9188 - acc: 0.4889 - val_loss: 2.1926 - val_acc: 0.4434\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9198 - acc: 0.4880 - val_loss: 2.1876 - val_acc: 0.4414\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9282 - acc: 0.4842 - val_loss: 2.1912 - val_acc: 0.4450\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9318 - acc: 0.4862 - val_loss: 2.2140 - val_acc: 0.4396\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9372 - acc: 0.4837 - val_loss: 2.2078 - val_acc: 0.4428\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9302 - acc: 0.4826 - val_loss: 2.1939 - val_acc: 0.4430\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1929 - acc: 0.4280 - val_loss: 2.2223 - val_acc: 0.4364\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1712 - acc: 0.4294 - val_loss: 2.2315 - val_acc: 0.4370\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1595 - acc: 0.4345 - val_loss: 2.2299 - val_acc: 0.4351\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1494 - acc: 0.4345 - val_loss: 2.2495 - val_acc: 0.4329\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1680 - acc: 0.4306 - val_loss: 2.2559 - val_acc: 0.4287\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1475 - acc: 0.4350 - val_loss: 2.2355 - val_acc: 0.4370\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1293 - acc: 0.4396 - val_loss: 2.2420 - val_acc: 0.4344\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1237 - acc: 0.4392 - val_loss: 2.2387 - val_acc: 0.4335\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1167 - acc: 0.4405 - val_loss: 2.2459 - val_acc: 0.4330\n",
      "Epoch 10/100\n",
      "2s - loss: 2.1112 - acc: 0.4432 - val_loss: 2.2487 - val_acc: 0.4319\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1103 - acc: 0.4428 - val_loss: 2.2465 - val_acc: 0.4316\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1045 - acc: 0.4438 - val_loss: 2.2485 - val_acc: 0.4306\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0987 - acc: 0.4447 - val_loss: 2.2406 - val_acc: 0.4324\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0928 - acc: 0.4463 - val_loss: 2.2386 - val_acc: 0.4346\n",
      "Epoch 15/100\n",
      "2s - loss: 2.0878 - acc: 0.4470 - val_loss: 2.2453 - val_acc: 0.4326\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0857 - acc: 0.4496 - val_loss: 2.2411 - val_acc: 0.4320\n",
      "Epoch 17/100\n",
      "2s - loss: 2.0848 - acc: 0.4480 - val_loss: 2.2417 - val_acc: 0.4323\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0779 - acc: 0.4507 - val_loss: 2.2408 - val_acc: 0.4308\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0789 - acc: 0.4509 - val_loss: 2.2565 - val_acc: 0.4297\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0779 - acc: 0.4500 - val_loss: 2.2397 - val_acc: 0.4319\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0779 - acc: 0.4478 - val_loss: 2.2474 - val_acc: 0.4320\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0731 - acc: 0.4518 - val_loss: 2.2363 - val_acc: 0.4333\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0652 - acc: 0.4524 - val_loss: 2.2411 - val_acc: 0.4331\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0615 - acc: 0.4534 - val_loss: 2.2435 - val_acc: 0.4305\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0622 - acc: 0.4523 - val_loss: 2.2329 - val_acc: 0.4338\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0532 - acc: 0.4545 - val_loss: 2.2305 - val_acc: 0.4339\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0521 - acc: 0.4560 - val_loss: 2.2376 - val_acc: 0.4325\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0567 - acc: 0.4541 - val_loss: 2.2611 - val_acc: 0.4283\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0623 - acc: 0.4513 - val_loss: 2.2353 - val_acc: 0.4319\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0503 - acc: 0.4551 - val_loss: 2.2407 - val_acc: 0.4323\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0502 - acc: 0.4567 - val_loss: 2.2298 - val_acc: 0.4330\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0416 - acc: 0.4572 - val_loss: 2.2507 - val_acc: 0.4302\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0549 - acc: 0.4543 - val_loss: 2.2420 - val_acc: 0.4317\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0506 - acc: 0.4559 - val_loss: 2.2773 - val_acc: 0.4270\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0549 - acc: 0.4554 - val_loss: 2.2374 - val_acc: 0.4337\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0414 - acc: 0.4574 - val_loss: 2.2385 - val_acc: 0.4314\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0303 - acc: 0.4586 - val_loss: 2.2394 - val_acc: 0.4333\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0329 - acc: 0.4591 - val_loss: 2.2495 - val_acc: 0.4312\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0256 - acc: 0.4614 - val_loss: 2.2321 - val_acc: 0.4344\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0208 - acc: 0.4608 - val_loss: 2.2317 - val_acc: 0.4315\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0222 - acc: 0.4620 - val_loss: 2.2330 - val_acc: 0.4336\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0264 - acc: 0.4607 - val_loss: 2.2556 - val_acc: 0.4303\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0260 - acc: 0.4608 - val_loss: 2.2296 - val_acc: 0.4332\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0220 - acc: 0.4605 - val_loss: 2.2501 - val_acc: 0.4301\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0319 - acc: 0.4628 - val_loss: 2.2440 - val_acc: 0.4302\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0254 - acc: 0.4614 - val_loss: 2.2593 - val_acc: 0.4330\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0411 - acc: 0.4583 - val_loss: 2.2632 - val_acc: 0.4286\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0298 - acc: 0.4604 - val_loss: 2.2398 - val_acc: 0.4324\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0091 - acc: 0.4655 - val_loss: 2.2375 - val_acc: 0.4334\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0087 - acc: 0.4638 - val_loss: 2.2358 - val_acc: 0.4318\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0113 - acc: 0.4631 - val_loss: 2.2381 - val_acc: 0.4326\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0068 - acc: 0.4642 - val_loss: 2.2365 - val_acc: 0.4310\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0042 - acc: 0.4663 - val_loss: 2.2394 - val_acc: 0.4328\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0052 - acc: 0.4655 - val_loss: 2.2247 - val_acc: 0.4332\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9933 - acc: 0.4687 - val_loss: 2.2391 - val_acc: 0.4317\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0068 - acc: 0.4647 - val_loss: 2.2248 - val_acc: 0.4342\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9917 - acc: 0.4702 - val_loss: 2.2366 - val_acc: 0.4329\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9930 - acc: 0.4708 - val_loss: 2.2217 - val_acc: 0.4346\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9877 - acc: 0.4694 - val_loss: 2.2441 - val_acc: 0.4349\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9982 - acc: 0.4661 - val_loss: 2.2396 - val_acc: 0.4319\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0021 - acc: 0.4654 - val_loss: 2.2297 - val_acc: 0.4330\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9896 - acc: 0.4686 - val_loss: 2.2285 - val_acc: 0.4364\n",
      "Epoch 63/100\n",
      "2s - loss: 1.9874 - acc: 0.4699 - val_loss: 2.2379 - val_acc: 0.4342\n",
      "Epoch 64/100\n",
      "2s - loss: 1.9807 - acc: 0.4706 - val_loss: 2.2259 - val_acc: 0.4353\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9900 - acc: 0.4693 - val_loss: 2.2401 - val_acc: 0.4297\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9859 - acc: 0.4704 - val_loss: 2.2252 - val_acc: 0.4332\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9788 - acc: 0.4701 - val_loss: 2.2401 - val_acc: 0.4327\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9810 - acc: 0.4694 - val_loss: 2.2246 - val_acc: 0.4329\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9776 - acc: 0.4709 - val_loss: 2.2661 - val_acc: 0.4316\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9858 - acc: 0.4686 - val_loss: 2.2754 - val_acc: 0.4273\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9905 - acc: 0.4679 - val_loss: 2.2486 - val_acc: 0.4327\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9850 - acc: 0.4701 - val_loss: 2.2682 - val_acc: 0.4304\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9823 - acc: 0.4689 - val_loss: 2.2353 - val_acc: 0.4357\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9716 - acc: 0.4725 - val_loss: 2.2612 - val_acc: 0.4306\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9733 - acc: 0.4717 - val_loss: 2.2578 - val_acc: 0.4322\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9841 - acc: 0.4702 - val_loss: 2.2466 - val_acc: 0.4314\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9717 - acc: 0.4726 - val_loss: 2.2607 - val_acc: 0.4315\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9728 - acc: 0.4724 - val_loss: 2.2493 - val_acc: 0.4328\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9708 - acc: 0.4708 - val_loss: 2.2316 - val_acc: 0.4344\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9644 - acc: 0.4732 - val_loss: 2.2499 - val_acc: 0.4300\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9623 - acc: 0.4741 - val_loss: 2.2365 - val_acc: 0.4342\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9663 - acc: 0.4749 - val_loss: 2.2293 - val_acc: 0.4356\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9556 - acc: 0.4772 - val_loss: 2.2264 - val_acc: 0.4351\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9497 - acc: 0.4787 - val_loss: 2.2229 - val_acc: 0.4322\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9488 - acc: 0.4785 - val_loss: 2.2358 - val_acc: 0.4335\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9585 - acc: 0.4736 - val_loss: 2.2378 - val_acc: 0.4315\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9550 - acc: 0.4774 - val_loss: 2.2383 - val_acc: 0.4323\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9570 - acc: 0.4769 - val_loss: 2.2338 - val_acc: 0.4342\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9468 - acc: 0.4797 - val_loss: 2.2567 - val_acc: 0.4326\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9550 - acc: 0.4773 - val_loss: 2.2468 - val_acc: 0.4319\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9489 - acc: 0.4783 - val_loss: 2.2343 - val_acc: 0.4344\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9502 - acc: 0.4784 - val_loss: 2.2612 - val_acc: 0.4295\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9526 - acc: 0.4783 - val_loss: 2.2530 - val_acc: 0.4321\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9482 - acc: 0.4777 - val_loss: 2.2558 - val_acc: 0.4322\n",
      "Epoch 95/100\n",
      "2s - loss: 1.9479 - acc: 0.4787 - val_loss: 2.2623 - val_acc: 0.4308\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9552 - acc: 0.4775 - val_loss: 2.2421 - val_acc: 0.4327\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9393 - acc: 0.4793 - val_loss: 2.2355 - val_acc: 0.4328\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9441 - acc: 0.4784 - val_loss: 2.2595 - val_acc: 0.4314\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9801 - acc: 0.4734 - val_loss: 2.2647 - val_acc: 0.4300\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9586 - acc: 0.4764 - val_loss: 2.2548 - val_acc: 0.4289\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1954 - acc: 0.4272 - val_loss: 2.2442 - val_acc: 0.4363\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1713 - acc: 0.4316 - val_loss: 2.2557 - val_acc: 0.4341\n",
      "Epoch 3/100\n",
      "2s - loss: 2.1643 - acc: 0.4324 - val_loss: 2.2495 - val_acc: 0.4355\n",
      "Epoch 4/100\n",
      "2s - loss: 2.1498 - acc: 0.4359 - val_loss: 2.2563 - val_acc: 0.4347\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1421 - acc: 0.4393 - val_loss: 2.2644 - val_acc: 0.4344\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1336 - acc: 0.4410 - val_loss: 2.2787 - val_acc: 0.4320\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1429 - acc: 0.4390 - val_loss: 2.2764 - val_acc: 0.4309\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1259 - acc: 0.4420 - val_loss: 2.2936 - val_acc: 0.4303\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1187 - acc: 0.4417 - val_loss: 2.2915 - val_acc: 0.4300\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1099 - acc: 0.4442 - val_loss: 2.2877 - val_acc: 0.4295\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1065 - acc: 0.4471 - val_loss: 2.2954 - val_acc: 0.4298\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1016 - acc: 0.4458 - val_loss: 2.2848 - val_acc: 0.4299\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0926 - acc: 0.4494 - val_loss: 2.2745 - val_acc: 0.4311\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0916 - acc: 0.4473 - val_loss: 2.2929 - val_acc: 0.4267\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0983 - acc: 0.4469 - val_loss: 2.2808 - val_acc: 0.4315\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0866 - acc: 0.4482 - val_loss: 2.2687 - val_acc: 0.4335\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0804 - acc: 0.4505 - val_loss: 2.2848 - val_acc: 0.4323\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0776 - acc: 0.4505 - val_loss: 2.2829 - val_acc: 0.4285\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0743 - acc: 0.4534 - val_loss: 2.2653 - val_acc: 0.4328\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0679 - acc: 0.4532 - val_loss: 2.2653 - val_acc: 0.4322\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0656 - acc: 0.4549 - val_loss: 2.2874 - val_acc: 0.4313\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0694 - acc: 0.4532 - val_loss: 2.2582 - val_acc: 0.4344\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0598 - acc: 0.4567 - val_loss: 2.2554 - val_acc: 0.4346\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0606 - acc: 0.4548 - val_loss: 2.2687 - val_acc: 0.4336\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0569 - acc: 0.4558 - val_loss: 2.2583 - val_acc: 0.4343\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0619 - acc: 0.4552 - val_loss: 2.2664 - val_acc: 0.4328\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0542 - acc: 0.4559 - val_loss: 2.2650 - val_acc: 0.4344\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0547 - acc: 0.4565 - val_loss: 2.2671 - val_acc: 0.4321\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0514 - acc: 0.4572 - val_loss: 2.2639 - val_acc: 0.4362\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0432 - acc: 0.4586 - val_loss: 2.2586 - val_acc: 0.4348\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0417 - acc: 0.4573 - val_loss: 2.2524 - val_acc: 0.4367\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0393 - acc: 0.4605 - val_loss: 2.2417 - val_acc: 0.4372\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0336 - acc: 0.4605 - val_loss: 2.2508 - val_acc: 0.4360\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0303 - acc: 0.4633 - val_loss: 2.2554 - val_acc: 0.4376\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0408 - acc: 0.4585 - val_loss: 2.2460 - val_acc: 0.4367\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0268 - acc: 0.4632 - val_loss: 2.2583 - val_acc: 0.4362\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0243 - acc: 0.4626 - val_loss: 2.2422 - val_acc: 0.4388\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0300 - acc: 0.4615 - val_loss: 2.2624 - val_acc: 0.4357\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0277 - acc: 0.4613 - val_loss: 2.2442 - val_acc: 0.4380\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0201 - acc: 0.4633 - val_loss: 2.2479 - val_acc: 0.4373\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0136 - acc: 0.4649 - val_loss: 2.2524 - val_acc: 0.4349\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0199 - acc: 0.4638 - val_loss: 2.2684 - val_acc: 0.4353\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0363 - acc: 0.4610 - val_loss: 2.2598 - val_acc: 0.4363\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0244 - acc: 0.4619 - val_loss: 2.2611 - val_acc: 0.4368\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0507 - acc: 0.4590 - val_loss: 2.2651 - val_acc: 0.4324\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0261 - acc: 0.4633 - val_loss: 2.2547 - val_acc: 0.4387\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9997 - acc: 0.4686 - val_loss: 2.2419 - val_acc: 0.4387\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9978 - acc: 0.4689 - val_loss: 2.2591 - val_acc: 0.4331\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0044 - acc: 0.4671 - val_loss: 2.2287 - val_acc: 0.4408\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0004 - acc: 0.4664 - val_loss: 2.2348 - val_acc: 0.4402\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9988 - acc: 0.4679 - val_loss: 2.2535 - val_acc: 0.4368\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0206 - acc: 0.4664 - val_loss: 2.2374 - val_acc: 0.4381\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9967 - acc: 0.4683 - val_loss: 2.2534 - val_acc: 0.4363\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0072 - acc: 0.4655 - val_loss: 2.2434 - val_acc: 0.4374\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9971 - acc: 0.4683 - val_loss: 2.2295 - val_acc: 0.4400\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9896 - acc: 0.4687 - val_loss: 2.2410 - val_acc: 0.4370\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9896 - acc: 0.4710 - val_loss: 2.2356 - val_acc: 0.4390\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9834 - acc: 0.4715 - val_loss: 2.2385 - val_acc: 0.4370\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9820 - acc: 0.4736 - val_loss: 2.2301 - val_acc: 0.4401\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9798 - acc: 0.4735 - val_loss: 2.2291 - val_acc: 0.4398\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9771 - acc: 0.4730 - val_loss: 2.2412 - val_acc: 0.4357\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9842 - acc: 0.4729 - val_loss: 2.2366 - val_acc: 0.4381\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9852 - acc: 0.4719 - val_loss: 2.2300 - val_acc: 0.4391\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9808 - acc: 0.4718 - val_loss: 2.2244 - val_acc: 0.4411\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9925 - acc: 0.4684 - val_loss: 2.2445 - val_acc: 0.4347\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9820 - acc: 0.4729 - val_loss: 2.2370 - val_acc: 0.4397\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9859 - acc: 0.4716 - val_loss: 2.2363 - val_acc: 0.4403\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9806 - acc: 0.4718 - val_loss: 2.2398 - val_acc: 0.4389\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9845 - acc: 0.4701 - val_loss: 2.2478 - val_acc: 0.4368\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9733 - acc: 0.4750 - val_loss: 2.2835 - val_acc: 0.4300\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9769 - acc: 0.4730 - val_loss: 2.2430 - val_acc: 0.4389\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9685 - acc: 0.4757 - val_loss: 2.2732 - val_acc: 0.4340\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9703 - acc: 0.4749 - val_loss: 2.2411 - val_acc: 0.4389\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9571 - acc: 0.4776 - val_loss: 2.2418 - val_acc: 0.4393\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9538 - acc: 0.4799 - val_loss: 2.2432 - val_acc: 0.4371\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9573 - acc: 0.4795 - val_loss: 2.2581 - val_acc: 0.4372\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9525 - acc: 0.4802 - val_loss: 2.2420 - val_acc: 0.4382\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9655 - acc: 0.4780 - val_loss: 2.2415 - val_acc: 0.4359\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9566 - acc: 0.4780 - val_loss: 2.2497 - val_acc: 0.4362\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9590 - acc: 0.4764 - val_loss: 2.2358 - val_acc: 0.4395\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9536 - acc: 0.4789 - val_loss: 2.2399 - val_acc: 0.4364\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9511 - acc: 0.4804 - val_loss: 2.2420 - val_acc: 0.4376\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9421 - acc: 0.4824 - val_loss: 2.2451 - val_acc: 0.4373\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9591 - acc: 0.4775 - val_loss: 2.2380 - val_acc: 0.4365\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9543 - acc: 0.4774 - val_loss: 2.2380 - val_acc: 0.4364\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9449 - acc: 0.4798 - val_loss: 2.2486 - val_acc: 0.4377\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9571 - acc: 0.4773 - val_loss: 2.2581 - val_acc: 0.4345\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9532 - acc: 0.4781 - val_loss: 2.2443 - val_acc: 0.4374\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9538 - acc: 0.4787 - val_loss: 2.2525 - val_acc: 0.4352\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9540 - acc: 0.4786 - val_loss: 2.2393 - val_acc: 0.4378\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9471 - acc: 0.4803 - val_loss: 2.2398 - val_acc: 0.4346\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9434 - acc: 0.4804 - val_loss: 2.2371 - val_acc: 0.4379\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9371 - acc: 0.4831 - val_loss: 2.2417 - val_acc: 0.4375\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9375 - acc: 0.4837 - val_loss: 2.2424 - val_acc: 0.4368\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9380 - acc: 0.4812 - val_loss: 2.2722 - val_acc: 0.4333\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9461 - acc: 0.4795 - val_loss: 2.2656 - val_acc: 0.4336\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9425 - acc: 0.4808 - val_loss: 2.2824 - val_acc: 0.4320\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9482 - acc: 0.4806 - val_loss: 2.2959 - val_acc: 0.4274\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9469 - acc: 0.4793 - val_loss: 2.2495 - val_acc: 0.4351\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9384 - acc: 0.4815 - val_loss: 2.2784 - val_acc: 0.4332\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2103 - acc: 0.4270 - val_loss: 2.1965 - val_acc: 0.4386\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1823 - acc: 0.4330 - val_loss: 2.1880 - val_acc: 0.4376\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1681 - acc: 0.4348 - val_loss: 2.1904 - val_acc: 0.4388\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1632 - acc: 0.4365 - val_loss: 2.2055 - val_acc: 0.4342\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1562 - acc: 0.4392 - val_loss: 2.1861 - val_acc: 0.4382\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1451 - acc: 0.4408 - val_loss: 2.1993 - val_acc: 0.4372\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1480 - acc: 0.4408 - val_loss: 2.2140 - val_acc: 0.4354\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1516 - acc: 0.4413 - val_loss: 2.2063 - val_acc: 0.4362\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1279 - acc: 0.4478 - val_loss: 2.2078 - val_acc: 0.4337\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1210 - acc: 0.4468 - val_loss: 2.2075 - val_acc: 0.4334\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1206 - acc: 0.4455 - val_loss: 2.2424 - val_acc: 0.4296\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1440 - acc: 0.4421 - val_loss: 2.2211 - val_acc: 0.4269\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1177 - acc: 0.4477 - val_loss: 2.2051 - val_acc: 0.4348\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1054 - acc: 0.4490 - val_loss: 2.2129 - val_acc: 0.4324\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1097 - acc: 0.4489 - val_loss: 2.2066 - val_acc: 0.4347\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1037 - acc: 0.4501 - val_loss: 2.2083 - val_acc: 0.4336\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0952 - acc: 0.4535 - val_loss: 2.2052 - val_acc: 0.4340\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0984 - acc: 0.4500 - val_loss: 2.2054 - val_acc: 0.4340\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0835 - acc: 0.4540 - val_loss: 2.1998 - val_acc: 0.4326\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0854 - acc: 0.4542 - val_loss: 2.2032 - val_acc: 0.4321\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0766 - acc: 0.4562 - val_loss: 2.2038 - val_acc: 0.4312\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0776 - acc: 0.4549 - val_loss: 2.2137 - val_acc: 0.4318\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0761 - acc: 0.4558 - val_loss: 2.1885 - val_acc: 0.4362\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0759 - acc: 0.4556 - val_loss: 2.1970 - val_acc: 0.4338\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0679 - acc: 0.4582 - val_loss: 2.1902 - val_acc: 0.4359\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0652 - acc: 0.4585 - val_loss: 2.2038 - val_acc: 0.4319\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0667 - acc: 0.4584 - val_loss: 2.2022 - val_acc: 0.4335\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0629 - acc: 0.4587 - val_loss: 2.1817 - val_acc: 0.4380\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0571 - acc: 0.4615 - val_loss: 2.1916 - val_acc: 0.4332\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0525 - acc: 0.4624 - val_loss: 2.2114 - val_acc: 0.4330\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0550 - acc: 0.4608 - val_loss: 2.2042 - val_acc: 0.4327\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0517 - acc: 0.4611 - val_loss: 2.2018 - val_acc: 0.4358\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0493 - acc: 0.4610 - val_loss: 2.2100 - val_acc: 0.4291\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0496 - acc: 0.4624 - val_loss: 2.1862 - val_acc: 0.4384\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0432 - acc: 0.4644 - val_loss: 2.1920 - val_acc: 0.4345\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0459 - acc: 0.4630 - val_loss: 2.1915 - val_acc: 0.4376\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0411 - acc: 0.4651 - val_loss: 2.2175 - val_acc: 0.4283\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0476 - acc: 0.4612 - val_loss: 2.1821 - val_acc: 0.4395\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0405 - acc: 0.4629 - val_loss: 2.2126 - val_acc: 0.4332\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0395 - acc: 0.4640 - val_loss: 2.1844 - val_acc: 0.4382\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0408 - acc: 0.4631 - val_loss: 2.2256 - val_acc: 0.4327\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0372 - acc: 0.4649 - val_loss: 2.1858 - val_acc: 0.4394\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0441 - acc: 0.4630 - val_loss: 2.2308 - val_acc: 0.4357\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0631 - acc: 0.4616 - val_loss: 2.2024 - val_acc: 0.4331\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0385 - acc: 0.4645 - val_loss: 2.1957 - val_acc: 0.4399\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0268 - acc: 0.4676 - val_loss: 2.1702 - val_acc: 0.4391\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0197 - acc: 0.4685 - val_loss: 2.1947 - val_acc: 0.4358\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0257 - acc: 0.4673 - val_loss: 2.1818 - val_acc: 0.4382\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0264 - acc: 0.4655 - val_loss: 2.2064 - val_acc: 0.4358\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0227 - acc: 0.4678 - val_loss: 2.1862 - val_acc: 0.4388\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0239 - acc: 0.4695 - val_loss: 2.1877 - val_acc: 0.4392\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0135 - acc: 0.4693 - val_loss: 2.1844 - val_acc: 0.4408\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0139 - acc: 0.4690 - val_loss: 2.2069 - val_acc: 0.4374\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0211 - acc: 0.4673 - val_loss: 2.1856 - val_acc: 0.4381\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0055 - acc: 0.4735 - val_loss: 2.1899 - val_acc: 0.4371\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0041 - acc: 0.4714 - val_loss: 2.1733 - val_acc: 0.4394\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0035 - acc: 0.4712 - val_loss: 2.1880 - val_acc: 0.4367\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0071 - acc: 0.4704 - val_loss: 2.1710 - val_acc: 0.4444\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0022 - acc: 0.4725 - val_loss: 2.2245 - val_acc: 0.4320\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0076 - acc: 0.4716 - val_loss: 2.1693 - val_acc: 0.4428\n",
      "Epoch 61/100\n",
      "2s - loss: 2.0069 - acc: 0.4708 - val_loss: 2.2006 - val_acc: 0.4354\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0099 - acc: 0.4690 - val_loss: 2.1698 - val_acc: 0.4422\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9993 - acc: 0.4719 - val_loss: 2.2444 - val_acc: 0.4353\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0236 - acc: 0.4683 - val_loss: 2.1877 - val_acc: 0.4390\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9953 - acc: 0.4748 - val_loss: 2.2086 - val_acc: 0.4360\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0007 - acc: 0.4721 - val_loss: 2.1850 - val_acc: 0.4367\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9967 - acc: 0.4725 - val_loss: 2.2108 - val_acc: 0.4339\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9888 - acc: 0.4756 - val_loss: 2.1905 - val_acc: 0.4381\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9895 - acc: 0.4771 - val_loss: 2.2137 - val_acc: 0.4394\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0155 - acc: 0.4710 - val_loss: 2.1856 - val_acc: 0.4391\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9983 - acc: 0.4742 - val_loss: 2.1832 - val_acc: 0.4385\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9950 - acc: 0.4758 - val_loss: 2.1786 - val_acc: 0.4427\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9855 - acc: 0.4761 - val_loss: 2.1983 - val_acc: 0.4372\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9775 - acc: 0.4782 - val_loss: 2.1766 - val_acc: 0.4433\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9875 - acc: 0.4758 - val_loss: 2.1978 - val_acc: 0.4380\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9775 - acc: 0.4791 - val_loss: 2.1797 - val_acc: 0.4421\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9836 - acc: 0.4767 - val_loss: 2.2244 - val_acc: 0.4314\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9863 - acc: 0.4752 - val_loss: 2.1748 - val_acc: 0.4414\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9736 - acc: 0.4798 - val_loss: 2.2011 - val_acc: 0.4352\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9693 - acc: 0.4798 - val_loss: 2.1955 - val_acc: 0.4356\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9834 - acc: 0.4760 - val_loss: 2.1992 - val_acc: 0.4357\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9665 - acc: 0.4786 - val_loss: 2.1799 - val_acc: 0.4394\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9680 - acc: 0.4791 - val_loss: 2.2102 - val_acc: 0.4347\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9718 - acc: 0.4759 - val_loss: 2.1819 - val_acc: 0.4403\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9745 - acc: 0.4796 - val_loss: 2.2207 - val_acc: 0.4337\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9753 - acc: 0.4782 - val_loss: 2.1760 - val_acc: 0.4400\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9592 - acc: 0.4827 - val_loss: 2.2092 - val_acc: 0.4379\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9816 - acc: 0.4782 - val_loss: 2.1886 - val_acc: 0.4388\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9575 - acc: 0.4813 - val_loss: 2.2126 - val_acc: 0.4340\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9547 - acc: 0.4848 - val_loss: 2.1741 - val_acc: 0.4410\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9606 - acc: 0.4809 - val_loss: 2.2000 - val_acc: 0.4346\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9604 - acc: 0.4813 - val_loss: 2.1782 - val_acc: 0.4397\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9665 - acc: 0.4799 - val_loss: 2.1979 - val_acc: 0.4342\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9624 - acc: 0.4808 - val_loss: 2.1745 - val_acc: 0.4413\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9512 - acc: 0.4829 - val_loss: 2.2072 - val_acc: 0.4353\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9520 - acc: 0.4833 - val_loss: 2.1777 - val_acc: 0.4420\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9496 - acc: 0.4843 - val_loss: 2.2015 - val_acc: 0.4375\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9398 - acc: 0.4857 - val_loss: 2.1813 - val_acc: 0.4387\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9582 - acc: 0.4825 - val_loss: 2.2466 - val_acc: 0.4323\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9661 - acc: 0.4807 - val_loss: 2.1998 - val_acc: 0.4394\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1869 - acc: 0.4258 - val_loss: 2.2017 - val_acc: 0.4404\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1539 - acc: 0.4314 - val_loss: 2.2039 - val_acc: 0.4406\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1413 - acc: 0.4367 - val_loss: 2.2226 - val_acc: 0.4371\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1334 - acc: 0.4373 - val_loss: 2.2206 - val_acc: 0.4392\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1207 - acc: 0.4400 - val_loss: 2.2282 - val_acc: 0.4378\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1168 - acc: 0.4393 - val_loss: 2.2223 - val_acc: 0.4378\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1081 - acc: 0.4416 - val_loss: 2.2213 - val_acc: 0.4361\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1039 - acc: 0.4416 - val_loss: 2.2198 - val_acc: 0.4381\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0987 - acc: 0.4426 - val_loss: 2.2159 - val_acc: 0.4366\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0924 - acc: 0.4437 - val_loss: 2.2136 - val_acc: 0.4386\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0826 - acc: 0.4477 - val_loss: 2.2148 - val_acc: 0.4389\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0809 - acc: 0.4475 - val_loss: 2.2237 - val_acc: 0.4373\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0791 - acc: 0.4461 - val_loss: 2.2307 - val_acc: 0.4362\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0731 - acc: 0.4486 - val_loss: 2.2284 - val_acc: 0.4382\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0683 - acc: 0.4485 - val_loss: 2.2214 - val_acc: 0.4386\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0611 - acc: 0.4515 - val_loss: 2.2196 - val_acc: 0.4378\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0593 - acc: 0.4511 - val_loss: 2.2319 - val_acc: 0.4359\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0639 - acc: 0.4502 - val_loss: 2.2266 - val_acc: 0.4385\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0603 - acc: 0.4505 - val_loss: 2.2166 - val_acc: 0.4363\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0561 - acc: 0.4521 - val_loss: 2.2138 - val_acc: 0.4396\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0545 - acc: 0.4522 - val_loss: 2.2124 - val_acc: 0.4399\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0463 - acc: 0.4540 - val_loss: 2.2231 - val_acc: 0.4398\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0579 - acc: 0.4519 - val_loss: 2.2136 - val_acc: 0.4387\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0418 - acc: 0.4549 - val_loss: 2.2223 - val_acc: 0.4372\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0406 - acc: 0.4567 - val_loss: 2.2100 - val_acc: 0.4380\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0362 - acc: 0.4544 - val_loss: 2.2229 - val_acc: 0.4407\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0314 - acc: 0.4584 - val_loss: 2.2090 - val_acc: 0.4401\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0279 - acc: 0.4583 - val_loss: 2.2132 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0238 - acc: 0.4602 - val_loss: 2.2055 - val_acc: 0.4398\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0247 - acc: 0.4606 - val_loss: 2.2070 - val_acc: 0.4403\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0187 - acc: 0.4616 - val_loss: 2.2068 - val_acc: 0.4398\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0162 - acc: 0.4608 - val_loss: 2.2177 - val_acc: 0.4409\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0251 - acc: 0.4603 - val_loss: 2.2149 - val_acc: 0.4384\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0157 - acc: 0.4605 - val_loss: 2.2323 - val_acc: 0.4412\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0145 - acc: 0.4615 - val_loss: 2.2157 - val_acc: 0.4381\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0140 - acc: 0.4603 - val_loss: 2.2072 - val_acc: 0.4426\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0059 - acc: 0.4627 - val_loss: 2.2178 - val_acc: 0.4408\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0088 - acc: 0.4625 - val_loss: 2.2062 - val_acc: 0.4415\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0119 - acc: 0.4611 - val_loss: 2.2040 - val_acc: 0.4407\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0043 - acc: 0.4639 - val_loss: 2.2072 - val_acc: 0.4430\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9996 - acc: 0.4659 - val_loss: 2.2080 - val_acc: 0.4388\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9980 - acc: 0.4650 - val_loss: 2.2118 - val_acc: 0.4441\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9949 - acc: 0.4647 - val_loss: 2.2128 - val_acc: 0.4392\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9992 - acc: 0.4648 - val_loss: 2.2065 - val_acc: 0.4406\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9898 - acc: 0.4683 - val_loss: 2.2015 - val_acc: 0.4416\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9909 - acc: 0.4675 - val_loss: 2.1959 - val_acc: 0.4433\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9878 - acc: 0.4679 - val_loss: 2.2008 - val_acc: 0.4425\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9887 - acc: 0.4682 - val_loss: 2.1961 - val_acc: 0.4419\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9815 - acc: 0.4695 - val_loss: 2.2012 - val_acc: 0.4424\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9813 - acc: 0.4684 - val_loss: 2.1988 - val_acc: 0.4424\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9815 - acc: 0.4696 - val_loss: 2.2073 - val_acc: 0.4438\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9752 - acc: 0.4723 - val_loss: 2.2178 - val_acc: 0.4378\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9826 - acc: 0.4676 - val_loss: 2.2066 - val_acc: 0.4426\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9737 - acc: 0.4712 - val_loss: 2.1995 - val_acc: 0.4400\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9734 - acc: 0.4715 - val_loss: 2.2197 - val_acc: 0.4432\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9855 - acc: 0.4692 - val_loss: 2.2047 - val_acc: 0.4397\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9769 - acc: 0.4680 - val_loss: 2.2196 - val_acc: 0.4440\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9800 - acc: 0.4697 - val_loss: 2.2107 - val_acc: 0.4422\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9709 - acc: 0.4729 - val_loss: 2.1966 - val_acc: 0.4451\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9652 - acc: 0.4732 - val_loss: 2.2127 - val_acc: 0.4368\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9743 - acc: 0.4696 - val_loss: 2.2188 - val_acc: 0.4416\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9745 - acc: 0.4683 - val_loss: 2.2347 - val_acc: 0.4362\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9685 - acc: 0.4714 - val_loss: 2.1949 - val_acc: 0.4450\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9598 - acc: 0.4724 - val_loss: 2.2023 - val_acc: 0.4417\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9613 - acc: 0.4718 - val_loss: 2.2042 - val_acc: 0.4423\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9563 - acc: 0.4736 - val_loss: 2.1946 - val_acc: 0.4445\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9504 - acc: 0.4761 - val_loss: 2.2127 - val_acc: 0.4404\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9583 - acc: 0.4736 - val_loss: 2.1974 - val_acc: 0.4453\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9543 - acc: 0.4758 - val_loss: 2.2098 - val_acc: 0.4407\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9675 - acc: 0.4716 - val_loss: 2.2000 - val_acc: 0.4449\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9637 - acc: 0.4741 - val_loss: 2.2105 - val_acc: 0.4430\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9525 - acc: 0.4755 - val_loss: 2.2159 - val_acc: 0.4384\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9502 - acc: 0.4757 - val_loss: 2.2250 - val_acc: 0.4431\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9818 - acc: 0.4712 - val_loss: 2.2406 - val_acc: 0.4317\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9678 - acc: 0.4716 - val_loss: 2.2032 - val_acc: 0.4451\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9467 - acc: 0.4773 - val_loss: 2.2038 - val_acc: 0.4399\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9407 - acc: 0.4776 - val_loss: 2.2141 - val_acc: 0.4436\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9429 - acc: 0.4780 - val_loss: 2.2039 - val_acc: 0.4399\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9429 - acc: 0.4772 - val_loss: 2.2291 - val_acc: 0.4404\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9602 - acc: 0.4730 - val_loss: 2.2131 - val_acc: 0.4393\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9398 - acc: 0.4792 - val_loss: 2.2015 - val_acc: 0.4470\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9314 - acc: 0.4813 - val_loss: 2.2153 - val_acc: 0.4402\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9427 - acc: 0.4782 - val_loss: 2.2076 - val_acc: 0.4428\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9388 - acc: 0.4791 - val_loss: 2.1952 - val_acc: 0.4424\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9297 - acc: 0.4790 - val_loss: 2.1989 - val_acc: 0.4456\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9249 - acc: 0.4825 - val_loss: 2.1992 - val_acc: 0.4436\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9281 - acc: 0.4796 - val_loss: 2.1990 - val_acc: 0.4434\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9258 - acc: 0.4826 - val_loss: 2.1971 - val_acc: 0.4445\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9311 - acc: 0.4815 - val_loss: 2.2070 - val_acc: 0.4414\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9202 - acc: 0.4829 - val_loss: 2.2058 - val_acc: 0.4417\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9160 - acc: 0.4839 - val_loss: 2.2037 - val_acc: 0.4407\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9122 - acc: 0.4852 - val_loss: 2.1918 - val_acc: 0.4449\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9118 - acc: 0.4841 - val_loss: 2.2100 - val_acc: 0.4401\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9137 - acc: 0.4830 - val_loss: 2.1922 - val_acc: 0.4448\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9166 - acc: 0.4845 - val_loss: 2.2341 - val_acc: 0.4354\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9282 - acc: 0.4821 - val_loss: 2.2088 - val_acc: 0.4432\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9190 - acc: 0.4837 - val_loss: 2.2075 - val_acc: 0.4398\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9174 - acc: 0.4828 - val_loss: 2.1915 - val_acc: 0.4462\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9054 - acc: 0.4867 - val_loss: 2.2127 - val_acc: 0.4450\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9173 - acc: 0.4825 - val_loss: 2.1994 - val_acc: 0.4428\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1847 - acc: 0.4302 - val_loss: 2.1640 - val_acc: 0.4454\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1556 - acc: 0.4351 - val_loss: 2.1688 - val_acc: 0.4461\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1455 - acc: 0.4372 - val_loss: 2.1895 - val_acc: 0.4435\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1545 - acc: 0.4364 - val_loss: 2.1940 - val_acc: 0.4394\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1304 - acc: 0.4398 - val_loss: 2.1810 - val_acc: 0.4431\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1168 - acc: 0.4427 - val_loss: 2.1749 - val_acc: 0.4425\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1094 - acc: 0.4460 - val_loss: 2.1790 - val_acc: 0.4426\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1046 - acc: 0.4464 - val_loss: 2.1834 - val_acc: 0.4412\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0981 - acc: 0.4470 - val_loss: 2.1791 - val_acc: 0.4429\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0948 - acc: 0.4467 - val_loss: 2.1874 - val_acc: 0.4417\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0895 - acc: 0.4497 - val_loss: 2.1918 - val_acc: 0.4399\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0853 - acc: 0.4498 - val_loss: 2.1850 - val_acc: 0.4398\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0771 - acc: 0.4512 - val_loss: 2.1941 - val_acc: 0.4417\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0755 - acc: 0.4515 - val_loss: 2.1789 - val_acc: 0.4408\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0721 - acc: 0.4526 - val_loss: 2.1845 - val_acc: 0.4397\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0670 - acc: 0.4545 - val_loss: 2.1788 - val_acc: 0.4423\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0634 - acc: 0.4532 - val_loss: 2.1857 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0551 - acc: 0.4569 - val_loss: 2.1862 - val_acc: 0.4422\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0568 - acc: 0.4570 - val_loss: 2.1959 - val_acc: 0.4399\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0578 - acc: 0.4556 - val_loss: 2.1933 - val_acc: 0.4400\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0473 - acc: 0.4584 - val_loss: 2.1911 - val_acc: 0.4399\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0478 - acc: 0.4581 - val_loss: 2.1846 - val_acc: 0.4446\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0450 - acc: 0.4593 - val_loss: 2.1813 - val_acc: 0.4421\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0401 - acc: 0.4591 - val_loss: 2.1882 - val_acc: 0.4401\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0394 - acc: 0.4616 - val_loss: 2.1879 - val_acc: 0.4387\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0394 - acc: 0.4601 - val_loss: 2.1829 - val_acc: 0.4444\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0284 - acc: 0.4620 - val_loss: 2.1788 - val_acc: 0.4428\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0266 - acc: 0.4629 - val_loss: 2.1810 - val_acc: 0.4390\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0235 - acc: 0.4634 - val_loss: 2.1827 - val_acc: 0.4420\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0298 - acc: 0.4620 - val_loss: 2.1823 - val_acc: 0.4402\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0195 - acc: 0.4633 - val_loss: 2.1836 - val_acc: 0.4401\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0215 - acc: 0.4630 - val_loss: 2.1940 - val_acc: 0.4393\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0210 - acc: 0.4659 - val_loss: 2.1918 - val_acc: 0.4403\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0108 - acc: 0.4673 - val_loss: 2.1862 - val_acc: 0.4411\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0113 - acc: 0.4652 - val_loss: 2.1837 - val_acc: 0.4438\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0083 - acc: 0.4672 - val_loss: 2.1942 - val_acc: 0.4409\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0043 - acc: 0.4673 - val_loss: 2.1890 - val_acc: 0.4396\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0122 - acc: 0.4653 - val_loss: 2.1808 - val_acc: 0.4419\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0007 - acc: 0.4660 - val_loss: 2.1754 - val_acc: 0.4420\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0096 - acc: 0.4673 - val_loss: 2.2273 - val_acc: 0.4326\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0207 - acc: 0.4648 - val_loss: 2.1762 - val_acc: 0.4427\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0030 - acc: 0.4668 - val_loss: 2.1960 - val_acc: 0.4422\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9958 - acc: 0.4694 - val_loss: 2.1812 - val_acc: 0.4409\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9978 - acc: 0.4675 - val_loss: 2.1908 - val_acc: 0.4410\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9932 - acc: 0.4708 - val_loss: 2.1899 - val_acc: 0.4382\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9942 - acc: 0.4705 - val_loss: 2.1861 - val_acc: 0.4425\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9886 - acc: 0.4713 - val_loss: 2.1845 - val_acc: 0.4441\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9837 - acc: 0.4725 - val_loss: 2.1867 - val_acc: 0.4397\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9818 - acc: 0.4717 - val_loss: 2.1942 - val_acc: 0.4402\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9811 - acc: 0.4725 - val_loss: 2.1771 - val_acc: 0.4430\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9760 - acc: 0.4731 - val_loss: 2.1887 - val_acc: 0.4413\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9725 - acc: 0.4734 - val_loss: 2.1783 - val_acc: 0.4405\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9719 - acc: 0.4737 - val_loss: 2.1770 - val_acc: 0.4431\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9725 - acc: 0.4723 - val_loss: 2.1816 - val_acc: 0.4420\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9734 - acc: 0.4731 - val_loss: 2.1914 - val_acc: 0.4397\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9750 - acc: 0.4723 - val_loss: 2.2080 - val_acc: 0.4348\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9825 - acc: 0.4725 - val_loss: 2.1936 - val_acc: 0.4404\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9757 - acc: 0.4748 - val_loss: 2.1839 - val_acc: 0.4393\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9744 - acc: 0.4734 - val_loss: 2.1878 - val_acc: 0.4408\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9664 - acc: 0.4761 - val_loss: 2.2027 - val_acc: 0.4361\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9696 - acc: 0.4748 - val_loss: 2.1947 - val_acc: 0.4407\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9608 - acc: 0.4761 - val_loss: 2.1872 - val_acc: 0.4386\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9612 - acc: 0.4763 - val_loss: 2.2090 - val_acc: 0.4398\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9768 - acc: 0.4746 - val_loss: 2.1909 - val_acc: 0.4374\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9610 - acc: 0.4755 - val_loss: 2.2007 - val_acc: 0.4394\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9713 - acc: 0.4748 - val_loss: 2.1924 - val_acc: 0.4376\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9605 - acc: 0.4777 - val_loss: 2.1804 - val_acc: 0.4455\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9614 - acc: 0.4785 - val_loss: 2.1847 - val_acc: 0.4402\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9491 - acc: 0.4807 - val_loss: 2.1830 - val_acc: 0.4414\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9507 - acc: 0.4801 - val_loss: 2.1780 - val_acc: 0.4428\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9447 - acc: 0.4800 - val_loss: 2.1844 - val_acc: 0.4403\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9447 - acc: 0.4805 - val_loss: 2.1767 - val_acc: 0.4429\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9449 - acc: 0.4801 - val_loss: 2.1833 - val_acc: 0.4423\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9442 - acc: 0.4799 - val_loss: 2.1865 - val_acc: 0.4408\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9490 - acc: 0.4802 - val_loss: 2.1990 - val_acc: 0.4407\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9575 - acc: 0.4785 - val_loss: 2.1883 - val_acc: 0.4373\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9519 - acc: 0.4768 - val_loss: 2.2181 - val_acc: 0.4412\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9661 - acc: 0.4755 - val_loss: 2.1922 - val_acc: 0.4400\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9474 - acc: 0.4802 - val_loss: 2.1966 - val_acc: 0.4405\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9415 - acc: 0.4809 - val_loss: 2.1822 - val_acc: 0.4404\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9341 - acc: 0.4829 - val_loss: 2.1966 - val_acc: 0.4416\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9367 - acc: 0.4823 - val_loss: 2.1798 - val_acc: 0.4414\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9336 - acc: 0.4834 - val_loss: 2.1942 - val_acc: 0.4397\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9279 - acc: 0.4858 - val_loss: 2.1902 - val_acc: 0.4436\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9274 - acc: 0.4837 - val_loss: 2.2010 - val_acc: 0.4418\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9381 - acc: 0.4817 - val_loss: 2.1864 - val_acc: 0.4386\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9262 - acc: 0.4834 - val_loss: 2.2065 - val_acc: 0.4390\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9271 - acc: 0.4844 - val_loss: 2.1862 - val_acc: 0.4389\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9313 - acc: 0.4843 - val_loss: 2.2097 - val_acc: 0.4404\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9303 - acc: 0.4815 - val_loss: 2.2136 - val_acc: 0.4337\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9404 - acc: 0.4824 - val_loss: 2.2057 - val_acc: 0.4416\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9256 - acc: 0.4862 - val_loss: 2.1837 - val_acc: 0.4412\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9175 - acc: 0.4847 - val_loss: 2.1781 - val_acc: 0.4435\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9094 - acc: 0.4892 - val_loss: 2.1977 - val_acc: 0.4356\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9092 - acc: 0.4895 - val_loss: 2.1905 - val_acc: 0.4405\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9085 - acc: 0.4875 - val_loss: 2.1944 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9166 - acc: 0.4871 - val_loss: 2.1994 - val_acc: 0.4399\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9162 - acc: 0.4890 - val_loss: 2.1851 - val_acc: 0.4396\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8998 - acc: 0.4895 - val_loss: 2.1876 - val_acc: 0.4434\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9053 - acc: 0.4886 - val_loss: 2.1885 - val_acc: 0.4408\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1927 - acc: 0.4243 - val_loss: 2.2149 - val_acc: 0.4399\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1688 - acc: 0.4285 - val_loss: 2.2146 - val_acc: 0.4399\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1543 - acc: 0.4345 - val_loss: 2.2411 - val_acc: 0.4376\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1544 - acc: 0.4329 - val_loss: 2.2288 - val_acc: 0.4393\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1373 - acc: 0.4354 - val_loss: 2.2344 - val_acc: 0.4364\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1278 - acc: 0.4363 - val_loss: 2.2430 - val_acc: 0.4352\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1242 - acc: 0.4372 - val_loss: 2.2457 - val_acc: 0.4339\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1174 - acc: 0.4384 - val_loss: 2.2345 - val_acc: 0.4340\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1122 - acc: 0.4374 - val_loss: 2.2406 - val_acc: 0.4355\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1051 - acc: 0.4410 - val_loss: 2.2588 - val_acc: 0.4332\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1010 - acc: 0.4417 - val_loss: 2.2722 - val_acc: 0.4298\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0975 - acc: 0.4424 - val_loss: 2.2474 - val_acc: 0.4320\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0982 - acc: 0.4417 - val_loss: 2.2569 - val_acc: 0.4309\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0890 - acc: 0.4448 - val_loss: 2.2385 - val_acc: 0.4342\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0848 - acc: 0.4477 - val_loss: 2.2448 - val_acc: 0.4346\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0781 - acc: 0.4466 - val_loss: 2.2423 - val_acc: 0.4343\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0794 - acc: 0.4476 - val_loss: 2.2483 - val_acc: 0.4328\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0738 - acc: 0.4478 - val_loss: 2.2504 - val_acc: 0.4329\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0705 - acc: 0.4481 - val_loss: 2.2280 - val_acc: 0.4366\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0684 - acc: 0.4494 - val_loss: 2.2584 - val_acc: 0.4328\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0782 - acc: 0.4449 - val_loss: 2.2307 - val_acc: 0.4346\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0688 - acc: 0.4478 - val_loss: 2.2445 - val_acc: 0.4326\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0651 - acc: 0.4484 - val_loss: 2.2279 - val_acc: 0.4390\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0590 - acc: 0.4498 - val_loss: 2.2293 - val_acc: 0.4373\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0587 - acc: 0.4489 - val_loss: 2.2383 - val_acc: 0.4356\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0467 - acc: 0.4540 - val_loss: 2.2149 - val_acc: 0.4379\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0492 - acc: 0.4512 - val_loss: 2.2389 - val_acc: 0.4333\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0468 - acc: 0.4530 - val_loss: 2.2217 - val_acc: 0.4377\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0422 - acc: 0.4531 - val_loss: 2.2317 - val_acc: 0.4345\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0411 - acc: 0.4562 - val_loss: 2.2387 - val_acc: 0.4359\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0525 - acc: 0.4519 - val_loss: 2.2323 - val_acc: 0.4368\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0433 - acc: 0.4519 - val_loss: 2.2100 - val_acc: 0.4417\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0316 - acc: 0.4552 - val_loss: 2.2311 - val_acc: 0.4363\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0325 - acc: 0.4569 - val_loss: 2.2193 - val_acc: 0.4410\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0334 - acc: 0.4565 - val_loss: 2.2138 - val_acc: 0.4426\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0238 - acc: 0.4575 - val_loss: 2.2380 - val_acc: 0.4354\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0271 - acc: 0.4566 - val_loss: 2.2325 - val_acc: 0.4394\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0340 - acc: 0.4572 - val_loss: 2.2193 - val_acc: 0.4385\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0197 - acc: 0.4579 - val_loss: 2.2181 - val_acc: 0.4391\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0162 - acc: 0.4582 - val_loss: 2.2111 - val_acc: 0.4397\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0200 - acc: 0.4573 - val_loss: 2.2162 - val_acc: 0.4406\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0126 - acc: 0.4601 - val_loss: 2.2109 - val_acc: 0.4413\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0105 - acc: 0.4604 - val_loss: 2.2148 - val_acc: 0.4414\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0047 - acc: 0.4618 - val_loss: 2.2108 - val_acc: 0.4416\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9984 - acc: 0.4624 - val_loss: 2.2099 - val_acc: 0.4412\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0027 - acc: 0.4625 - val_loss: 2.2135 - val_acc: 0.4413\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0120 - acc: 0.4615 - val_loss: 2.2192 - val_acc: 0.4417\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0022 - acc: 0.4642 - val_loss: 2.2124 - val_acc: 0.4400\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0046 - acc: 0.4612 - val_loss: 2.2274 - val_acc: 0.4377\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0008 - acc: 0.4628 - val_loss: 2.2201 - val_acc: 0.4377\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0012 - acc: 0.4644 - val_loss: 2.2600 - val_acc: 0.4343\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0207 - acc: 0.4594 - val_loss: 2.2213 - val_acc: 0.4378\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0010 - acc: 0.4619 - val_loss: 2.2253 - val_acc: 0.4400\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9926 - acc: 0.4637 - val_loss: 2.2102 - val_acc: 0.4408\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0040 - acc: 0.4638 - val_loss: 2.2204 - val_acc: 0.4398\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9901 - acc: 0.4657 - val_loss: 2.2303 - val_acc: 0.4361\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9918 - acc: 0.4664 - val_loss: 2.2123 - val_acc: 0.4408\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9835 - acc: 0.4658 - val_loss: 2.2072 - val_acc: 0.4394\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9803 - acc: 0.4659 - val_loss: 2.2265 - val_acc: 0.4393\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9790 - acc: 0.4691 - val_loss: 2.2037 - val_acc: 0.4404\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9808 - acc: 0.4668 - val_loss: 2.2330 - val_acc: 0.4381\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9868 - acc: 0.4666 - val_loss: 2.2299 - val_acc: 0.4383\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9814 - acc: 0.4678 - val_loss: 2.2126 - val_acc: 0.4409\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9752 - acc: 0.4701 - val_loss: 2.2179 - val_acc: 0.4374\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9777 - acc: 0.4687 - val_loss: 2.2244 - val_acc: 0.4367\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9705 - acc: 0.4692 - val_loss: 2.2325 - val_acc: 0.4370\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9931 - acc: 0.4647 - val_loss: 2.2090 - val_acc: 0.4434\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9821 - acc: 0.4679 - val_loss: 2.2276 - val_acc: 0.4377\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9737 - acc: 0.4708 - val_loss: 2.2022 - val_acc: 0.4436\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9648 - acc: 0.4704 - val_loss: 2.2239 - val_acc: 0.4391\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9622 - acc: 0.4721 - val_loss: 2.2090 - val_acc: 0.4426\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9588 - acc: 0.4722 - val_loss: 2.2071 - val_acc: 0.4409\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9569 - acc: 0.4726 - val_loss: 2.2062 - val_acc: 0.4446\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9730 - acc: 0.4703 - val_loss: 2.2191 - val_acc: 0.4387\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9659 - acc: 0.4723 - val_loss: 2.2086 - val_acc: 0.4419\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9688 - acc: 0.4711 - val_loss: 2.2283 - val_acc: 0.4398\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9761 - acc: 0.4681 - val_loss: 2.2094 - val_acc: 0.4419\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9597 - acc: 0.4719 - val_loss: 2.2211 - val_acc: 0.4386\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9597 - acc: 0.4730 - val_loss: 2.2305 - val_acc: 0.4381\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9544 - acc: 0.4734 - val_loss: 2.2222 - val_acc: 0.4397\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9639 - acc: 0.4717 - val_loss: 2.2131 - val_acc: 0.4391\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9417 - acc: 0.4766 - val_loss: 2.2009 - val_acc: 0.4439\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9359 - acc: 0.4768 - val_loss: 2.2047 - val_acc: 0.4421\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9379 - acc: 0.4779 - val_loss: 2.2073 - val_acc: 0.4452\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9383 - acc: 0.4774 - val_loss: 2.2219 - val_acc: 0.4386\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9410 - acc: 0.4785 - val_loss: 2.2112 - val_acc: 0.4416\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9401 - acc: 0.4754 - val_loss: 2.2172 - val_acc: 0.4380\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9391 - acc: 0.4772 - val_loss: 2.2409 - val_acc: 0.4367\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9643 - acc: 0.4710 - val_loss: 2.2161 - val_acc: 0.4403\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9366 - acc: 0.4755 - val_loss: 2.2000 - val_acc: 0.4444\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9386 - acc: 0.4779 - val_loss: 2.2403 - val_acc: 0.4379\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9545 - acc: 0.4747 - val_loss: 2.2128 - val_acc: 0.4416\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9480 - acc: 0.4740 - val_loss: 2.2277 - val_acc: 0.4385\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9456 - acc: 0.4757 - val_loss: 2.1970 - val_acc: 0.4430\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9269 - acc: 0.4782 - val_loss: 2.2125 - val_acc: 0.4391\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9296 - acc: 0.4792 - val_loss: 2.2337 - val_acc: 0.4402\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9353 - acc: 0.4803 - val_loss: 2.2557 - val_acc: 0.4380\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9776 - acc: 0.4696 - val_loss: 2.2525 - val_acc: 0.4325\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9634 - acc: 0.4726 - val_loss: 2.2275 - val_acc: 0.4384\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9344 - acc: 0.4777 - val_loss: 2.2186 - val_acc: 0.4402\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2138 - acc: 0.4242 - val_loss: 2.2016 - val_acc: 0.4420\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1840 - acc: 0.4290 - val_loss: 2.1924 - val_acc: 0.4432\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1695 - acc: 0.4308 - val_loss: 2.2069 - val_acc: 0.4431\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1613 - acc: 0.4326 - val_loss: 2.2196 - val_acc: 0.4404\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1527 - acc: 0.4339 - val_loss: 2.2208 - val_acc: 0.4395\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1441 - acc: 0.4367 - val_loss: 2.2323 - val_acc: 0.4346\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1445 - acc: 0.4360 - val_loss: 2.2349 - val_acc: 0.4399\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1757 - acc: 0.4317 - val_loss: 2.2683 - val_acc: 0.4262\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1653 - acc: 0.4346 - val_loss: 2.2528 - val_acc: 0.4296\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1358 - acc: 0.4391 - val_loss: 2.2340 - val_acc: 0.4364\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1184 - acc: 0.4418 - val_loss: 2.2290 - val_acc: 0.4357\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1137 - acc: 0.4421 - val_loss: 2.2261 - val_acc: 0.4377\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1050 - acc: 0.4457 - val_loss: 2.2154 - val_acc: 0.4374\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1073 - acc: 0.4440 - val_loss: 2.2250 - val_acc: 0.4365\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0999 - acc: 0.4468 - val_loss: 2.2193 - val_acc: 0.4391\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1007 - acc: 0.4456 - val_loss: 2.2384 - val_acc: 0.4360\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0998 - acc: 0.4453 - val_loss: 2.2262 - val_acc: 0.4390\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1155 - acc: 0.4447 - val_loss: 2.2362 - val_acc: 0.4343\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0962 - acc: 0.4476 - val_loss: 2.2095 - val_acc: 0.4383\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0845 - acc: 0.4495 - val_loss: 2.2246 - val_acc: 0.4373\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0821 - acc: 0.4506 - val_loss: 2.2625 - val_acc: 0.4381\n",
      "Epoch 22/100\n",
      "1s - loss: 2.1445 - acc: 0.4397 - val_loss: 2.2844 - val_acc: 0.4229\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1468 - acc: 0.4367 - val_loss: 2.2738 - val_acc: 0.4271\n",
      "Epoch 24/100\n",
      "1s - loss: 2.1241 - acc: 0.4408 - val_loss: 2.2676 - val_acc: 0.4285\n",
      "Epoch 25/100\n",
      "1s - loss: 2.1132 - acc: 0.4438 - val_loss: 2.2429 - val_acc: 0.4333\n",
      "Epoch 26/100\n",
      "1s - loss: 2.1001 - acc: 0.4474 - val_loss: 2.2613 - val_acc: 0.4372\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0900 - acc: 0.4497 - val_loss: 2.2306 - val_acc: 0.4399\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0683 - acc: 0.4543 - val_loss: 2.2286 - val_acc: 0.4395\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0636 - acc: 0.4547 - val_loss: 2.2294 - val_acc: 0.4405\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0655 - acc: 0.4535 - val_loss: 2.2227 - val_acc: 0.4411\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0658 - acc: 0.4531 - val_loss: 2.2222 - val_acc: 0.4412\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0632 - acc: 0.4527 - val_loss: 2.2221 - val_acc: 0.4405\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0588 - acc: 0.4539 - val_loss: 2.2255 - val_acc: 0.4398\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0554 - acc: 0.4559 - val_loss: 2.2214 - val_acc: 0.4408\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0433 - acc: 0.4568 - val_loss: 2.2173 - val_acc: 0.4408\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0455 - acc: 0.4589 - val_loss: 2.2097 - val_acc: 0.4412\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0395 - acc: 0.4593 - val_loss: 2.2000 - val_acc: 0.4432\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0374 - acc: 0.4595 - val_loss: 2.2071 - val_acc: 0.4430\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0355 - acc: 0.4603 - val_loss: 2.2012 - val_acc: 0.4444\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0384 - acc: 0.4577 - val_loss: 2.2224 - val_acc: 0.4418\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0361 - acc: 0.4603 - val_loss: 2.2106 - val_acc: 0.4456\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0293 - acc: 0.4607 - val_loss: 2.2231 - val_acc: 0.4418\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0297 - acc: 0.4633 - val_loss: 2.2105 - val_acc: 0.4429\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0403 - acc: 0.4590 - val_loss: 2.2139 - val_acc: 0.4449\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0341 - acc: 0.4608 - val_loss: 2.2211 - val_acc: 0.4414\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0362 - acc: 0.4608 - val_loss: 2.2245 - val_acc: 0.4432\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0337 - acc: 0.4610 - val_loss: 2.2015 - val_acc: 0.4437\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0331 - acc: 0.4592 - val_loss: 2.2069 - val_acc: 0.4450\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0269 - acc: 0.4626 - val_loss: 2.2012 - val_acc: 0.4462\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0313 - acc: 0.4607 - val_loss: 2.2111 - val_acc: 0.4444\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0195 - acc: 0.4637 - val_loss: 2.2593 - val_acc: 0.4356\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0477 - acc: 0.4592 - val_loss: 2.2165 - val_acc: 0.4430\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0145 - acc: 0.4644 - val_loss: 2.2074 - val_acc: 0.4421\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0125 - acc: 0.4632 - val_loss: 2.1956 - val_acc: 0.4450\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0068 - acc: 0.4669 - val_loss: 2.2079 - val_acc: 0.4428\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0238 - acc: 0.4628 - val_loss: 2.2225 - val_acc: 0.4420\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0218 - acc: 0.4629 - val_loss: 2.2088 - val_acc: 0.4416\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0100 - acc: 0.4669 - val_loss: 2.2011 - val_acc: 0.4446\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0032 - acc: 0.4660 - val_loss: 2.1965 - val_acc: 0.4439\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0008 - acc: 0.4699 - val_loss: 2.2139 - val_acc: 0.4421\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9978 - acc: 0.4679 - val_loss: 2.2192 - val_acc: 0.4412\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0133 - acc: 0.4660 - val_loss: 2.2296 - val_acc: 0.4412\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0096 - acc: 0.4654 - val_loss: 2.2670 - val_acc: 0.4327\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0196 - acc: 0.4627 - val_loss: 2.2013 - val_acc: 0.4445\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9941 - acc: 0.4701 - val_loss: 2.1928 - val_acc: 0.4446\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9864 - acc: 0.4704 - val_loss: 2.2071 - val_acc: 0.4434\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9939 - acc: 0.4685 - val_loss: 2.2023 - val_acc: 0.4430\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9960 - acc: 0.4695 - val_loss: 2.2564 - val_acc: 0.4375\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0257 - acc: 0.4610 - val_loss: 2.2357 - val_acc: 0.4410\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9993 - acc: 0.4664 - val_loss: 2.2055 - val_acc: 0.4458\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9925 - acc: 0.4688 - val_loss: 2.2034 - val_acc: 0.4435\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9924 - acc: 0.4702 - val_loss: 2.1983 - val_acc: 0.4438\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9836 - acc: 0.4722 - val_loss: 2.1956 - val_acc: 0.4454\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9851 - acc: 0.4715 - val_loss: 2.2096 - val_acc: 0.4408\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9890 - acc: 0.4706 - val_loss: 2.1967 - val_acc: 0.4429\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9937 - acc: 0.4686 - val_loss: 2.2060 - val_acc: 0.4444\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9818 - acc: 0.4726 - val_loss: 2.1986 - val_acc: 0.4445\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9841 - acc: 0.4714 - val_loss: 2.2173 - val_acc: 0.4418\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9911 - acc: 0.4701 - val_loss: 2.2013 - val_acc: 0.4462\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9850 - acc: 0.4704 - val_loss: 2.2202 - val_acc: 0.4398\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9862 - acc: 0.4715 - val_loss: 2.2000 - val_acc: 0.4452\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9729 - acc: 0.4749 - val_loss: 2.2105 - val_acc: 0.4406\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9654 - acc: 0.4772 - val_loss: 2.2075 - val_acc: 0.4439\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9760 - acc: 0.4741 - val_loss: 2.2343 - val_acc: 0.4423\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9648 - acc: 0.4759 - val_loss: 2.2093 - val_acc: 0.4430\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9702 - acc: 0.4760 - val_loss: 2.2687 - val_acc: 0.4354\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9896 - acc: 0.4711 - val_loss: 2.2333 - val_acc: 0.4417\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9913 - acc: 0.4712 - val_loss: 2.2113 - val_acc: 0.4428\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9746 - acc: 0.4739 - val_loss: 2.2172 - val_acc: 0.4447\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9817 - acc: 0.4712 - val_loss: 2.2353 - val_acc: 0.4387\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9724 - acc: 0.4718 - val_loss: 2.1962 - val_acc: 0.4439\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9543 - acc: 0.4784 - val_loss: 2.2024 - val_acc: 0.4444\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9520 - acc: 0.4791 - val_loss: 2.2205 - val_acc: 0.4420\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9610 - acc: 0.4756 - val_loss: 2.2169 - val_acc: 0.4406\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9709 - acc: 0.4729 - val_loss: 2.2326 - val_acc: 0.4402\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9694 - acc: 0.4750 - val_loss: 2.2285 - val_acc: 0.4403\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9571 - acc: 0.4781 - val_loss: 2.2160 - val_acc: 0.4434\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9499 - acc: 0.4802 - val_loss: 2.2128 - val_acc: 0.4406\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9644 - acc: 0.4755 - val_loss: 2.2171 - val_acc: 0.4422\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9572 - acc: 0.4788 - val_loss: 2.1968 - val_acc: 0.4449\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1748 - acc: 0.4265 - val_loss: 2.1785 - val_acc: 0.4421\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1541 - acc: 0.4315 - val_loss: 2.1923 - val_acc: 0.4369\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1549 - acc: 0.4324 - val_loss: 2.1991 - val_acc: 0.4323\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1322 - acc: 0.4351 - val_loss: 2.1974 - val_acc: 0.4340\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1223 - acc: 0.4398 - val_loss: 2.1977 - val_acc: 0.4366\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1109 - acc: 0.4395 - val_loss: 2.2004 - val_acc: 0.4346\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1067 - acc: 0.4411 - val_loss: 2.2164 - val_acc: 0.4311\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1007 - acc: 0.4414 - val_loss: 2.2082 - val_acc: 0.4337\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0964 - acc: 0.4435 - val_loss: 2.2016 - val_acc: 0.4351\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0875 - acc: 0.4454 - val_loss: 2.1999 - val_acc: 0.4348\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0842 - acc: 0.4461 - val_loss: 2.2111 - val_acc: 0.4343\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0799 - acc: 0.4484 - val_loss: 2.2066 - val_acc: 0.4325\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0744 - acc: 0.4502 - val_loss: 2.2245 - val_acc: 0.4296\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0745 - acc: 0.4479 - val_loss: 2.2166 - val_acc: 0.4324\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0756 - acc: 0.4487 - val_loss: 2.2154 - val_acc: 0.4302\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0677 - acc: 0.4509 - val_loss: 2.2057 - val_acc: 0.4350\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0603 - acc: 0.4524 - val_loss: 2.2263 - val_acc: 0.4310\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0604 - acc: 0.4524 - val_loss: 2.1996 - val_acc: 0.4366\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0539 - acc: 0.4518 - val_loss: 2.2060 - val_acc: 0.4361\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0603 - acc: 0.4532 - val_loss: 2.2088 - val_acc: 0.4328\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0446 - acc: 0.4547 - val_loss: 2.2043 - val_acc: 0.4336\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0398 - acc: 0.4567 - val_loss: 2.2086 - val_acc: 0.4352\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0463 - acc: 0.4551 - val_loss: 2.2030 - val_acc: 0.4347\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0392 - acc: 0.4554 - val_loss: 2.2198 - val_acc: 0.4340\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0391 - acc: 0.4559 - val_loss: 2.2066 - val_acc: 0.4358\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0335 - acc: 0.4583 - val_loss: 2.1967 - val_acc: 0.4361\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0293 - acc: 0.4588 - val_loss: 2.2171 - val_acc: 0.4342\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0476 - acc: 0.4559 - val_loss: 2.2153 - val_acc: 0.4342\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0263 - acc: 0.4581 - val_loss: 2.1987 - val_acc: 0.4392\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0330 - acc: 0.4579 - val_loss: 2.1975 - val_acc: 0.4378\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0231 - acc: 0.4597 - val_loss: 2.1912 - val_acc: 0.4388\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0174 - acc: 0.4621 - val_loss: 2.1982 - val_acc: 0.4363\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0138 - acc: 0.4618 - val_loss: 2.1904 - val_acc: 0.4393\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0089 - acc: 0.4625 - val_loss: 2.2107 - val_acc: 0.4354\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0141 - acc: 0.4616 - val_loss: 2.2049 - val_acc: 0.4328\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0130 - acc: 0.4624 - val_loss: 2.1914 - val_acc: 0.4396\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0106 - acc: 0.4616 - val_loss: 2.1904 - val_acc: 0.4379\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0095 - acc: 0.4622 - val_loss: 2.1964 - val_acc: 0.4388\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0105 - acc: 0.4624 - val_loss: 2.1877 - val_acc: 0.4376\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0061 - acc: 0.4635 - val_loss: 2.1979 - val_acc: 0.4374\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0033 - acc: 0.4630 - val_loss: 2.1892 - val_acc: 0.4388\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9981 - acc: 0.4649 - val_loss: 2.1832 - val_acc: 0.4407\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9923 - acc: 0.4672 - val_loss: 2.1832 - val_acc: 0.4406\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9968 - acc: 0.4661 - val_loss: 2.1768 - val_acc: 0.4432\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9953 - acc: 0.4667 - val_loss: 2.1836 - val_acc: 0.4429\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9923 - acc: 0.4683 - val_loss: 2.1889 - val_acc: 0.4401\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9875 - acc: 0.4671 - val_loss: 2.1967 - val_acc: 0.4380\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9891 - acc: 0.4695 - val_loss: 2.1925 - val_acc: 0.4394\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9864 - acc: 0.4686 - val_loss: 2.1897 - val_acc: 0.4405\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9851 - acc: 0.4679 - val_loss: 2.1817 - val_acc: 0.4403\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9850 - acc: 0.4692 - val_loss: 2.2050 - val_acc: 0.4366\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9969 - acc: 0.4666 - val_loss: 2.1788 - val_acc: 0.4429\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9723 - acc: 0.4719 - val_loss: 2.1720 - val_acc: 0.4453\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9714 - acc: 0.4720 - val_loss: 2.1912 - val_acc: 0.4408\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9713 - acc: 0.4708 - val_loss: 2.1914 - val_acc: 0.4427\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0038 - acc: 0.4660 - val_loss: 2.2041 - val_acc: 0.4359\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9788 - acc: 0.4692 - val_loss: 2.1823 - val_acc: 0.4431\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9647 - acc: 0.4715 - val_loss: 2.1791 - val_acc: 0.4427\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9643 - acc: 0.4727 - val_loss: 2.1767 - val_acc: 0.4453\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9739 - acc: 0.4703 - val_loss: 2.1811 - val_acc: 0.4443\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9622 - acc: 0.4738 - val_loss: 2.1753 - val_acc: 0.4453\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9568 - acc: 0.4749 - val_loss: 2.1798 - val_acc: 0.4454\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9535 - acc: 0.4763 - val_loss: 2.1802 - val_acc: 0.4403\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9578 - acc: 0.4766 - val_loss: 2.1743 - val_acc: 0.4433\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9619 - acc: 0.4727 - val_loss: 2.1795 - val_acc: 0.4441\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9566 - acc: 0.4754 - val_loss: 2.1843 - val_acc: 0.4434\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9606 - acc: 0.4741 - val_loss: 2.2052 - val_acc: 0.4381\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9588 - acc: 0.4737 - val_loss: 2.1875 - val_acc: 0.4420\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9565 - acc: 0.4765 - val_loss: 2.2069 - val_acc: 0.4378\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9645 - acc: 0.4735 - val_loss: 2.1858 - val_acc: 0.4428\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9530 - acc: 0.4769 - val_loss: 2.1895 - val_acc: 0.4426\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9369 - acc: 0.4798 - val_loss: 2.1860 - val_acc: 0.4416\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9437 - acc: 0.4778 - val_loss: 2.1844 - val_acc: 0.4426\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9381 - acc: 0.4802 - val_loss: 2.1816 - val_acc: 0.4421\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9407 - acc: 0.4799 - val_loss: 2.2034 - val_acc: 0.4397\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9530 - acc: 0.4740 - val_loss: 2.1985 - val_acc: 0.4380\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9431 - acc: 0.4775 - val_loss: 2.1769 - val_acc: 0.4417\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9441 - acc: 0.4788 - val_loss: 2.1957 - val_acc: 0.4401\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9402 - acc: 0.4786 - val_loss: 2.1780 - val_acc: 0.4438\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9392 - acc: 0.4777 - val_loss: 2.2026 - val_acc: 0.4387\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9384 - acc: 0.4781 - val_loss: 2.2183 - val_acc: 0.4360\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9461 - acc: 0.4761 - val_loss: 2.1947 - val_acc: 0.4421\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9436 - acc: 0.4768 - val_loss: 2.2055 - val_acc: 0.4406\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9400 - acc: 0.4795 - val_loss: 2.1866 - val_acc: 0.4425\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9288 - acc: 0.4815 - val_loss: 2.2014 - val_acc: 0.4392\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9240 - acc: 0.4831 - val_loss: 2.1908 - val_acc: 0.4427\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9336 - acc: 0.4803 - val_loss: 2.2015 - val_acc: 0.4387\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9324 - acc: 0.4823 - val_loss: 2.1909 - val_acc: 0.4419\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9274 - acc: 0.4811 - val_loss: 2.2147 - val_acc: 0.4379\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9215 - acc: 0.4836 - val_loss: 2.1830 - val_acc: 0.4462\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9180 - acc: 0.4811 - val_loss: 2.2137 - val_acc: 0.4381\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9186 - acc: 0.4816 - val_loss: 2.1817 - val_acc: 0.4444\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9135 - acc: 0.4822 - val_loss: 2.1992 - val_acc: 0.4428\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9158 - acc: 0.4846 - val_loss: 2.1834 - val_acc: 0.4415\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9125 - acc: 0.4855 - val_loss: 2.1942 - val_acc: 0.4408\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9205 - acc: 0.4849 - val_loss: 2.1903 - val_acc: 0.4404\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9231 - acc: 0.4807 - val_loss: 2.1982 - val_acc: 0.4392\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9173 - acc: 0.4827 - val_loss: 2.2054 - val_acc: 0.4426\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9281 - acc: 0.4830 - val_loss: 2.1931 - val_acc: 0.4390\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9155 - acc: 0.4838 - val_loss: 2.2040 - val_acc: 0.4433\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1766 - acc: 0.4291 - val_loss: 2.2283 - val_acc: 0.4361\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1502 - acc: 0.4355 - val_loss: 2.2351 - val_acc: 0.4324\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1383 - acc: 0.4371 - val_loss: 2.2417 - val_acc: 0.4316\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1324 - acc: 0.4387 - val_loss: 2.2510 - val_acc: 0.4300\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1252 - acc: 0.4403 - val_loss: 2.2389 - val_acc: 0.4304\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1125 - acc: 0.4430 - val_loss: 2.2585 - val_acc: 0.4295\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1079 - acc: 0.4437 - val_loss: 2.2627 - val_acc: 0.4278\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0998 - acc: 0.4450 - val_loss: 2.2610 - val_acc: 0.4284\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0937 - acc: 0.4454 - val_loss: 2.2851 - val_acc: 0.4261\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0903 - acc: 0.4475 - val_loss: 2.2594 - val_acc: 0.4299\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0857 - acc: 0.4463 - val_loss: 2.2581 - val_acc: 0.4269\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0784 - acc: 0.4494 - val_loss: 2.2874 - val_acc: 0.4261\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0822 - acc: 0.4488 - val_loss: 2.2690 - val_acc: 0.4261\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0704 - acc: 0.4509 - val_loss: 2.2767 - val_acc: 0.4260\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0698 - acc: 0.4503 - val_loss: 2.2536 - val_acc: 0.4276\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0665 - acc: 0.4524 - val_loss: 2.2656 - val_acc: 0.4245\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0609 - acc: 0.4528 - val_loss: 2.2728 - val_acc: 0.4231\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0578 - acc: 0.4540 - val_loss: 2.2616 - val_acc: 0.4232\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0534 - acc: 0.4557 - val_loss: 2.2664 - val_acc: 0.4259\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0479 - acc: 0.4554 - val_loss: 2.2685 - val_acc: 0.4257\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0456 - acc: 0.4582 - val_loss: 2.2779 - val_acc: 0.4272\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0528 - acc: 0.4537 - val_loss: 2.2632 - val_acc: 0.4259\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0401 - acc: 0.4581 - val_loss: 2.2607 - val_acc: 0.4228\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0370 - acc: 0.4579 - val_loss: 2.2967 - val_acc: 0.4223\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0323 - acc: 0.4601 - val_loss: 2.2627 - val_acc: 0.4254\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0307 - acc: 0.4602 - val_loss: 2.2792 - val_acc: 0.4215\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0227 - acc: 0.4619 - val_loss: 2.2571 - val_acc: 0.4260\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0258 - acc: 0.4609 - val_loss: 2.2884 - val_acc: 0.4230\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0229 - acc: 0.4606 - val_loss: 2.2571 - val_acc: 0.4247\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0199 - acc: 0.4614 - val_loss: 2.2908 - val_acc: 0.4227\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0261 - acc: 0.4606 - val_loss: 2.2692 - val_acc: 0.4248\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0147 - acc: 0.4655 - val_loss: 2.2794 - val_acc: 0.4223\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0186 - acc: 0.4624 - val_loss: 2.2654 - val_acc: 0.4230\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0121 - acc: 0.4654 - val_loss: 2.2643 - val_acc: 0.4252\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0121 - acc: 0.4626 - val_loss: 2.2705 - val_acc: 0.4264\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0061 - acc: 0.4663 - val_loss: 2.2632 - val_acc: 0.4268\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0027 - acc: 0.4657 - val_loss: 2.2530 - val_acc: 0.4264\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0020 - acc: 0.4650 - val_loss: 2.2719 - val_acc: 0.4237\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9981 - acc: 0.4671 - val_loss: 2.2591 - val_acc: 0.4244\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9960 - acc: 0.4682 - val_loss: 2.2639 - val_acc: 0.4246\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9901 - acc: 0.4687 - val_loss: 2.2608 - val_acc: 0.4295\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9929 - acc: 0.4670 - val_loss: 2.2498 - val_acc: 0.4302\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9894 - acc: 0.4699 - val_loss: 2.2858 - val_acc: 0.4240\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9960 - acc: 0.4665 - val_loss: 2.2564 - val_acc: 0.4271\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9944 - acc: 0.4671 - val_loss: 2.2967 - val_acc: 0.4229\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9927 - acc: 0.4676 - val_loss: 2.2577 - val_acc: 0.4263\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9943 - acc: 0.4679 - val_loss: 2.2816 - val_acc: 0.4277\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9940 - acc: 0.4676 - val_loss: 2.2790 - val_acc: 0.4254\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9891 - acc: 0.4712 - val_loss: 2.2576 - val_acc: 0.4294\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9849 - acc: 0.4705 - val_loss: 2.2804 - val_acc: 0.4244\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9876 - acc: 0.4714 - val_loss: 2.2500 - val_acc: 0.4285\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9741 - acc: 0.4726 - val_loss: 2.2527 - val_acc: 0.4278\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9763 - acc: 0.4716 - val_loss: 2.2404 - val_acc: 0.4296\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9737 - acc: 0.4733 - val_loss: 2.2623 - val_acc: 0.4257\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9701 - acc: 0.4738 - val_loss: 2.2492 - val_acc: 0.4301\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9685 - acc: 0.4741 - val_loss: 2.2423 - val_acc: 0.4304\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9661 - acc: 0.4736 - val_loss: 2.2526 - val_acc: 0.4309\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9660 - acc: 0.4749 - val_loss: 2.2647 - val_acc: 0.4267\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9664 - acc: 0.4747 - val_loss: 2.2730 - val_acc: 0.4263\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9706 - acc: 0.4742 - val_loss: 2.2660 - val_acc: 0.4254\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9724 - acc: 0.4733 - val_loss: 2.2707 - val_acc: 0.4312\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9759 - acc: 0.4714 - val_loss: 2.2525 - val_acc: 0.4291\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9675 - acc: 0.4725 - val_loss: 2.2702 - val_acc: 0.4313\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9755 - acc: 0.4718 - val_loss: 2.2481 - val_acc: 0.4302\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9637 - acc: 0.4753 - val_loss: 2.2680 - val_acc: 0.4300\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9650 - acc: 0.4743 - val_loss: 2.2346 - val_acc: 0.4346\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9586 - acc: 0.4753 - val_loss: 2.2562 - val_acc: 0.4297\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9595 - acc: 0.4754 - val_loss: 2.2361 - val_acc: 0.4318\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9451 - acc: 0.4806 - val_loss: 2.2476 - val_acc: 0.4304\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9441 - acc: 0.4804 - val_loss: 2.2412 - val_acc: 0.4312\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9461 - acc: 0.4796 - val_loss: 2.2473 - val_acc: 0.4328\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9479 - acc: 0.4779 - val_loss: 2.2344 - val_acc: 0.4323\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9504 - acc: 0.4764 - val_loss: 2.2532 - val_acc: 0.4309\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9499 - acc: 0.4779 - val_loss: 2.2390 - val_acc: 0.4328\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9430 - acc: 0.4806 - val_loss: 2.2560 - val_acc: 0.4291\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9407 - acc: 0.4809 - val_loss: 2.2329 - val_acc: 0.4347\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9344 - acc: 0.4827 - val_loss: 2.2499 - val_acc: 0.4291\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9370 - acc: 0.4816 - val_loss: 2.2394 - val_acc: 0.4334\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9346 - acc: 0.4827 - val_loss: 2.2515 - val_acc: 0.4297\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9354 - acc: 0.4803 - val_loss: 2.2320 - val_acc: 0.4344\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9350 - acc: 0.4834 - val_loss: 2.2451 - val_acc: 0.4307\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9279 - acc: 0.4833 - val_loss: 2.2301 - val_acc: 0.4335\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9344 - acc: 0.4839 - val_loss: 2.2704 - val_acc: 0.4276\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9303 - acc: 0.4842 - val_loss: 2.2340 - val_acc: 0.4334\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9304 - acc: 0.4824 - val_loss: 2.2572 - val_acc: 0.4323\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9368 - acc: 0.4821 - val_loss: 2.2491 - val_acc: 0.4314\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9303 - acc: 0.4817 - val_loss: 2.2605 - val_acc: 0.4308\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9351 - acc: 0.4811 - val_loss: 2.2477 - val_acc: 0.4326\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9297 - acc: 0.4823 - val_loss: 2.2490 - val_acc: 0.4312\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9203 - acc: 0.4852 - val_loss: 2.2630 - val_acc: 0.4274\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9269 - acc: 0.4843 - val_loss: 2.2696 - val_acc: 0.4294\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9239 - acc: 0.4856 - val_loss: 2.2637 - val_acc: 0.4286\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9281 - acc: 0.4823 - val_loss: 2.2759 - val_acc: 0.4287\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9279 - acc: 0.4839 - val_loss: 2.2852 - val_acc: 0.4262\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9257 - acc: 0.4831 - val_loss: 2.2708 - val_acc: 0.4313\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9320 - acc: 0.4830 - val_loss: 2.2552 - val_acc: 0.4289\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9180 - acc: 0.4851 - val_loss: 2.2508 - val_acc: 0.4318\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9086 - acc: 0.4882 - val_loss: 2.2411 - val_acc: 0.4328\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9090 - acc: 0.4854 - val_loss: 2.2500 - val_acc: 0.4315\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9134 - acc: 0.4876 - val_loss: 2.2463 - val_acc: 0.4317\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1900 - acc: 0.4267 - val_loss: 2.2216 - val_acc: 0.4371\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1657 - acc: 0.4332 - val_loss: 2.2284 - val_acc: 0.4367\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1606 - acc: 0.4345 - val_loss: 2.2236 - val_acc: 0.4390\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1479 - acc: 0.4376 - val_loss: 2.2110 - val_acc: 0.4406\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1348 - acc: 0.4392 - val_loss: 2.2147 - val_acc: 0.4389\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1269 - acc: 0.4411 - val_loss: 2.2202 - val_acc: 0.4404\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1219 - acc: 0.4408 - val_loss: 2.2089 - val_acc: 0.4411\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1126 - acc: 0.4437 - val_loss: 2.2157 - val_acc: 0.4404\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1081 - acc: 0.4455 - val_loss: 2.2372 - val_acc: 0.4366\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1025 - acc: 0.4442 - val_loss: 2.2295 - val_acc: 0.4389\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0994 - acc: 0.4465 - val_loss: 2.2298 - val_acc: 0.4379\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0904 - acc: 0.4466 - val_loss: 2.2240 - val_acc: 0.4402\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0914 - acc: 0.4484 - val_loss: 2.2378 - val_acc: 0.4400\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0842 - acc: 0.4493 - val_loss: 2.2273 - val_acc: 0.4392\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0806 - acc: 0.4500 - val_loss: 2.2169 - val_acc: 0.4405\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0789 - acc: 0.4527 - val_loss: 2.2248 - val_acc: 0.4396\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0795 - acc: 0.4513 - val_loss: 2.2199 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0718 - acc: 0.4526 - val_loss: 2.2374 - val_acc: 0.4379\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0666 - acc: 0.4525 - val_loss: 2.2164 - val_acc: 0.4416\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0607 - acc: 0.4552 - val_loss: 2.2279 - val_acc: 0.4394\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0601 - acc: 0.4543 - val_loss: 2.2177 - val_acc: 0.4390\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0619 - acc: 0.4537 - val_loss: 2.2247 - val_acc: 0.4375\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0601 - acc: 0.4544 - val_loss: 2.2171 - val_acc: 0.4419\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0516 - acc: 0.4565 - val_loss: 2.2340 - val_acc: 0.4382\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0528 - acc: 0.4552 - val_loss: 2.2400 - val_acc: 0.4382\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0622 - acc: 0.4529 - val_loss: 2.2557 - val_acc: 0.4348\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0468 - acc: 0.4577 - val_loss: 2.2116 - val_acc: 0.4408\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0387 - acc: 0.4584 - val_loss: 2.2306 - val_acc: 0.4381\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0418 - acc: 0.4592 - val_loss: 2.2182 - val_acc: 0.4385\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0369 - acc: 0.4587 - val_loss: 2.2227 - val_acc: 0.4413\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0325 - acc: 0.4615 - val_loss: 2.2295 - val_acc: 0.4369\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0348 - acc: 0.4604 - val_loss: 2.2131 - val_acc: 0.4411\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0289 - acc: 0.4602 - val_loss: 2.2242 - val_acc: 0.4395\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0259 - acc: 0.4623 - val_loss: 2.2048 - val_acc: 0.4441\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0258 - acc: 0.4621 - val_loss: 2.2170 - val_acc: 0.4414\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0243 - acc: 0.4612 - val_loss: 2.2255 - val_acc: 0.4398\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0261 - acc: 0.4609 - val_loss: 2.2225 - val_acc: 0.4387\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0170 - acc: 0.4651 - val_loss: 2.2128 - val_acc: 0.4425\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0196 - acc: 0.4629 - val_loss: 2.2376 - val_acc: 0.4373\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0197 - acc: 0.4639 - val_loss: 2.2101 - val_acc: 0.4417\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0226 - acc: 0.4635 - val_loss: 2.2211 - val_acc: 0.4416\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0066 - acc: 0.4660 - val_loss: 2.2082 - val_acc: 0.4412\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0100 - acc: 0.4641 - val_loss: 2.2362 - val_acc: 0.4391\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0079 - acc: 0.4657 - val_loss: 2.2076 - val_acc: 0.4408\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0045 - acc: 0.4663 - val_loss: 2.2273 - val_acc: 0.4419\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0263 - acc: 0.4635 - val_loss: 2.2361 - val_acc: 0.4383\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0100 - acc: 0.4648 - val_loss: 2.2214 - val_acc: 0.4388\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9958 - acc: 0.4674 - val_loss: 2.2125 - val_acc: 0.4422\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9999 - acc: 0.4677 - val_loss: 2.2212 - val_acc: 0.4390\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9983 - acc: 0.4685 - val_loss: 2.2663 - val_acc: 0.4390\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0556 - acc: 0.4589 - val_loss: 2.2797 - val_acc: 0.4277\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0457 - acc: 0.4595 - val_loss: 2.2480 - val_acc: 0.4358\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0121 - acc: 0.4644 - val_loss: 2.2115 - val_acc: 0.4390\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9944 - acc: 0.4683 - val_loss: 2.2311 - val_acc: 0.4409\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9856 - acc: 0.4701 - val_loss: 2.2124 - val_acc: 0.4377\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9844 - acc: 0.4697 - val_loss: 2.2376 - val_acc: 0.4385\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9922 - acc: 0.4681 - val_loss: 2.2142 - val_acc: 0.4399\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9886 - acc: 0.4701 - val_loss: 2.2438 - val_acc: 0.4383\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9879 - acc: 0.4690 - val_loss: 2.2208 - val_acc: 0.4386\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9795 - acc: 0.4714 - val_loss: 2.2233 - val_acc: 0.4388\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9770 - acc: 0.4706 - val_loss: 2.2127 - val_acc: 0.4413\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9857 - acc: 0.4687 - val_loss: 2.2366 - val_acc: 0.4396\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9821 - acc: 0.4719 - val_loss: 2.2358 - val_acc: 0.4351\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9762 - acc: 0.4729 - val_loss: 2.2265 - val_acc: 0.4409\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9720 - acc: 0.4735 - val_loss: 2.2369 - val_acc: 0.4341\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9804 - acc: 0.4727 - val_loss: 2.2330 - val_acc: 0.4409\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9742 - acc: 0.4735 - val_loss: 2.2306 - val_acc: 0.4386\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9654 - acc: 0.4754 - val_loss: 2.2238 - val_acc: 0.4403\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9648 - acc: 0.4749 - val_loss: 2.2442 - val_acc: 0.4344\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9743 - acc: 0.4743 - val_loss: 2.2402 - val_acc: 0.4410\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9735 - acc: 0.4735 - val_loss: 2.2685 - val_acc: 0.4329\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9679 - acc: 0.4744 - val_loss: 2.2177 - val_acc: 0.4404\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9609 - acc: 0.4755 - val_loss: 2.2250 - val_acc: 0.4386\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9614 - acc: 0.4754 - val_loss: 2.2281 - val_acc: 0.4392\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9560 - acc: 0.4775 - val_loss: 2.2226 - val_acc: 0.4370\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9603 - acc: 0.4759 - val_loss: 2.2235 - val_acc: 0.4401\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9450 - acc: 0.4804 - val_loss: 2.2142 - val_acc: 0.4382\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9520 - acc: 0.4774 - val_loss: 2.2493 - val_acc: 0.4380\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9646 - acc: 0.4742 - val_loss: 2.2311 - val_acc: 0.4366\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9623 - acc: 0.4750 - val_loss: 2.2439 - val_acc: 0.4400\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9677 - acc: 0.4755 - val_loss: 2.2140 - val_acc: 0.4391\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9484 - acc: 0.4770 - val_loss: 2.2306 - val_acc: 0.4382\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9549 - acc: 0.4771 - val_loss: 2.2163 - val_acc: 0.4394\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9443 - acc: 0.4820 - val_loss: 2.2306 - val_acc: 0.4385\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9404 - acc: 0.4807 - val_loss: 2.2097 - val_acc: 0.4395\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9426 - acc: 0.4799 - val_loss: 2.2356 - val_acc: 0.4366\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9459 - acc: 0.4800 - val_loss: 2.2200 - val_acc: 0.4408\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9440 - acc: 0.4796 - val_loss: 2.2732 - val_acc: 0.4297\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9483 - acc: 0.4802 - val_loss: 2.2181 - val_acc: 0.4402\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9491 - acc: 0.4765 - val_loss: 2.2656 - val_acc: 0.4329\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9439 - acc: 0.4803 - val_loss: 2.2354 - val_acc: 0.4357\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9479 - acc: 0.4805 - val_loss: 2.2331 - val_acc: 0.4374\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9304 - acc: 0.4823 - val_loss: 2.2190 - val_acc: 0.4398\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9296 - acc: 0.4839 - val_loss: 2.2316 - val_acc: 0.4385\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9229 - acc: 0.4861 - val_loss: 2.2082 - val_acc: 0.4411\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9309 - acc: 0.4832 - val_loss: 2.2422 - val_acc: 0.4363\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9320 - acc: 0.4826 - val_loss: 2.2274 - val_acc: 0.4371\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9350 - acc: 0.4817 - val_loss: 2.2385 - val_acc: 0.4378\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9248 - acc: 0.4826 - val_loss: 2.2230 - val_acc: 0.4404\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9262 - acc: 0.4848 - val_loss: 2.2401 - val_acc: 0.4352\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2025 - acc: 0.4214 - val_loss: 2.1661 - val_acc: 0.4489\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1795 - acc: 0.4261 - val_loss: 2.1801 - val_acc: 0.4450\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1697 - acc: 0.4277 - val_loss: 2.1948 - val_acc: 0.4407\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1572 - acc: 0.4310 - val_loss: 2.1810 - val_acc: 0.4436\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1470 - acc: 0.4352 - val_loss: 2.1897 - val_acc: 0.4425\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1391 - acc: 0.4345 - val_loss: 2.1928 - val_acc: 0.4418\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1341 - acc: 0.4337 - val_loss: 2.2059 - val_acc: 0.4383\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1295 - acc: 0.4372 - val_loss: 2.2092 - val_acc: 0.4376\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1214 - acc: 0.4354 - val_loss: 2.2022 - val_acc: 0.4393\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1169 - acc: 0.4360 - val_loss: 2.2049 - val_acc: 0.4414\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1126 - acc: 0.4400 - val_loss: 2.2067 - val_acc: 0.4418\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1130 - acc: 0.4404 - val_loss: 2.1915 - val_acc: 0.4434\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1039 - acc: 0.4429 - val_loss: 2.2202 - val_acc: 0.4368\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1031 - acc: 0.4422 - val_loss: 2.1975 - val_acc: 0.4422\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0946 - acc: 0.4422 - val_loss: 2.1957 - val_acc: 0.4417\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0892 - acc: 0.4454 - val_loss: 2.1983 - val_acc: 0.4406\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0873 - acc: 0.4452 - val_loss: 2.1970 - val_acc: 0.4418\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0827 - acc: 0.4451 - val_loss: 2.1938 - val_acc: 0.4448\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0808 - acc: 0.4459 - val_loss: 2.1922 - val_acc: 0.4429\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0808 - acc: 0.4462 - val_loss: 2.2001 - val_acc: 0.4403\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0773 - acc: 0.4475 - val_loss: 2.1918 - val_acc: 0.4421\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0738 - acc: 0.4474 - val_loss: 2.2009 - val_acc: 0.4462\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0771 - acc: 0.4457 - val_loss: 2.1933 - val_acc: 0.4446\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0655 - acc: 0.4487 - val_loss: 2.2051 - val_acc: 0.4417\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0679 - acc: 0.4470 - val_loss: 2.2133 - val_acc: 0.4434\n",
      "Epoch 26/100\n",
      "1s - loss: 2.1062 - acc: 0.4424 - val_loss: 2.2474 - val_acc: 0.4308\n",
      "Epoch 27/100\n",
      "1s - loss: 2.1021 - acc: 0.4423 - val_loss: 2.2277 - val_acc: 0.4373\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0755 - acc: 0.4477 - val_loss: 2.1945 - val_acc: 0.4435\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0556 - acc: 0.4524 - val_loss: 2.1909 - val_acc: 0.4434\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0456 - acc: 0.4558 - val_loss: 2.1806 - val_acc: 0.4469\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0458 - acc: 0.4551 - val_loss: 2.1790 - val_acc: 0.4457\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0442 - acc: 0.4513 - val_loss: 2.1857 - val_acc: 0.4445\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0419 - acc: 0.4544 - val_loss: 2.1873 - val_acc: 0.4427\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0413 - acc: 0.4544 - val_loss: 2.1787 - val_acc: 0.4443\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0332 - acc: 0.4561 - val_loss: 2.1723 - val_acc: 0.4452\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0387 - acc: 0.4578 - val_loss: 2.1805 - val_acc: 0.4468\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0305 - acc: 0.4574 - val_loss: 2.1742 - val_acc: 0.4467\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0255 - acc: 0.4576 - val_loss: 2.1722 - val_acc: 0.4479\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0231 - acc: 0.4599 - val_loss: 2.1811 - val_acc: 0.4453\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0287 - acc: 0.4568 - val_loss: 2.1829 - val_acc: 0.4433\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0208 - acc: 0.4584 - val_loss: 2.1771 - val_acc: 0.4477\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0207 - acc: 0.4598 - val_loss: 2.1928 - val_acc: 0.4439\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0249 - acc: 0.4579 - val_loss: 2.1794 - val_acc: 0.4470\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0214 - acc: 0.4600 - val_loss: 2.1838 - val_acc: 0.4466\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0165 - acc: 0.4601 - val_loss: 2.1847 - val_acc: 0.4462\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0112 - acc: 0.4621 - val_loss: 2.1707 - val_acc: 0.4473\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0109 - acc: 0.4619 - val_loss: 2.1689 - val_acc: 0.4499\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0109 - acc: 0.4611 - val_loss: 2.1848 - val_acc: 0.4470\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0069 - acc: 0.4636 - val_loss: 2.1736 - val_acc: 0.4484\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0092 - acc: 0.4616 - val_loss: 2.1840 - val_acc: 0.4467\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0098 - acc: 0.4611 - val_loss: 2.1724 - val_acc: 0.4484\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0046 - acc: 0.4627 - val_loss: 2.1800 - val_acc: 0.4488\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0067 - acc: 0.4619 - val_loss: 2.2149 - val_acc: 0.4417\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0256 - acc: 0.4552 - val_loss: 2.1855 - val_acc: 0.4478\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0058 - acc: 0.4636 - val_loss: 2.2060 - val_acc: 0.4454\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0089 - acc: 0.4619 - val_loss: 2.1723 - val_acc: 0.4500\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9963 - acc: 0.4638 - val_loss: 2.1898 - val_acc: 0.4477\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0019 - acc: 0.4655 - val_loss: 2.1808 - val_acc: 0.4495\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9952 - acc: 0.4649 - val_loss: 2.1748 - val_acc: 0.4481\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9891 - acc: 0.4657 - val_loss: 2.1806 - val_acc: 0.4472\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9955 - acc: 0.4659 - val_loss: 2.2131 - val_acc: 0.4402\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0059 - acc: 0.4630 - val_loss: 2.2025 - val_acc: 0.4480\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0116 - acc: 0.4628 - val_loss: 2.1867 - val_acc: 0.4459\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9884 - acc: 0.4656 - val_loss: 2.2069 - val_acc: 0.4481\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0109 - acc: 0.4642 - val_loss: 2.1961 - val_acc: 0.4439\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9931 - acc: 0.4662 - val_loss: 2.1705 - val_acc: 0.4477\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9790 - acc: 0.4693 - val_loss: 2.1785 - val_acc: 0.4474\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9831 - acc: 0.4681 - val_loss: 2.1832 - val_acc: 0.4478\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9720 - acc: 0.4695 - val_loss: 2.1799 - val_acc: 0.4467\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9777 - acc: 0.4669 - val_loss: 2.1728 - val_acc: 0.4521\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9798 - acc: 0.4691 - val_loss: 2.1742 - val_acc: 0.4472\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9701 - acc: 0.4696 - val_loss: 2.1795 - val_acc: 0.4474\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9706 - acc: 0.4705 - val_loss: 2.1811 - val_acc: 0.4463\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9723 - acc: 0.4698 - val_loss: 2.1783 - val_acc: 0.4505\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9702 - acc: 0.4698 - val_loss: 2.1758 - val_acc: 0.4457\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9695 - acc: 0.4702 - val_loss: 2.1765 - val_acc: 0.4470\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9621 - acc: 0.4718 - val_loss: 2.1675 - val_acc: 0.4494\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9648 - acc: 0.4720 - val_loss: 2.1842 - val_acc: 0.4447\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9621 - acc: 0.4741 - val_loss: 2.1780 - val_acc: 0.4449\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9592 - acc: 0.4741 - val_loss: 2.1797 - val_acc: 0.4479\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9615 - acc: 0.4727 - val_loss: 2.1670 - val_acc: 0.4487\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9666 - acc: 0.4718 - val_loss: 2.1892 - val_acc: 0.4490\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9697 - acc: 0.4708 - val_loss: 2.1757 - val_acc: 0.4457\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9723 - acc: 0.4719 - val_loss: 2.1871 - val_acc: 0.4465\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9629 - acc: 0.4728 - val_loss: 2.1719 - val_acc: 0.4472\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9578 - acc: 0.4725 - val_loss: 2.1940 - val_acc: 0.4484\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9506 - acc: 0.4750 - val_loss: 2.1660 - val_acc: 0.4491\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9565 - acc: 0.4750 - val_loss: 2.2072 - val_acc: 0.4463\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9655 - acc: 0.4717 - val_loss: 2.1781 - val_acc: 0.4474\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9510 - acc: 0.4752 - val_loss: 2.1929 - val_acc: 0.4482\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9465 - acc: 0.4750 - val_loss: 2.1728 - val_acc: 0.4518\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9421 - acc: 0.4763 - val_loss: 2.1855 - val_acc: 0.4472\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9469 - acc: 0.4758 - val_loss: 2.1779 - val_acc: 0.4486\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9502 - acc: 0.4761 - val_loss: 2.1890 - val_acc: 0.4437\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9465 - acc: 0.4758 - val_loss: 2.1699 - val_acc: 0.4487\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9356 - acc: 0.4784 - val_loss: 2.1734 - val_acc: 0.4490\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9286 - acc: 0.4818 - val_loss: 2.1629 - val_acc: 0.4517\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9277 - acc: 0.4809 - val_loss: 2.1815 - val_acc: 0.4488\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9350 - acc: 0.4793 - val_loss: 2.2128 - val_acc: 0.4426\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9578 - acc: 0.4744 - val_loss: 2.2018 - val_acc: 0.4487\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1909 - acc: 0.4267 - val_loss: 2.1901 - val_acc: 0.4424\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1683 - acc: 0.4298 - val_loss: 2.2155 - val_acc: 0.4401\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1541 - acc: 0.4345 - val_loss: 2.2109 - val_acc: 0.4414\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1458 - acc: 0.4357 - val_loss: 2.2287 - val_acc: 0.4375\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1356 - acc: 0.4370 - val_loss: 2.2257 - val_acc: 0.4357\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1311 - acc: 0.4371 - val_loss: 2.2131 - val_acc: 0.4379\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1246 - acc: 0.4399 - val_loss: 2.2165 - val_acc: 0.4389\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1175 - acc: 0.4408 - val_loss: 2.2421 - val_acc: 0.4350\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1158 - acc: 0.4417 - val_loss: 2.2173 - val_acc: 0.4359\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1034 - acc: 0.4446 - val_loss: 2.2384 - val_acc: 0.4325\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1023 - acc: 0.4446 - val_loss: 2.2323 - val_acc: 0.4341\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0960 - acc: 0.4455 - val_loss: 2.2502 - val_acc: 0.4330\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0949 - acc: 0.4474 - val_loss: 2.2347 - val_acc: 0.4339\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0879 - acc: 0.4474 - val_loss: 2.2504 - val_acc: 0.4296\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0894 - acc: 0.4481 - val_loss: 2.2319 - val_acc: 0.4373\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0847 - acc: 0.4485 - val_loss: 2.2347 - val_acc: 0.4348\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0779 - acc: 0.4493 - val_loss: 2.2291 - val_acc: 0.4341\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0777 - acc: 0.4503 - val_loss: 2.2760 - val_acc: 0.4328\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0969 - acc: 0.4468 - val_loss: 2.2623 - val_acc: 0.4298\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0753 - acc: 0.4503 - val_loss: 2.2398 - val_acc: 0.4341\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0663 - acc: 0.4529 - val_loss: 2.2220 - val_acc: 0.4366\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0609 - acc: 0.4533 - val_loss: 2.2264 - val_acc: 0.4381\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0666 - acc: 0.4532 - val_loss: 2.2201 - val_acc: 0.4370\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0602 - acc: 0.4511 - val_loss: 2.2630 - val_acc: 0.4312\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0642 - acc: 0.4524 - val_loss: 2.2354 - val_acc: 0.4328\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0607 - acc: 0.4544 - val_loss: 2.2629 - val_acc: 0.4312\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0662 - acc: 0.4538 - val_loss: 2.2131 - val_acc: 0.4380\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0405 - acc: 0.4576 - val_loss: 2.2368 - val_acc: 0.4335\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0404 - acc: 0.4578 - val_loss: 2.2256 - val_acc: 0.4350\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0459 - acc: 0.4582 - val_loss: 2.2710 - val_acc: 0.4296\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0466 - acc: 0.4561 - val_loss: 2.2255 - val_acc: 0.4364\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0355 - acc: 0.4585 - val_loss: 2.2164 - val_acc: 0.4358\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0282 - acc: 0.4608 - val_loss: 2.2086 - val_acc: 0.4389\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0252 - acc: 0.4616 - val_loss: 2.2158 - val_acc: 0.4383\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0343 - acc: 0.4601 - val_loss: 2.2509 - val_acc: 0.4325\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0331 - acc: 0.4598 - val_loss: 2.2189 - val_acc: 0.4372\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0314 - acc: 0.4619 - val_loss: 2.2237 - val_acc: 0.4335\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0230 - acc: 0.4619 - val_loss: 2.2173 - val_acc: 0.4368\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0145 - acc: 0.4625 - val_loss: 2.2077 - val_acc: 0.4387\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0141 - acc: 0.4642 - val_loss: 2.2165 - val_acc: 0.4367\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0156 - acc: 0.4645 - val_loss: 2.2191 - val_acc: 0.4374\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0087 - acc: 0.4668 - val_loss: 2.2156 - val_acc: 0.4368\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0104 - acc: 0.4653 - val_loss: 2.2407 - val_acc: 0.4382\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0341 - acc: 0.4611 - val_loss: 2.2201 - val_acc: 0.4378\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0228 - acc: 0.4624 - val_loss: 2.2417 - val_acc: 0.4365\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0249 - acc: 0.4618 - val_loss: 2.2018 - val_acc: 0.4409\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0107 - acc: 0.4651 - val_loss: 2.2343 - val_acc: 0.4397\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0118 - acc: 0.4643 - val_loss: 2.1993 - val_acc: 0.4421\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0081 - acc: 0.4653 - val_loss: 2.2416 - val_acc: 0.4352\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0109 - acc: 0.4654 - val_loss: 2.2064 - val_acc: 0.4382\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0111 - acc: 0.4648 - val_loss: 2.2256 - val_acc: 0.4371\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0029 - acc: 0.4663 - val_loss: 2.2100 - val_acc: 0.4391\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0051 - acc: 0.4649 - val_loss: 2.2156 - val_acc: 0.4378\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9958 - acc: 0.4677 - val_loss: 2.2070 - val_acc: 0.4417\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9975 - acc: 0.4681 - val_loss: 2.2035 - val_acc: 0.4427\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9907 - acc: 0.4694 - val_loss: 2.2013 - val_acc: 0.4435\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9842 - acc: 0.4719 - val_loss: 2.2103 - val_acc: 0.4421\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9822 - acc: 0.4711 - val_loss: 2.1990 - val_acc: 0.4445\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9854 - acc: 0.4685 - val_loss: 2.2266 - val_acc: 0.4395\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9937 - acc: 0.4670 - val_loss: 2.2041 - val_acc: 0.4421\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9906 - acc: 0.4702 - val_loss: 2.2194 - val_acc: 0.4402\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9834 - acc: 0.4706 - val_loss: 2.2066 - val_acc: 0.4379\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9849 - acc: 0.4704 - val_loss: 2.2529 - val_acc: 0.4384\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9848 - acc: 0.4705 - val_loss: 2.2047 - val_acc: 0.4403\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9788 - acc: 0.4714 - val_loss: 2.2258 - val_acc: 0.4406\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9859 - acc: 0.4708 - val_loss: 2.1984 - val_acc: 0.4422\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9755 - acc: 0.4715 - val_loss: 2.2104 - val_acc: 0.4411\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9741 - acc: 0.4737 - val_loss: 2.2166 - val_acc: 0.4391\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9831 - acc: 0.4719 - val_loss: 2.2420 - val_acc: 0.4381\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9767 - acc: 0.4729 - val_loss: 2.2088 - val_acc: 0.4371\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9691 - acc: 0.4738 - val_loss: 2.2030 - val_acc: 0.4422\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9683 - acc: 0.4756 - val_loss: 2.1932 - val_acc: 0.4447\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9688 - acc: 0.4767 - val_loss: 2.2142 - val_acc: 0.4398\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9704 - acc: 0.4737 - val_loss: 2.1972 - val_acc: 0.4420\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9636 - acc: 0.4772 - val_loss: 2.2086 - val_acc: 0.4415\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9587 - acc: 0.4774 - val_loss: 2.1919 - val_acc: 0.4423\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9566 - acc: 0.4753 - val_loss: 2.2081 - val_acc: 0.4407\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9577 - acc: 0.4758 - val_loss: 2.1938 - val_acc: 0.4447\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9544 - acc: 0.4780 - val_loss: 2.1991 - val_acc: 0.4423\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9515 - acc: 0.4792 - val_loss: 2.1943 - val_acc: 0.4441\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9511 - acc: 0.4792 - val_loss: 2.1999 - val_acc: 0.4422\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9459 - acc: 0.4790 - val_loss: 2.2168 - val_acc: 0.4370\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9665 - acc: 0.4752 - val_loss: 2.2277 - val_acc: 0.4382\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9635 - acc: 0.4748 - val_loss: 2.2074 - val_acc: 0.4403\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9611 - acc: 0.4756 - val_loss: 2.2163 - val_acc: 0.4400\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9562 - acc: 0.4782 - val_loss: 2.1935 - val_acc: 0.4414\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9515 - acc: 0.4761 - val_loss: 2.2330 - val_acc: 0.4383\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9525 - acc: 0.4769 - val_loss: 2.1945 - val_acc: 0.4421\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9421 - acc: 0.4816 - val_loss: 2.2344 - val_acc: 0.4391\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9413 - acc: 0.4823 - val_loss: 2.2045 - val_acc: 0.4395\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9576 - acc: 0.4793 - val_loss: 2.2559 - val_acc: 0.4373\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9733 - acc: 0.4751 - val_loss: 2.1996 - val_acc: 0.4414\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9335 - acc: 0.4829 - val_loss: 2.2189 - val_acc: 0.4408\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9438 - acc: 0.4810 - val_loss: 2.2093 - val_acc: 0.4406\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9442 - acc: 0.4811 - val_loss: 2.2401 - val_acc: 0.4371\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9442 - acc: 0.4812 - val_loss: 2.2349 - val_acc: 0.4362\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9307 - acc: 0.4834 - val_loss: 2.2373 - val_acc: 0.4397\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9680 - acc: 0.4747 - val_loss: 2.2262 - val_acc: 0.4358\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9334 - acc: 0.4825 - val_loss: 2.1990 - val_acc: 0.4425\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9225 - acc: 0.4854 - val_loss: 2.1938 - val_acc: 0.4440\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1950 - acc: 0.4285 - val_loss: 2.2239 - val_acc: 0.4352\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1716 - acc: 0.4346 - val_loss: 2.2224 - val_acc: 0.4315\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1588 - acc: 0.4340 - val_loss: 2.2327 - val_acc: 0.4293\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1540 - acc: 0.4363 - val_loss: 2.2344 - val_acc: 0.4317\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1409 - acc: 0.4396 - val_loss: 2.2317 - val_acc: 0.4288\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1334 - acc: 0.4404 - val_loss: 2.2407 - val_acc: 0.4268\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1279 - acc: 0.4398 - val_loss: 2.2489 - val_acc: 0.4253\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1238 - acc: 0.4418 - val_loss: 2.2475 - val_acc: 0.4254\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1136 - acc: 0.4440 - val_loss: 2.2552 - val_acc: 0.4251\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1116 - acc: 0.4444 - val_loss: 2.2541 - val_acc: 0.4249\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1088 - acc: 0.4454 - val_loss: 2.2572 - val_acc: 0.4259\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0982 - acc: 0.4466 - val_loss: 2.2565 - val_acc: 0.4233\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0965 - acc: 0.4482 - val_loss: 2.2560 - val_acc: 0.4223\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0918 - acc: 0.4479 - val_loss: 2.2940 - val_acc: 0.4232\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1308 - acc: 0.4410 - val_loss: 2.3029 - val_acc: 0.4167\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1175 - acc: 0.4437 - val_loss: 2.2754 - val_acc: 0.4250\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0858 - acc: 0.4507 - val_loss: 2.2650 - val_acc: 0.4235\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0781 - acc: 0.4511 - val_loss: 2.2648 - val_acc: 0.4246\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0726 - acc: 0.4534 - val_loss: 2.2601 - val_acc: 0.4276\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0809 - acc: 0.4528 - val_loss: 2.2591 - val_acc: 0.4263\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0673 - acc: 0.4543 - val_loss: 2.2624 - val_acc: 0.4254\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0673 - acc: 0.4515 - val_loss: 2.2537 - val_acc: 0.4251\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0625 - acc: 0.4553 - val_loss: 2.2680 - val_acc: 0.4236\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0599 - acc: 0.4567 - val_loss: 2.2556 - val_acc: 0.4256\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0548 - acc: 0.4573 - val_loss: 2.2662 - val_acc: 0.4242\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0489 - acc: 0.4584 - val_loss: 2.2613 - val_acc: 0.4272\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0506 - acc: 0.4565 - val_loss: 2.2504 - val_acc: 0.4273\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0484 - acc: 0.4580 - val_loss: 2.2605 - val_acc: 0.4261\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0444 - acc: 0.4590 - val_loss: 2.2705 - val_acc: 0.4234\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0474 - acc: 0.4593 - val_loss: 2.3116 - val_acc: 0.4231\n",
      "Epoch 31/100\n",
      "1s - loss: 2.1051 - acc: 0.4473 - val_loss: 2.3253 - val_acc: 0.4106\n",
      "Epoch 32/100\n",
      "1s - loss: 2.1039 - acc: 0.4480 - val_loss: 2.3302 - val_acc: 0.4131\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0936 - acc: 0.4476 - val_loss: 2.2764 - val_acc: 0.4241\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0653 - acc: 0.4530 - val_loss: 2.2849 - val_acc: 0.4237\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0402 - acc: 0.4587 - val_loss: 2.2440 - val_acc: 0.4301\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0311 - acc: 0.4615 - val_loss: 2.2531 - val_acc: 0.4267\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0257 - acc: 0.4626 - val_loss: 2.2444 - val_acc: 0.4302\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0272 - acc: 0.4623 - val_loss: 2.2546 - val_acc: 0.4278\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0258 - acc: 0.4636 - val_loss: 2.2661 - val_acc: 0.4257\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0267 - acc: 0.4636 - val_loss: 2.2723 - val_acc: 0.4251\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0264 - acc: 0.4624 - val_loss: 2.2507 - val_acc: 0.4267\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0178 - acc: 0.4634 - val_loss: 2.2581 - val_acc: 0.4271\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0190 - acc: 0.4643 - val_loss: 2.2427 - val_acc: 0.4285\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0172 - acc: 0.4634 - val_loss: 2.2596 - val_acc: 0.4286\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0199 - acc: 0.4647 - val_loss: 2.2408 - val_acc: 0.4293\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0178 - acc: 0.4649 - val_loss: 2.2567 - val_acc: 0.4265\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0168 - acc: 0.4646 - val_loss: 2.2355 - val_acc: 0.4323\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0200 - acc: 0.4623 - val_loss: 2.2676 - val_acc: 0.4274\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0231 - acc: 0.4637 - val_loss: 2.2374 - val_acc: 0.4305\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0018 - acc: 0.4676 - val_loss: 2.2440 - val_acc: 0.4295\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9981 - acc: 0.4681 - val_loss: 2.2354 - val_acc: 0.4315\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9992 - acc: 0.4685 - val_loss: 2.2324 - val_acc: 0.4303\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9910 - acc: 0.4701 - val_loss: 2.2360 - val_acc: 0.4321\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0021 - acc: 0.4687 - val_loss: 2.2691 - val_acc: 0.4266\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0212 - acc: 0.4629 - val_loss: 2.2462 - val_acc: 0.4302\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0045 - acc: 0.4672 - val_loss: 2.2641 - val_acc: 0.4287\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0085 - acc: 0.4667 - val_loss: 2.2330 - val_acc: 0.4311\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9956 - acc: 0.4699 - val_loss: 2.2445 - val_acc: 0.4307\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9899 - acc: 0.4710 - val_loss: 2.2254 - val_acc: 0.4312\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9948 - acc: 0.4689 - val_loss: 2.2368 - val_acc: 0.4318\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9787 - acc: 0.4734 - val_loss: 2.2304 - val_acc: 0.4323\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9788 - acc: 0.4734 - val_loss: 2.2419 - val_acc: 0.4306\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9795 - acc: 0.4727 - val_loss: 2.2353 - val_acc: 0.4320\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9778 - acc: 0.4747 - val_loss: 2.2388 - val_acc: 0.4295\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9791 - acc: 0.4734 - val_loss: 2.2378 - val_acc: 0.4307\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9816 - acc: 0.4734 - val_loss: 2.2643 - val_acc: 0.4298\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9888 - acc: 0.4717 - val_loss: 2.3411 - val_acc: 0.4171\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0078 - acc: 0.4675 - val_loss: 2.2576 - val_acc: 0.4277\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9782 - acc: 0.4731 - val_loss: 2.2652 - val_acc: 0.4261\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9852 - acc: 0.4717 - val_loss: 2.2483 - val_acc: 0.4294\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9687 - acc: 0.4749 - val_loss: 2.2515 - val_acc: 0.4285\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9711 - acc: 0.4743 - val_loss: 2.2618 - val_acc: 0.4264\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9742 - acc: 0.4745 - val_loss: 2.2427 - val_acc: 0.4317\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9743 - acc: 0.4755 - val_loss: 2.2454 - val_acc: 0.4281\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9698 - acc: 0.4762 - val_loss: 2.2364 - val_acc: 0.4307\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9647 - acc: 0.4754 - val_loss: 2.2498 - val_acc: 0.4281\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9643 - acc: 0.4791 - val_loss: 2.2314 - val_acc: 0.4318\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9754 - acc: 0.4751 - val_loss: 2.2424 - val_acc: 0.4334\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9617 - acc: 0.4770 - val_loss: 2.2322 - val_acc: 0.4324\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9569 - acc: 0.4786 - val_loss: 2.2408 - val_acc: 0.4321\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9606 - acc: 0.4767 - val_loss: 2.2237 - val_acc: 0.4321\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9555 - acc: 0.4783 - val_loss: 2.2412 - val_acc: 0.4318\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9507 - acc: 0.4787 - val_loss: 2.2441 - val_acc: 0.4283\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9563 - acc: 0.4788 - val_loss: 2.2667 - val_acc: 0.4292\n",
      "Epoch 85/100\n",
      "1s - loss: 2.0006 - acc: 0.4705 - val_loss: 2.3116 - val_acc: 0.4192\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9905 - acc: 0.4702 - val_loss: 2.2515 - val_acc: 0.4272\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9649 - acc: 0.4749 - val_loss: 2.2403 - val_acc: 0.4326\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9468 - acc: 0.4807 - val_loss: 2.2459 - val_acc: 0.4305\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9479 - acc: 0.4812 - val_loss: 2.2340 - val_acc: 0.4323\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9468 - acc: 0.4813 - val_loss: 2.2543 - val_acc: 0.4291\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9492 - acc: 0.4800 - val_loss: 2.2449 - val_acc: 0.4275\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9450 - acc: 0.4794 - val_loss: 2.2458 - val_acc: 0.4303\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9430 - acc: 0.4801 - val_loss: 2.2889 - val_acc: 0.4220\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9547 - acc: 0.4796 - val_loss: 2.2446 - val_acc: 0.4292\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9437 - acc: 0.4813 - val_loss: 2.2708 - val_acc: 0.4261\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9465 - acc: 0.4807 - val_loss: 2.2500 - val_acc: 0.4283\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9545 - acc: 0.4778 - val_loss: 2.2458 - val_acc: 0.4312\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9398 - acc: 0.4811 - val_loss: 2.2286 - val_acc: 0.4303\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9309 - acc: 0.4842 - val_loss: 2.2239 - val_acc: 0.4305\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9299 - acc: 0.4837 - val_loss: 2.2281 - val_acc: 0.4308\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1558 - acc: 0.4300 - val_loss: 2.1770 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1320 - acc: 0.4348 - val_loss: 2.1804 - val_acc: 0.4479\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1205 - acc: 0.4383 - val_loss: 2.1902 - val_acc: 0.4422\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1096 - acc: 0.4417 - val_loss: 2.1896 - val_acc: 0.4432\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1002 - acc: 0.4428 - val_loss: 2.2044 - val_acc: 0.4365\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0938 - acc: 0.4448 - val_loss: 2.2013 - val_acc: 0.4386\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0874 - acc: 0.4459 - val_loss: 2.1989 - val_acc: 0.4386\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0792 - acc: 0.4465 - val_loss: 2.2097 - val_acc: 0.4347\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0745 - acc: 0.4493 - val_loss: 2.2088 - val_acc: 0.4346\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0750 - acc: 0.4481 - val_loss: 2.2086 - val_acc: 0.4351\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0655 - acc: 0.4505 - val_loss: 2.2051 - val_acc: 0.4352\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0620 - acc: 0.4513 - val_loss: 2.2057 - val_acc: 0.4350\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0564 - acc: 0.4527 - val_loss: 2.2106 - val_acc: 0.4333\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0541 - acc: 0.4522 - val_loss: 2.2034 - val_acc: 0.4364\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0479 - acc: 0.4536 - val_loss: 2.2072 - val_acc: 0.4345\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0408 - acc: 0.4551 - val_loss: 2.2076 - val_acc: 0.4352\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0433 - acc: 0.4558 - val_loss: 2.2186 - val_acc: 0.4325\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0383 - acc: 0.4554 - val_loss: 2.1995 - val_acc: 0.4367\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0335 - acc: 0.4557 - val_loss: 2.2086 - val_acc: 0.4358\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0312 - acc: 0.4586 - val_loss: 2.2098 - val_acc: 0.4330\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0283 - acc: 0.4573 - val_loss: 2.2155 - val_acc: 0.4362\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0535 - acc: 0.4555 - val_loss: 2.2403 - val_acc: 0.4294\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0378 - acc: 0.4575 - val_loss: 2.2104 - val_acc: 0.4336\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0193 - acc: 0.4593 - val_loss: 2.2098 - val_acc: 0.4340\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0158 - acc: 0.4607 - val_loss: 2.2062 - val_acc: 0.4336\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0143 - acc: 0.4606 - val_loss: 2.2013 - val_acc: 0.4356\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0145 - acc: 0.4616 - val_loss: 2.2130 - val_acc: 0.4326\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0052 - acc: 0.4642 - val_loss: 2.2064 - val_acc: 0.4330\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0051 - acc: 0.4640 - val_loss: 2.2010 - val_acc: 0.4374\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0056 - acc: 0.4616 - val_loss: 2.2076 - val_acc: 0.4323\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0000 - acc: 0.4633 - val_loss: 2.2010 - val_acc: 0.4342\n",
      "Epoch 32/100\n",
      "1s - loss: 1.9969 - acc: 0.4649 - val_loss: 2.2061 - val_acc: 0.4346\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9957 - acc: 0.4652 - val_loss: 2.1999 - val_acc: 0.4344\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9895 - acc: 0.4681 - val_loss: 2.2019 - val_acc: 0.4350\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9896 - acc: 0.4648 - val_loss: 2.2008 - val_acc: 0.4319\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9932 - acc: 0.4659 - val_loss: 2.1956 - val_acc: 0.4364\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9879 - acc: 0.4671 - val_loss: 2.2136 - val_acc: 0.4340\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9912 - acc: 0.4684 - val_loss: 2.2236 - val_acc: 0.4336\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9863 - acc: 0.4671 - val_loss: 2.2029 - val_acc: 0.4353\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9827 - acc: 0.4667 - val_loss: 2.2042 - val_acc: 0.4344\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9779 - acc: 0.4698 - val_loss: 2.2115 - val_acc: 0.4354\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9777 - acc: 0.4697 - val_loss: 2.2057 - val_acc: 0.4351\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9838 - acc: 0.4679 - val_loss: 2.1971 - val_acc: 0.4351\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9734 - acc: 0.4707 - val_loss: 2.1976 - val_acc: 0.4371\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9713 - acc: 0.4721 - val_loss: 2.1928 - val_acc: 0.4379\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9684 - acc: 0.4703 - val_loss: 2.1867 - val_acc: 0.4370\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9644 - acc: 0.4715 - val_loss: 2.2090 - val_acc: 0.4335\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9676 - acc: 0.4714 - val_loss: 2.1898 - val_acc: 0.4377\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9614 - acc: 0.4741 - val_loss: 2.1930 - val_acc: 0.4368\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9593 - acc: 0.4718 - val_loss: 2.1979 - val_acc: 0.4354\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9621 - acc: 0.4731 - val_loss: 2.2043 - val_acc: 0.4352\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9643 - acc: 0.4727 - val_loss: 2.2231 - val_acc: 0.4333\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9674 - acc: 0.4713 - val_loss: 2.1964 - val_acc: 0.4374\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9555 - acc: 0.4737 - val_loss: 2.2063 - val_acc: 0.4338\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9580 - acc: 0.4741 - val_loss: 2.2004 - val_acc: 0.4396\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9551 - acc: 0.4748 - val_loss: 2.1961 - val_acc: 0.4376\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9485 - acc: 0.4765 - val_loss: 2.1904 - val_acc: 0.4389\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9493 - acc: 0.4749 - val_loss: 2.1851 - val_acc: 0.4380\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9458 - acc: 0.4761 - val_loss: 2.1941 - val_acc: 0.4376\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9437 - acc: 0.4766 - val_loss: 2.1879 - val_acc: 0.4369\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9383 - acc: 0.4779 - val_loss: 2.2058 - val_acc: 0.4348\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9395 - acc: 0.4774 - val_loss: 2.2046 - val_acc: 0.4380\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9428 - acc: 0.4785 - val_loss: 2.1973 - val_acc: 0.4390\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9434 - acc: 0.4762 - val_loss: 2.1878 - val_acc: 0.4406\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9413 - acc: 0.4774 - val_loss: 2.2028 - val_acc: 0.4365\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9384 - acc: 0.4795 - val_loss: 2.1854 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9320 - acc: 0.4776 - val_loss: 2.1904 - val_acc: 0.4401\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9296 - acc: 0.4793 - val_loss: 2.1893 - val_acc: 0.4392\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9243 - acc: 0.4812 - val_loss: 2.1909 - val_acc: 0.4391\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9277 - acc: 0.4800 - val_loss: 2.1939 - val_acc: 0.4401\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9259 - acc: 0.4812 - val_loss: 2.2003 - val_acc: 0.4378\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9348 - acc: 0.4803 - val_loss: 2.2145 - val_acc: 0.4379\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9346 - acc: 0.4759 - val_loss: 2.2469 - val_acc: 0.4309\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9391 - acc: 0.4790 - val_loss: 2.1911 - val_acc: 0.4414\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9236 - acc: 0.4807 - val_loss: 2.2266 - val_acc: 0.4344\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9270 - acc: 0.4813 - val_loss: 2.1899 - val_acc: 0.4413\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9225 - acc: 0.4815 - val_loss: 2.2099 - val_acc: 0.4342\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9240 - acc: 0.4813 - val_loss: 2.1896 - val_acc: 0.4410\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9182 - acc: 0.4825 - val_loss: 2.1959 - val_acc: 0.4375\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9161 - acc: 0.4820 - val_loss: 2.1818 - val_acc: 0.4410\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9078 - acc: 0.4835 - val_loss: 2.1943 - val_acc: 0.4397\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9070 - acc: 0.4858 - val_loss: 2.1860 - val_acc: 0.4416\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9076 - acc: 0.4858 - val_loss: 2.2130 - val_acc: 0.4378\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9084 - acc: 0.4857 - val_loss: 2.1824 - val_acc: 0.4421\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9146 - acc: 0.4818 - val_loss: 2.1936 - val_acc: 0.4377\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9062 - acc: 0.4853 - val_loss: 2.1982 - val_acc: 0.4391\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9202 - acc: 0.4819 - val_loss: 2.2153 - val_acc: 0.4373\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9190 - acc: 0.4820 - val_loss: 2.2313 - val_acc: 0.4333\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9141 - acc: 0.4826 - val_loss: 2.2077 - val_acc: 0.4389\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9051 - acc: 0.4873 - val_loss: 2.1983 - val_acc: 0.4381\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8963 - acc: 0.4867 - val_loss: 2.1957 - val_acc: 0.4407\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9007 - acc: 0.4872 - val_loss: 2.2184 - val_acc: 0.4369\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9112 - acc: 0.4837 - val_loss: 2.2050 - val_acc: 0.4398\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9084 - acc: 0.4849 - val_loss: 2.2169 - val_acc: 0.4354\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9106 - acc: 0.4871 - val_loss: 2.2105 - val_acc: 0.4393\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8981 - acc: 0.4871 - val_loss: 2.2056 - val_acc: 0.4380\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8937 - acc: 0.4874 - val_loss: 2.1977 - val_acc: 0.4419\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8898 - acc: 0.4879 - val_loss: 2.1924 - val_acc: 0.4379\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8907 - acc: 0.4887 - val_loss: 2.2120 - val_acc: 0.4427\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9231 - acc: 0.4840 - val_loss: 2.2531 - val_acc: 0.4285\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1810 - acc: 0.4298 - val_loss: 2.2566 - val_acc: 0.4306\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1400 - acc: 0.4377 - val_loss: 2.2571 - val_acc: 0.4320\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1256 - acc: 0.4384 - val_loss: 2.2533 - val_acc: 0.4327\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1180 - acc: 0.4413 - val_loss: 2.2665 - val_acc: 0.4315\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1094 - acc: 0.4430 - val_loss: 2.2580 - val_acc: 0.4336\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1001 - acc: 0.4460 - val_loss: 2.2673 - val_acc: 0.4325\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0974 - acc: 0.4457 - val_loss: 2.2716 - val_acc: 0.4333\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0862 - acc: 0.4465 - val_loss: 2.2938 - val_acc: 0.4255\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0894 - acc: 0.4465 - val_loss: 2.2616 - val_acc: 0.4340\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0808 - acc: 0.4490 - val_loss: 2.2749 - val_acc: 0.4292\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0769 - acc: 0.4499 - val_loss: 2.2749 - val_acc: 0.4300\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0683 - acc: 0.4513 - val_loss: 2.2775 - val_acc: 0.4293\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0732 - acc: 0.4494 - val_loss: 2.2804 - val_acc: 0.4269\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0608 - acc: 0.4537 - val_loss: 2.2777 - val_acc: 0.4293\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0562 - acc: 0.4549 - val_loss: 2.2726 - val_acc: 0.4300\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0527 - acc: 0.4552 - val_loss: 2.2719 - val_acc: 0.4250\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0499 - acc: 0.4568 - val_loss: 2.2912 - val_acc: 0.4291\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0518 - acc: 0.4553 - val_loss: 2.2694 - val_acc: 0.4282\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0489 - acc: 0.4567 - val_loss: 2.2676 - val_acc: 0.4311\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0457 - acc: 0.4584 - val_loss: 2.2732 - val_acc: 0.4292\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0400 - acc: 0.4584 - val_loss: 2.2739 - val_acc: 0.4309\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0338 - acc: 0.4597 - val_loss: 2.2753 - val_acc: 0.4269\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0326 - acc: 0.4600 - val_loss: 2.2674 - val_acc: 0.4305\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0275 - acc: 0.4609 - val_loss: 2.2630 - val_acc: 0.4326\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0259 - acc: 0.4609 - val_loss: 2.2813 - val_acc: 0.4272\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0212 - acc: 0.4616 - val_loss: 2.2622 - val_acc: 0.4325\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0229 - acc: 0.4627 - val_loss: 2.2732 - val_acc: 0.4290\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0172 - acc: 0.4625 - val_loss: 2.2679 - val_acc: 0.4284\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0118 - acc: 0.4635 - val_loss: 2.2826 - val_acc: 0.4269\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0128 - acc: 0.4641 - val_loss: 2.2812 - val_acc: 0.4300\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0081 - acc: 0.4650 - val_loss: 2.2630 - val_acc: 0.4300\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0105 - acc: 0.4641 - val_loss: 2.2690 - val_acc: 0.4298\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0057 - acc: 0.4656 - val_loss: 2.2652 - val_acc: 0.4293\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9998 - acc: 0.4664 - val_loss: 2.2681 - val_acc: 0.4309\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9969 - acc: 0.4681 - val_loss: 2.2702 - val_acc: 0.4300\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0051 - acc: 0.4656 - val_loss: 2.2762 - val_acc: 0.4292\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0024 - acc: 0.4672 - val_loss: 2.2623 - val_acc: 0.4321\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0025 - acc: 0.4665 - val_loss: 2.2856 - val_acc: 0.4292\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0110 - acc: 0.4650 - val_loss: 2.3061 - val_acc: 0.4263\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0023 - acc: 0.4652 - val_loss: 2.2675 - val_acc: 0.4308\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9957 - acc: 0.4679 - val_loss: 2.2795 - val_acc: 0.4287\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9898 - acc: 0.4679 - val_loss: 2.2802 - val_acc: 0.4312\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0105 - acc: 0.4654 - val_loss: 2.2854 - val_acc: 0.4299\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0005 - acc: 0.4671 - val_loss: 2.2588 - val_acc: 0.4324\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9835 - acc: 0.4715 - val_loss: 2.2638 - val_acc: 0.4343\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9882 - acc: 0.4700 - val_loss: 2.2774 - val_acc: 0.4288\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9800 - acc: 0.4726 - val_loss: 2.2817 - val_acc: 0.4298\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9771 - acc: 0.4717 - val_loss: 2.2688 - val_acc: 0.4325\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9838 - acc: 0.4715 - val_loss: 2.2879 - val_acc: 0.4289\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9822 - acc: 0.4710 - val_loss: 2.2657 - val_acc: 0.4316\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9723 - acc: 0.4713 - val_loss: 2.2507 - val_acc: 0.4329\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9643 - acc: 0.4750 - val_loss: 2.2541 - val_acc: 0.4318\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9606 - acc: 0.4774 - val_loss: 2.2646 - val_acc: 0.4318\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9606 - acc: 0.4758 - val_loss: 2.2664 - val_acc: 0.4317\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9600 - acc: 0.4775 - val_loss: 2.2500 - val_acc: 0.4342\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9645 - acc: 0.4744 - val_loss: 2.2739 - val_acc: 0.4309\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9650 - acc: 0.4733 - val_loss: 2.2651 - val_acc: 0.4312\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9671 - acc: 0.4748 - val_loss: 2.2584 - val_acc: 0.4359\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9603 - acc: 0.4746 - val_loss: 2.2878 - val_acc: 0.4266\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9699 - acc: 0.4736 - val_loss: 2.2786 - val_acc: 0.4323\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9764 - acc: 0.4724 - val_loss: 2.2754 - val_acc: 0.4302\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9593 - acc: 0.4768 - val_loss: 2.2666 - val_acc: 0.4343\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9541 - acc: 0.4774 - val_loss: 2.2588 - val_acc: 0.4351\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9470 - acc: 0.4778 - val_loss: 2.2708 - val_acc: 0.4334\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9568 - acc: 0.4773 - val_loss: 2.2883 - val_acc: 0.4295\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9691 - acc: 0.4718 - val_loss: 2.2882 - val_acc: 0.4276\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9513 - acc: 0.4792 - val_loss: 2.2694 - val_acc: 0.4291\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9463 - acc: 0.4779 - val_loss: 2.2624 - val_acc: 0.4338\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9356 - acc: 0.4827 - val_loss: 2.2482 - val_acc: 0.4340\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9296 - acc: 0.4811 - val_loss: 2.2600 - val_acc: 0.4329\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9297 - acc: 0.4825 - val_loss: 2.2603 - val_acc: 0.4328\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9339 - acc: 0.4824 - val_loss: 2.2563 - val_acc: 0.4337\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9407 - acc: 0.4792 - val_loss: 2.2723 - val_acc: 0.4326\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9380 - acc: 0.4811 - val_loss: 2.2567 - val_acc: 0.4339\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9259 - acc: 0.4837 - val_loss: 2.2627 - val_acc: 0.4332\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9301 - acc: 0.4845 - val_loss: 2.2595 - val_acc: 0.4295\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9229 - acc: 0.4831 - val_loss: 2.2601 - val_acc: 0.4312\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9312 - acc: 0.4814 - val_loss: 2.3041 - val_acc: 0.4214\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9405 - acc: 0.4793 - val_loss: 2.3000 - val_acc: 0.4261\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9407 - acc: 0.4799 - val_loss: 2.3078 - val_acc: 0.4235\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9386 - acc: 0.4820 - val_loss: 2.2745 - val_acc: 0.4289\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9309 - acc: 0.4830 - val_loss: 2.2705 - val_acc: 0.4289\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9196 - acc: 0.4844 - val_loss: 2.2636 - val_acc: 0.4311\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9183 - acc: 0.4860 - val_loss: 2.2843 - val_acc: 0.4296\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9374 - acc: 0.4810 - val_loss: 2.2479 - val_acc: 0.4319\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9293 - acc: 0.4817 - val_loss: 2.2863 - val_acc: 0.4268\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9276 - acc: 0.4837 - val_loss: 2.2508 - val_acc: 0.4345\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9306 - acc: 0.4825 - val_loss: 2.2695 - val_acc: 0.4311\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9196 - acc: 0.4858 - val_loss: 2.2489 - val_acc: 0.4329\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9120 - acc: 0.4876 - val_loss: 2.2720 - val_acc: 0.4320\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9208 - acc: 0.4837 - val_loss: 2.2856 - val_acc: 0.4292\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9259 - acc: 0.4836 - val_loss: 2.2814 - val_acc: 0.4279\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9222 - acc: 0.4842 - val_loss: 2.2756 - val_acc: 0.4304\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9223 - acc: 0.4842 - val_loss: 2.2611 - val_acc: 0.4331\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9067 - acc: 0.4871 - val_loss: 2.2535 - val_acc: 0.4329\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9051 - acc: 0.4874 - val_loss: 2.2688 - val_acc: 0.4329\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9045 - acc: 0.4903 - val_loss: 2.2499 - val_acc: 0.4316\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8951 - acc: 0.4906 - val_loss: 2.2749 - val_acc: 0.4292\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9128 - acc: 0.4871 - val_loss: 2.2627 - val_acc: 0.4301\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9098 - acc: 0.4864 - val_loss: 2.2737 - val_acc: 0.4301\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1820 - acc: 0.4312 - val_loss: 2.1977 - val_acc: 0.4419\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1567 - acc: 0.4358 - val_loss: 2.2009 - val_acc: 0.4403\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1420 - acc: 0.4395 - val_loss: 2.2233 - val_acc: 0.4394\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1516 - acc: 0.4382 - val_loss: 2.2128 - val_acc: 0.4390\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1291 - acc: 0.4406 - val_loss: 2.2123 - val_acc: 0.4367\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1135 - acc: 0.4433 - val_loss: 2.2197 - val_acc: 0.4370\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1117 - acc: 0.4433 - val_loss: 2.2183 - val_acc: 0.4359\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1035 - acc: 0.4456 - val_loss: 2.2063 - val_acc: 0.4405\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1014 - acc: 0.4467 - val_loss: 2.2210 - val_acc: 0.4363\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1019 - acc: 0.4468 - val_loss: 2.2110 - val_acc: 0.4384\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0903 - acc: 0.4488 - val_loss: 2.2247 - val_acc: 0.4392\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0892 - acc: 0.4492 - val_loss: 2.2231 - val_acc: 0.4362\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0953 - acc: 0.4463 - val_loss: 2.2361 - val_acc: 0.4358\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0896 - acc: 0.4471 - val_loss: 2.2258 - val_acc: 0.4385\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0815 - acc: 0.4490 - val_loss: 2.2079 - val_acc: 0.4396\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0724 - acc: 0.4534 - val_loss: 2.2197 - val_acc: 0.4369\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0650 - acc: 0.4551 - val_loss: 2.2141 - val_acc: 0.4393\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0654 - acc: 0.4555 - val_loss: 2.2107 - val_acc: 0.4390\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0604 - acc: 0.4551 - val_loss: 2.2192 - val_acc: 0.4365\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0603 - acc: 0.4559 - val_loss: 2.2273 - val_acc: 0.4362\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0630 - acc: 0.4546 - val_loss: 2.2263 - val_acc: 0.4372\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0529 - acc: 0.4574 - val_loss: 2.2150 - val_acc: 0.4381\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0502 - acc: 0.4585 - val_loss: 2.2196 - val_acc: 0.4369\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0429 - acc: 0.4593 - val_loss: 2.2206 - val_acc: 0.4390\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0428 - acc: 0.4582 - val_loss: 2.2299 - val_acc: 0.4380\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0509 - acc: 0.4582 - val_loss: 2.2064 - val_acc: 0.4409\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0480 - acc: 0.4576 - val_loss: 2.2513 - val_acc: 0.4367\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0507 - acc: 0.4573 - val_loss: 2.2213 - val_acc: 0.4359\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0350 - acc: 0.4614 - val_loss: 2.2191 - val_acc: 0.4411\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0343 - acc: 0.4618 - val_loss: 2.2132 - val_acc: 0.4364\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0311 - acc: 0.4622 - val_loss: 2.2209 - val_acc: 0.4365\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0226 - acc: 0.4634 - val_loss: 2.2112 - val_acc: 0.4401\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0270 - acc: 0.4616 - val_loss: 2.2154 - val_acc: 0.4387\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0254 - acc: 0.4618 - val_loss: 2.1969 - val_acc: 0.4430\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0220 - acc: 0.4637 - val_loss: 2.2053 - val_acc: 0.4398\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0185 - acc: 0.4643 - val_loss: 2.2077 - val_acc: 0.4410\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0289 - acc: 0.4635 - val_loss: 2.2140 - val_acc: 0.4406\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0224 - acc: 0.4647 - val_loss: 2.2140 - val_acc: 0.4402\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0265 - acc: 0.4622 - val_loss: 2.2338 - val_acc: 0.4387\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0105 - acc: 0.4657 - val_loss: 2.2095 - val_acc: 0.4386\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0199 - acc: 0.4620 - val_loss: 2.2192 - val_acc: 0.4398\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0108 - acc: 0.4660 - val_loss: 2.2087 - val_acc: 0.4392\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0020 - acc: 0.4681 - val_loss: 2.2204 - val_acc: 0.4403\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0315 - acc: 0.4628 - val_loss: 2.2249 - val_acc: 0.4363\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0086 - acc: 0.4684 - val_loss: 2.1967 - val_acc: 0.4423\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9922 - acc: 0.4709 - val_loss: 2.1986 - val_acc: 0.4424\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9864 - acc: 0.4718 - val_loss: 2.1992 - val_acc: 0.4408\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9881 - acc: 0.4702 - val_loss: 2.2095 - val_acc: 0.4400\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9984 - acc: 0.4685 - val_loss: 2.2105 - val_acc: 0.4386\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0134 - acc: 0.4647 - val_loss: 2.2370 - val_acc: 0.4388\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0030 - acc: 0.4684 - val_loss: 2.2397 - val_acc: 0.4326\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0076 - acc: 0.4685 - val_loss: 2.2182 - val_acc: 0.4404\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9927 - acc: 0.4694 - val_loss: 2.2111 - val_acc: 0.4407\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9955 - acc: 0.4698 - val_loss: 2.2185 - val_acc: 0.4413\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9995 - acc: 0.4699 - val_loss: 2.2088 - val_acc: 0.4394\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9866 - acc: 0.4701 - val_loss: 2.2247 - val_acc: 0.4411\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0135 - acc: 0.4667 - val_loss: 2.2217 - val_acc: 0.4363\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9984 - acc: 0.4708 - val_loss: 2.2508 - val_acc: 0.4362\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9994 - acc: 0.4688 - val_loss: 2.2083 - val_acc: 0.4425\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9764 - acc: 0.4744 - val_loss: 2.2156 - val_acc: 0.4423\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9772 - acc: 0.4729 - val_loss: 2.2022 - val_acc: 0.4392\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9684 - acc: 0.4758 - val_loss: 2.2036 - val_acc: 0.4431\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9624 - acc: 0.4770 - val_loss: 2.2011 - val_acc: 0.4374\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9653 - acc: 0.4765 - val_loss: 2.1961 - val_acc: 0.4470\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9595 - acc: 0.4779 - val_loss: 2.2019 - val_acc: 0.4418\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9695 - acc: 0.4744 - val_loss: 2.2287 - val_acc: 0.4393\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0106 - acc: 0.4663 - val_loss: 2.2034 - val_acc: 0.4450\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9744 - acc: 0.4740 - val_loss: 2.2117 - val_acc: 0.4418\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9606 - acc: 0.4782 - val_loss: 2.2031 - val_acc: 0.4401\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9656 - acc: 0.4768 - val_loss: 2.2160 - val_acc: 0.4401\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9758 - acc: 0.4741 - val_loss: 2.1994 - val_acc: 0.4429\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9597 - acc: 0.4768 - val_loss: 2.2120 - val_acc: 0.4414\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9611 - acc: 0.4789 - val_loss: 2.1948 - val_acc: 0.4439\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9600 - acc: 0.4779 - val_loss: 2.2157 - val_acc: 0.4384\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9494 - acc: 0.4818 - val_loss: 2.2009 - val_acc: 0.4432\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9494 - acc: 0.4816 - val_loss: 2.2084 - val_acc: 0.4389\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9568 - acc: 0.4782 - val_loss: 2.2013 - val_acc: 0.4431\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9477 - acc: 0.4810 - val_loss: 2.2232 - val_acc: 0.4396\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9438 - acc: 0.4811 - val_loss: 2.1997 - val_acc: 0.4429\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9498 - acc: 0.4806 - val_loss: 2.2362 - val_acc: 0.4382\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9600 - acc: 0.4798 - val_loss: 2.2021 - val_acc: 0.4425\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9548 - acc: 0.4778 - val_loss: 2.2244 - val_acc: 0.4388\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9404 - acc: 0.4815 - val_loss: 2.1970 - val_acc: 0.4433\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9505 - acc: 0.4817 - val_loss: 2.2178 - val_acc: 0.4394\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9454 - acc: 0.4815 - val_loss: 2.1944 - val_acc: 0.4440\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9344 - acc: 0.4849 - val_loss: 2.2269 - val_acc: 0.4375\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9454 - acc: 0.4835 - val_loss: 2.2206 - val_acc: 0.4424\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9477 - acc: 0.4815 - val_loss: 2.2359 - val_acc: 0.4334\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9445 - acc: 0.4834 - val_loss: 2.2099 - val_acc: 0.4419\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9543 - acc: 0.4810 - val_loss: 2.2309 - val_acc: 0.4335\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9335 - acc: 0.4828 - val_loss: 2.2144 - val_acc: 0.4429\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9363 - acc: 0.4839 - val_loss: 2.2163 - val_acc: 0.4370\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9320 - acc: 0.4846 - val_loss: 2.2368 - val_acc: 0.4391\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9346 - acc: 0.4831 - val_loss: 2.2259 - val_acc: 0.4389\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9475 - acc: 0.4802 - val_loss: 2.2256 - val_acc: 0.4407\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9332 - acc: 0.4833 - val_loss: 2.2281 - val_acc: 0.4376\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9456 - acc: 0.4808 - val_loss: 2.2405 - val_acc: 0.4363\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9335 - acc: 0.4847 - val_loss: 2.2234 - val_acc: 0.4385\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9209 - acc: 0.4872 - val_loss: 2.2307 - val_acc: 0.4403\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9445 - acc: 0.4831 - val_loss: 2.2147 - val_acc: 0.4398\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1908 - acc: 0.4319 - val_loss: 2.1332 - val_acc: 0.4558\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1679 - acc: 0.4350 - val_loss: 2.1371 - val_acc: 0.4533\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1507 - acc: 0.4380 - val_loss: 2.1351 - val_acc: 0.4523\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1449 - acc: 0.4400 - val_loss: 2.1364 - val_acc: 0.4526\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1350 - acc: 0.4393 - val_loss: 2.1396 - val_acc: 0.4528\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1212 - acc: 0.4446 - val_loss: 2.1446 - val_acc: 0.4512\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1174 - acc: 0.4428 - val_loss: 2.1431 - val_acc: 0.4517\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1132 - acc: 0.4459 - val_loss: 2.1521 - val_acc: 0.4501\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1077 - acc: 0.4458 - val_loss: 2.1486 - val_acc: 0.4502\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1033 - acc: 0.4474 - val_loss: 2.1524 - val_acc: 0.4502\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1120 - acc: 0.4473 - val_loss: 2.1588 - val_acc: 0.4494\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0915 - acc: 0.4507 - val_loss: 2.1573 - val_acc: 0.4482\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0910 - acc: 0.4512 - val_loss: 2.1647 - val_acc: 0.4475\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0873 - acc: 0.4487 - val_loss: 2.1742 - val_acc: 0.4486\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1056 - acc: 0.4474 - val_loss: 2.1694 - val_acc: 0.4472\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0847 - acc: 0.4520 - val_loss: 2.1556 - val_acc: 0.4489\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0732 - acc: 0.4533 - val_loss: 2.1560 - val_acc: 0.4507\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0685 - acc: 0.4540 - val_loss: 2.1648 - val_acc: 0.4472\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0680 - acc: 0.4549 - val_loss: 2.1674 - val_acc: 0.4502\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0651 - acc: 0.4560 - val_loss: 2.1592 - val_acc: 0.4512\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0602 - acc: 0.4558 - val_loss: 2.1455 - val_acc: 0.4536\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0573 - acc: 0.4560 - val_loss: 2.1575 - val_acc: 0.4515\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0561 - acc: 0.4581 - val_loss: 2.1481 - val_acc: 0.4513\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0524 - acc: 0.4577 - val_loss: 2.1515 - val_acc: 0.4531\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0512 - acc: 0.4591 - val_loss: 2.1470 - val_acc: 0.4510\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0441 - acc: 0.4608 - val_loss: 2.1542 - val_acc: 0.4515\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0460 - acc: 0.4584 - val_loss: 2.1622 - val_acc: 0.4473\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0454 - acc: 0.4616 - val_loss: 2.1651 - val_acc: 0.4466\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0543 - acc: 0.4590 - val_loss: 2.1665 - val_acc: 0.4478\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0484 - acc: 0.4593 - val_loss: 2.1531 - val_acc: 0.4496\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0401 - acc: 0.4586 - val_loss: 2.1533 - val_acc: 0.4507\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0389 - acc: 0.4625 - val_loss: 2.1479 - val_acc: 0.4505\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0399 - acc: 0.4593 - val_loss: 2.1503 - val_acc: 0.4527\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0359 - acc: 0.4627 - val_loss: 2.1548 - val_acc: 0.4507\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0238 - acc: 0.4670 - val_loss: 2.1480 - val_acc: 0.4523\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0285 - acc: 0.4643 - val_loss: 2.1507 - val_acc: 0.4499\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0253 - acc: 0.4626 - val_loss: 2.1377 - val_acc: 0.4546\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0184 - acc: 0.4658 - val_loss: 2.1466 - val_acc: 0.4516\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0133 - acc: 0.4676 - val_loss: 2.1481 - val_acc: 0.4517\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0097 - acc: 0.4682 - val_loss: 2.1470 - val_acc: 0.4536\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0081 - acc: 0.4679 - val_loss: 2.1523 - val_acc: 0.4524\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0089 - acc: 0.4681 - val_loss: 2.1485 - val_acc: 0.4516\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0068 - acc: 0.4696 - val_loss: 2.1411 - val_acc: 0.4536\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0055 - acc: 0.4679 - val_loss: 2.1555 - val_acc: 0.4514\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0060 - acc: 0.4668 - val_loss: 2.2053 - val_acc: 0.4427\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0284 - acc: 0.4651 - val_loss: 2.1778 - val_acc: 0.4486\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0065 - acc: 0.4681 - val_loss: 2.1657 - val_acc: 0.4508\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0064 - acc: 0.4690 - val_loss: 2.1664 - val_acc: 0.4511\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0224 - acc: 0.4676 - val_loss: 2.1413 - val_acc: 0.4552\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0039 - acc: 0.4699 - val_loss: 2.1341 - val_acc: 0.4536\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9903 - acc: 0.4733 - val_loss: 2.1348 - val_acc: 0.4526\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9889 - acc: 0.4723 - val_loss: 2.1349 - val_acc: 0.4532\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9860 - acc: 0.4721 - val_loss: 2.1387 - val_acc: 0.4538\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9831 - acc: 0.4728 - val_loss: 2.1381 - val_acc: 0.4527\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9784 - acc: 0.4764 - val_loss: 2.1419 - val_acc: 0.4522\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9804 - acc: 0.4750 - val_loss: 2.1722 - val_acc: 0.4493\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0004 - acc: 0.4727 - val_loss: 2.1440 - val_acc: 0.4521\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9818 - acc: 0.4760 - val_loss: 2.1519 - val_acc: 0.4520\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9844 - acc: 0.4739 - val_loss: 2.1499 - val_acc: 0.4536\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9889 - acc: 0.4726 - val_loss: 2.1462 - val_acc: 0.4531\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9792 - acc: 0.4738 - val_loss: 2.1373 - val_acc: 0.4530\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9798 - acc: 0.4737 - val_loss: 2.1415 - val_acc: 0.4523\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9782 - acc: 0.4738 - val_loss: 2.1499 - val_acc: 0.4528\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9934 - acc: 0.4720 - val_loss: 2.1495 - val_acc: 0.4523\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9952 - acc: 0.4719 - val_loss: 2.1506 - val_acc: 0.4502\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9841 - acc: 0.4733 - val_loss: 2.1414 - val_acc: 0.4526\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9870 - acc: 0.4721 - val_loss: 2.1561 - val_acc: 0.4489\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9801 - acc: 0.4738 - val_loss: 2.1397 - val_acc: 0.4530\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9735 - acc: 0.4741 - val_loss: 2.1498 - val_acc: 0.4483\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9746 - acc: 0.4762 - val_loss: 2.1433 - val_acc: 0.4508\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9745 - acc: 0.4747 - val_loss: 2.1646 - val_acc: 0.4487\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9937 - acc: 0.4720 - val_loss: 2.1569 - val_acc: 0.4488\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9700 - acc: 0.4781 - val_loss: 2.1671 - val_acc: 0.4495\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9796 - acc: 0.4757 - val_loss: 2.1430 - val_acc: 0.4530\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9561 - acc: 0.4807 - val_loss: 2.1518 - val_acc: 0.4502\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9595 - acc: 0.4793 - val_loss: 2.1451 - val_acc: 0.4520\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9606 - acc: 0.4787 - val_loss: 2.1902 - val_acc: 0.4445\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9676 - acc: 0.4802 - val_loss: 2.1557 - val_acc: 0.4508\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9635 - acc: 0.4787 - val_loss: 2.1754 - val_acc: 0.4474\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9630 - acc: 0.4785 - val_loss: 2.1548 - val_acc: 0.4527\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9604 - acc: 0.4797 - val_loss: 2.1824 - val_acc: 0.4475\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9605 - acc: 0.4787 - val_loss: 2.1571 - val_acc: 0.4494\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9468 - acc: 0.4815 - val_loss: 2.1624 - val_acc: 0.4493\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9666 - acc: 0.4794 - val_loss: 2.1616 - val_acc: 0.4517\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9528 - acc: 0.4838 - val_loss: 2.1769 - val_acc: 0.4479\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9467 - acc: 0.4840 - val_loss: 2.1509 - val_acc: 0.4520\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9375 - acc: 0.4864 - val_loss: 2.1604 - val_acc: 0.4510\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9637 - acc: 0.4791 - val_loss: 2.1670 - val_acc: 0.4523\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9457 - acc: 0.4817 - val_loss: 2.1716 - val_acc: 0.4488\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9525 - acc: 0.4815 - val_loss: 2.1650 - val_acc: 0.4499\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9394 - acc: 0.4853 - val_loss: 2.1572 - val_acc: 0.4499\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9384 - acc: 0.4864 - val_loss: 2.1445 - val_acc: 0.4530\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9445 - acc: 0.4843 - val_loss: 2.1573 - val_acc: 0.4500\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9363 - acc: 0.4844 - val_loss: 2.1656 - val_acc: 0.4514\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9324 - acc: 0.4847 - val_loss: 2.1492 - val_acc: 0.4530\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9259 - acc: 0.4888 - val_loss: 2.1471 - val_acc: 0.4518\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9257 - acc: 0.4885 - val_loss: 2.1539 - val_acc: 0.4509\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9308 - acc: 0.4876 - val_loss: 2.1427 - val_acc: 0.4509\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9239 - acc: 0.4874 - val_loss: 2.1615 - val_acc: 0.4487\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9270 - acc: 0.4863 - val_loss: 2.1590 - val_acc: 0.4512\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1930 - acc: 0.4304 - val_loss: 2.1531 - val_acc: 0.4498\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1640 - acc: 0.4356 - val_loss: 2.1686 - val_acc: 0.4479\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1553 - acc: 0.4368 - val_loss: 2.1727 - val_acc: 0.4489\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1423 - acc: 0.4402 - val_loss: 2.1765 - val_acc: 0.4452\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1392 - acc: 0.4412 - val_loss: 2.1798 - val_acc: 0.4454\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1275 - acc: 0.4430 - val_loss: 2.1886 - val_acc: 0.4416\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1247 - acc: 0.4416 - val_loss: 2.1832 - val_acc: 0.4450\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1138 - acc: 0.4442 - val_loss: 2.1968 - val_acc: 0.4434\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1074 - acc: 0.4451 - val_loss: 2.1991 - val_acc: 0.4422\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1094 - acc: 0.4447 - val_loss: 2.1950 - val_acc: 0.4414\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0968 - acc: 0.4487 - val_loss: 2.2395 - val_acc: 0.3960\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0969 - acc: 0.4484 - val_loss: 2.2093 - val_acc: 0.4400\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0906 - acc: 0.4492 - val_loss: 2.2080 - val_acc: 0.4405\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0842 - acc: 0.4506 - val_loss: 2.1984 - val_acc: 0.4412\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0780 - acc: 0.4522 - val_loss: 2.2059 - val_acc: 0.4403\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0745 - acc: 0.4527 - val_loss: 2.2093 - val_acc: 0.4393\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0768 - acc: 0.4520 - val_loss: 2.1929 - val_acc: 0.4417\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0683 - acc: 0.4540 - val_loss: 2.2178 - val_acc: 0.4380\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0719 - acc: 0.4541 - val_loss: 2.1903 - val_acc: 0.4421\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0615 - acc: 0.4573 - val_loss: 2.2176 - val_acc: 0.4388\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0688 - acc: 0.4540 - val_loss: 2.2020 - val_acc: 0.4400\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0555 - acc: 0.4596 - val_loss: 2.1983 - val_acc: 0.4416\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0526 - acc: 0.4573 - val_loss: 2.2013 - val_acc: 0.4396\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0487 - acc: 0.4568 - val_loss: 2.2473 - val_acc: 0.4325\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0598 - acc: 0.4589 - val_loss: 2.2045 - val_acc: 0.4413\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0435 - acc: 0.4609 - val_loss: 2.2041 - val_acc: 0.4405\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0403 - acc: 0.4601 - val_loss: 2.1922 - val_acc: 0.4421\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0383 - acc: 0.4624 - val_loss: 2.1915 - val_acc: 0.4428\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0389 - acc: 0.4615 - val_loss: 2.1822 - val_acc: 0.4442\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0363 - acc: 0.4626 - val_loss: 2.1882 - val_acc: 0.4417\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0307 - acc: 0.4632 - val_loss: 2.1849 - val_acc: 0.4446\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0315 - acc: 0.4618 - val_loss: 2.2033 - val_acc: 0.4425\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0454 - acc: 0.4600 - val_loss: 2.1856 - val_acc: 0.4462\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0367 - acc: 0.4599 - val_loss: 2.2114 - val_acc: 0.4445\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0602 - acc: 0.4571 - val_loss: 2.1926 - val_acc: 0.4422\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0341 - acc: 0.4637 - val_loss: 2.1871 - val_acc: 0.4471\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0385 - acc: 0.4615 - val_loss: 2.1867 - val_acc: 0.4436\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0257 - acc: 0.4618 - val_loss: 2.1947 - val_acc: 0.4453\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0318 - acc: 0.4625 - val_loss: 2.1776 - val_acc: 0.4447\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0148 - acc: 0.4675 - val_loss: 2.1871 - val_acc: 0.4480\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0206 - acc: 0.4632 - val_loss: 2.1723 - val_acc: 0.4479\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0060 - acc: 0.4676 - val_loss: 2.1785 - val_acc: 0.4473\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0088 - acc: 0.4656 - val_loss: 2.1935 - val_acc: 0.4450\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0103 - acc: 0.4669 - val_loss: 2.1788 - val_acc: 0.4464\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0052 - acc: 0.4679 - val_loss: 2.1832 - val_acc: 0.4470\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0108 - acc: 0.4679 - val_loss: 2.1898 - val_acc: 0.4432\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0093 - acc: 0.4657 - val_loss: 2.2105 - val_acc: 0.4427\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0213 - acc: 0.4643 - val_loss: 2.1801 - val_acc: 0.4466\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0044 - acc: 0.4701 - val_loss: 2.1746 - val_acc: 0.4507\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0065 - acc: 0.4678 - val_loss: 2.1672 - val_acc: 0.4468\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9952 - acc: 0.4703 - val_loss: 2.1773 - val_acc: 0.4483\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9901 - acc: 0.4726 - val_loss: 2.1581 - val_acc: 0.4502\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9922 - acc: 0.4720 - val_loss: 2.1775 - val_acc: 0.4477\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9923 - acc: 0.4728 - val_loss: 2.1706 - val_acc: 0.4477\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9926 - acc: 0.4693 - val_loss: 2.1841 - val_acc: 0.4477\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0066 - acc: 0.4664 - val_loss: 2.1882 - val_acc: 0.4423\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0007 - acc: 0.4679 - val_loss: 2.1813 - val_acc: 0.4474\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9867 - acc: 0.4712 - val_loss: 2.1687 - val_acc: 0.4498\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9880 - acc: 0.4713 - val_loss: 2.1677 - val_acc: 0.4505\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9863 - acc: 0.4713 - val_loss: 2.1576 - val_acc: 0.4496\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9854 - acc: 0.4742 - val_loss: 2.1757 - val_acc: 0.4490\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9755 - acc: 0.4753 - val_loss: 2.1525 - val_acc: 0.4507\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9736 - acc: 0.4761 - val_loss: 2.1641 - val_acc: 0.4515\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9708 - acc: 0.4770 - val_loss: 2.2122 - val_acc: 0.4388\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9888 - acc: 0.4731 - val_loss: 2.1696 - val_acc: 0.4470\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9638 - acc: 0.4777 - val_loss: 2.1660 - val_acc: 0.4486\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9596 - acc: 0.4792 - val_loss: 2.1678 - val_acc: 0.4497\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9647 - acc: 0.4792 - val_loss: 2.1755 - val_acc: 0.4477\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9707 - acc: 0.4740 - val_loss: 2.1781 - val_acc: 0.4465\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9596 - acc: 0.4788 - val_loss: 2.1638 - val_acc: 0.4519\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9590 - acc: 0.4785 - val_loss: 2.1824 - val_acc: 0.4470\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9530 - acc: 0.4805 - val_loss: 2.1897 - val_acc: 0.4460\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9593 - acc: 0.4792 - val_loss: 2.1922 - val_acc: 0.4444\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9604 - acc: 0.4791 - val_loss: 2.1914 - val_acc: 0.4471\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9705 - acc: 0.4768 - val_loss: 2.1830 - val_acc: 0.4451\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9619 - acc: 0.4785 - val_loss: 2.1737 - val_acc: 0.4512\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9558 - acc: 0.4799 - val_loss: 2.1657 - val_acc: 0.4481\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9562 - acc: 0.4797 - val_loss: 2.1779 - val_acc: 0.4467\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9580 - acc: 0.4804 - val_loss: 2.1645 - val_acc: 0.4500\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9573 - acc: 0.4803 - val_loss: 2.1980 - val_acc: 0.4487\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9761 - acc: 0.4754 - val_loss: 2.1764 - val_acc: 0.4471\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9596 - acc: 0.4792 - val_loss: 2.1706 - val_acc: 0.4486\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9447 - acc: 0.4825 - val_loss: 2.1794 - val_acc: 0.4465\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9406 - acc: 0.4822 - val_loss: 2.1707 - val_acc: 0.4501\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9435 - acc: 0.4808 - val_loss: 2.1716 - val_acc: 0.4485\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9470 - acc: 0.4825 - val_loss: 2.1739 - val_acc: 0.4509\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9471 - acc: 0.4817 - val_loss: 2.1724 - val_acc: 0.4466\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9420 - acc: 0.4827 - val_loss: 2.1729 - val_acc: 0.4503\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9487 - acc: 0.4817 - val_loss: 2.1695 - val_acc: 0.4470\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9410 - acc: 0.4827 - val_loss: 2.1902 - val_acc: 0.4466\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9543 - acc: 0.4795 - val_loss: 2.1732 - val_acc: 0.4486\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9409 - acc: 0.4821 - val_loss: 2.1860 - val_acc: 0.4453\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9449 - acc: 0.4826 - val_loss: 2.1676 - val_acc: 0.4482\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9339 - acc: 0.4847 - val_loss: 2.1706 - val_acc: 0.4514\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9307 - acc: 0.4852 - val_loss: 2.1684 - val_acc: 0.4500\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9317 - acc: 0.4837 - val_loss: 2.1903 - val_acc: 0.4474\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9387 - acc: 0.4842 - val_loss: 2.2475 - val_acc: 0.4340\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9573 - acc: 0.4805 - val_loss: 2.1682 - val_acc: 0.4497\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9199 - acc: 0.4887 - val_loss: 2.2259 - val_acc: 0.4397\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9391 - acc: 0.4839 - val_loss: 2.1789 - val_acc: 0.4478\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1774 - acc: 0.4311 - val_loss: 2.2027 - val_acc: 0.4382\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1520 - acc: 0.4351 - val_loss: 2.2104 - val_acc: 0.4383\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1353 - acc: 0.4388 - val_loss: 2.2079 - val_acc: 0.4413\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1282 - acc: 0.4407 - val_loss: 2.2120 - val_acc: 0.4395\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1193 - acc: 0.4426 - val_loss: 2.2231 - val_acc: 0.4388\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1121 - acc: 0.4436 - val_loss: 2.2299 - val_acc: 0.4365\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1078 - acc: 0.4436 - val_loss: 2.2284 - val_acc: 0.4376\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1009 - acc: 0.4450 - val_loss: 2.2266 - val_acc: 0.4382\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0991 - acc: 0.4455 - val_loss: 2.2344 - val_acc: 0.4382\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0891 - acc: 0.4487 - val_loss: 2.2319 - val_acc: 0.4380\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0841 - acc: 0.4490 - val_loss: 2.2379 - val_acc: 0.4367\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0776 - acc: 0.4497 - val_loss: 2.2363 - val_acc: 0.4370\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0762 - acc: 0.4513 - val_loss: 2.2356 - val_acc: 0.4352\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0713 - acc: 0.4517 - val_loss: 2.2250 - val_acc: 0.4394\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0678 - acc: 0.4524 - val_loss: 2.2234 - val_acc: 0.4380\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0631 - acc: 0.4525 - val_loss: 2.2343 - val_acc: 0.4362\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0575 - acc: 0.4550 - val_loss: 2.2343 - val_acc: 0.4380\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0556 - acc: 0.4555 - val_loss: 2.2443 - val_acc: 0.4362\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0571 - acc: 0.4553 - val_loss: 2.2512 - val_acc: 0.4359\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0540 - acc: 0.4561 - val_loss: 2.2292 - val_acc: 0.4373\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0461 - acc: 0.4563 - val_loss: 2.2571 - val_acc: 0.4351\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0469 - acc: 0.4586 - val_loss: 2.2291 - val_acc: 0.4370\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0412 - acc: 0.4592 - val_loss: 2.2292 - val_acc: 0.4379\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0409 - acc: 0.4575 - val_loss: 2.2148 - val_acc: 0.4378\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0353 - acc: 0.4592 - val_loss: 2.2503 - val_acc: 0.4345\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0298 - acc: 0.4607 - val_loss: 2.2279 - val_acc: 0.4386\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0303 - acc: 0.4612 - val_loss: 2.2553 - val_acc: 0.4335\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0278 - acc: 0.4604 - val_loss: 2.2413 - val_acc: 0.4337\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0242 - acc: 0.4629 - val_loss: 2.2263 - val_acc: 0.4392\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0190 - acc: 0.4630 - val_loss: 2.2389 - val_acc: 0.4390\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0172 - acc: 0.4630 - val_loss: 2.2363 - val_acc: 0.4384\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0156 - acc: 0.4636 - val_loss: 2.2411 - val_acc: 0.4374\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0154 - acc: 0.4635 - val_loss: 2.2253 - val_acc: 0.4366\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0151 - acc: 0.4652 - val_loss: 2.2366 - val_acc: 0.4361\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0198 - acc: 0.4623 - val_loss: 2.2170 - val_acc: 0.4382\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0140 - acc: 0.4628 - val_loss: 2.2444 - val_acc: 0.4372\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0113 - acc: 0.4651 - val_loss: 2.2309 - val_acc: 0.4395\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0090 - acc: 0.4653 - val_loss: 2.2535 - val_acc: 0.4359\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0255 - acc: 0.4623 - val_loss: 2.2191 - val_acc: 0.4393\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0013 - acc: 0.4685 - val_loss: 2.2470 - val_acc: 0.4363\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9996 - acc: 0.4657 - val_loss: 2.2213 - val_acc: 0.4383\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9914 - acc: 0.4707 - val_loss: 2.2370 - val_acc: 0.4376\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9860 - acc: 0.4704 - val_loss: 2.2258 - val_acc: 0.4402\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9889 - acc: 0.4692 - val_loss: 2.2226 - val_acc: 0.4374\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0010 - acc: 0.4673 - val_loss: 2.2188 - val_acc: 0.4372\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9853 - acc: 0.4707 - val_loss: 2.2098 - val_acc: 0.4401\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9863 - acc: 0.4706 - val_loss: 2.2243 - val_acc: 0.4374\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9797 - acc: 0.4712 - val_loss: 2.2196 - val_acc: 0.4377\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9914 - acc: 0.4691 - val_loss: 2.2416 - val_acc: 0.4374\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9821 - acc: 0.4719 - val_loss: 2.2128 - val_acc: 0.4399\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9770 - acc: 0.4733 - val_loss: 2.2317 - val_acc: 0.4389\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9843 - acc: 0.4689 - val_loss: 2.2098 - val_acc: 0.4402\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9780 - acc: 0.4732 - val_loss: 2.2338 - val_acc: 0.4384\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9808 - acc: 0.4724 - val_loss: 2.2252 - val_acc: 0.4397\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9923 - acc: 0.4697 - val_loss: 2.2369 - val_acc: 0.4356\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9771 - acc: 0.4731 - val_loss: 2.2157 - val_acc: 0.4418\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9766 - acc: 0.4731 - val_loss: 2.2378 - val_acc: 0.4354\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9720 - acc: 0.4729 - val_loss: 2.2191 - val_acc: 0.4397\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9648 - acc: 0.4763 - val_loss: 2.2192 - val_acc: 0.4400\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9794 - acc: 0.4717 - val_loss: 2.2226 - val_acc: 0.4402\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9787 - acc: 0.4723 - val_loss: 2.2271 - val_acc: 0.4394\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9709 - acc: 0.4728 - val_loss: 2.2365 - val_acc: 0.4372\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9706 - acc: 0.4734 - val_loss: 2.2292 - val_acc: 0.4393\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9629 - acc: 0.4748 - val_loss: 2.2395 - val_acc: 0.4373\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9532 - acc: 0.4773 - val_loss: 2.2425 - val_acc: 0.4358\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9552 - acc: 0.4773 - val_loss: 2.2039 - val_acc: 0.4405\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9522 - acc: 0.4774 - val_loss: 2.2234 - val_acc: 0.4392\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9539 - acc: 0.4778 - val_loss: 2.2310 - val_acc: 0.4389\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9737 - acc: 0.4727 - val_loss: 2.2372 - val_acc: 0.4348\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9460 - acc: 0.4791 - val_loss: 2.2102 - val_acc: 0.4405\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9415 - acc: 0.4810 - val_loss: 2.2303 - val_acc: 0.4376\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9469 - acc: 0.4782 - val_loss: 2.2071 - val_acc: 0.4418\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9427 - acc: 0.4813 - val_loss: 2.2313 - val_acc: 0.4382\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9418 - acc: 0.4815 - val_loss: 2.2209 - val_acc: 0.4395\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9439 - acc: 0.4809 - val_loss: 2.2368 - val_acc: 0.4345\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9452 - acc: 0.4780 - val_loss: 2.2424 - val_acc: 0.4381\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9479 - acc: 0.4803 - val_loss: 2.2360 - val_acc: 0.4364\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9448 - acc: 0.4809 - val_loss: 2.2152 - val_acc: 0.4395\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9354 - acc: 0.4841 - val_loss: 2.2111 - val_acc: 0.4376\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9301 - acc: 0.4828 - val_loss: 2.2045 - val_acc: 0.4410\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9251 - acc: 0.4828 - val_loss: 2.2171 - val_acc: 0.4386\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9286 - acc: 0.4844 - val_loss: 2.2030 - val_acc: 0.4424\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9297 - acc: 0.4843 - val_loss: 2.2383 - val_acc: 0.4381\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9251 - acc: 0.4857 - val_loss: 2.2166 - val_acc: 0.4401\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9239 - acc: 0.4848 - val_loss: 2.2316 - val_acc: 0.4378\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9243 - acc: 0.4865 - val_loss: 2.2243 - val_acc: 0.4389\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9271 - acc: 0.4835 - val_loss: 2.2357 - val_acc: 0.4342\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9288 - acc: 0.4831 - val_loss: 2.2283 - val_acc: 0.4385\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9347 - acc: 0.4824 - val_loss: 2.2272 - val_acc: 0.4396\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9402 - acc: 0.4790 - val_loss: 2.2570 - val_acc: 0.4362\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9515 - acc: 0.4783 - val_loss: 2.2190 - val_acc: 0.4385\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9330 - acc: 0.4835 - val_loss: 2.2231 - val_acc: 0.4367\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9291 - acc: 0.4828 - val_loss: 2.2376 - val_acc: 0.4389\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9269 - acc: 0.4845 - val_loss: 2.2541 - val_acc: 0.4355\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9243 - acc: 0.4858 - val_loss: 2.2183 - val_acc: 0.4418\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9196 - acc: 0.4866 - val_loss: 2.2326 - val_acc: 0.4367\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9169 - acc: 0.4879 - val_loss: 2.2204 - val_acc: 0.4406\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9254 - acc: 0.4830 - val_loss: 2.2346 - val_acc: 0.4358\n",
      "Epoch 99/100\n",
      "2s - loss: 1.9258 - acc: 0.4842 - val_loss: 2.2239 - val_acc: 0.4402\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9165 - acc: 0.4856 - val_loss: 2.2249 - val_acc: 0.4371\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1964 - acc: 0.4272 - val_loss: 2.1838 - val_acc: 0.4434\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1751 - acc: 0.4310 - val_loss: 2.1831 - val_acc: 0.4451\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1610 - acc: 0.4349 - val_loss: 2.2057 - val_acc: 0.4411\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1511 - acc: 0.4344 - val_loss: 2.1926 - val_acc: 0.4432\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1452 - acc: 0.4363 - val_loss: 2.1986 - val_acc: 0.4391\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1378 - acc: 0.4398 - val_loss: 2.2090 - val_acc: 0.4405\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1334 - acc: 0.4401 - val_loss: 2.2008 - val_acc: 0.4397\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1228 - acc: 0.4408 - val_loss: 2.1986 - val_acc: 0.4408\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1159 - acc: 0.4429 - val_loss: 2.1996 - val_acc: 0.4437\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1155 - acc: 0.4419 - val_loss: 2.2007 - val_acc: 0.4394\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1083 - acc: 0.4442 - val_loss: 2.2168 - val_acc: 0.4401\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1088 - acc: 0.4447 - val_loss: 2.2216 - val_acc: 0.4383\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1021 - acc: 0.4452 - val_loss: 2.2095 - val_acc: 0.4394\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0930 - acc: 0.4474 - val_loss: 2.2083 - val_acc: 0.4392\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0883 - acc: 0.4452 - val_loss: 2.2089 - val_acc: 0.4409\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0909 - acc: 0.4467 - val_loss: 2.2037 - val_acc: 0.4397\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0844 - acc: 0.4490 - val_loss: 2.1995 - val_acc: 0.4414\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0737 - acc: 0.4510 - val_loss: 2.2305 - val_acc: 0.4391\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1184 - acc: 0.4434 - val_loss: 2.2502 - val_acc: 0.4322\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1005 - acc: 0.4465 - val_loss: 2.2164 - val_acc: 0.4397\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0771 - acc: 0.4522 - val_loss: 2.1988 - val_acc: 0.4425\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0674 - acc: 0.4526 - val_loss: 2.1905 - val_acc: 0.4431\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0608 - acc: 0.4543 - val_loss: 2.2122 - val_acc: 0.4435\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0608 - acc: 0.4524 - val_loss: 2.1953 - val_acc: 0.4414\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0584 - acc: 0.4540 - val_loss: 2.2011 - val_acc: 0.4434\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0595 - acc: 0.4548 - val_loss: 2.2029 - val_acc: 0.4427\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0580 - acc: 0.4548 - val_loss: 2.2133 - val_acc: 0.4421\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0566 - acc: 0.4544 - val_loss: 2.2009 - val_acc: 0.4411\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0503 - acc: 0.4569 - val_loss: 2.2057 - val_acc: 0.4404\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0442 - acc: 0.4565 - val_loss: 2.1925 - val_acc: 0.4430\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0442 - acc: 0.4562 - val_loss: 2.1942 - val_acc: 0.4432\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0391 - acc: 0.4597 - val_loss: 2.1881 - val_acc: 0.4444\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0345 - acc: 0.4597 - val_loss: 2.1905 - val_acc: 0.4444\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0368 - acc: 0.4580 - val_loss: 2.1933 - val_acc: 0.4453\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0295 - acc: 0.4599 - val_loss: 2.1899 - val_acc: 0.4434\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0297 - acc: 0.4607 - val_loss: 2.1881 - val_acc: 0.4452\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0294 - acc: 0.4611 - val_loss: 2.1853 - val_acc: 0.4451\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0259 - acc: 0.4623 - val_loss: 2.2070 - val_acc: 0.4410\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0249 - acc: 0.4620 - val_loss: 2.1866 - val_acc: 0.4440\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0206 - acc: 0.4629 - val_loss: 2.1939 - val_acc: 0.4447\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0219 - acc: 0.4623 - val_loss: 2.1967 - val_acc: 0.4451\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0287 - acc: 0.4613 - val_loss: 2.2144 - val_acc: 0.4398\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0316 - acc: 0.4621 - val_loss: 2.1933 - val_acc: 0.4450\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0209 - acc: 0.4646 - val_loss: 2.1964 - val_acc: 0.4449\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0155 - acc: 0.4639 - val_loss: 2.1897 - val_acc: 0.4444\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0111 - acc: 0.4656 - val_loss: 2.1836 - val_acc: 0.4439\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0079 - acc: 0.4666 - val_loss: 2.1869 - val_acc: 0.4451\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0086 - acc: 0.4657 - val_loss: 2.1829 - val_acc: 0.4450\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0101 - acc: 0.4636 - val_loss: 2.2055 - val_acc: 0.4393\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0121 - acc: 0.4643 - val_loss: 2.1929 - val_acc: 0.4438\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0093 - acc: 0.4651 - val_loss: 2.2031 - val_acc: 0.4423\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0025 - acc: 0.4676 - val_loss: 2.1898 - val_acc: 0.4428\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0005 - acc: 0.4677 - val_loss: 2.2004 - val_acc: 0.4433\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9983 - acc: 0.4678 - val_loss: 2.1825 - val_acc: 0.4458\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9951 - acc: 0.4673 - val_loss: 2.1862 - val_acc: 0.4430\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9907 - acc: 0.4692 - val_loss: 2.1855 - val_acc: 0.4426\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9852 - acc: 0.4713 - val_loss: 2.1934 - val_acc: 0.4431\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9850 - acc: 0.4703 - val_loss: 2.1940 - val_acc: 0.4427\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9916 - acc: 0.4677 - val_loss: 2.1922 - val_acc: 0.4437\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9834 - acc: 0.4718 - val_loss: 2.1874 - val_acc: 0.4444\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9814 - acc: 0.4723 - val_loss: 2.1839 - val_acc: 0.4459\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9824 - acc: 0.4715 - val_loss: 2.2023 - val_acc: 0.4426\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9831 - acc: 0.4721 - val_loss: 2.1881 - val_acc: 0.4458\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9832 - acc: 0.4719 - val_loss: 2.1913 - val_acc: 0.4438\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9825 - acc: 0.4718 - val_loss: 2.2127 - val_acc: 0.4405\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0086 - acc: 0.4654 - val_loss: 2.2030 - val_acc: 0.4401\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9909 - acc: 0.4707 - val_loss: 2.2058 - val_acc: 0.4410\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9778 - acc: 0.4725 - val_loss: 2.1790 - val_acc: 0.4432\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9670 - acc: 0.4750 - val_loss: 2.2066 - val_acc: 0.4416\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9915 - acc: 0.4715 - val_loss: 2.1915 - val_acc: 0.4419\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9772 - acc: 0.4744 - val_loss: 2.1988 - val_acc: 0.4430\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9702 - acc: 0.4756 - val_loss: 2.1841 - val_acc: 0.4423\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9663 - acc: 0.4736 - val_loss: 2.2069 - val_acc: 0.4404\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9656 - acc: 0.4777 - val_loss: 2.1908 - val_acc: 0.4424\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9660 - acc: 0.4750 - val_loss: 2.1994 - val_acc: 0.4435\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9638 - acc: 0.4752 - val_loss: 2.1891 - val_acc: 0.4446\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9655 - acc: 0.4765 - val_loss: 2.1860 - val_acc: 0.4469\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9501 - acc: 0.4796 - val_loss: 2.1769 - val_acc: 0.4454\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9548 - acc: 0.4766 - val_loss: 2.1942 - val_acc: 0.4435\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9673 - acc: 0.4740 - val_loss: 2.1912 - val_acc: 0.4428\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9572 - acc: 0.4772 - val_loss: 2.1971 - val_acc: 0.4420\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9590 - acc: 0.4767 - val_loss: 2.1841 - val_acc: 0.4444\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9500 - acc: 0.4795 - val_loss: 2.1894 - val_acc: 0.4455\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9438 - acc: 0.4810 - val_loss: 2.1918 - val_acc: 0.4427\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9403 - acc: 0.4811 - val_loss: 2.1870 - val_acc: 0.4428\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9415 - acc: 0.4822 - val_loss: 2.2077 - val_acc: 0.4412\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9535 - acc: 0.4789 - val_loss: 2.2108 - val_acc: 0.4415\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9437 - acc: 0.4813 - val_loss: 2.2220 - val_acc: 0.4369\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9546 - acc: 0.4789 - val_loss: 2.2073 - val_acc: 0.4400\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9422 - acc: 0.4814 - val_loss: 2.1979 - val_acc: 0.4437\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9432 - acc: 0.4803 - val_loss: 2.2055 - val_acc: 0.4424\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9530 - acc: 0.4777 - val_loss: 2.1982 - val_acc: 0.4434\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9467 - acc: 0.4787 - val_loss: 2.2030 - val_acc: 0.4402\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9410 - acc: 0.4808 - val_loss: 2.1892 - val_acc: 0.4454\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9461 - acc: 0.4800 - val_loss: 2.1926 - val_acc: 0.4420\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9341 - acc: 0.4837 - val_loss: 2.1905 - val_acc: 0.4432\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9319 - acc: 0.4844 - val_loss: 2.1995 - val_acc: 0.4429\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9321 - acc: 0.4845 - val_loss: 2.1930 - val_acc: 0.4442\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9404 - acc: 0.4816 - val_loss: 2.2440 - val_acc: 0.4363\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9514 - acc: 0.4798 - val_loss: 2.1925 - val_acc: 0.4408\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1713 - acc: 0.4321 - val_loss: 2.1711 - val_acc: 0.4414\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1515 - acc: 0.4350 - val_loss: 2.1702 - val_acc: 0.4426\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1326 - acc: 0.4387 - val_loss: 2.1840 - val_acc: 0.4409\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1302 - acc: 0.4399 - val_loss: 2.1842 - val_acc: 0.4396\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1171 - acc: 0.4426 - val_loss: 2.1943 - val_acc: 0.4373\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1105 - acc: 0.4432 - val_loss: 2.1946 - val_acc: 0.4406\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1030 - acc: 0.4442 - val_loss: 2.1983 - val_acc: 0.4402\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0946 - acc: 0.4473 - val_loss: 2.2007 - val_acc: 0.4388\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0919 - acc: 0.4487 - val_loss: 2.1916 - val_acc: 0.4406\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0839 - acc: 0.4498 - val_loss: 2.1915 - val_acc: 0.4406\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0775 - acc: 0.4496 - val_loss: 2.2097 - val_acc: 0.4373\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0768 - acc: 0.4497 - val_loss: 2.2032 - val_acc: 0.4390\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0738 - acc: 0.4515 - val_loss: 2.2001 - val_acc: 0.4389\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0664 - acc: 0.4519 - val_loss: 2.2312 - val_acc: 0.4358\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0782 - acc: 0.4507 - val_loss: 2.1979 - val_acc: 0.4412\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0606 - acc: 0.4542 - val_loss: 2.1998 - val_acc: 0.4389\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0520 - acc: 0.4552 - val_loss: 2.1905 - val_acc: 0.4398\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0501 - acc: 0.4556 - val_loss: 2.2014 - val_acc: 0.4385\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0466 - acc: 0.4580 - val_loss: 2.1942 - val_acc: 0.4429\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0482 - acc: 0.4565 - val_loss: 2.1946 - val_acc: 0.4398\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0414 - acc: 0.4593 - val_loss: 2.2009 - val_acc: 0.4399\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0392 - acc: 0.4603 - val_loss: 2.1985 - val_acc: 0.4393\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0359 - acc: 0.4603 - val_loss: 2.1946 - val_acc: 0.4412\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0325 - acc: 0.4601 - val_loss: 2.2050 - val_acc: 0.4389\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0308 - acc: 0.4600 - val_loss: 2.2021 - val_acc: 0.4407\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0457 - acc: 0.4572 - val_loss: 2.1973 - val_acc: 0.4413\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0236 - acc: 0.4627 - val_loss: 2.1916 - val_acc: 0.4411\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0233 - acc: 0.4598 - val_loss: 2.1874 - val_acc: 0.4429\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0236 - acc: 0.4625 - val_loss: 2.2145 - val_acc: 0.4400\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0510 - acc: 0.4579 - val_loss: 2.2256 - val_acc: 0.4357\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0333 - acc: 0.4610 - val_loss: 2.1897 - val_acc: 0.4430\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0122 - acc: 0.4649 - val_loss: 2.2021 - val_acc: 0.4371\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0118 - acc: 0.4644 - val_loss: 2.1834 - val_acc: 0.4413\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0051 - acc: 0.4668 - val_loss: 2.1824 - val_acc: 0.4412\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0047 - acc: 0.4686 - val_loss: 2.1893 - val_acc: 0.4433\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0015 - acc: 0.4659 - val_loss: 2.1883 - val_acc: 0.4421\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0000 - acc: 0.4687 - val_loss: 2.1827 - val_acc: 0.4413\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9999 - acc: 0.4684 - val_loss: 2.1928 - val_acc: 0.4414\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9954 - acc: 0.4685 - val_loss: 2.1903 - val_acc: 0.4418\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0056 - acc: 0.4682 - val_loss: 2.1889 - val_acc: 0.4420\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9934 - acc: 0.4716 - val_loss: 2.1841 - val_acc: 0.4427\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9927 - acc: 0.4687 - val_loss: 2.1891 - val_acc: 0.4414\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9904 - acc: 0.4701 - val_loss: 2.1812 - val_acc: 0.4430\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9927 - acc: 0.4702 - val_loss: 2.1989 - val_acc: 0.4407\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9850 - acc: 0.4714 - val_loss: 2.1811 - val_acc: 0.4421\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9842 - acc: 0.4723 - val_loss: 2.1767 - val_acc: 0.4437\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9827 - acc: 0.4723 - val_loss: 2.1790 - val_acc: 0.4441\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9772 - acc: 0.4734 - val_loss: 2.1834 - val_acc: 0.4409\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9797 - acc: 0.4723 - val_loss: 2.1729 - val_acc: 0.4451\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9743 - acc: 0.4742 - val_loss: 2.1712 - val_acc: 0.4440\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9699 - acc: 0.4748 - val_loss: 2.1797 - val_acc: 0.4438\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9739 - acc: 0.4741 - val_loss: 2.1830 - val_acc: 0.4412\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9725 - acc: 0.4737 - val_loss: 2.1838 - val_acc: 0.4434\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9764 - acc: 0.4725 - val_loss: 2.1815 - val_acc: 0.4420\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9765 - acc: 0.4720 - val_loss: 2.1847 - val_acc: 0.4427\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9694 - acc: 0.4739 - val_loss: 2.1769 - val_acc: 0.4444\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9650 - acc: 0.4763 - val_loss: 2.1761 - val_acc: 0.4426\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9589 - acc: 0.4757 - val_loss: 2.1755 - val_acc: 0.4440\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9605 - acc: 0.4762 - val_loss: 2.1653 - val_acc: 0.4453\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9556 - acc: 0.4781 - val_loss: 2.1664 - val_acc: 0.4450\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9533 - acc: 0.4794 - val_loss: 2.1814 - val_acc: 0.4407\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9578 - acc: 0.4773 - val_loss: 2.1635 - val_acc: 0.4463\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9508 - acc: 0.4782 - val_loss: 2.1866 - val_acc: 0.4444\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9588 - acc: 0.4782 - val_loss: 2.1853 - val_acc: 0.4429\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9633 - acc: 0.4772 - val_loss: 2.1786 - val_acc: 0.4432\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9540 - acc: 0.4765 - val_loss: 2.2003 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9660 - acc: 0.4769 - val_loss: 2.1897 - val_acc: 0.4420\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9649 - acc: 0.4769 - val_loss: 2.1833 - val_acc: 0.4444\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9627 - acc: 0.4781 - val_loss: 2.1834 - val_acc: 0.4420\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9534 - acc: 0.4790 - val_loss: 2.1760 - val_acc: 0.4412\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9462 - acc: 0.4787 - val_loss: 2.1826 - val_acc: 0.4439\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9472 - acc: 0.4814 - val_loss: 2.2011 - val_acc: 0.4404\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9414 - acc: 0.4820 - val_loss: 2.1758 - val_acc: 0.4440\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9481 - acc: 0.4793 - val_loss: 2.1905 - val_acc: 0.4410\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9422 - acc: 0.4843 - val_loss: 2.1773 - val_acc: 0.4465\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9343 - acc: 0.4848 - val_loss: 2.1893 - val_acc: 0.4428\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9363 - acc: 0.4819 - val_loss: 2.1672 - val_acc: 0.4457\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9275 - acc: 0.4849 - val_loss: 2.1654 - val_acc: 0.4449\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9229 - acc: 0.4837 - val_loss: 2.1701 - val_acc: 0.4470\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9230 - acc: 0.4860 - val_loss: 2.2035 - val_acc: 0.4408\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9554 - acc: 0.4788 - val_loss: 2.1830 - val_acc: 0.4433\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9315 - acc: 0.4855 - val_loss: 2.1665 - val_acc: 0.4460\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9209 - acc: 0.4850 - val_loss: 2.1644 - val_acc: 0.4485\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9348 - acc: 0.4833 - val_loss: 2.1721 - val_acc: 0.4430\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9219 - acc: 0.4851 - val_loss: 2.1703 - val_acc: 0.4459\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9296 - acc: 0.4839 - val_loss: 2.1758 - val_acc: 0.4436\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9325 - acc: 0.4828 - val_loss: 2.1837 - val_acc: 0.4442\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9255 - acc: 0.4846 - val_loss: 2.1774 - val_acc: 0.4452\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9221 - acc: 0.4861 - val_loss: 2.1777 - val_acc: 0.4433\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9234 - acc: 0.4851 - val_loss: 2.2134 - val_acc: 0.4432\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9773 - acc: 0.4739 - val_loss: 2.2426 - val_acc: 0.4312\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9670 - acc: 0.4759 - val_loss: 2.2230 - val_acc: 0.4388\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9406 - acc: 0.4820 - val_loss: 2.1900 - val_acc: 0.4382\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9180 - acc: 0.4867 - val_loss: 2.1887 - val_acc: 0.4431\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9161 - acc: 0.4879 - val_loss: 2.1830 - val_acc: 0.4421\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9049 - acc: 0.4903 - val_loss: 2.1808 - val_acc: 0.4441\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9202 - acc: 0.4864 - val_loss: 2.1746 - val_acc: 0.4470\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9065 - acc: 0.4898 - val_loss: 2.1666 - val_acc: 0.4433\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9062 - acc: 0.4910 - val_loss: 2.1735 - val_acc: 0.4430\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9077 - acc: 0.4890 - val_loss: 2.1759 - val_acc: 0.4442\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1811 - acc: 0.4265 - val_loss: 2.1890 - val_acc: 0.4423\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1581 - acc: 0.4328 - val_loss: 2.1938 - val_acc: 0.4412\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1442 - acc: 0.4347 - val_loss: 2.1947 - val_acc: 0.4409\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1411 - acc: 0.4373 - val_loss: 2.1958 - val_acc: 0.4399\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1273 - acc: 0.4389 - val_loss: 2.2150 - val_acc: 0.4388\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1237 - acc: 0.4373 - val_loss: 2.2049 - val_acc: 0.4393\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1139 - acc: 0.4409 - val_loss: 2.2154 - val_acc: 0.4364\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1117 - acc: 0.4424 - val_loss: 2.1983 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1031 - acc: 0.4421 - val_loss: 2.2067 - val_acc: 0.4367\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0969 - acc: 0.4439 - val_loss: 2.2179 - val_acc: 0.4345\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0950 - acc: 0.4463 - val_loss: 2.2058 - val_acc: 0.4370\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0868 - acc: 0.4460 - val_loss: 2.2040 - val_acc: 0.4376\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0846 - acc: 0.4479 - val_loss: 2.2010 - val_acc: 0.4386\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0819 - acc: 0.4475 - val_loss: 2.2092 - val_acc: 0.4359\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0778 - acc: 0.4494 - val_loss: 2.2065 - val_acc: 0.4359\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0726 - acc: 0.4506 - val_loss: 2.2142 - val_acc: 0.4326\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0707 - acc: 0.4495 - val_loss: 2.1963 - val_acc: 0.4392\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0642 - acc: 0.4523 - val_loss: 2.2000 - val_acc: 0.4374\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0607 - acc: 0.4527 - val_loss: 2.2008 - val_acc: 0.4392\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0563 - acc: 0.4541 - val_loss: 2.1937 - val_acc: 0.4386\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0568 - acc: 0.4534 - val_loss: 2.2014 - val_acc: 0.4368\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0542 - acc: 0.4552 - val_loss: 2.2051 - val_acc: 0.4382\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0514 - acc: 0.4554 - val_loss: 2.2008 - val_acc: 0.4395\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0460 - acc: 0.4549 - val_loss: 2.1896 - val_acc: 0.4398\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0466 - acc: 0.4570 - val_loss: 2.1960 - val_acc: 0.4383\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0382 - acc: 0.4579 - val_loss: 2.1963 - val_acc: 0.4391\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0340 - acc: 0.4601 - val_loss: 2.2030 - val_acc: 0.4368\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0342 - acc: 0.4575 - val_loss: 2.2089 - val_acc: 0.4363\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0323 - acc: 0.4582 - val_loss: 2.2066 - val_acc: 0.4354\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0248 - acc: 0.4617 - val_loss: 2.1988 - val_acc: 0.4371\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0266 - acc: 0.4620 - val_loss: 2.2002 - val_acc: 0.4372\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0283 - acc: 0.4615 - val_loss: 2.1927 - val_acc: 0.4398\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0398 - acc: 0.4567 - val_loss: 2.2106 - val_acc: 0.4360\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0347 - acc: 0.4608 - val_loss: 2.2051 - val_acc: 0.4382\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0233 - acc: 0.4617 - val_loss: 2.2105 - val_acc: 0.4367\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0209 - acc: 0.4601 - val_loss: 2.1916 - val_acc: 0.4368\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0164 - acc: 0.4630 - val_loss: 2.2132 - val_acc: 0.4369\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0174 - acc: 0.4632 - val_loss: 2.2031 - val_acc: 0.4388\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0131 - acc: 0.4628 - val_loss: 2.2033 - val_acc: 0.4362\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0065 - acc: 0.4642 - val_loss: 2.1935 - val_acc: 0.4397\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0057 - acc: 0.4661 - val_loss: 2.2098 - val_acc: 0.4351\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0097 - acc: 0.4649 - val_loss: 2.1989 - val_acc: 0.4365\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0013 - acc: 0.4654 - val_loss: 2.2004 - val_acc: 0.4375\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9993 - acc: 0.4672 - val_loss: 2.1947 - val_acc: 0.4381\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9989 - acc: 0.4678 - val_loss: 2.1961 - val_acc: 0.4404\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0033 - acc: 0.4670 - val_loss: 2.1943 - val_acc: 0.4391\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9988 - acc: 0.4669 - val_loss: 2.2039 - val_acc: 0.4376\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9958 - acc: 0.4674 - val_loss: 2.1919 - val_acc: 0.4386\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9898 - acc: 0.4676 - val_loss: 2.2004 - val_acc: 0.4375\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9905 - acc: 0.4686 - val_loss: 2.1910 - val_acc: 0.4375\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9846 - acc: 0.4691 - val_loss: 2.1991 - val_acc: 0.4379\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9827 - acc: 0.4698 - val_loss: 2.1900 - val_acc: 0.4386\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9811 - acc: 0.4697 - val_loss: 2.2035 - val_acc: 0.4370\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9838 - acc: 0.4694 - val_loss: 2.1892 - val_acc: 0.4409\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9804 - acc: 0.4713 - val_loss: 2.1998 - val_acc: 0.4381\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9870 - acc: 0.4694 - val_loss: 2.1961 - val_acc: 0.4364\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9886 - acc: 0.4676 - val_loss: 2.1979 - val_acc: 0.4414\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9781 - acc: 0.4711 - val_loss: 2.1825 - val_acc: 0.4422\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9691 - acc: 0.4724 - val_loss: 2.1974 - val_acc: 0.4403\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9728 - acc: 0.4727 - val_loss: 2.1914 - val_acc: 0.4400\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9680 - acc: 0.4728 - val_loss: 2.1885 - val_acc: 0.4415\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9672 - acc: 0.4736 - val_loss: 2.1869 - val_acc: 0.4416\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9662 - acc: 0.4755 - val_loss: 2.1932 - val_acc: 0.4381\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9640 - acc: 0.4749 - val_loss: 2.1985 - val_acc: 0.4393\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9633 - acc: 0.4752 - val_loss: 2.1971 - val_acc: 0.4405\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9706 - acc: 0.4716 - val_loss: 2.1920 - val_acc: 0.4401\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9688 - acc: 0.4742 - val_loss: 2.2070 - val_acc: 0.4392\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9694 - acc: 0.4724 - val_loss: 2.2044 - val_acc: 0.4382\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9737 - acc: 0.4728 - val_loss: 2.2054 - val_acc: 0.4398\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9578 - acc: 0.4752 - val_loss: 2.1938 - val_acc: 0.4400\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9527 - acc: 0.4767 - val_loss: 2.2027 - val_acc: 0.4419\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9567 - acc: 0.4761 - val_loss: 2.1986 - val_acc: 0.4386\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9657 - acc: 0.4734 - val_loss: 2.2115 - val_acc: 0.4366\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9568 - acc: 0.4752 - val_loss: 2.1981 - val_acc: 0.4377\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9590 - acc: 0.4769 - val_loss: 2.2076 - val_acc: 0.4387\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9575 - acc: 0.4768 - val_loss: 2.2158 - val_acc: 0.4360\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9565 - acc: 0.4753 - val_loss: 2.2108 - val_acc: 0.4384\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9454 - acc: 0.4788 - val_loss: 2.2160 - val_acc: 0.4349\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9466 - acc: 0.4783 - val_loss: 2.2040 - val_acc: 0.4385\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9444 - acc: 0.4801 - val_loss: 2.1986 - val_acc: 0.4404\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9382 - acc: 0.4790 - val_loss: 2.1971 - val_acc: 0.4404\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9315 - acc: 0.4818 - val_loss: 2.2094 - val_acc: 0.4364\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9402 - acc: 0.4792 - val_loss: 2.2185 - val_acc: 0.4390\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9518 - acc: 0.4771 - val_loss: 2.2208 - val_acc: 0.4356\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9501 - acc: 0.4753 - val_loss: 2.1974 - val_acc: 0.4424\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9353 - acc: 0.4801 - val_loss: 2.2020 - val_acc: 0.4384\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9333 - acc: 0.4802 - val_loss: 2.2101 - val_acc: 0.4387\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9385 - acc: 0.4801 - val_loss: 2.1968 - val_acc: 0.4390\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9311 - acc: 0.4801 - val_loss: 2.1996 - val_acc: 0.4398\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9310 - acc: 0.4826 - val_loss: 2.1928 - val_acc: 0.4411\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9310 - acc: 0.4816 - val_loss: 2.2080 - val_acc: 0.4384\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9266 - acc: 0.4838 - val_loss: 2.1911 - val_acc: 0.4423\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9264 - acc: 0.4830 - val_loss: 2.1995 - val_acc: 0.4385\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9245 - acc: 0.4798 - val_loss: 2.1992 - val_acc: 0.4403\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9268 - acc: 0.4822 - val_loss: 2.2405 - val_acc: 0.4357\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9561 - acc: 0.4757 - val_loss: 2.2039 - val_acc: 0.4391\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9352 - acc: 0.4806 - val_loss: 2.2257 - val_acc: 0.4360\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9349 - acc: 0.4801 - val_loss: 2.1996 - val_acc: 0.4377\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9233 - acc: 0.4847 - val_loss: 2.2108 - val_acc: 0.4369\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9185 - acc: 0.4852 - val_loss: 2.1978 - val_acc: 0.4414\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1951 - acc: 0.4297 - val_loss: 2.1687 - val_acc: 0.4456\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1705 - acc: 0.4338 - val_loss: 2.1693 - val_acc: 0.4444\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1532 - acc: 0.4371 - val_loss: 2.1810 - val_acc: 0.4450\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1507 - acc: 0.4357 - val_loss: 2.1811 - val_acc: 0.4440\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1329 - acc: 0.4407 - val_loss: 2.1749 - val_acc: 0.4447\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1305 - acc: 0.4397 - val_loss: 2.1852 - val_acc: 0.4445\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1229 - acc: 0.4426 - val_loss: 2.1861 - val_acc: 0.4454\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1228 - acc: 0.4432 - val_loss: 2.1939 - val_acc: 0.4028\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1122 - acc: 0.4451 - val_loss: 2.1954 - val_acc: 0.4038\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1074 - acc: 0.4450 - val_loss: 2.1864 - val_acc: 0.4042\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1019 - acc: 0.4472 - val_loss: 2.1751 - val_acc: 0.4481\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0935 - acc: 0.4496 - val_loss: 2.1767 - val_acc: 0.4483\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0902 - acc: 0.4488 - val_loss: 2.1983 - val_acc: 0.4006\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0858 - acc: 0.4526 - val_loss: 2.1949 - val_acc: 0.4023\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0903 - acc: 0.4504 - val_loss: 2.1957 - val_acc: 0.3997\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0809 - acc: 0.4506 - val_loss: 2.1995 - val_acc: 0.4415\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0801 - acc: 0.4517 - val_loss: 2.2087 - val_acc: 0.4022\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1068 - acc: 0.4463 - val_loss: 2.2086 - val_acc: 0.4398\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0884 - acc: 0.4520 - val_loss: 2.1957 - val_acc: 0.4020\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0667 - acc: 0.4537 - val_loss: 2.1789 - val_acc: 0.4478\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0740 - acc: 0.4528 - val_loss: 2.1924 - val_acc: 0.4030\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0663 - acc: 0.4560 - val_loss: 2.1722 - val_acc: 0.4479\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0600 - acc: 0.4565 - val_loss: 2.1806 - val_acc: 0.4456\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0535 - acc: 0.4588 - val_loss: 2.1848 - val_acc: 0.4458\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0495 - acc: 0.4573 - val_loss: 2.1750 - val_acc: 0.4463\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0479 - acc: 0.4597 - val_loss: 2.1672 - val_acc: 0.4470\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0454 - acc: 0.4607 - val_loss: 2.1760 - val_acc: 0.4449\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0441 - acc: 0.4613 - val_loss: 2.1774 - val_acc: 0.4441\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0452 - acc: 0.4588 - val_loss: 2.1722 - val_acc: 0.4439\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0408 - acc: 0.4603 - val_loss: 2.1675 - val_acc: 0.4442\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0371 - acc: 0.4603 - val_loss: 2.1846 - val_acc: 0.4444\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0390 - acc: 0.4616 - val_loss: 2.1616 - val_acc: 0.4462\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0337 - acc: 0.4613 - val_loss: 2.1728 - val_acc: 0.4450\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0300 - acc: 0.4625 - val_loss: 2.1649 - val_acc: 0.4467\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0292 - acc: 0.4624 - val_loss: 2.1721 - val_acc: 0.4458\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0287 - acc: 0.4603 - val_loss: 2.1714 - val_acc: 0.4459\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0256 - acc: 0.4631 - val_loss: 2.1886 - val_acc: 0.4416\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0311 - acc: 0.4624 - val_loss: 2.2110 - val_acc: 0.4390\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0354 - acc: 0.4621 - val_loss: 2.1765 - val_acc: 0.4459\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0267 - acc: 0.4650 - val_loss: 2.1922 - val_acc: 0.4424\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0206 - acc: 0.4659 - val_loss: 2.1647 - val_acc: 0.4482\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0169 - acc: 0.4631 - val_loss: 2.1658 - val_acc: 0.4479\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0193 - acc: 0.4655 - val_loss: 2.1565 - val_acc: 0.4475\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0097 - acc: 0.4682 - val_loss: 2.1500 - val_acc: 0.4475\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0035 - acc: 0.4687 - val_loss: 2.1605 - val_acc: 0.4473\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0072 - acc: 0.4656 - val_loss: 2.1519 - val_acc: 0.4493\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0009 - acc: 0.4697 - val_loss: 2.1686 - val_acc: 0.4454\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0021 - acc: 0.4692 - val_loss: 2.1663 - val_acc: 0.4489\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0232 - acc: 0.4655 - val_loss: 2.1947 - val_acc: 0.4408\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0095 - acc: 0.4679 - val_loss: 2.1563 - val_acc: 0.4489\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0033 - acc: 0.4677 - val_loss: 2.1659 - val_acc: 0.4478\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9987 - acc: 0.4697 - val_loss: 2.1586 - val_acc: 0.4482\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9904 - acc: 0.4708 - val_loss: 2.1541 - val_acc: 0.4490\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9940 - acc: 0.4695 - val_loss: 2.1679 - val_acc: 0.4475\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9959 - acc: 0.4685 - val_loss: 2.1638 - val_acc: 0.4471\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9878 - acc: 0.4719 - val_loss: 2.1494 - val_acc: 0.4496\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9897 - acc: 0.4723 - val_loss: 2.1598 - val_acc: 0.4472\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9876 - acc: 0.4721 - val_loss: 2.1594 - val_acc: 0.4465\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9895 - acc: 0.4727 - val_loss: 2.1592 - val_acc: 0.4494\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9969 - acc: 0.4722 - val_loss: 2.1646 - val_acc: 0.4466\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9926 - acc: 0.4722 - val_loss: 2.1663 - val_acc: 0.4471\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9981 - acc: 0.4712 - val_loss: 2.1722 - val_acc: 0.4451\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9902 - acc: 0.4719 - val_loss: 2.1743 - val_acc: 0.4465\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9873 - acc: 0.4724 - val_loss: 2.1891 - val_acc: 0.4417\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9920 - acc: 0.4721 - val_loss: 2.2015 - val_acc: 0.4405\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9999 - acc: 0.4687 - val_loss: 2.1906 - val_acc: 0.4403\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9864 - acc: 0.4711 - val_loss: 2.1743 - val_acc: 0.4441\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9764 - acc: 0.4753 - val_loss: 2.1825 - val_acc: 0.4426\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9721 - acc: 0.4762 - val_loss: 2.1620 - val_acc: 0.4470\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9685 - acc: 0.4768 - val_loss: 2.1675 - val_acc: 0.4455\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9703 - acc: 0.4779 - val_loss: 2.1589 - val_acc: 0.4492\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9668 - acc: 0.4782 - val_loss: 2.1695 - val_acc: 0.4463\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9614 - acc: 0.4762 - val_loss: 2.1540 - val_acc: 0.4481\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9563 - acc: 0.4793 - val_loss: 2.1567 - val_acc: 0.4489\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9577 - acc: 0.4797 - val_loss: 2.1519 - val_acc: 0.4487\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9570 - acc: 0.4790 - val_loss: 2.1950 - val_acc: 0.4407\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9694 - acc: 0.4771 - val_loss: 2.1733 - val_acc: 0.4428\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9636 - acc: 0.4776 - val_loss: 2.1873 - val_acc: 0.4433\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9648 - acc: 0.4782 - val_loss: 2.1676 - val_acc: 0.4471\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9578 - acc: 0.4784 - val_loss: 2.1857 - val_acc: 0.4423\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9632 - acc: 0.4795 - val_loss: 2.1685 - val_acc: 0.4472\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9649 - acc: 0.4788 - val_loss: 2.1778 - val_acc: 0.4446\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9544 - acc: 0.4814 - val_loss: 2.1579 - val_acc: 0.4489\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9437 - acc: 0.4825 - val_loss: 2.1927 - val_acc: 0.4439\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9452 - acc: 0.4829 - val_loss: 2.1570 - val_acc: 0.4496\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9419 - acc: 0.4833 - val_loss: 2.1656 - val_acc: 0.4485\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9412 - acc: 0.4837 - val_loss: 2.1583 - val_acc: 0.4497\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9446 - acc: 0.4837 - val_loss: 2.1641 - val_acc: 0.4483\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9424 - acc: 0.4835 - val_loss: 2.1699 - val_acc: 0.4473\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9521 - acc: 0.4797 - val_loss: 2.1587 - val_acc: 0.4481\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9498 - acc: 0.4790 - val_loss: 2.1502 - val_acc: 0.4505\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9366 - acc: 0.4842 - val_loss: 2.1536 - val_acc: 0.4479\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9411 - acc: 0.4816 - val_loss: 2.1602 - val_acc: 0.4485\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9424 - acc: 0.4809 - val_loss: 2.1669 - val_acc: 0.4465\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9385 - acc: 0.4829 - val_loss: 2.1734 - val_acc: 0.4444\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9365 - acc: 0.4848 - val_loss: 2.1776 - val_acc: 0.4461\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9384 - acc: 0.4850 - val_loss: 2.1762 - val_acc: 0.4460\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9416 - acc: 0.4822 - val_loss: 2.1828 - val_acc: 0.4417\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9390 - acc: 0.4822 - val_loss: 2.1813 - val_acc: 0.4440\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9388 - acc: 0.4841 - val_loss: 2.1753 - val_acc: 0.4449\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1718 - acc: 0.4308 - val_loss: 2.1654 - val_acc: 0.4476\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1458 - acc: 0.4364 - val_loss: 2.1655 - val_acc: 0.4498\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1321 - acc: 0.4370 - val_loss: 2.1749 - val_acc: 0.4458\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1199 - acc: 0.4406 - val_loss: 2.1737 - val_acc: 0.4465\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1197 - acc: 0.4406 - val_loss: 2.1705 - val_acc: 0.4469\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1067 - acc: 0.4437 - val_loss: 2.1768 - val_acc: 0.4436\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1008 - acc: 0.4439 - val_loss: 2.1736 - val_acc: 0.4456\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0936 - acc: 0.4452 - val_loss: 2.1849 - val_acc: 0.4454\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1122 - acc: 0.4430 - val_loss: 2.1962 - val_acc: 0.4419\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0911 - acc: 0.4478 - val_loss: 2.1881 - val_acc: 0.4433\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0828 - acc: 0.4502 - val_loss: 2.1906 - val_acc: 0.4441\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0794 - acc: 0.4496 - val_loss: 2.1857 - val_acc: 0.4421\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0707 - acc: 0.4496 - val_loss: 2.1903 - val_acc: 0.4438\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0723 - acc: 0.4501 - val_loss: 2.1760 - val_acc: 0.4434\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0631 - acc: 0.4530 - val_loss: 2.1851 - val_acc: 0.4424\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0569 - acc: 0.4530 - val_loss: 2.1795 - val_acc: 0.4425\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0595 - acc: 0.4546 - val_loss: 2.1774 - val_acc: 0.4459\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0520 - acc: 0.4542 - val_loss: 2.1834 - val_acc: 0.4452\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0538 - acc: 0.4543 - val_loss: 2.1729 - val_acc: 0.4444\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0470 - acc: 0.4572 - val_loss: 2.1811 - val_acc: 0.4448\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0389 - acc: 0.4566 - val_loss: 2.1896 - val_acc: 0.4441\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0442 - acc: 0.4555 - val_loss: 2.1735 - val_acc: 0.4472\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0424 - acc: 0.4578 - val_loss: 2.1856 - val_acc: 0.4434\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0367 - acc: 0.4579 - val_loss: 2.1782 - val_acc: 0.4457\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0299 - acc: 0.4601 - val_loss: 2.1743 - val_acc: 0.4472\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0268 - acc: 0.4598 - val_loss: 2.1807 - val_acc: 0.4450\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0219 - acc: 0.4606 - val_loss: 2.1752 - val_acc: 0.4476\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0186 - acc: 0.4603 - val_loss: 2.1774 - val_acc: 0.4435\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0197 - acc: 0.4624 - val_loss: 2.1772 - val_acc: 0.4455\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0185 - acc: 0.4620 - val_loss: 2.1835 - val_acc: 0.4440\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0193 - acc: 0.4613 - val_loss: 2.1828 - val_acc: 0.4464\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0233 - acc: 0.4600 - val_loss: 2.1855 - val_acc: 0.4443\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0131 - acc: 0.4629 - val_loss: 2.1899 - val_acc: 0.4457\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0293 - acc: 0.4600 - val_loss: 2.1824 - val_acc: 0.4458\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0047 - acc: 0.4655 - val_loss: 2.1817 - val_acc: 0.4458\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0022 - acc: 0.4660 - val_loss: 2.1747 - val_acc: 0.4467\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9997 - acc: 0.4657 - val_loss: 2.1751 - val_acc: 0.4473\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9955 - acc: 0.4680 - val_loss: 2.1729 - val_acc: 0.4485\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9979 - acc: 0.4660 - val_loss: 2.1678 - val_acc: 0.4492\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9896 - acc: 0.4684 - val_loss: 2.1788 - val_acc: 0.4475\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9887 - acc: 0.4703 - val_loss: 2.1727 - val_acc: 0.4466\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9990 - acc: 0.4670 - val_loss: 2.1789 - val_acc: 0.4477\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9913 - acc: 0.4673 - val_loss: 2.1627 - val_acc: 0.4478\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9901 - acc: 0.4700 - val_loss: 2.2072 - val_acc: 0.4425\n",
      "Epoch 45/100\n",
      "2s - loss: 1.9942 - acc: 0.4686 - val_loss: 2.1745 - val_acc: 0.4465\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9936 - acc: 0.4665 - val_loss: 2.1853 - val_acc: 0.4465\n",
      "Epoch 47/100\n",
      "2s - loss: 1.9906 - acc: 0.4676 - val_loss: 2.2027 - val_acc: 0.4416\n",
      "Epoch 48/100\n",
      "2s - loss: 1.9931 - acc: 0.4670 - val_loss: 2.2064 - val_acc: 0.4464\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0163 - acc: 0.4652 - val_loss: 2.2121 - val_acc: 0.4406\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9931 - acc: 0.4690 - val_loss: 2.1701 - val_acc: 0.4520\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9722 - acc: 0.4723 - val_loss: 2.1794 - val_acc: 0.4477\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9692 - acc: 0.4736 - val_loss: 2.1779 - val_acc: 0.4463\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9678 - acc: 0.4748 - val_loss: 2.1647 - val_acc: 0.4475\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9683 - acc: 0.4738 - val_loss: 2.1849 - val_acc: 0.4472\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9788 - acc: 0.4702 - val_loss: 2.1885 - val_acc: 0.4445\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9718 - acc: 0.4723 - val_loss: 2.1757 - val_acc: 0.4485\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9626 - acc: 0.4744 - val_loss: 2.2102 - val_acc: 0.4422\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9675 - acc: 0.4750 - val_loss: 2.1783 - val_acc: 0.4459\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9596 - acc: 0.4740 - val_loss: 2.1796 - val_acc: 0.4475\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9613 - acc: 0.4741 - val_loss: 2.1882 - val_acc: 0.4453\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9673 - acc: 0.4737 - val_loss: 2.1894 - val_acc: 0.4452\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9610 - acc: 0.4753 - val_loss: 2.1894 - val_acc: 0.4465\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9658 - acc: 0.4738 - val_loss: 2.1763 - val_acc: 0.4482\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9573 - acc: 0.4752 - val_loss: 2.1877 - val_acc: 0.4455\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9521 - acc: 0.4779 - val_loss: 2.1631 - val_acc: 0.4504\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9487 - acc: 0.4788 - val_loss: 2.1828 - val_acc: 0.4441\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9506 - acc: 0.4777 - val_loss: 2.2126 - val_acc: 0.4434\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9535 - acc: 0.4767 - val_loss: 2.1884 - val_acc: 0.4468\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9476 - acc: 0.4783 - val_loss: 2.1707 - val_acc: 0.4489\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9504 - acc: 0.4782 - val_loss: 2.2020 - val_acc: 0.4448\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9479 - acc: 0.4783 - val_loss: 2.1670 - val_acc: 0.4492\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9431 - acc: 0.4793 - val_loss: 2.1860 - val_acc: 0.4475\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9509 - acc: 0.4774 - val_loss: 2.1830 - val_acc: 0.4480\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9367 - acc: 0.4814 - val_loss: 2.1886 - val_acc: 0.4455\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9430 - acc: 0.4811 - val_loss: 2.1901 - val_acc: 0.4462\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9384 - acc: 0.4809 - val_loss: 2.2074 - val_acc: 0.4442\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9472 - acc: 0.4783 - val_loss: 2.1832 - val_acc: 0.4457\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9314 - acc: 0.4808 - val_loss: 2.1966 - val_acc: 0.4452\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9484 - acc: 0.4783 - val_loss: 2.2294 - val_acc: 0.4385\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9492 - acc: 0.4781 - val_loss: 2.2123 - val_acc: 0.4451\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9398 - acc: 0.4789 - val_loss: 2.1998 - val_acc: 0.4439\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9291 - acc: 0.4817 - val_loss: 2.1825 - val_acc: 0.4462\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9310 - acc: 0.4828 - val_loss: 2.1871 - val_acc: 0.4471\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9199 - acc: 0.4858 - val_loss: 2.1771 - val_acc: 0.4470\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9148 - acc: 0.4872 - val_loss: 2.1769 - val_acc: 0.4479\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9134 - acc: 0.4867 - val_loss: 2.1898 - val_acc: 0.4491\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9174 - acc: 0.4839 - val_loss: 2.2022 - val_acc: 0.4445\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9173 - acc: 0.4862 - val_loss: 2.1991 - val_acc: 0.4458\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9130 - acc: 0.4865 - val_loss: 2.1775 - val_acc: 0.4459\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9119 - acc: 0.4863 - val_loss: 2.2224 - val_acc: 0.4407\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9299 - acc: 0.4839 - val_loss: 2.2044 - val_acc: 0.4420\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9150 - acc: 0.4859 - val_loss: 2.1860 - val_acc: 0.4475\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9291 - acc: 0.4850 - val_loss: 2.1980 - val_acc: 0.4440\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9195 - acc: 0.4859 - val_loss: 2.1819 - val_acc: 0.4490\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9276 - acc: 0.4824 - val_loss: 2.2399 - val_acc: 0.4429\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9541 - acc: 0.4777 - val_loss: 2.2021 - val_acc: 0.4461\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9206 - acc: 0.4869 - val_loss: 2.1883 - val_acc: 0.4475\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9052 - acc: 0.4872 - val_loss: 2.1875 - val_acc: 0.4451\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9048 - acc: 0.4879 - val_loss: 2.2058 - val_acc: 0.4454\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9340 - acc: 0.4833 - val_loss: 2.2019 - val_acc: 0.4419\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2295 - acc: 0.4192 - val_loss: 2.1695 - val_acc: 0.4469\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1924 - acc: 0.4261 - val_loss: 2.1657 - val_acc: 0.4475\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1755 - acc: 0.4321 - val_loss: 2.1872 - val_acc: 0.4443\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1707 - acc: 0.4315 - val_loss: 2.1840 - val_acc: 0.4456\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1612 - acc: 0.4340 - val_loss: 2.1985 - val_acc: 0.4405\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1588 - acc: 0.4337 - val_loss: 2.1759 - val_acc: 0.4464\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1404 - acc: 0.4383 - val_loss: 2.1938 - val_acc: 0.4401\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1414 - acc: 0.4372 - val_loss: 2.1923 - val_acc: 0.4414\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1404 - acc: 0.4378 - val_loss: 2.1948 - val_acc: 0.4407\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1239 - acc: 0.4398 - val_loss: 2.1900 - val_acc: 0.4444\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1200 - acc: 0.4437 - val_loss: 2.1903 - val_acc: 0.4453\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1201 - acc: 0.4435 - val_loss: 2.1871 - val_acc: 0.4424\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1138 - acc: 0.4440 - val_loss: 2.1904 - val_acc: 0.4434\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1140 - acc: 0.4452 - val_loss: 2.1864 - val_acc: 0.4426\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1081 - acc: 0.4467 - val_loss: 2.1820 - val_acc: 0.4435\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0995 - acc: 0.4467 - val_loss: 2.1869 - val_acc: 0.4413\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1011 - acc: 0.4453 - val_loss: 2.1934 - val_acc: 0.4399\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1030 - acc: 0.4446 - val_loss: 2.1831 - val_acc: 0.4428\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0939 - acc: 0.4479 - val_loss: 2.1727 - val_acc: 0.4469\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0886 - acc: 0.4487 - val_loss: 2.1788 - val_acc: 0.4464\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0827 - acc: 0.4511 - val_loss: 2.1781 - val_acc: 0.4461\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0976 - acc: 0.4493 - val_loss: 2.1815 - val_acc: 0.4433\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0830 - acc: 0.4502 - val_loss: 2.1710 - val_acc: 0.4442\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0800 - acc: 0.4508 - val_loss: 2.1932 - val_acc: 0.4418\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0775 - acc: 0.4507 - val_loss: 2.1825 - val_acc: 0.4414\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0709 - acc: 0.4539 - val_loss: 2.1742 - val_acc: 0.4401\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0676 - acc: 0.4536 - val_loss: 2.1709 - val_acc: 0.4439\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0633 - acc: 0.4530 - val_loss: 2.1646 - val_acc: 0.4424\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0590 - acc: 0.4566 - val_loss: 2.1795 - val_acc: 0.4410\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0553 - acc: 0.4557 - val_loss: 2.1684 - val_acc: 0.4435\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0551 - acc: 0.4551 - val_loss: 2.1771 - val_acc: 0.4418\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0560 - acc: 0.4556 - val_loss: 2.1629 - val_acc: 0.4472\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0516 - acc: 0.4552 - val_loss: 2.1759 - val_acc: 0.4460\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0546 - acc: 0.4559 - val_loss: 2.1610 - val_acc: 0.4469\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0504 - acc: 0.4577 - val_loss: 2.1773 - val_acc: 0.4425\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0627 - acc: 0.4527 - val_loss: 2.1681 - val_acc: 0.4457\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0736 - acc: 0.4527 - val_loss: 2.1689 - val_acc: 0.4467\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0507 - acc: 0.4566 - val_loss: 2.2015 - val_acc: 0.4404\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0556 - acc: 0.4554 - val_loss: 2.1551 - val_acc: 0.4495\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0338 - acc: 0.4598 - val_loss: 2.1614 - val_acc: 0.4488\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0332 - acc: 0.4591 - val_loss: 2.1555 - val_acc: 0.4482\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0273 - acc: 0.4609 - val_loss: 2.1547 - val_acc: 0.4467\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0287 - acc: 0.4616 - val_loss: 2.1735 - val_acc: 0.4457\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0377 - acc: 0.4589 - val_loss: 2.1606 - val_acc: 0.4472\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0301 - acc: 0.4619 - val_loss: 2.1700 - val_acc: 0.4467\n",
      "Epoch 46/100\n",
      "2s - loss: 2.0330 - acc: 0.4601 - val_loss: 2.1632 - val_acc: 0.4465\n",
      "Epoch 47/100\n",
      "2s - loss: 2.0339 - acc: 0.4612 - val_loss: 2.1689 - val_acc: 0.4460\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0324 - acc: 0.4598 - val_loss: 2.1674 - val_acc: 0.4463\n",
      "Epoch 49/100\n",
      "2s - loss: 2.0313 - acc: 0.4617 - val_loss: 2.1670 - val_acc: 0.4473\n",
      "Epoch 50/100\n",
      "2s - loss: 2.0276 - acc: 0.4615 - val_loss: 2.1578 - val_acc: 0.4500\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0348 - acc: 0.4599 - val_loss: 2.1716 - val_acc: 0.4470\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0274 - acc: 0.4624 - val_loss: 2.1590 - val_acc: 0.4470\n",
      "Epoch 53/100\n",
      "2s - loss: 2.0256 - acc: 0.4617 - val_loss: 2.1756 - val_acc: 0.4471\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0212 - acc: 0.4636 - val_loss: 2.1703 - val_acc: 0.4475\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0191 - acc: 0.4638 - val_loss: 2.1791 - val_acc: 0.4485\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0179 - acc: 0.4616 - val_loss: 2.1563 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0099 - acc: 0.4663 - val_loss: 2.1769 - val_acc: 0.4479\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0148 - acc: 0.4646 - val_loss: 2.1606 - val_acc: 0.4486\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0143 - acc: 0.4635 - val_loss: 2.1677 - val_acc: 0.4476\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0021 - acc: 0.4651 - val_loss: 2.1545 - val_acc: 0.4498\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9990 - acc: 0.4682 - val_loss: 2.1768 - val_acc: 0.4462\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0192 - acc: 0.4631 - val_loss: 2.1559 - val_acc: 0.4496\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0075 - acc: 0.4660 - val_loss: 2.1714 - val_acc: 0.4482\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0007 - acc: 0.4674 - val_loss: 2.1482 - val_acc: 0.4477\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9922 - acc: 0.4696 - val_loss: 2.1469 - val_acc: 0.4496\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9867 - acc: 0.4712 - val_loss: 2.1400 - val_acc: 0.4489\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9846 - acc: 0.4722 - val_loss: 2.1542 - val_acc: 0.4505\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9842 - acc: 0.4703 - val_loss: 2.1576 - val_acc: 0.4479\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9815 - acc: 0.4704 - val_loss: 2.1446 - val_acc: 0.4506\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9765 - acc: 0.4726 - val_loss: 2.1631 - val_acc: 0.4446\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9858 - acc: 0.4715 - val_loss: 2.1674 - val_acc: 0.4472\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0014 - acc: 0.4677 - val_loss: 2.1858 - val_acc: 0.4427\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9980 - acc: 0.4690 - val_loss: 2.1729 - val_acc: 0.4453\n",
      "Epoch 74/100\n",
      "1s - loss: 2.0080 - acc: 0.4663 - val_loss: 2.2022 - val_acc: 0.4411\n",
      "Epoch 75/100\n",
      "1s - loss: 2.0022 - acc: 0.4662 - val_loss: 2.1588 - val_acc: 0.4478\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9941 - acc: 0.4685 - val_loss: 2.1963 - val_acc: 0.4460\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9926 - acc: 0.4692 - val_loss: 2.1647 - val_acc: 0.4461\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9811 - acc: 0.4717 - val_loss: 2.1636 - val_acc: 0.4478\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9773 - acc: 0.4744 - val_loss: 2.1825 - val_acc: 0.4436\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9837 - acc: 0.4713 - val_loss: 2.1700 - val_acc: 0.4485\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9657 - acc: 0.4748 - val_loss: 2.1671 - val_acc: 0.4452\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9765 - acc: 0.4732 - val_loss: 2.1704 - val_acc: 0.4482\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9830 - acc: 0.4719 - val_loss: 2.1821 - val_acc: 0.4446\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9949 - acc: 0.4675 - val_loss: 2.1841 - val_acc: 0.4430\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9809 - acc: 0.4735 - val_loss: 2.1598 - val_acc: 0.4483\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9742 - acc: 0.4736 - val_loss: 2.1808 - val_acc: 0.4427\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9787 - acc: 0.4718 - val_loss: 2.1589 - val_acc: 0.4500\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9671 - acc: 0.4749 - val_loss: 2.1810 - val_acc: 0.4442\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9738 - acc: 0.4741 - val_loss: 2.1632 - val_acc: 0.4488\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9866 - acc: 0.4704 - val_loss: 2.1970 - val_acc: 0.4416\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9760 - acc: 0.4720 - val_loss: 2.1639 - val_acc: 0.4493\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9776 - acc: 0.4725 - val_loss: 2.1798 - val_acc: 0.4437\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9683 - acc: 0.4767 - val_loss: 2.1516 - val_acc: 0.4479\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9620 - acc: 0.4770 - val_loss: 2.1728 - val_acc: 0.4434\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9693 - acc: 0.4737 - val_loss: 2.1571 - val_acc: 0.4505\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9638 - acc: 0.4762 - val_loss: 2.1789 - val_acc: 0.4440\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9541 - acc: 0.4783 - val_loss: 2.1580 - val_acc: 0.4477\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9495 - acc: 0.4779 - val_loss: 2.1780 - val_acc: 0.4441\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9484 - acc: 0.4805 - val_loss: 2.1552 - val_acc: 0.4498\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9608 - acc: 0.4768 - val_loss: 2.1896 - val_acc: 0.4409\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1567 - acc: 0.4316 - val_loss: 2.2078 - val_acc: 0.4389\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1329 - acc: 0.4369 - val_loss: 2.1952 - val_acc: 0.4367\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1151 - acc: 0.4405 - val_loss: 2.2080 - val_acc: 0.4347\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1028 - acc: 0.4414 - val_loss: 2.2084 - val_acc: 0.4357\n",
      "Epoch 5/100\n",
      "1s - loss: 2.0941 - acc: 0.4456 - val_loss: 2.2254 - val_acc: 0.4310\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0935 - acc: 0.4429 - val_loss: 2.2115 - val_acc: 0.4367\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0823 - acc: 0.4468 - val_loss: 2.2082 - val_acc: 0.4343\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0705 - acc: 0.4475 - val_loss: 2.2127 - val_acc: 0.4361\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0661 - acc: 0.4479 - val_loss: 2.2328 - val_acc: 0.4324\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0644 - acc: 0.4496 - val_loss: 2.2163 - val_acc: 0.4364\n",
      "Epoch 11/100\n",
      "2s - loss: 2.0578 - acc: 0.4536 - val_loss: 2.2257 - val_acc: 0.4335\n",
      "Epoch 12/100\n",
      "2s - loss: 2.0595 - acc: 0.4530 - val_loss: 2.2183 - val_acc: 0.4354\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0502 - acc: 0.4514 - val_loss: 2.2192 - val_acc: 0.4336\n",
      "Epoch 14/100\n",
      "2s - loss: 2.0461 - acc: 0.4531 - val_loss: 2.2176 - val_acc: 0.4340\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0427 - acc: 0.4540 - val_loss: 2.2301 - val_acc: 0.4337\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0432 - acc: 0.4550 - val_loss: 2.2292 - val_acc: 0.4326\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0335 - acc: 0.4581 - val_loss: 2.2069 - val_acc: 0.4352\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0331 - acc: 0.4563 - val_loss: 2.2258 - val_acc: 0.4325\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0329 - acc: 0.4579 - val_loss: 2.2173 - val_acc: 0.4328\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0250 - acc: 0.4590 - val_loss: 2.2192 - val_acc: 0.4343\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0220 - acc: 0.4595 - val_loss: 2.2229 - val_acc: 0.4343\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0203 - acc: 0.4576 - val_loss: 2.2179 - val_acc: 0.4334\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0153 - acc: 0.4590 - val_loss: 2.2126 - val_acc: 0.4357\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0123 - acc: 0.4593 - val_loss: 2.2198 - val_acc: 0.4336\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0117 - acc: 0.4614 - val_loss: 2.2115 - val_acc: 0.4350\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0044 - acc: 0.4622 - val_loss: 2.2022 - val_acc: 0.4357\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0048 - acc: 0.4611 - val_loss: 2.2034 - val_acc: 0.4346\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0027 - acc: 0.4624 - val_loss: 2.2155 - val_acc: 0.4357\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0015 - acc: 0.4624 - val_loss: 2.1926 - val_acc: 0.4387\n",
      "Epoch 30/100\n",
      "2s - loss: 2.0038 - acc: 0.4634 - val_loss: 2.1951 - val_acc: 0.4391\n",
      "Epoch 31/100\n",
      "2s - loss: 1.9945 - acc: 0.4634 - val_loss: 2.2059 - val_acc: 0.4356\n",
      "Epoch 32/100\n",
      "2s - loss: 1.9923 - acc: 0.4658 - val_loss: 2.2133 - val_acc: 0.4347\n",
      "Epoch 33/100\n",
      "2s - loss: 1.9927 - acc: 0.4644 - val_loss: 2.1974 - val_acc: 0.4353\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9910 - acc: 0.4652 - val_loss: 2.2043 - val_acc: 0.4349\n",
      "Epoch 35/100\n",
      "2s - loss: 1.9860 - acc: 0.4672 - val_loss: 2.2024 - val_acc: 0.4396\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9886 - acc: 0.4674 - val_loss: 2.1927 - val_acc: 0.4397\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9803 - acc: 0.4680 - val_loss: 2.1833 - val_acc: 0.4397\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9779 - acc: 0.4672 - val_loss: 2.1830 - val_acc: 0.4399\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9762 - acc: 0.4699 - val_loss: 2.1866 - val_acc: 0.4375\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9771 - acc: 0.4689 - val_loss: 2.1902 - val_acc: 0.4400\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9831 - acc: 0.4671 - val_loss: 2.2115 - val_acc: 0.4324\n",
      "Epoch 42/100\n",
      "2s - loss: 1.9736 - acc: 0.4698 - val_loss: 2.2020 - val_acc: 0.4370\n",
      "Epoch 43/100\n",
      "2s - loss: 1.9707 - acc: 0.4689 - val_loss: 2.1942 - val_acc: 0.4382\n",
      "Epoch 44/100\n",
      "2s - loss: 1.9654 - acc: 0.4696 - val_loss: 2.1926 - val_acc: 0.4392\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9665 - acc: 0.4708 - val_loss: 2.2242 - val_acc: 0.4336\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9726 - acc: 0.4704 - val_loss: 2.1905 - val_acc: 0.4418\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9620 - acc: 0.4733 - val_loss: 2.1898 - val_acc: 0.4407\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9600 - acc: 0.4719 - val_loss: 2.1888 - val_acc: 0.4395\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9581 - acc: 0.4742 - val_loss: 2.1985 - val_acc: 0.4373\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9634 - acc: 0.4723 - val_loss: 2.1947 - val_acc: 0.4395\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9646 - acc: 0.4715 - val_loss: 2.2032 - val_acc: 0.4373\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9623 - acc: 0.4715 - val_loss: 2.2035 - val_acc: 0.4400\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9911 - acc: 0.4666 - val_loss: 2.2194 - val_acc: 0.4347\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9700 - acc: 0.4712 - val_loss: 2.1952 - val_acc: 0.4396\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9530 - acc: 0.4742 - val_loss: 2.1930 - val_acc: 0.4412\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9455 - acc: 0.4757 - val_loss: 2.1832 - val_acc: 0.4412\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9427 - acc: 0.4754 - val_loss: 2.1761 - val_acc: 0.4402\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9390 - acc: 0.4784 - val_loss: 2.1960 - val_acc: 0.4390\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9441 - acc: 0.4761 - val_loss: 2.1863 - val_acc: 0.4408\n",
      "Epoch 60/100\n",
      "2s - loss: 1.9406 - acc: 0.4765 - val_loss: 2.1814 - val_acc: 0.4425\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9435 - acc: 0.4763 - val_loss: 2.2038 - val_acc: 0.4420\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9484 - acc: 0.4746 - val_loss: 2.1859 - val_acc: 0.4412\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9354 - acc: 0.4787 - val_loss: 2.2003 - val_acc: 0.4403\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9348 - acc: 0.4779 - val_loss: 2.1798 - val_acc: 0.4408\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9297 - acc: 0.4781 - val_loss: 2.2016 - val_acc: 0.4385\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9344 - acc: 0.4788 - val_loss: 2.1809 - val_acc: 0.4409\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9292 - acc: 0.4791 - val_loss: 2.1901 - val_acc: 0.4385\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9261 - acc: 0.4794 - val_loss: 2.1771 - val_acc: 0.4418\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9183 - acc: 0.4827 - val_loss: 2.1820 - val_acc: 0.4410\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9203 - acc: 0.4809 - val_loss: 2.1758 - val_acc: 0.4431\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9213 - acc: 0.4807 - val_loss: 2.1823 - val_acc: 0.4401\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9212 - acc: 0.4811 - val_loss: 2.1916 - val_acc: 0.4388\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9260 - acc: 0.4802 - val_loss: 2.1945 - val_acc: 0.4387\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9334 - acc: 0.4775 - val_loss: 2.1900 - val_acc: 0.4381\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9283 - acc: 0.4781 - val_loss: 2.1955 - val_acc: 0.4396\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9288 - acc: 0.4788 - val_loss: 2.1995 - val_acc: 0.4381\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9158 - acc: 0.4823 - val_loss: 2.1891 - val_acc: 0.4399\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9128 - acc: 0.4832 - val_loss: 2.1903 - val_acc: 0.4388\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9095 - acc: 0.4833 - val_loss: 2.1886 - val_acc: 0.4385\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9044 - acc: 0.4854 - val_loss: 2.1834 - val_acc: 0.4417\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9114 - acc: 0.4838 - val_loss: 2.1936 - val_acc: 0.4382\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9146 - acc: 0.4813 - val_loss: 2.1801 - val_acc: 0.4430\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9049 - acc: 0.4835 - val_loss: 2.1843 - val_acc: 0.4403\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9033 - acc: 0.4845 - val_loss: 2.1722 - val_acc: 0.4437\n",
      "Epoch 85/100\n",
      "1s - loss: 1.8933 - acc: 0.4892 - val_loss: 2.1806 - val_acc: 0.4425\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9005 - acc: 0.4858 - val_loss: 2.1862 - val_acc: 0.4395\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9045 - acc: 0.4856 - val_loss: 2.2628 - val_acc: 0.4309\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9211 - acc: 0.4822 - val_loss: 2.1902 - val_acc: 0.4410\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9029 - acc: 0.4849 - val_loss: 2.1974 - val_acc: 0.4394\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9012 - acc: 0.4870 - val_loss: 2.1835 - val_acc: 0.4418\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8922 - acc: 0.4887 - val_loss: 2.2001 - val_acc: 0.4380\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9012 - acc: 0.4860 - val_loss: 2.1943 - val_acc: 0.4404\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8894 - acc: 0.4887 - val_loss: 2.2307 - val_acc: 0.4349\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8987 - acc: 0.4870 - val_loss: 2.1966 - val_acc: 0.4392\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9078 - acc: 0.4852 - val_loss: 2.2000 - val_acc: 0.4410\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8888 - acc: 0.4889 - val_loss: 2.1844 - val_acc: 0.4409\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8903 - acc: 0.4887 - val_loss: 2.2080 - val_acc: 0.4388\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8873 - acc: 0.4899 - val_loss: 2.1863 - val_acc: 0.4395\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9003 - acc: 0.4860 - val_loss: 2.2355 - val_acc: 0.4363\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8997 - acc: 0.4862 - val_loss: 2.1872 - val_acc: 0.4431\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1610 - acc: 0.4342 - val_loss: 2.1943 - val_acc: 0.4430\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1396 - acc: 0.4372 - val_loss: 2.2013 - val_acc: 0.4407\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1301 - acc: 0.4397 - val_loss: 2.2080 - val_acc: 0.4404\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1172 - acc: 0.4417 - val_loss: 2.2203 - val_acc: 0.4388\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1110 - acc: 0.4446 - val_loss: 2.2292 - val_acc: 0.4373\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1015 - acc: 0.4463 - val_loss: 2.2278 - val_acc: 0.4366\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0940 - acc: 0.4467 - val_loss: 2.2262 - val_acc: 0.4341\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0881 - acc: 0.4492 - val_loss: 2.2302 - val_acc: 0.4360\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0825 - acc: 0.4482 - val_loss: 2.2262 - val_acc: 0.4361\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0794 - acc: 0.4506 - val_loss: 2.2306 - val_acc: 0.4343\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0705 - acc: 0.4533 - val_loss: 2.2316 - val_acc: 0.4336\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0706 - acc: 0.4506 - val_loss: 2.2236 - val_acc: 0.4362\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0664 - acc: 0.4529 - val_loss: 2.2256 - val_acc: 0.4343\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0631 - acc: 0.4531 - val_loss: 2.2257 - val_acc: 0.4339\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0563 - acc: 0.4541 - val_loss: 2.2186 - val_acc: 0.4361\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0524 - acc: 0.4552 - val_loss: 2.2233 - val_acc: 0.4352\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0506 - acc: 0.4557 - val_loss: 2.2221 - val_acc: 0.4355\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0445 - acc: 0.4580 - val_loss: 2.2243 - val_acc: 0.4340\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0404 - acc: 0.4573 - val_loss: 2.2372 - val_acc: 0.4331\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0518 - acc: 0.4546 - val_loss: 2.2279 - val_acc: 0.4347\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0334 - acc: 0.4601 - val_loss: 2.2216 - val_acc: 0.4354\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0359 - acc: 0.4575 - val_loss: 2.2360 - val_acc: 0.4313\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0287 - acc: 0.4633 - val_loss: 2.2229 - val_acc: 0.4354\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0252 - acc: 0.4616 - val_loss: 2.2122 - val_acc: 0.4351\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0235 - acc: 0.4613 - val_loss: 2.2255 - val_acc: 0.4332\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0223 - acc: 0.4617 - val_loss: 2.2148 - val_acc: 0.4363\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0153 - acc: 0.4636 - val_loss: 2.2237 - val_acc: 0.4336\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0173 - acc: 0.4626 - val_loss: 2.2183 - val_acc: 0.4353\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0168 - acc: 0.4624 - val_loss: 2.2207 - val_acc: 0.4353\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0091 - acc: 0.4636 - val_loss: 2.2102 - val_acc: 0.4379\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0066 - acc: 0.4663 - val_loss: 2.2197 - val_acc: 0.4369\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0074 - acc: 0.4654 - val_loss: 2.2015 - val_acc: 0.4394\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0002 - acc: 0.4659 - val_loss: 2.2089 - val_acc: 0.4380\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0023 - acc: 0.4655 - val_loss: 2.2054 - val_acc: 0.4375\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0008 - acc: 0.4671 - val_loss: 2.2284 - val_acc: 0.4351\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9976 - acc: 0.4652 - val_loss: 2.2137 - val_acc: 0.4362\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0053 - acc: 0.4652 - val_loss: 2.2142 - val_acc: 0.4357\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9954 - acc: 0.4661 - val_loss: 2.1975 - val_acc: 0.4401\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9874 - acc: 0.4699 - val_loss: 2.2192 - val_acc: 0.4356\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9857 - acc: 0.4708 - val_loss: 2.2012 - val_acc: 0.4386\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9857 - acc: 0.4687 - val_loss: 2.2039 - val_acc: 0.4375\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9894 - acc: 0.4668 - val_loss: 2.2050 - val_acc: 0.4391\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9887 - acc: 0.4703 - val_loss: 2.2185 - val_acc: 0.4369\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9902 - acc: 0.4706 - val_loss: 2.2012 - val_acc: 0.4377\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9739 - acc: 0.4735 - val_loss: 2.2068 - val_acc: 0.4376\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9740 - acc: 0.4718 - val_loss: 2.1986 - val_acc: 0.4398\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9749 - acc: 0.4731 - val_loss: 2.2106 - val_acc: 0.4370\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9726 - acc: 0.4713 - val_loss: 2.2028 - val_acc: 0.4387\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9687 - acc: 0.4735 - val_loss: 2.2239 - val_acc: 0.4332\n",
      "Epoch 50/100\n",
      "2s - loss: 1.9741 - acc: 0.4725 - val_loss: 2.2079 - val_acc: 0.4368\n",
      "Epoch 51/100\n",
      "2s - loss: 1.9730 - acc: 0.4723 - val_loss: 2.2276 - val_acc: 0.4364\n",
      "Epoch 52/100\n",
      "2s - loss: 1.9738 - acc: 0.4715 - val_loss: 2.2133 - val_acc: 0.4380\n",
      "Epoch 53/100\n",
      "2s - loss: 1.9724 - acc: 0.4724 - val_loss: 2.2409 - val_acc: 0.4302\n",
      "Epoch 54/100\n",
      "2s - loss: 1.9746 - acc: 0.4733 - val_loss: 2.2090 - val_acc: 0.4412\n",
      "Epoch 55/100\n",
      "2s - loss: 1.9571 - acc: 0.4754 - val_loss: 2.1996 - val_acc: 0.4383\n",
      "Epoch 56/100\n",
      "2s - loss: 1.9579 - acc: 0.4768 - val_loss: 2.2027 - val_acc: 0.4406\n",
      "Epoch 57/100\n",
      "2s - loss: 1.9526 - acc: 0.4763 - val_loss: 2.2099 - val_acc: 0.4380\n",
      "Epoch 58/100\n",
      "2s - loss: 1.9496 - acc: 0.4777 - val_loss: 2.2158 - val_acc: 0.4357\n",
      "Epoch 59/100\n",
      "2s - loss: 1.9505 - acc: 0.4794 - val_loss: 2.2056 - val_acc: 0.4384\n",
      "Epoch 60/100\n",
      "2s - loss: 1.9507 - acc: 0.4773 - val_loss: 2.2044 - val_acc: 0.4350\n",
      "Epoch 61/100\n",
      "2s - loss: 1.9504 - acc: 0.4771 - val_loss: 2.2101 - val_acc: 0.4389\n",
      "Epoch 62/100\n",
      "2s - loss: 1.9480 - acc: 0.4782 - val_loss: 2.2115 - val_acc: 0.4353\n",
      "Epoch 63/100\n",
      "2s - loss: 1.9486 - acc: 0.4789 - val_loss: 2.1892 - val_acc: 0.4415\n",
      "Epoch 64/100\n",
      "2s - loss: 1.9382 - acc: 0.4810 - val_loss: 2.2063 - val_acc: 0.4370\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9389 - acc: 0.4813 - val_loss: 2.2095 - val_acc: 0.4395\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9391 - acc: 0.4813 - val_loss: 2.2044 - val_acc: 0.4380\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9411 - acc: 0.4779 - val_loss: 2.1912 - val_acc: 0.4402\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9372 - acc: 0.4807 - val_loss: 2.1966 - val_acc: 0.4395\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9281 - acc: 0.4823 - val_loss: 2.2133 - val_acc: 0.4381\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9437 - acc: 0.4798 - val_loss: 2.2141 - val_acc: 0.4381\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9369 - acc: 0.4797 - val_loss: 2.2292 - val_acc: 0.4377\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9623 - acc: 0.4769 - val_loss: 2.2145 - val_acc: 0.4336\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9415 - acc: 0.4800 - val_loss: 2.2089 - val_acc: 0.4380\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9329 - acc: 0.4825 - val_loss: 2.1932 - val_acc: 0.4396\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9318 - acc: 0.4838 - val_loss: 2.1966 - val_acc: 0.4409\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9321 - acc: 0.4814 - val_loss: 2.1947 - val_acc: 0.4410\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9274 - acc: 0.4850 - val_loss: 2.2129 - val_acc: 0.4373\n",
      "Epoch 78/100\n",
      "2s - loss: 1.9203 - acc: 0.4858 - val_loss: 2.2017 - val_acc: 0.4388\n",
      "Epoch 79/100\n",
      "2s - loss: 1.9236 - acc: 0.4842 - val_loss: 2.2272 - val_acc: 0.4366\n",
      "Epoch 80/100\n",
      "2s - loss: 1.9216 - acc: 0.4850 - val_loss: 2.1971 - val_acc: 0.4385\n",
      "Epoch 81/100\n",
      "2s - loss: 1.9148 - acc: 0.4865 - val_loss: 2.2174 - val_acc: 0.4351\n",
      "Epoch 82/100\n",
      "2s - loss: 1.9251 - acc: 0.4827 - val_loss: 2.2044 - val_acc: 0.4394\n",
      "Epoch 83/100\n",
      "2s - loss: 1.9129 - acc: 0.4879 - val_loss: 2.2062 - val_acc: 0.4372\n",
      "Epoch 84/100\n",
      "2s - loss: 1.9089 - acc: 0.4877 - val_loss: 2.2056 - val_acc: 0.4391\n",
      "Epoch 85/100\n",
      "2s - loss: 1.9134 - acc: 0.4874 - val_loss: 2.2223 - val_acc: 0.4342\n",
      "Epoch 86/100\n",
      "2s - loss: 1.9094 - acc: 0.4888 - val_loss: 2.2034 - val_acc: 0.4377\n",
      "Epoch 87/100\n",
      "2s - loss: 1.9103 - acc: 0.4869 - val_loss: 2.2070 - val_acc: 0.4389\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9090 - acc: 0.4874 - val_loss: 2.1991 - val_acc: 0.4362\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9197 - acc: 0.4849 - val_loss: 2.2408 - val_acc: 0.4358\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9296 - acc: 0.4841 - val_loss: 2.2054 - val_acc: 0.4349\n",
      "Epoch 91/100\n",
      "2s - loss: 1.9147 - acc: 0.4872 - val_loss: 2.2465 - val_acc: 0.4380\n",
      "Epoch 92/100\n",
      "2s - loss: 1.9323 - acc: 0.4808 - val_loss: 2.2121 - val_acc: 0.4379\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9076 - acc: 0.4892 - val_loss: 2.2215 - val_acc: 0.4365\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9149 - acc: 0.4868 - val_loss: 2.2160 - val_acc: 0.4371\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9026 - acc: 0.4895 - val_loss: 2.2028 - val_acc: 0.4416\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8967 - acc: 0.4911 - val_loss: 2.2265 - val_acc: 0.4369\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9016 - acc: 0.4897 - val_loss: 2.1920 - val_acc: 0.4409\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8895 - acc: 0.4931 - val_loss: 2.2169 - val_acc: 0.4390\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8921 - acc: 0.4917 - val_loss: 2.2088 - val_acc: 0.4400\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8961 - acc: 0.4907 - val_loss: 2.2252 - val_acc: 0.4366\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1828 - acc: 0.4292 - val_loss: 2.1958 - val_acc: 0.4373\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1602 - acc: 0.4318 - val_loss: 2.2078 - val_acc: 0.4365\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1468 - acc: 0.4363 - val_loss: 2.1998 - val_acc: 0.4369\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1339 - acc: 0.4375 - val_loss: 2.1981 - val_acc: 0.4350\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1268 - acc: 0.4375 - val_loss: 2.2118 - val_acc: 0.4349\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1181 - acc: 0.4404 - val_loss: 2.2167 - val_acc: 0.4350\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1135 - acc: 0.4442 - val_loss: 2.2085 - val_acc: 0.4356\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1119 - acc: 0.4420 - val_loss: 2.2125 - val_acc: 0.4353\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1014 - acc: 0.4456 - val_loss: 2.2069 - val_acc: 0.4363\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0968 - acc: 0.4462 - val_loss: 2.2116 - val_acc: 0.4360\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0954 - acc: 0.4453 - val_loss: 2.2104 - val_acc: 0.4336\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0855 - acc: 0.4464 - val_loss: 2.2155 - val_acc: 0.4363\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0847 - acc: 0.4491 - val_loss: 2.2020 - val_acc: 0.4382\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0779 - acc: 0.4489 - val_loss: 2.2962 - val_acc: 0.4291\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1365 - acc: 0.4393 - val_loss: 2.2570 - val_acc: 0.4263\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1202 - acc: 0.4409 - val_loss: 2.2939 - val_acc: 0.4249\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0989 - acc: 0.4447 - val_loss: 2.2274 - val_acc: 0.4325\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0735 - acc: 0.4497 - val_loss: 2.2091 - val_acc: 0.4354\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0620 - acc: 0.4534 - val_loss: 2.2065 - val_acc: 0.4369\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0598 - acc: 0.4548 - val_loss: 2.2228 - val_acc: 0.4358\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0517 - acc: 0.4544 - val_loss: 2.2023 - val_acc: 0.4356\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0508 - acc: 0.4548 - val_loss: 2.1990 - val_acc: 0.4364\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0501 - acc: 0.4548 - val_loss: 2.2148 - val_acc: 0.4334\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0435 - acc: 0.4570 - val_loss: 2.1940 - val_acc: 0.4368\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0442 - acc: 0.4562 - val_loss: 2.1918 - val_acc: 0.4380\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0364 - acc: 0.4564 - val_loss: 2.1991 - val_acc: 0.4375\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0359 - acc: 0.4570 - val_loss: 2.1871 - val_acc: 0.4382\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0333 - acc: 0.4571 - val_loss: 2.1956 - val_acc: 0.4367\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0323 - acc: 0.4579 - val_loss: 2.1938 - val_acc: 0.4385\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0312 - acc: 0.4595 - val_loss: 2.1936 - val_acc: 0.4369\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0306 - acc: 0.4592 - val_loss: 2.1940 - val_acc: 0.4381\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0261 - acc: 0.4593 - val_loss: 2.1902 - val_acc: 0.4374\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0191 - acc: 0.4612 - val_loss: 2.1995 - val_acc: 0.4364\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0184 - acc: 0.4594 - val_loss: 2.1912 - val_acc: 0.4358\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0173 - acc: 0.4599 - val_loss: 2.1959 - val_acc: 0.4380\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0159 - acc: 0.4619 - val_loss: 2.1899 - val_acc: 0.4389\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0177 - acc: 0.4605 - val_loss: 2.2004 - val_acc: 0.4344\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0165 - acc: 0.4615 - val_loss: 2.1895 - val_acc: 0.4372\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0045 - acc: 0.4638 - val_loss: 2.1874 - val_acc: 0.4378\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0026 - acc: 0.4644 - val_loss: 2.1875 - val_acc: 0.4384\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0102 - acc: 0.4603 - val_loss: 2.2227 - val_acc: 0.4362\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0160 - acc: 0.4623 - val_loss: 2.1897 - val_acc: 0.4386\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9991 - acc: 0.4634 - val_loss: 2.1979 - val_acc: 0.4386\n",
      "Epoch 44/100\n",
      "2s - loss: 2.0027 - acc: 0.4649 - val_loss: 2.1911 - val_acc: 0.4362\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9980 - acc: 0.4670 - val_loss: 2.1870 - val_acc: 0.4390\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9946 - acc: 0.4666 - val_loss: 2.2000 - val_acc: 0.4360\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9921 - acc: 0.4687 - val_loss: 2.1916 - val_acc: 0.4383\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9956 - acc: 0.4663 - val_loss: 2.1836 - val_acc: 0.4381\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9868 - acc: 0.4692 - val_loss: 2.1904 - val_acc: 0.4388\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9858 - acc: 0.4704 - val_loss: 2.1814 - val_acc: 0.4361\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9825 - acc: 0.4697 - val_loss: 2.1876 - val_acc: 0.4371\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9851 - acc: 0.4673 - val_loss: 2.1946 - val_acc: 0.4341\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9948 - acc: 0.4658 - val_loss: 2.1985 - val_acc: 0.4373\n",
      "Epoch 54/100\n",
      "2s - loss: 1.9871 - acc: 0.4692 - val_loss: 2.1936 - val_acc: 0.4345\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9820 - acc: 0.4680 - val_loss: 2.1890 - val_acc: 0.4383\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9740 - acc: 0.4718 - val_loss: 2.1853 - val_acc: 0.4364\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9814 - acc: 0.4695 - val_loss: 2.1939 - val_acc: 0.4383\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9760 - acc: 0.4705 - val_loss: 2.1797 - val_acc: 0.4384\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9683 - acc: 0.4720 - val_loss: 2.1988 - val_acc: 0.4382\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9670 - acc: 0.4731 - val_loss: 2.2098 - val_acc: 0.4337\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9792 - acc: 0.4703 - val_loss: 2.2131 - val_acc: 0.4374\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9698 - acc: 0.4722 - val_loss: 2.2195 - val_acc: 0.4329\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9787 - acc: 0.4708 - val_loss: 2.2021 - val_acc: 0.4389\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9646 - acc: 0.4718 - val_loss: 2.2078 - val_acc: 0.4370\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9634 - acc: 0.4728 - val_loss: 2.1896 - val_acc: 0.4374\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9571 - acc: 0.4752 - val_loss: 2.1855 - val_acc: 0.4384\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9602 - acc: 0.4744 - val_loss: 2.2041 - val_acc: 0.4369\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9634 - acc: 0.4720 - val_loss: 2.2140 - val_acc: 0.4353\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9591 - acc: 0.4741 - val_loss: 2.1934 - val_acc: 0.4393\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9577 - acc: 0.4782 - val_loss: 2.2000 - val_acc: 0.4361\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9558 - acc: 0.4749 - val_loss: 2.1800 - val_acc: 0.4390\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9512 - acc: 0.4765 - val_loss: 2.1880 - val_acc: 0.4374\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9502 - acc: 0.4762 - val_loss: 2.1826 - val_acc: 0.4374\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9450 - acc: 0.4771 - val_loss: 2.1848 - val_acc: 0.4379\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9472 - acc: 0.4764 - val_loss: 2.1777 - val_acc: 0.4387\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9427 - acc: 0.4808 - val_loss: 2.1985 - val_acc: 0.4383\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9468 - acc: 0.4793 - val_loss: 2.1899 - val_acc: 0.4400\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9436 - acc: 0.4774 - val_loss: 2.1876 - val_acc: 0.4398\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9447 - acc: 0.4787 - val_loss: 2.1776 - val_acc: 0.4404\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9351 - acc: 0.4806 - val_loss: 2.1829 - val_acc: 0.4414\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9338 - acc: 0.4787 - val_loss: 2.1945 - val_acc: 0.4383\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9349 - acc: 0.4799 - val_loss: 2.1949 - val_acc: 0.4379\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9310 - acc: 0.4802 - val_loss: 2.1883 - val_acc: 0.4402\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9317 - acc: 0.4824 - val_loss: 2.1957 - val_acc: 0.4356\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9315 - acc: 0.4817 - val_loss: 2.2053 - val_acc: 0.4388\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9449 - acc: 0.4777 - val_loss: 2.2255 - val_acc: 0.4317\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9421 - acc: 0.4774 - val_loss: 2.2024 - val_acc: 0.4383\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9304 - acc: 0.4798 - val_loss: 2.2031 - val_acc: 0.4359\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9316 - acc: 0.4810 - val_loss: 2.1948 - val_acc: 0.4388\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9270 - acc: 0.4832 - val_loss: 2.1914 - val_acc: 0.4366\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9285 - acc: 0.4828 - val_loss: 2.1978 - val_acc: 0.4395\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9322 - acc: 0.4809 - val_loss: 2.2150 - val_acc: 0.4339\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9322 - acc: 0.4800 - val_loss: 2.2094 - val_acc: 0.4381\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9364 - acc: 0.4782 - val_loss: 2.1878 - val_acc: 0.4388\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9201 - acc: 0.4822 - val_loss: 2.1965 - val_acc: 0.4389\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9154 - acc: 0.4845 - val_loss: 2.1916 - val_acc: 0.4379\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9214 - acc: 0.4815 - val_loss: 2.2058 - val_acc: 0.4393\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9297 - acc: 0.4818 - val_loss: 2.1961 - val_acc: 0.4343\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9219 - acc: 0.4839 - val_loss: 2.2063 - val_acc: 0.4390\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9255 - acc: 0.4815 - val_loss: 2.2080 - val_acc: 0.4353\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1454 - acc: 0.4342 - val_loss: 2.2054 - val_acc: 0.4431\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1228 - acc: 0.4411 - val_loss: 2.2163 - val_acc: 0.4426\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1198 - acc: 0.4416 - val_loss: 2.2195 - val_acc: 0.4418\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1016 - acc: 0.4448 - val_loss: 2.2201 - val_acc: 0.4403\n",
      "Epoch 5/100\n",
      "1s - loss: 2.0913 - acc: 0.4469 - val_loss: 2.2395 - val_acc: 0.4381\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0853 - acc: 0.4480 - val_loss: 2.2274 - val_acc: 0.4380\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0810 - acc: 0.4482 - val_loss: 2.2234 - val_acc: 0.4410\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0733 - acc: 0.4503 - val_loss: 2.2252 - val_acc: 0.4397\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0649 - acc: 0.4511 - val_loss: 2.2333 - val_acc: 0.4359\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0616 - acc: 0.4507 - val_loss: 2.2420 - val_acc: 0.4353\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0595 - acc: 0.4523 - val_loss: 2.2329 - val_acc: 0.4372\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0535 - acc: 0.4518 - val_loss: 2.2416 - val_acc: 0.4366\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0490 - acc: 0.4530 - val_loss: 2.2412 - val_acc: 0.4350\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0464 - acc: 0.4546 - val_loss: 2.2581 - val_acc: 0.4349\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0504 - acc: 0.4544 - val_loss: 2.2340 - val_acc: 0.4385\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0382 - acc: 0.4571 - val_loss: 2.2359 - val_acc: 0.4379\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0319 - acc: 0.4568 - val_loss: 2.2303 - val_acc: 0.4393\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0296 - acc: 0.4576 - val_loss: 2.2320 - val_acc: 0.4372\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0265 - acc: 0.4600 - val_loss: 2.2182 - val_acc: 0.4414\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0252 - acc: 0.4579 - val_loss: 2.2275 - val_acc: 0.4390\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0210 - acc: 0.4593 - val_loss: 2.2231 - val_acc: 0.4397\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0170 - acc: 0.4618 - val_loss: 2.2396 - val_acc: 0.4391\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0223 - acc: 0.4625 - val_loss: 2.2251 - val_acc: 0.4404\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0148 - acc: 0.4620 - val_loss: 2.2305 - val_acc: 0.4387\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0100 - acc: 0.4642 - val_loss: 2.2270 - val_acc: 0.4385\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0046 - acc: 0.4638 - val_loss: 2.2139 - val_acc: 0.4417\n",
      "Epoch 27/100\n",
      "1s - loss: 1.9999 - acc: 0.4657 - val_loss: 2.2250 - val_acc: 0.4413\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0006 - acc: 0.4656 - val_loss: 2.2205 - val_acc: 0.4404\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0035 - acc: 0.4639 - val_loss: 2.2403 - val_acc: 0.4358\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0005 - acc: 0.4643 - val_loss: 2.2120 - val_acc: 0.4435\n",
      "Epoch 31/100\n",
      "1s - loss: 1.9934 - acc: 0.4674 - val_loss: 2.2250 - val_acc: 0.4415\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0003 - acc: 0.4631 - val_loss: 2.2154 - val_acc: 0.4425\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9900 - acc: 0.4665 - val_loss: 2.2339 - val_acc: 0.4397\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9891 - acc: 0.4674 - val_loss: 2.2248 - val_acc: 0.4422\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9971 - acc: 0.4668 - val_loss: 2.2131 - val_acc: 0.4406\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9843 - acc: 0.4684 - val_loss: 2.2324 - val_acc: 0.4435\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9973 - acc: 0.4637 - val_loss: 2.2177 - val_acc: 0.4399\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9886 - acc: 0.4666 - val_loss: 2.2177 - val_acc: 0.4435\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9792 - acc: 0.4696 - val_loss: 2.2247 - val_acc: 0.4383\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9750 - acc: 0.4691 - val_loss: 2.2000 - val_acc: 0.4441\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9727 - acc: 0.4710 - val_loss: 2.2388 - val_acc: 0.4362\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9713 - acc: 0.4698 - val_loss: 2.2292 - val_acc: 0.4425\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9870 - acc: 0.4656 - val_loss: 2.2299 - val_acc: 0.4390\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9690 - acc: 0.4717 - val_loss: 2.2035 - val_acc: 0.4455\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9646 - acc: 0.4717 - val_loss: 2.2366 - val_acc: 0.4367\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9739 - acc: 0.4701 - val_loss: 2.2192 - val_acc: 0.4434\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9689 - acc: 0.4721 - val_loss: 2.2320 - val_acc: 0.4383\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9644 - acc: 0.4704 - val_loss: 2.2233 - val_acc: 0.4441\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9650 - acc: 0.4724 - val_loss: 2.2205 - val_acc: 0.4406\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9531 - acc: 0.4747 - val_loss: 2.2164 - val_acc: 0.4435\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9520 - acc: 0.4748 - val_loss: 2.2100 - val_acc: 0.4431\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9512 - acc: 0.4761 - val_loss: 2.2148 - val_acc: 0.4437\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9451 - acc: 0.4784 - val_loss: 2.2042 - val_acc: 0.4425\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9468 - acc: 0.4770 - val_loss: 2.2101 - val_acc: 0.4428\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9430 - acc: 0.4762 - val_loss: 2.2070 - val_acc: 0.4450\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9425 - acc: 0.4762 - val_loss: 2.2126 - val_acc: 0.4466\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9445 - acc: 0.4775 - val_loss: 2.2128 - val_acc: 0.4422\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9438 - acc: 0.4768 - val_loss: 2.2122 - val_acc: 0.4422\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9523 - acc: 0.4769 - val_loss: 2.2388 - val_acc: 0.4377\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9436 - acc: 0.4766 - val_loss: 2.2209 - val_acc: 0.4454\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9419 - acc: 0.4771 - val_loss: 2.2195 - val_acc: 0.4406\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9404 - acc: 0.4785 - val_loss: 2.2025 - val_acc: 0.4460\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9314 - acc: 0.4781 - val_loss: 2.2188 - val_acc: 0.4420\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9304 - acc: 0.4817 - val_loss: 2.2086 - val_acc: 0.4443\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9283 - acc: 0.4818 - val_loss: 2.2231 - val_acc: 0.4396\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9320 - acc: 0.4812 - val_loss: 2.2296 - val_acc: 0.4466\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9382 - acc: 0.4780 - val_loss: 2.2231 - val_acc: 0.4420\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9312 - acc: 0.4809 - val_loss: 2.2190 - val_acc: 0.4439\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9322 - acc: 0.4785 - val_loss: 2.2140 - val_acc: 0.4432\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9282 - acc: 0.4804 - val_loss: 2.2250 - val_acc: 0.4446\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9322 - acc: 0.4798 - val_loss: 2.2219 - val_acc: 0.4407\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9229 - acc: 0.4831 - val_loss: 2.2189 - val_acc: 0.4454\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9240 - acc: 0.4799 - val_loss: 2.2187 - val_acc: 0.4433\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9233 - acc: 0.4812 - val_loss: 2.2073 - val_acc: 0.4469\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9115 - acc: 0.4841 - val_loss: 2.2247 - val_acc: 0.4429\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9128 - acc: 0.4842 - val_loss: 2.2092 - val_acc: 0.4455\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9103 - acc: 0.4852 - val_loss: 2.2170 - val_acc: 0.4415\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9117 - acc: 0.4830 - val_loss: 2.2032 - val_acc: 0.4448\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9118 - acc: 0.4840 - val_loss: 2.2148 - val_acc: 0.4387\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9078 - acc: 0.4863 - val_loss: 2.2099 - val_acc: 0.4467\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9086 - acc: 0.4856 - val_loss: 2.2250 - val_acc: 0.4390\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9093 - acc: 0.4871 - val_loss: 2.2149 - val_acc: 0.4446\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9221 - acc: 0.4805 - val_loss: 2.2311 - val_acc: 0.4403\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9091 - acc: 0.4876 - val_loss: 2.2101 - val_acc: 0.4442\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9009 - acc: 0.4865 - val_loss: 2.2104 - val_acc: 0.4429\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9011 - acc: 0.4864 - val_loss: 2.2475 - val_acc: 0.4412\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9227 - acc: 0.4834 - val_loss: 2.2382 - val_acc: 0.4385\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9089 - acc: 0.4863 - val_loss: 2.2203 - val_acc: 0.4433\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9035 - acc: 0.4865 - val_loss: 2.2459 - val_acc: 0.4336\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9025 - acc: 0.4853 - val_loss: 2.2187 - val_acc: 0.4462\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8943 - acc: 0.4873 - val_loss: 2.2346 - val_acc: 0.4355\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8929 - acc: 0.4898 - val_loss: 2.2033 - val_acc: 0.4451\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8884 - acc: 0.4889 - val_loss: 2.2275 - val_acc: 0.4400\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8889 - acc: 0.4899 - val_loss: 2.2166 - val_acc: 0.4434\n",
      "Epoch 95/100\n",
      "1s - loss: 1.8880 - acc: 0.4894 - val_loss: 2.2364 - val_acc: 0.4409\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9041 - acc: 0.4878 - val_loss: 2.2189 - val_acc: 0.4419\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8873 - acc: 0.4925 - val_loss: 2.2144 - val_acc: 0.4433\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8883 - acc: 0.4895 - val_loss: 2.2013 - val_acc: 0.4432\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8860 - acc: 0.4923 - val_loss: 2.2382 - val_acc: 0.4412\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8887 - acc: 0.4885 - val_loss: 2.2238 - val_acc: 0.4400\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1582 - acc: 0.4340 - val_loss: 2.1639 - val_acc: 0.4492\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1320 - acc: 0.4392 - val_loss: 2.1594 - val_acc: 0.4496\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1160 - acc: 0.4417 - val_loss: 2.1687 - val_acc: 0.4462\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1088 - acc: 0.4432 - val_loss: 2.1867 - val_acc: 0.4425\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1002 - acc: 0.4462 - val_loss: 2.1796 - val_acc: 0.4441\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0930 - acc: 0.4462 - val_loss: 2.1991 - val_acc: 0.4410\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0991 - acc: 0.4464 - val_loss: 2.1815 - val_acc: 0.4460\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0808 - acc: 0.4513 - val_loss: 2.2047 - val_acc: 0.4412\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0760 - acc: 0.4527 - val_loss: 2.1917 - val_acc: 0.4465\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0706 - acc: 0.4505 - val_loss: 2.1908 - val_acc: 0.4426\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0676 - acc: 0.4519 - val_loss: 2.1890 - val_acc: 0.4421\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0608 - acc: 0.4543 - val_loss: 2.1908 - val_acc: 0.4415\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0553 - acc: 0.4563 - val_loss: 2.1721 - val_acc: 0.4460\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0516 - acc: 0.4549 - val_loss: 2.1964 - val_acc: 0.4423\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0506 - acc: 0.4559 - val_loss: 2.1847 - val_acc: 0.4433\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0467 - acc: 0.4558 - val_loss: 2.1830 - val_acc: 0.4455\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0416 - acc: 0.4595 - val_loss: 2.1864 - val_acc: 0.4446\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0388 - acc: 0.4594 - val_loss: 2.1813 - val_acc: 0.4466\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0346 - acc: 0.4586 - val_loss: 2.1870 - val_acc: 0.4432\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0299 - acc: 0.4606 - val_loss: 2.1871 - val_acc: 0.4454\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0288 - acc: 0.4604 - val_loss: 2.1956 - val_acc: 0.4419\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0292 - acc: 0.4596 - val_loss: 2.1887 - val_acc: 0.4436\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0215 - acc: 0.4617 - val_loss: 2.1838 - val_acc: 0.4436\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0183 - acc: 0.4640 - val_loss: 2.1925 - val_acc: 0.4407\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0217 - acc: 0.4620 - val_loss: 2.1748 - val_acc: 0.4448\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0142 - acc: 0.4624 - val_loss: 2.1781 - val_acc: 0.4448\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0161 - acc: 0.4647 - val_loss: 2.1803 - val_acc: 0.4447\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0087 - acc: 0.4655 - val_loss: 2.1762 - val_acc: 0.4443\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0027 - acc: 0.4651 - val_loss: 2.1672 - val_acc: 0.4476\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0012 - acc: 0.4671 - val_loss: 2.1774 - val_acc: 0.4433\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0023 - acc: 0.4659 - val_loss: 2.2120 - val_acc: 0.4402\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0065 - acc: 0.4632 - val_loss: 2.1841 - val_acc: 0.4452\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0031 - acc: 0.4663 - val_loss: 2.1829 - val_acc: 0.4427\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9954 - acc: 0.4668 - val_loss: 2.1864 - val_acc: 0.4411\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9947 - acc: 0.4678 - val_loss: 2.1705 - val_acc: 0.4461\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9927 - acc: 0.4683 - val_loss: 2.1874 - val_acc: 0.4445\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9915 - acc: 0.4659 - val_loss: 2.1722 - val_acc: 0.4448\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9879 - acc: 0.4698 - val_loss: 2.2002 - val_acc: 0.4428\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9869 - acc: 0.4692 - val_loss: 2.1761 - val_acc: 0.4446\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9831 - acc: 0.4714 - val_loss: 2.2036 - val_acc: 0.4396\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9849 - acc: 0.4702 - val_loss: 2.1740 - val_acc: 0.4440\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9776 - acc: 0.4719 - val_loss: 2.1734 - val_acc: 0.4462\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9745 - acc: 0.4738 - val_loss: 2.1963 - val_acc: 0.4426\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9718 - acc: 0.4728 - val_loss: 2.1782 - val_acc: 0.4468\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9782 - acc: 0.4688 - val_loss: 2.1723 - val_acc: 0.4470\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9745 - acc: 0.4732 - val_loss: 2.1789 - val_acc: 0.4465\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9711 - acc: 0.4737 - val_loss: 2.1916 - val_acc: 0.4441\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9824 - acc: 0.4725 - val_loss: 2.1661 - val_acc: 0.4455\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9722 - acc: 0.4741 - val_loss: 2.1863 - val_acc: 0.4443\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9691 - acc: 0.4731 - val_loss: 2.1699 - val_acc: 0.4449\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9654 - acc: 0.4735 - val_loss: 2.1944 - val_acc: 0.4447\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9560 - acc: 0.4773 - val_loss: 2.1958 - val_acc: 0.4413\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9655 - acc: 0.4753 - val_loss: 2.1734 - val_acc: 0.4461\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9537 - acc: 0.4769 - val_loss: 2.1743 - val_acc: 0.4452\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9578 - acc: 0.4770 - val_loss: 2.1955 - val_acc: 0.4449\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9738 - acc: 0.4728 - val_loss: 2.1992 - val_acc: 0.4416\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9553 - acc: 0.4759 - val_loss: 2.1740 - val_acc: 0.4457\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9452 - acc: 0.4780 - val_loss: 2.1827 - val_acc: 0.4443\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9478 - acc: 0.4801 - val_loss: 2.1700 - val_acc: 0.4444\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9452 - acc: 0.4804 - val_loss: 2.2070 - val_acc: 0.4419\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9449 - acc: 0.4783 - val_loss: 2.1638 - val_acc: 0.4466\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9391 - acc: 0.4798 - val_loss: 2.1852 - val_acc: 0.4453\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9355 - acc: 0.4823 - val_loss: 2.1754 - val_acc: 0.4478\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9348 - acc: 0.4833 - val_loss: 2.1652 - val_acc: 0.4468\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9334 - acc: 0.4814 - val_loss: 2.1667 - val_acc: 0.4485\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9337 - acc: 0.4825 - val_loss: 2.1731 - val_acc: 0.4450\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9426 - acc: 0.4793 - val_loss: 2.1799 - val_acc: 0.4501\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9337 - acc: 0.4820 - val_loss: 2.1723 - val_acc: 0.4466\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9346 - acc: 0.4801 - val_loss: 2.2043 - val_acc: 0.4451\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9434 - acc: 0.4807 - val_loss: 2.1698 - val_acc: 0.4483\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9322 - acc: 0.4836 - val_loss: 2.2041 - val_acc: 0.4421\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9384 - acc: 0.4814 - val_loss: 2.1777 - val_acc: 0.4454\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9231 - acc: 0.4841 - val_loss: 2.1976 - val_acc: 0.4432\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9352 - acc: 0.4828 - val_loss: 2.1794 - val_acc: 0.4467\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9239 - acc: 0.4838 - val_loss: 2.1804 - val_acc: 0.4436\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9153 - acc: 0.4849 - val_loss: 2.1747 - val_acc: 0.4456\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9296 - acc: 0.4826 - val_loss: 2.1892 - val_acc: 0.4438\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9294 - acc: 0.4836 - val_loss: 2.1804 - val_acc: 0.4482\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9273 - acc: 0.4830 - val_loss: 2.1843 - val_acc: 0.4468\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9180 - acc: 0.4861 - val_loss: 2.1663 - val_acc: 0.4481\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9126 - acc: 0.4864 - val_loss: 2.1828 - val_acc: 0.4488\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9319 - acc: 0.4827 - val_loss: 2.1706 - val_acc: 0.4463\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9118 - acc: 0.4869 - val_loss: 2.1946 - val_acc: 0.4468\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9347 - acc: 0.4822 - val_loss: 2.1782 - val_acc: 0.4483\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9220 - acc: 0.4853 - val_loss: 2.2135 - val_acc: 0.4420\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9251 - acc: 0.4848 - val_loss: 2.1784 - val_acc: 0.4457\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9149 - acc: 0.4842 - val_loss: 2.2216 - val_acc: 0.4429\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9336 - acc: 0.4837 - val_loss: 2.1744 - val_acc: 0.4465\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9017 - acc: 0.4904 - val_loss: 2.1619 - val_acc: 0.4503\n",
      "Epoch 90/100\n",
      "1s - loss: 1.8956 - acc: 0.4899 - val_loss: 2.1876 - val_acc: 0.4474\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9067 - acc: 0.4908 - val_loss: 2.1695 - val_acc: 0.4482\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8953 - acc: 0.4915 - val_loss: 2.1896 - val_acc: 0.4463\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8973 - acc: 0.4894 - val_loss: 2.1765 - val_acc: 0.4440\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9022 - acc: 0.4908 - val_loss: 2.2183 - val_acc: 0.4393\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9079 - acc: 0.4891 - val_loss: 2.1780 - val_acc: 0.4469\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8929 - acc: 0.4900 - val_loss: 2.2032 - val_acc: 0.4443\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8998 - acc: 0.4889 - val_loss: 2.1750 - val_acc: 0.4477\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8999 - acc: 0.4898 - val_loss: 2.1937 - val_acc: 0.4431\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8913 - acc: 0.4919 - val_loss: 2.1867 - val_acc: 0.4452\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9081 - acc: 0.4863 - val_loss: 2.1907 - val_acc: 0.4469\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1648 - acc: 0.4326 - val_loss: 2.2405 - val_acc: 0.4339\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1407 - acc: 0.4357 - val_loss: 2.2487 - val_acc: 0.4326\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1285 - acc: 0.4395 - val_loss: 2.2515 - val_acc: 0.4343\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1201 - acc: 0.4400 - val_loss: 2.2527 - val_acc: 0.4321\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1131 - acc: 0.4417 - val_loss: 2.2583 - val_acc: 0.4326\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1061 - acc: 0.4444 - val_loss: 2.2540 - val_acc: 0.4322\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1027 - acc: 0.4441 - val_loss: 2.2574 - val_acc: 0.4324\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0899 - acc: 0.4467 - val_loss: 2.2608 - val_acc: 0.4313\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0887 - acc: 0.4485 - val_loss: 2.2671 - val_acc: 0.4279\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0800 - acc: 0.4491 - val_loss: 2.2596 - val_acc: 0.4302\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0797 - acc: 0.4493 - val_loss: 2.2797 - val_acc: 0.4266\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0769 - acc: 0.4516 - val_loss: 2.2637 - val_acc: 0.4305\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0677 - acc: 0.4512 - val_loss: 2.2640 - val_acc: 0.4292\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0648 - acc: 0.4524 - val_loss: 2.2576 - val_acc: 0.4323\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0596 - acc: 0.4544 - val_loss: 2.2791 - val_acc: 0.4278\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0567 - acc: 0.4565 - val_loss: 2.2877 - val_acc: 0.4232\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0571 - acc: 0.4552 - val_loss: 2.2757 - val_acc: 0.4281\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0512 - acc: 0.4562 - val_loss: 2.2801 - val_acc: 0.4302\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0478 - acc: 0.4560 - val_loss: 2.2659 - val_acc: 0.4295\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0431 - acc: 0.4568 - val_loss: 2.2668 - val_acc: 0.4306\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0446 - acc: 0.4566 - val_loss: 2.2724 - val_acc: 0.4289\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0376 - acc: 0.4589 - val_loss: 2.2716 - val_acc: 0.4293\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0341 - acc: 0.4595 - val_loss: 2.2770 - val_acc: 0.4279\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0318 - acc: 0.4591 - val_loss: 2.2667 - val_acc: 0.4290\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0273 - acc: 0.4608 - val_loss: 2.2627 - val_acc: 0.4303\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0245 - acc: 0.4618 - val_loss: 2.2719 - val_acc: 0.4309\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0223 - acc: 0.4619 - val_loss: 2.2707 - val_acc: 0.4294\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0225 - acc: 0.4623 - val_loss: 2.2656 - val_acc: 0.4307\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0224 - acc: 0.4617 - val_loss: 2.2640 - val_acc: 0.4306\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0141 - acc: 0.4633 - val_loss: 2.2825 - val_acc: 0.4297\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0151 - acc: 0.4642 - val_loss: 2.2809 - val_acc: 0.4260\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0184 - acc: 0.4641 - val_loss: 2.2759 - val_acc: 0.4289\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0163 - acc: 0.4633 - val_loss: 2.2829 - val_acc: 0.4279\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0091 - acc: 0.4645 - val_loss: 2.2562 - val_acc: 0.4308\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0042 - acc: 0.4661 - val_loss: 2.2656 - val_acc: 0.4315\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0065 - acc: 0.4651 - val_loss: 2.2618 - val_acc: 0.4318\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9998 - acc: 0.4672 - val_loss: 2.2608 - val_acc: 0.4291\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9980 - acc: 0.4669 - val_loss: 2.2663 - val_acc: 0.4305\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9959 - acc: 0.4680 - val_loss: 2.2664 - val_acc: 0.4307\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0009 - acc: 0.4667 - val_loss: 2.2539 - val_acc: 0.4322\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9903 - acc: 0.4681 - val_loss: 2.2611 - val_acc: 0.4303\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9941 - acc: 0.4664 - val_loss: 2.2641 - val_acc: 0.4303\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9933 - acc: 0.4687 - val_loss: 2.2561 - val_acc: 0.4305\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9920 - acc: 0.4681 - val_loss: 2.2650 - val_acc: 0.4293\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9919 - acc: 0.4694 - val_loss: 2.2600 - val_acc: 0.4289\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9819 - acc: 0.4714 - val_loss: 2.2492 - val_acc: 0.4327\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9779 - acc: 0.4731 - val_loss: 2.2571 - val_acc: 0.4286\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9761 - acc: 0.4728 - val_loss: 2.2497 - val_acc: 0.4328\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9742 - acc: 0.4729 - val_loss: 2.2498 - val_acc: 0.4300\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9734 - acc: 0.4726 - val_loss: 2.2662 - val_acc: 0.4300\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9895 - acc: 0.4692 - val_loss: 2.2602 - val_acc: 0.4308\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9721 - acc: 0.4723 - val_loss: 2.2538 - val_acc: 0.4333\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9689 - acc: 0.4745 - val_loss: 2.2477 - val_acc: 0.4336\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9645 - acc: 0.4748 - val_loss: 2.2557 - val_acc: 0.4312\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9618 - acc: 0.4751 - val_loss: 2.2486 - val_acc: 0.4327\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9626 - acc: 0.4743 - val_loss: 2.2467 - val_acc: 0.4335\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9658 - acc: 0.4731 - val_loss: 2.2659 - val_acc: 0.4307\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9760 - acc: 0.4715 - val_loss: 2.2602 - val_acc: 0.4277\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9742 - acc: 0.4745 - val_loss: 2.2704 - val_acc: 0.4294\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9718 - acc: 0.4736 - val_loss: 2.2606 - val_acc: 0.4301\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9679 - acc: 0.4738 - val_loss: 2.2674 - val_acc: 0.4302\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9552 - acc: 0.4763 - val_loss: 2.2547 - val_acc: 0.4309\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9506 - acc: 0.4773 - val_loss: 2.2604 - val_acc: 0.4325\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9581 - acc: 0.4762 - val_loss: 2.2884 - val_acc: 0.4257\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9559 - acc: 0.4760 - val_loss: 2.2755 - val_acc: 0.4284\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9619 - acc: 0.4748 - val_loss: 2.2747 - val_acc: 0.4282\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9510 - acc: 0.4787 - val_loss: 2.2427 - val_acc: 0.4321\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9438 - acc: 0.4790 - val_loss: 2.2550 - val_acc: 0.4312\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9465 - acc: 0.4772 - val_loss: 2.2550 - val_acc: 0.4303\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9461 - acc: 0.4789 - val_loss: 2.2641 - val_acc: 0.4306\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9535 - acc: 0.4770 - val_loss: 2.2435 - val_acc: 0.4331\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9484 - acc: 0.4767 - val_loss: 2.2608 - val_acc: 0.4306\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9434 - acc: 0.4789 - val_loss: 2.2515 - val_acc: 0.4302\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9473 - acc: 0.4782 - val_loss: 2.2680 - val_acc: 0.4303\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9496 - acc: 0.4781 - val_loss: 2.2570 - val_acc: 0.4287\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9417 - acc: 0.4795 - val_loss: 2.2549 - val_acc: 0.4312\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9359 - acc: 0.4796 - val_loss: 2.2612 - val_acc: 0.4285\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9400 - acc: 0.4812 - val_loss: 2.2580 - val_acc: 0.4299\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9322 - acc: 0.4811 - val_loss: 2.2732 - val_acc: 0.4293\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9392 - acc: 0.4813 - val_loss: 2.2567 - val_acc: 0.4313\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9337 - acc: 0.4815 - val_loss: 2.2801 - val_acc: 0.4283\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9281 - acc: 0.4835 - val_loss: 2.2455 - val_acc: 0.4315\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9249 - acc: 0.4856 - val_loss: 2.2692 - val_acc: 0.4292\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9360 - acc: 0.4805 - val_loss: 2.2566 - val_acc: 0.4284\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9313 - acc: 0.4819 - val_loss: 2.2685 - val_acc: 0.4291\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9240 - acc: 0.4843 - val_loss: 2.2486 - val_acc: 0.4318\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9242 - acc: 0.4824 - val_loss: 2.2768 - val_acc: 0.4273\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9346 - acc: 0.4807 - val_loss: 2.2532 - val_acc: 0.4295\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9293 - acc: 0.4825 - val_loss: 2.2876 - val_acc: 0.4275\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9371 - acc: 0.4789 - val_loss: 2.2490 - val_acc: 0.4294\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9306 - acc: 0.4822 - val_loss: 2.2748 - val_acc: 0.4268\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9353 - acc: 0.4799 - val_loss: 2.2443 - val_acc: 0.4309\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9246 - acc: 0.4835 - val_loss: 2.2613 - val_acc: 0.4287\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9090 - acc: 0.4883 - val_loss: 2.2413 - val_acc: 0.4306\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9038 - acc: 0.4869 - val_loss: 2.2510 - val_acc: 0.4318\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9038 - acc: 0.4874 - val_loss: 2.2615 - val_acc: 0.4290\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9142 - acc: 0.4849 - val_loss: 2.2818 - val_acc: 0.4279\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9131 - acc: 0.4863 - val_loss: 2.2513 - val_acc: 0.4309\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9046 - acc: 0.4880 - val_loss: 2.2712 - val_acc: 0.4285\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9113 - acc: 0.4873 - val_loss: 2.2656 - val_acc: 0.4303\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1735 - acc: 0.4307 - val_loss: 2.1475 - val_acc: 0.4478\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1429 - acc: 0.4379 - val_loss: 2.1491 - val_acc: 0.4470\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1268 - acc: 0.4415 - val_loss: 2.1569 - val_acc: 0.4486\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1256 - acc: 0.4410 - val_loss: 2.1528 - val_acc: 0.4468\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1134 - acc: 0.4424 - val_loss: 2.1623 - val_acc: 0.4468\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1065 - acc: 0.4450 - val_loss: 2.1618 - val_acc: 0.4459\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0969 - acc: 0.4475 - val_loss: 2.1566 - val_acc: 0.4461\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0899 - acc: 0.4479 - val_loss: 2.1628 - val_acc: 0.4453\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0882 - acc: 0.4492 - val_loss: 2.1668 - val_acc: 0.4436\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0829 - acc: 0.4500 - val_loss: 2.1632 - val_acc: 0.4444\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0735 - acc: 0.4512 - val_loss: 2.1656 - val_acc: 0.4449\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0748 - acc: 0.4515 - val_loss: 2.1627 - val_acc: 0.4456\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0671 - acc: 0.4540 - val_loss: 2.1660 - val_acc: 0.4444\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0616 - acc: 0.4555 - val_loss: 2.1691 - val_acc: 0.4441\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0563 - acc: 0.4570 - val_loss: 2.1612 - val_acc: 0.4460\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0543 - acc: 0.4563 - val_loss: 2.1693 - val_acc: 0.4445\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0515 - acc: 0.4584 - val_loss: 2.1679 - val_acc: 0.4435\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0486 - acc: 0.4601 - val_loss: 2.1672 - val_acc: 0.4444\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0487 - acc: 0.4570 - val_loss: 2.1800 - val_acc: 0.4420\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0503 - acc: 0.4578 - val_loss: 2.1640 - val_acc: 0.4465\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0395 - acc: 0.4601 - val_loss: 2.1903 - val_acc: 0.4423\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0422 - acc: 0.4596 - val_loss: 2.1668 - val_acc: 0.4430\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0385 - acc: 0.4586 - val_loss: 2.1899 - val_acc: 0.4415\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0326 - acc: 0.4590 - val_loss: 2.1640 - val_acc: 0.4463\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0242 - acc: 0.4642 - val_loss: 2.1659 - val_acc: 0.4447\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0253 - acc: 0.4632 - val_loss: 2.1583 - val_acc: 0.4446\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0209 - acc: 0.4631 - val_loss: 2.1873 - val_acc: 0.4434\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0283 - acc: 0.4628 - val_loss: 2.1678 - val_acc: 0.4445\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0181 - acc: 0.4649 - val_loss: 2.1718 - val_acc: 0.4449\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0186 - acc: 0.4652 - val_loss: 2.1580 - val_acc: 0.4468\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0113 - acc: 0.4650 - val_loss: 2.1716 - val_acc: 0.4465\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0091 - acc: 0.4670 - val_loss: 2.1758 - val_acc: 0.4434\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0141 - acc: 0.4658 - val_loss: 2.1614 - val_acc: 0.4446\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0031 - acc: 0.4667 - val_loss: 2.1567 - val_acc: 0.4458\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0073 - acc: 0.4660 - val_loss: 2.1672 - val_acc: 0.4443\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0097 - acc: 0.4655 - val_loss: 2.1723 - val_acc: 0.4433\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0072 - acc: 0.4642 - val_loss: 2.1819 - val_acc: 0.4450\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0132 - acc: 0.4667 - val_loss: 2.1655 - val_acc: 0.4431\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9999 - acc: 0.4687 - val_loss: 2.1814 - val_acc: 0.4439\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9937 - acc: 0.4687 - val_loss: 2.1596 - val_acc: 0.4419\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9902 - acc: 0.4701 - val_loss: 2.1612 - val_acc: 0.4435\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9854 - acc: 0.4719 - val_loss: 2.1505 - val_acc: 0.4453\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9815 - acc: 0.4732 - val_loss: 2.1557 - val_acc: 0.4445\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9806 - acc: 0.4741 - val_loss: 2.1685 - val_acc: 0.4432\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9824 - acc: 0.4719 - val_loss: 2.1503 - val_acc: 0.4474\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9792 - acc: 0.4737 - val_loss: 2.1584 - val_acc: 0.4465\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9800 - acc: 0.4728 - val_loss: 2.1532 - val_acc: 0.4481\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9750 - acc: 0.4747 - val_loss: 2.1646 - val_acc: 0.4455\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9771 - acc: 0.4733 - val_loss: 2.1584 - val_acc: 0.4469\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9720 - acc: 0.4743 - val_loss: 2.1538 - val_acc: 0.4463\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9711 - acc: 0.4717 - val_loss: 2.1580 - val_acc: 0.4465\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9700 - acc: 0.4754 - val_loss: 2.1524 - val_acc: 0.4442\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9678 - acc: 0.4742 - val_loss: 2.1722 - val_acc: 0.4448\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9776 - acc: 0.4742 - val_loss: 2.1539 - val_acc: 0.4442\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9688 - acc: 0.4738 - val_loss: 2.1565 - val_acc: 0.4465\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9584 - acc: 0.4788 - val_loss: 2.1456 - val_acc: 0.4472\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9609 - acc: 0.4771 - val_loss: 2.1588 - val_acc: 0.4482\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9600 - acc: 0.4777 - val_loss: 2.1635 - val_acc: 0.4437\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9640 - acc: 0.4750 - val_loss: 2.1755 - val_acc: 0.4466\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9604 - acc: 0.4784 - val_loss: 2.1479 - val_acc: 0.4483\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9513 - acc: 0.4792 - val_loss: 2.1510 - val_acc: 0.4469\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9530 - acc: 0.4806 - val_loss: 2.1505 - val_acc: 0.4486\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9535 - acc: 0.4800 - val_loss: 2.1797 - val_acc: 0.4412\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9600 - acc: 0.4769 - val_loss: 2.1577 - val_acc: 0.4487\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9527 - acc: 0.4790 - val_loss: 2.1713 - val_acc: 0.4450\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9469 - acc: 0.4813 - val_loss: 2.1767 - val_acc: 0.4460\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9541 - acc: 0.4786 - val_loss: 2.2059 - val_acc: 0.4411\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9611 - acc: 0.4770 - val_loss: 2.1689 - val_acc: 0.4477\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9483 - acc: 0.4804 - val_loss: 2.1655 - val_acc: 0.4446\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9531 - acc: 0.4783 - val_loss: 2.1806 - val_acc: 0.4451\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9550 - acc: 0.4787 - val_loss: 2.1720 - val_acc: 0.4440\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9440 - acc: 0.4792 - val_loss: 2.1589 - val_acc: 0.4452\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9451 - acc: 0.4798 - val_loss: 2.1678 - val_acc: 0.4443\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9314 - acc: 0.4837 - val_loss: 2.1624 - val_acc: 0.4475\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9366 - acc: 0.4826 - val_loss: 2.1526 - val_acc: 0.4459\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9303 - acc: 0.4839 - val_loss: 2.1623 - val_acc: 0.4471\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9372 - acc: 0.4831 - val_loss: 2.1836 - val_acc: 0.4444\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9420 - acc: 0.4839 - val_loss: 2.1690 - val_acc: 0.4470\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9352 - acc: 0.4821 - val_loss: 2.1696 - val_acc: 0.4455\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9273 - acc: 0.4841 - val_loss: 2.1703 - val_acc: 0.4470\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9380 - acc: 0.4817 - val_loss: 2.1999 - val_acc: 0.4389\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9354 - acc: 0.4821 - val_loss: 2.1854 - val_acc: 0.4449\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9472 - acc: 0.4831 - val_loss: 2.1765 - val_acc: 0.4425\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9275 - acc: 0.4830 - val_loss: 2.1797 - val_acc: 0.4464\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9368 - acc: 0.4816 - val_loss: 2.1713 - val_acc: 0.4453\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9200 - acc: 0.4871 - val_loss: 2.1641 - val_acc: 0.4443\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9164 - acc: 0.4887 - val_loss: 2.1505 - val_acc: 0.4465\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9196 - acc: 0.4858 - val_loss: 2.1563 - val_acc: 0.4463\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9095 - acc: 0.4883 - val_loss: 2.1517 - val_acc: 0.4476\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9058 - acc: 0.4890 - val_loss: 2.1572 - val_acc: 0.4445\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9053 - acc: 0.4899 - val_loss: 2.1551 - val_acc: 0.4472\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9170 - acc: 0.4896 - val_loss: 2.1933 - val_acc: 0.4432\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9331 - acc: 0.4832 - val_loss: 2.1617 - val_acc: 0.4470\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9300 - acc: 0.4824 - val_loss: 2.1661 - val_acc: 0.4493\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9110 - acc: 0.4885 - val_loss: 2.1760 - val_acc: 0.4421\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9075 - acc: 0.4901 - val_loss: 2.1716 - val_acc: 0.4473\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9117 - acc: 0.4872 - val_loss: 2.1878 - val_acc: 0.4422\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9134 - acc: 0.4885 - val_loss: 2.1701 - val_acc: 0.4461\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9062 - acc: 0.4899 - val_loss: 2.2166 - val_acc: 0.4369\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9089 - acc: 0.4885 - val_loss: 2.1706 - val_acc: 0.4458\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2244 - acc: 0.4209 - val_loss: 2.1482 - val_acc: 0.4517\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1981 - acc: 0.4231 - val_loss: 2.1533 - val_acc: 0.4499\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1881 - acc: 0.4269 - val_loss: 2.1911 - val_acc: 0.4420\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1919 - acc: 0.4258 - val_loss: 2.1766 - val_acc: 0.4470\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1657 - acc: 0.4311 - val_loss: 2.1810 - val_acc: 0.4474\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1622 - acc: 0.4329 - val_loss: 2.1722 - val_acc: 0.4502\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1601 - acc: 0.4325 - val_loss: 2.1991 - val_acc: 0.4435\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1710 - acc: 0.4311 - val_loss: 2.2122 - val_acc: 0.4376\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1515 - acc: 0.4354 - val_loss: 2.1965 - val_acc: 0.4010\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1586 - acc: 0.4332 - val_loss: 2.2072 - val_acc: 0.3977\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1435 - acc: 0.4347 - val_loss: 2.1967 - val_acc: 0.4011\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1286 - acc: 0.4396 - val_loss: 2.1924 - val_acc: 0.4430\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1249 - acc: 0.4398 - val_loss: 2.2069 - val_acc: 0.4371\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1216 - acc: 0.4392 - val_loss: 2.2067 - val_acc: 0.4373\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1160 - acc: 0.4409 - val_loss: 2.1845 - val_acc: 0.4424\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1127 - acc: 0.4412 - val_loss: 2.2065 - val_acc: 0.4386\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1321 - acc: 0.4373 - val_loss: 2.2024 - val_acc: 0.4389\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1168 - acc: 0.4422 - val_loss: 2.2267 - val_acc: 0.4340\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1186 - acc: 0.4412 - val_loss: 2.1913 - val_acc: 0.4407\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0949 - acc: 0.4462 - val_loss: 2.1859 - val_acc: 0.4441\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0948 - acc: 0.4468 - val_loss: 2.1957 - val_acc: 0.4398\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0946 - acc: 0.4484 - val_loss: 2.1870 - val_acc: 0.4421\n",
      "Epoch 23/100\n",
      "1s - loss: 2.1049 - acc: 0.4441 - val_loss: 2.2048 - val_acc: 0.4371\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0869 - acc: 0.4469 - val_loss: 2.1930 - val_acc: 0.4373\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0847 - acc: 0.4481 - val_loss: 2.2081 - val_acc: 0.4363\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0902 - acc: 0.4473 - val_loss: 2.2026 - val_acc: 0.4370\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0796 - acc: 0.4495 - val_loss: 2.1926 - val_acc: 0.4397\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0757 - acc: 0.4508 - val_loss: 2.1820 - val_acc: 0.4417\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0765 - acc: 0.4499 - val_loss: 2.2263 - val_acc: 0.4338\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0984 - acc: 0.4465 - val_loss: 2.2046 - val_acc: 0.4379\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0817 - acc: 0.4471 - val_loss: 2.1748 - val_acc: 0.4430\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0775 - acc: 0.4482 - val_loss: 2.1850 - val_acc: 0.4435\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0788 - acc: 0.4478 - val_loss: 2.1839 - val_acc: 0.4413\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0771 - acc: 0.4486 - val_loss: 2.1834 - val_acc: 0.4420\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0624 - acc: 0.4514 - val_loss: 2.1711 - val_acc: 0.4449\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0587 - acc: 0.4529 - val_loss: 2.1866 - val_acc: 0.4397\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0628 - acc: 0.4530 - val_loss: 2.1687 - val_acc: 0.4441\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0591 - acc: 0.4537 - val_loss: 2.1862 - val_acc: 0.4395\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0734 - acc: 0.4511 - val_loss: 2.1709 - val_acc: 0.4449\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0580 - acc: 0.4524 - val_loss: 2.1868 - val_acc: 0.4431\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0736 - acc: 0.4498 - val_loss: 2.1637 - val_acc: 0.4456\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0539 - acc: 0.4551 - val_loss: 2.1801 - val_acc: 0.4424\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0422 - acc: 0.4569 - val_loss: 2.1801 - val_acc: 0.4441\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0478 - acc: 0.4556 - val_loss: 2.1777 - val_acc: 0.4426\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0391 - acc: 0.4565 - val_loss: 2.1797 - val_acc: 0.4414\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0448 - acc: 0.4589 - val_loss: 2.1806 - val_acc: 0.4433\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0434 - acc: 0.4589 - val_loss: 2.1799 - val_acc: 0.4421\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0436 - acc: 0.4563 - val_loss: 2.1888 - val_acc: 0.4412\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0295 - acc: 0.4584 - val_loss: 2.1522 - val_acc: 0.4485\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0253 - acc: 0.4601 - val_loss: 2.1615 - val_acc: 0.4477\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0205 - acc: 0.4610 - val_loss: 2.1723 - val_acc: 0.4431\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0270 - acc: 0.4617 - val_loss: 2.1729 - val_acc: 0.4445\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0314 - acc: 0.4602 - val_loss: 2.1676 - val_acc: 0.4425\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0260 - acc: 0.4619 - val_loss: 2.1744 - val_acc: 0.4444\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0243 - acc: 0.4626 - val_loss: 2.1755 - val_acc: 0.4452\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0393 - acc: 0.4595 - val_loss: 2.1728 - val_acc: 0.4414\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0326 - acc: 0.4576 - val_loss: 2.1959 - val_acc: 0.4417\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0579 - acc: 0.4547 - val_loss: 2.1586 - val_acc: 0.4478\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0311 - acc: 0.4600 - val_loss: 2.1733 - val_acc: 0.4441\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0222 - acc: 0.4623 - val_loss: 2.1610 - val_acc: 0.4460\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0253 - acc: 0.4592 - val_loss: 2.1755 - val_acc: 0.4450\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0161 - acc: 0.4628 - val_loss: 2.1613 - val_acc: 0.4460\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0156 - acc: 0.4631 - val_loss: 2.1843 - val_acc: 0.4401\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0092 - acc: 0.4655 - val_loss: 2.1591 - val_acc: 0.4464\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0047 - acc: 0.4661 - val_loss: 2.1840 - val_acc: 0.4409\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0045 - acc: 0.4652 - val_loss: 2.1572 - val_acc: 0.4472\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0031 - acc: 0.4648 - val_loss: 2.1874 - val_acc: 0.4402\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0070 - acc: 0.4660 - val_loss: 2.1688 - val_acc: 0.4421\n",
      "Epoch 69/100\n",
      "1s - loss: 2.0250 - acc: 0.4601 - val_loss: 2.1818 - val_acc: 0.4423\n",
      "Epoch 70/100\n",
      "1s - loss: 2.0239 - acc: 0.4623 - val_loss: 2.2297 - val_acc: 0.4321\n",
      "Epoch 71/100\n",
      "1s - loss: 2.0464 - acc: 0.4568 - val_loss: 2.1959 - val_acc: 0.4412\n",
      "Epoch 72/100\n",
      "1s - loss: 2.0033 - acc: 0.4659 - val_loss: 2.1524 - val_acc: 0.4478\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9949 - acc: 0.4662 - val_loss: 2.1620 - val_acc: 0.4446\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9926 - acc: 0.4687 - val_loss: 2.1576 - val_acc: 0.4460\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9975 - acc: 0.4660 - val_loss: 2.1691 - val_acc: 0.4426\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0013 - acc: 0.4648 - val_loss: 2.1580 - val_acc: 0.4468\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9972 - acc: 0.4669 - val_loss: 2.1893 - val_acc: 0.4398\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0222 - acc: 0.4623 - val_loss: 2.1608 - val_acc: 0.4460\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9908 - acc: 0.4660 - val_loss: 2.1795 - val_acc: 0.4398\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9957 - acc: 0.4670 - val_loss: 2.1638 - val_acc: 0.4422\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9939 - acc: 0.4666 - val_loss: 2.1869 - val_acc: 0.4390\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9765 - acc: 0.4706 - val_loss: 2.1666 - val_acc: 0.4425\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9889 - acc: 0.4695 - val_loss: 2.2032 - val_acc: 0.4387\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9949 - acc: 0.4670 - val_loss: 2.1718 - val_acc: 0.4415\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9966 - acc: 0.4661 - val_loss: 2.1982 - val_acc: 0.4379\n",
      "Epoch 86/100\n",
      "1s - loss: 2.0021 - acc: 0.4660 - val_loss: 2.1703 - val_acc: 0.4429\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9781 - acc: 0.4709 - val_loss: 2.1798 - val_acc: 0.4424\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9789 - acc: 0.4738 - val_loss: 2.1746 - val_acc: 0.4424\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9811 - acc: 0.4707 - val_loss: 2.1622 - val_acc: 0.4464\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9638 - acc: 0.4737 - val_loss: 2.1739 - val_acc: 0.4413\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9644 - acc: 0.4741 - val_loss: 2.1558 - val_acc: 0.4480\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9591 - acc: 0.4754 - val_loss: 2.1650 - val_acc: 0.4427\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9684 - acc: 0.4735 - val_loss: 2.1629 - val_acc: 0.4442\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9672 - acc: 0.4736 - val_loss: 2.2038 - val_acc: 0.4356\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9693 - acc: 0.4741 - val_loss: 2.1665 - val_acc: 0.4427\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9658 - acc: 0.4741 - val_loss: 2.1926 - val_acc: 0.4402\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9889 - acc: 0.4695 - val_loss: 2.1684 - val_acc: 0.4437\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9808 - acc: 0.4691 - val_loss: 2.2047 - val_acc: 0.4364\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9884 - acc: 0.4692 - val_loss: 2.1672 - val_acc: 0.4448\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9689 - acc: 0.4730 - val_loss: 2.1823 - val_acc: 0.4408\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1926 - acc: 0.4267 - val_loss: 2.2263 - val_acc: 0.4280\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1550 - acc: 0.4333 - val_loss: 2.2330 - val_acc: 0.4270\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1405 - acc: 0.4366 - val_loss: 2.2301 - val_acc: 0.4277\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1285 - acc: 0.4392 - val_loss: 2.2266 - val_acc: 0.4256\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1226 - acc: 0.4418 - val_loss: 2.2432 - val_acc: 0.4241\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1139 - acc: 0.4422 - val_loss: 2.2388 - val_acc: 0.4241\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1058 - acc: 0.4417 - val_loss: 2.2482 - val_acc: 0.4234\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1094 - acc: 0.4420 - val_loss: 2.2586 - val_acc: 0.4214\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0971 - acc: 0.4440 - val_loss: 2.2353 - val_acc: 0.4268\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0884 - acc: 0.4463 - val_loss: 2.2336 - val_acc: 0.4292\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0854 - acc: 0.4483 - val_loss: 2.2354 - val_acc: 0.4257\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0833 - acc: 0.4468 - val_loss: 2.2370 - val_acc: 0.4260\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0755 - acc: 0.4495 - val_loss: 2.2509 - val_acc: 0.4283\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1057 - acc: 0.4426 - val_loss: 2.2600 - val_acc: 0.4235\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0877 - acc: 0.4481 - val_loss: 2.2448 - val_acc: 0.4273\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0683 - acc: 0.4531 - val_loss: 2.2330 - val_acc: 0.4298\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0630 - acc: 0.4512 - val_loss: 2.2298 - val_acc: 0.4270\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0587 - acc: 0.4527 - val_loss: 2.2293 - val_acc: 0.4289\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0563 - acc: 0.4533 - val_loss: 2.2254 - val_acc: 0.4314\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0498 - acc: 0.4548 - val_loss: 2.2326 - val_acc: 0.4275\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0488 - acc: 0.4556 - val_loss: 2.2275 - val_acc: 0.4294\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0444 - acc: 0.4572 - val_loss: 2.2250 - val_acc: 0.4300\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0411 - acc: 0.4561 - val_loss: 2.2207 - val_acc: 0.4305\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0386 - acc: 0.4573 - val_loss: 2.2221 - val_acc: 0.4299\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0350 - acc: 0.4580 - val_loss: 2.2288 - val_acc: 0.4300\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0330 - acc: 0.4589 - val_loss: 2.2347 - val_acc: 0.4263\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0310 - acc: 0.4588 - val_loss: 2.2226 - val_acc: 0.4295\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0297 - acc: 0.4584 - val_loss: 2.2304 - val_acc: 0.4276\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0306 - acc: 0.4590 - val_loss: 2.2267 - val_acc: 0.4310\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0268 - acc: 0.4604 - val_loss: 2.2192 - val_acc: 0.4289\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0230 - acc: 0.4597 - val_loss: 2.2239 - val_acc: 0.4278\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0229 - acc: 0.4622 - val_loss: 2.2409 - val_acc: 0.4263\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0287 - acc: 0.4588 - val_loss: 2.2212 - val_acc: 0.4303\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0145 - acc: 0.4629 - val_loss: 2.2189 - val_acc: 0.4316\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0070 - acc: 0.4644 - val_loss: 2.2391 - val_acc: 0.4276\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0158 - acc: 0.4612 - val_loss: 2.2247 - val_acc: 0.4299\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0034 - acc: 0.4673 - val_loss: 2.2166 - val_acc: 0.4311\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0043 - acc: 0.4637 - val_loss: 2.2126 - val_acc: 0.4307\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0059 - acc: 0.4665 - val_loss: 2.2236 - val_acc: 0.4297\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0011 - acc: 0.4654 - val_loss: 2.2198 - val_acc: 0.4318\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9957 - acc: 0.4654 - val_loss: 2.2199 - val_acc: 0.4313\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9950 - acc: 0.4666 - val_loss: 2.2196 - val_acc: 0.4314\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9929 - acc: 0.4652 - val_loss: 2.2156 - val_acc: 0.4311\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0063 - acc: 0.4649 - val_loss: 2.2148 - val_acc: 0.4298\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9872 - acc: 0.4701 - val_loss: 2.2079 - val_acc: 0.4313\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9849 - acc: 0.4699 - val_loss: 2.2155 - val_acc: 0.4318\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9832 - acc: 0.4684 - val_loss: 2.2158 - val_acc: 0.4319\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9843 - acc: 0.4701 - val_loss: 2.2132 - val_acc: 0.4300\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9876 - acc: 0.4671 - val_loss: 2.2227 - val_acc: 0.4301\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0035 - acc: 0.4645 - val_loss: 2.2127 - val_acc: 0.4299\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9774 - acc: 0.4717 - val_loss: 2.2102 - val_acc: 0.4309\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9721 - acc: 0.4703 - val_loss: 2.2234 - val_acc: 0.4295\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9805 - acc: 0.4705 - val_loss: 2.2164 - val_acc: 0.4333\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9721 - acc: 0.4727 - val_loss: 2.2098 - val_acc: 0.4324\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9794 - acc: 0.4701 - val_loss: 2.2234 - val_acc: 0.4320\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9743 - acc: 0.4697 - val_loss: 2.2096 - val_acc: 0.4329\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9660 - acc: 0.4732 - val_loss: 2.2228 - val_acc: 0.4331\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9663 - acc: 0.4750 - val_loss: 2.2119 - val_acc: 0.4304\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9654 - acc: 0.4731 - val_loss: 2.2226 - val_acc: 0.4323\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9674 - acc: 0.4729 - val_loss: 2.2247 - val_acc: 0.4301\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9655 - acc: 0.4749 - val_loss: 2.2216 - val_acc: 0.4308\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9607 - acc: 0.4749 - val_loss: 2.2379 - val_acc: 0.4308\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9894 - acc: 0.4699 - val_loss: 2.2431 - val_acc: 0.4261\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9754 - acc: 0.4717 - val_loss: 2.2257 - val_acc: 0.4316\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9600 - acc: 0.4749 - val_loss: 2.2328 - val_acc: 0.4305\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9658 - acc: 0.4731 - val_loss: 2.2481 - val_acc: 0.4279\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9652 - acc: 0.4732 - val_loss: 2.2304 - val_acc: 0.4296\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9652 - acc: 0.4726 - val_loss: 2.2510 - val_acc: 0.4293\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9717 - acc: 0.4728 - val_loss: 2.2349 - val_acc: 0.4302\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9508 - acc: 0.4766 - val_loss: 2.2148 - val_acc: 0.4333\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9431 - acc: 0.4800 - val_loss: 2.2114 - val_acc: 0.4321\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9461 - acc: 0.4775 - val_loss: 2.2167 - val_acc: 0.4320\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9416 - acc: 0.4786 - val_loss: 2.2088 - val_acc: 0.4357\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9361 - acc: 0.4809 - val_loss: 2.2080 - val_acc: 0.4308\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9332 - acc: 0.4809 - val_loss: 2.2100 - val_acc: 0.4342\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9343 - acc: 0.4803 - val_loss: 2.2233 - val_acc: 0.4304\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9516 - acc: 0.4774 - val_loss: 2.2187 - val_acc: 0.4284\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9349 - acc: 0.4808 - val_loss: 2.2060 - val_acc: 0.4315\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9279 - acc: 0.4821 - val_loss: 2.2069 - val_acc: 0.4304\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9285 - acc: 0.4803 - val_loss: 2.2109 - val_acc: 0.4331\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9294 - acc: 0.4822 - val_loss: 2.2358 - val_acc: 0.4280\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9374 - acc: 0.4808 - val_loss: 2.2196 - val_acc: 0.4301\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9319 - acc: 0.4805 - val_loss: 2.2471 - val_acc: 0.4265\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9543 - acc: 0.4767 - val_loss: 2.2170 - val_acc: 0.4315\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9358 - acc: 0.4808 - val_loss: 2.2189 - val_acc: 0.4324\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9307 - acc: 0.4809 - val_loss: 2.2079 - val_acc: 0.4321\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9269 - acc: 0.4831 - val_loss: 2.2384 - val_acc: 0.4277\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9262 - acc: 0.4825 - val_loss: 2.2159 - val_acc: 0.4296\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9202 - acc: 0.4857 - val_loss: 2.2500 - val_acc: 0.4276\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9291 - acc: 0.4814 - val_loss: 2.2278 - val_acc: 0.4286\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9321 - acc: 0.4803 - val_loss: 2.2355 - val_acc: 0.4282\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9282 - acc: 0.4834 - val_loss: 2.2496 - val_acc: 0.4262\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9367 - acc: 0.4805 - val_loss: 2.2503 - val_acc: 0.4264\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9420 - acc: 0.4788 - val_loss: 2.2551 - val_acc: 0.4268\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9211 - acc: 0.4855 - val_loss: 2.2209 - val_acc: 0.4311\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9153 - acc: 0.4852 - val_loss: 2.2411 - val_acc: 0.4273\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9120 - acc: 0.4845 - val_loss: 2.2196 - val_acc: 0.4320\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9077 - acc: 0.4871 - val_loss: 2.2426 - val_acc: 0.4282\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9252 - acc: 0.4837 - val_loss: 2.2562 - val_acc: 0.4255\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9199 - acc: 0.4843 - val_loss: 2.2433 - val_acc: 0.4267\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2033 - acc: 0.4272 - val_loss: 2.2133 - val_acc: 0.4385\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1663 - acc: 0.4317 - val_loss: 2.2090 - val_acc: 0.4347\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1489 - acc: 0.4372 - val_loss: 2.2220 - val_acc: 0.4330\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1393 - acc: 0.4396 - val_loss: 2.2265 - val_acc: 0.4324\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1371 - acc: 0.4386 - val_loss: 2.2559 - val_acc: 0.4280\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1313 - acc: 0.4400 - val_loss: 2.2380 - val_acc: 0.4281\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1179 - acc: 0.4430 - val_loss: 2.2409 - val_acc: 0.4282\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1115 - acc: 0.4442 - val_loss: 2.2452 - val_acc: 0.4288\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1148 - acc: 0.4440 - val_loss: 2.2398 - val_acc: 0.4277\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1015 - acc: 0.4469 - val_loss: 2.2410 - val_acc: 0.4294\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0966 - acc: 0.4455 - val_loss: 2.2417 - val_acc: 0.4281\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0967 - acc: 0.4473 - val_loss: 2.2444 - val_acc: 0.4305\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0966 - acc: 0.4464 - val_loss: 2.2292 - val_acc: 0.4333\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0860 - acc: 0.4504 - val_loss: 2.2419 - val_acc: 0.4308\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0822 - acc: 0.4487 - val_loss: 2.2334 - val_acc: 0.4319\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0771 - acc: 0.4515 - val_loss: 2.2337 - val_acc: 0.4298\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0769 - acc: 0.4517 - val_loss: 2.2299 - val_acc: 0.4298\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0736 - acc: 0.4510 - val_loss: 2.2334 - val_acc: 0.4308\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0651 - acc: 0.4544 - val_loss: 2.2418 - val_acc: 0.4293\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0623 - acc: 0.4545 - val_loss: 2.2289 - val_acc: 0.4321\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0574 - acc: 0.4539 - val_loss: 2.2299 - val_acc: 0.4315\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0570 - acc: 0.4547 - val_loss: 2.2360 - val_acc: 0.4344\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0636 - acc: 0.4541 - val_loss: 2.2379 - val_acc: 0.4308\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0637 - acc: 0.4537 - val_loss: 2.2505 - val_acc: 0.4296\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0557 - acc: 0.4561 - val_loss: 2.2324 - val_acc: 0.4334\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0460 - acc: 0.4594 - val_loss: 2.2253 - val_acc: 0.4326\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0457 - acc: 0.4571 - val_loss: 2.2493 - val_acc: 0.4274\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0623 - acc: 0.4538 - val_loss: 2.2352 - val_acc: 0.4324\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0433 - acc: 0.4601 - val_loss: 2.2260 - val_acc: 0.4321\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0468 - acc: 0.4576 - val_loss: 2.2408 - val_acc: 0.4318\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0479 - acc: 0.4566 - val_loss: 2.2183 - val_acc: 0.4370\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0482 - acc: 0.4573 - val_loss: 2.2267 - val_acc: 0.4345\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0318 - acc: 0.4611 - val_loss: 2.2160 - val_acc: 0.4358\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0356 - acc: 0.4596 - val_loss: 2.2338 - val_acc: 0.4330\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0310 - acc: 0.4609 - val_loss: 2.2238 - val_acc: 0.4341\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0260 - acc: 0.4626 - val_loss: 2.2366 - val_acc: 0.4312\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0315 - acc: 0.4596 - val_loss: 2.2145 - val_acc: 0.4333\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0282 - acc: 0.4625 - val_loss: 2.2329 - val_acc: 0.4349\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0263 - acc: 0.4620 - val_loss: 2.2237 - val_acc: 0.4354\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0170 - acc: 0.4637 - val_loss: 2.2380 - val_acc: 0.4312\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0132 - acc: 0.4665 - val_loss: 2.2147 - val_acc: 0.4345\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0113 - acc: 0.4658 - val_loss: 2.2250 - val_acc: 0.4355\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0157 - acc: 0.4648 - val_loss: 2.2218 - val_acc: 0.4358\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0088 - acc: 0.4678 - val_loss: 2.2242 - val_acc: 0.4353\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0131 - acc: 0.4657 - val_loss: 2.2096 - val_acc: 0.4379\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0002 - acc: 0.4685 - val_loss: 2.2297 - val_acc: 0.4347\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0052 - acc: 0.4678 - val_loss: 2.2113 - val_acc: 0.4366\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9940 - acc: 0.4690 - val_loss: 2.2406 - val_acc: 0.4324\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0121 - acc: 0.4634 - val_loss: 2.2125 - val_acc: 0.4383\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0035 - acc: 0.4679 - val_loss: 2.2343 - val_acc: 0.4343\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0037 - acc: 0.4647 - val_loss: 2.2103 - val_acc: 0.4353\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0047 - acc: 0.4665 - val_loss: 2.2422 - val_acc: 0.4347\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0015 - acc: 0.4672 - val_loss: 2.2175 - val_acc: 0.4348\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0057 - acc: 0.4663 - val_loss: 2.2242 - val_acc: 0.4370\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9920 - acc: 0.4717 - val_loss: 2.2140 - val_acc: 0.4374\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9810 - acc: 0.4729 - val_loss: 2.2310 - val_acc: 0.4352\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9905 - acc: 0.4684 - val_loss: 2.2091 - val_acc: 0.4376\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9978 - acc: 0.4682 - val_loss: 2.2495 - val_acc: 0.4334\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9879 - acc: 0.4716 - val_loss: 2.2132 - val_acc: 0.4374\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9981 - acc: 0.4664 - val_loss: 2.2201 - val_acc: 0.4370\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9740 - acc: 0.4739 - val_loss: 2.2157 - val_acc: 0.4374\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9788 - acc: 0.4726 - val_loss: 2.2362 - val_acc: 0.4337\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9959 - acc: 0.4685 - val_loss: 2.2098 - val_acc: 0.4390\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9860 - acc: 0.4716 - val_loss: 2.2467 - val_acc: 0.4340\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9840 - acc: 0.4695 - val_loss: 2.2131 - val_acc: 0.4372\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9759 - acc: 0.4741 - val_loss: 2.2227 - val_acc: 0.4357\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9711 - acc: 0.4747 - val_loss: 2.2054 - val_acc: 0.4398\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9716 - acc: 0.4744 - val_loss: 2.2497 - val_acc: 0.4331\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9794 - acc: 0.4706 - val_loss: 2.2069 - val_acc: 0.4364\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9680 - acc: 0.4748 - val_loss: 2.2365 - val_acc: 0.4338\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9836 - acc: 0.4711 - val_loss: 2.2198 - val_acc: 0.4380\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9684 - acc: 0.4737 - val_loss: 2.2400 - val_acc: 0.4355\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9659 - acc: 0.4768 - val_loss: 2.2165 - val_acc: 0.4360\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9588 - acc: 0.4778 - val_loss: 2.2332 - val_acc: 0.4355\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9611 - acc: 0.4769 - val_loss: 2.2079 - val_acc: 0.4377\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9624 - acc: 0.4749 - val_loss: 2.2275 - val_acc: 0.4344\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9593 - acc: 0.4767 - val_loss: 2.2155 - val_acc: 0.4378\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9808 - acc: 0.4725 - val_loss: 2.2632 - val_acc: 0.4305\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9680 - acc: 0.4755 - val_loss: 2.2261 - val_acc: 0.4356\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9793 - acc: 0.4732 - val_loss: 2.2379 - val_acc: 0.4368\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9585 - acc: 0.4757 - val_loss: 2.2129 - val_acc: 0.4368\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9531 - acc: 0.4797 - val_loss: 2.2397 - val_acc: 0.4360\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9461 - acc: 0.4799 - val_loss: 2.2059 - val_acc: 0.4384\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9414 - acc: 0.4832 - val_loss: 2.2268 - val_acc: 0.4348\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9446 - acc: 0.4781 - val_loss: 2.2271 - val_acc: 0.4376\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9906 - acc: 0.4702 - val_loss: 2.2785 - val_acc: 0.4279\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9700 - acc: 0.4766 - val_loss: 2.2183 - val_acc: 0.4362\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9391 - acc: 0.4815 - val_loss: 2.2342 - val_acc: 0.4353\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9493 - acc: 0.4806 - val_loss: 2.2118 - val_acc: 0.4377\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9506 - acc: 0.4778 - val_loss: 2.2514 - val_acc: 0.4323\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9407 - acc: 0.4812 - val_loss: 2.2254 - val_acc: 0.4365\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9392 - acc: 0.4819 - val_loss: 2.2501 - val_acc: 0.4345\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9427 - acc: 0.4833 - val_loss: 2.2123 - val_acc: 0.4363\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9315 - acc: 0.4828 - val_loss: 2.2267 - val_acc: 0.4377\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9247 - acc: 0.4850 - val_loss: 2.2131 - val_acc: 0.4364\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9323 - acc: 0.4837 - val_loss: 2.2238 - val_acc: 0.4367\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9249 - acc: 0.4855 - val_loss: 2.2224 - val_acc: 0.4351\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9329 - acc: 0.4825 - val_loss: 2.2348 - val_acc: 0.4374\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9325 - acc: 0.4823 - val_loss: 2.2126 - val_acc: 0.4368\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9272 - acc: 0.4835 - val_loss: 2.2419 - val_acc: 0.4355\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1770 - acc: 0.4343 - val_loss: 2.1229 - val_acc: 0.4497\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1423 - acc: 0.4401 - val_loss: 2.1266 - val_acc: 0.4488\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1306 - acc: 0.4432 - val_loss: 2.1415 - val_acc: 0.4473\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1196 - acc: 0.4460 - val_loss: 2.1440 - val_acc: 0.4441\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1088 - acc: 0.4467 - val_loss: 2.1295 - val_acc: 0.4462\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1056 - acc: 0.4496 - val_loss: 2.1577 - val_acc: 0.4443\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0986 - acc: 0.4501 - val_loss: 2.1453 - val_acc: 0.4457\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0967 - acc: 0.4506 - val_loss: 2.1569 - val_acc: 0.4430\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0853 - acc: 0.4532 - val_loss: 2.1632 - val_acc: 0.4452\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1064 - acc: 0.4488 - val_loss: 2.1778 - val_acc: 0.4407\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0824 - acc: 0.4525 - val_loss: 2.1308 - val_acc: 0.4492\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0713 - acc: 0.4540 - val_loss: 2.1477 - val_acc: 0.4464\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0653 - acc: 0.4573 - val_loss: 2.1519 - val_acc: 0.4454\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0641 - acc: 0.4595 - val_loss: 2.1396 - val_acc: 0.4485\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0605 - acc: 0.4572 - val_loss: 2.1478 - val_acc: 0.4465\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0609 - acc: 0.4582 - val_loss: 2.1432 - val_acc: 0.4444\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0521 - acc: 0.4581 - val_loss: 2.1405 - val_acc: 0.4466\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0534 - acc: 0.4611 - val_loss: 2.1444 - val_acc: 0.4465\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0541 - acc: 0.4594 - val_loss: 2.1265 - val_acc: 0.4491\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0455 - acc: 0.4605 - val_loss: 2.1321 - val_acc: 0.4457\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0401 - acc: 0.4609 - val_loss: 2.1375 - val_acc: 0.4446\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0458 - acc: 0.4612 - val_loss: 2.1514 - val_acc: 0.4439\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0355 - acc: 0.4624 - val_loss: 2.1327 - val_acc: 0.4467\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0306 - acc: 0.4640 - val_loss: 2.1528 - val_acc: 0.4469\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0300 - acc: 0.4648 - val_loss: 2.1449 - val_acc: 0.4438\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0347 - acc: 0.4633 - val_loss: 2.1386 - val_acc: 0.4483\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0238 - acc: 0.4643 - val_loss: 2.1342 - val_acc: 0.4479\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0227 - acc: 0.4655 - val_loss: 2.1249 - val_acc: 0.4512\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0198 - acc: 0.4672 - val_loss: 2.1373 - val_acc: 0.4471\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0141 - acc: 0.4681 - val_loss: 2.1376 - val_acc: 0.4500\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0173 - acc: 0.4677 - val_loss: 2.1322 - val_acc: 0.4492\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0135 - acc: 0.4675 - val_loss: 2.1438 - val_acc: 0.4461\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0109 - acc: 0.4692 - val_loss: 2.1294 - val_acc: 0.4500\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0023 - acc: 0.4708 - val_loss: 2.1305 - val_acc: 0.4498\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0211 - acc: 0.4658 - val_loss: 2.1329 - val_acc: 0.4473\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0102 - acc: 0.4688 - val_loss: 2.1276 - val_acc: 0.4481\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9983 - acc: 0.4715 - val_loss: 2.1290 - val_acc: 0.4479\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0037 - acc: 0.4727 - val_loss: 2.1258 - val_acc: 0.4491\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9922 - acc: 0.4722 - val_loss: 2.1279 - val_acc: 0.4486\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0000 - acc: 0.4703 - val_loss: 2.1247 - val_acc: 0.4497\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9911 - acc: 0.4717 - val_loss: 2.1089 - val_acc: 0.4545\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9895 - acc: 0.4723 - val_loss: 2.1229 - val_acc: 0.4501\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9863 - acc: 0.4725 - val_loss: 2.1393 - val_acc: 0.4486\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9987 - acc: 0.4699 - val_loss: 2.1259 - val_acc: 0.4519\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9846 - acc: 0.4736 - val_loss: 2.1445 - val_acc: 0.4508\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0267 - acc: 0.4664 - val_loss: 2.1545 - val_acc: 0.4439\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0114 - acc: 0.4686 - val_loss: 2.1335 - val_acc: 0.4492\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9839 - acc: 0.4735 - val_loss: 2.1156 - val_acc: 0.4525\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9771 - acc: 0.4751 - val_loss: 2.1222 - val_acc: 0.4500\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9719 - acc: 0.4773 - val_loss: 2.1316 - val_acc: 0.4498\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9758 - acc: 0.4753 - val_loss: 2.1341 - val_acc: 0.4507\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9707 - acc: 0.4764 - val_loss: 2.1105 - val_acc: 0.4532\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9717 - acc: 0.4786 - val_loss: 2.1256 - val_acc: 0.4504\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9710 - acc: 0.4772 - val_loss: 2.1241 - val_acc: 0.4522\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9671 - acc: 0.4765 - val_loss: 2.1184 - val_acc: 0.4492\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9665 - acc: 0.4773 - val_loss: 2.1162 - val_acc: 0.4502\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9635 - acc: 0.4801 - val_loss: 2.1798 - val_acc: 0.4417\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9983 - acc: 0.4714 - val_loss: 2.1369 - val_acc: 0.4465\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9642 - acc: 0.4787 - val_loss: 2.1367 - val_acc: 0.4486\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9622 - acc: 0.4795 - val_loss: 2.1522 - val_acc: 0.4504\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0165 - acc: 0.4677 - val_loss: 2.1742 - val_acc: 0.4399\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0034 - acc: 0.4706 - val_loss: 2.1412 - val_acc: 0.4463\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9777 - acc: 0.4739 - val_loss: 2.1309 - val_acc: 0.4495\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9636 - acc: 0.4773 - val_loss: 2.1318 - val_acc: 0.4512\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9602 - acc: 0.4799 - val_loss: 2.1282 - val_acc: 0.4507\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9515 - acc: 0.4811 - val_loss: 2.1211 - val_acc: 0.4528\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9486 - acc: 0.4833 - val_loss: 2.1148 - val_acc: 0.4529\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9485 - acc: 0.4820 - val_loss: 2.1084 - val_acc: 0.4529\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9400 - acc: 0.4828 - val_loss: 2.1214 - val_acc: 0.4487\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9406 - acc: 0.4823 - val_loss: 2.1151 - val_acc: 0.4524\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9393 - acc: 0.4845 - val_loss: 2.1178 - val_acc: 0.4533\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9382 - acc: 0.4838 - val_loss: 2.1213 - val_acc: 0.4514\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9378 - acc: 0.4861 - val_loss: 2.1165 - val_acc: 0.4508\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9416 - acc: 0.4830 - val_loss: 2.1549 - val_acc: 0.4454\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9465 - acc: 0.4832 - val_loss: 2.1326 - val_acc: 0.4484\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9386 - acc: 0.4843 - val_loss: 2.1386 - val_acc: 0.4506\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9554 - acc: 0.4815 - val_loss: 2.1461 - val_acc: 0.4478\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9356 - acc: 0.4836 - val_loss: 2.1212 - val_acc: 0.4507\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9346 - acc: 0.4847 - val_loss: 2.1393 - val_acc: 0.4478\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9474 - acc: 0.4826 - val_loss: 2.1345 - val_acc: 0.4485\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9420 - acc: 0.4813 - val_loss: 2.1133 - val_acc: 0.4508\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9234 - acc: 0.4867 - val_loss: 2.1217 - val_acc: 0.4518\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9270 - acc: 0.4868 - val_loss: 2.1127 - val_acc: 0.4520\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9266 - acc: 0.4871 - val_loss: 2.1488 - val_acc: 0.4468\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9317 - acc: 0.4865 - val_loss: 2.1230 - val_acc: 0.4471\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9240 - acc: 0.4897 - val_loss: 2.1212 - val_acc: 0.4485\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9160 - acc: 0.4900 - val_loss: 2.1171 - val_acc: 0.4505\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9163 - acc: 0.4902 - val_loss: 2.1263 - val_acc: 0.4470\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9180 - acc: 0.4887 - val_loss: 2.1264 - val_acc: 0.4489\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9162 - acc: 0.4905 - val_loss: 2.1214 - val_acc: 0.4513\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9116 - acc: 0.4893 - val_loss: 2.1198 - val_acc: 0.4497\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9144 - acc: 0.4895 - val_loss: 2.1339 - val_acc: 0.4483\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9132 - acc: 0.4917 - val_loss: 2.1220 - val_acc: 0.4523\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9223 - acc: 0.4851 - val_loss: 2.1531 - val_acc: 0.4468\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9284 - acc: 0.4849 - val_loss: 2.1173 - val_acc: 0.4505\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9168 - acc: 0.4890 - val_loss: 2.1389 - val_acc: 0.4435\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9156 - acc: 0.4880 - val_loss: 2.1298 - val_acc: 0.4492\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9250 - acc: 0.4874 - val_loss: 2.1647 - val_acc: 0.4428\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9170 - acc: 0.4903 - val_loss: 2.1198 - val_acc: 0.4484\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9063 - acc: 0.4912 - val_loss: 2.1404 - val_acc: 0.4478\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1877 - acc: 0.4271 - val_loss: 2.1887 - val_acc: 0.4427\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1639 - acc: 0.4304 - val_loss: 2.1899 - val_acc: 0.4415\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1507 - acc: 0.4318 - val_loss: 2.1933 - val_acc: 0.4412\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1466 - acc: 0.4342 - val_loss: 2.1890 - val_acc: 0.4418\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1321 - acc: 0.4388 - val_loss: 2.2037 - val_acc: 0.4399\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1287 - acc: 0.4376 - val_loss: 2.2033 - val_acc: 0.4389\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1251 - acc: 0.4393 - val_loss: 2.2046 - val_acc: 0.4406\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1140 - acc: 0.4434 - val_loss: 2.2134 - val_acc: 0.4381\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1098 - acc: 0.4421 - val_loss: 2.2101 - val_acc: 0.4384\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1032 - acc: 0.4423 - val_loss: 2.2190 - val_acc: 0.4395\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1250 - acc: 0.4409 - val_loss: 2.2293 - val_acc: 0.4372\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1091 - acc: 0.4426 - val_loss: 2.2278 - val_acc: 0.4370\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0910 - acc: 0.4471 - val_loss: 2.2191 - val_acc: 0.4361\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0853 - acc: 0.4474 - val_loss: 2.2251 - val_acc: 0.4374\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0842 - acc: 0.4478 - val_loss: 2.2052 - val_acc: 0.4406\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0787 - acc: 0.4484 - val_loss: 2.2192 - val_acc: 0.4392\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0769 - acc: 0.4501 - val_loss: 2.2087 - val_acc: 0.4411\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0754 - acc: 0.4496 - val_loss: 2.2112 - val_acc: 0.4390\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0709 - acc: 0.4505 - val_loss: 2.2099 - val_acc: 0.4415\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0649 - acc: 0.4533 - val_loss: 2.1999 - val_acc: 0.4420\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0661 - acc: 0.4524 - val_loss: 2.2162 - val_acc: 0.4407\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0618 - acc: 0.4526 - val_loss: 2.2213 - val_acc: 0.4380\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0564 - acc: 0.4545 - val_loss: 2.2161 - val_acc: 0.4398\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0518 - acc: 0.4568 - val_loss: 2.2149 - val_acc: 0.4410\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0542 - acc: 0.4542 - val_loss: 2.2107 - val_acc: 0.4408\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0445 - acc: 0.4569 - val_loss: 2.2168 - val_acc: 0.4421\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0461 - acc: 0.4572 - val_loss: 2.2364 - val_acc: 0.4379\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0471 - acc: 0.4556 - val_loss: 2.2110 - val_acc: 0.4396\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0385 - acc: 0.4581 - val_loss: 2.2203 - val_acc: 0.4402\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0400 - acc: 0.4563 - val_loss: 2.2203 - val_acc: 0.4410\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0364 - acc: 0.4607 - val_loss: 2.2090 - val_acc: 0.4386\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0322 - acc: 0.4571 - val_loss: 2.2154 - val_acc: 0.4403\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0315 - acc: 0.4589 - val_loss: 2.2213 - val_acc: 0.4371\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0249 - acc: 0.4611 - val_loss: 2.2117 - val_acc: 0.4433\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0270 - acc: 0.4595 - val_loss: 2.2084 - val_acc: 0.4398\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0239 - acc: 0.4616 - val_loss: 2.2048 - val_acc: 0.4398\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0177 - acc: 0.4621 - val_loss: 2.2237 - val_acc: 0.4383\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0248 - acc: 0.4613 - val_loss: 2.2394 - val_acc: 0.4426\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0776 - acc: 0.4522 - val_loss: 2.2529 - val_acc: 0.4317\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0538 - acc: 0.4549 - val_loss: 2.2342 - val_acc: 0.4388\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0255 - acc: 0.4601 - val_loss: 2.2188 - val_acc: 0.4414\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0167 - acc: 0.4616 - val_loss: 2.2082 - val_acc: 0.4437\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0113 - acc: 0.4638 - val_loss: 2.2179 - val_acc: 0.4411\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0107 - acc: 0.4636 - val_loss: 2.1995 - val_acc: 0.4446\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0013 - acc: 0.4666 - val_loss: 2.2145 - val_acc: 0.4431\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0000 - acc: 0.4685 - val_loss: 2.2110 - val_acc: 0.4428\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0029 - acc: 0.4663 - val_loss: 2.2022 - val_acc: 0.4425\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9968 - acc: 0.4676 - val_loss: 2.2132 - val_acc: 0.4420\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9951 - acc: 0.4682 - val_loss: 2.2016 - val_acc: 0.4435\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0042 - acc: 0.4667 - val_loss: 2.2085 - val_acc: 0.4426\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0005 - acc: 0.4655 - val_loss: 2.2153 - val_acc: 0.4400\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9996 - acc: 0.4679 - val_loss: 2.2167 - val_acc: 0.4403\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9961 - acc: 0.4670 - val_loss: 2.2046 - val_acc: 0.4424\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9844 - acc: 0.4709 - val_loss: 2.2132 - val_acc: 0.4434\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9889 - acc: 0.4712 - val_loss: 2.2070 - val_acc: 0.4442\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9899 - acc: 0.4692 - val_loss: 2.2147 - val_acc: 0.4419\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9916 - acc: 0.4674 - val_loss: 2.2192 - val_acc: 0.4386\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9877 - acc: 0.4698 - val_loss: 2.1980 - val_acc: 0.4438\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9793 - acc: 0.4722 - val_loss: 2.2103 - val_acc: 0.4440\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9873 - acc: 0.4708 - val_loss: 2.2052 - val_acc: 0.4433\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9804 - acc: 0.4707 - val_loss: 2.2132 - val_acc: 0.4405\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9772 - acc: 0.4738 - val_loss: 2.2134 - val_acc: 0.4402\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9775 - acc: 0.4728 - val_loss: 2.2328 - val_acc: 0.4370\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9860 - acc: 0.4677 - val_loss: 2.2321 - val_acc: 0.4400\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9864 - acc: 0.4693 - val_loss: 2.2427 - val_acc: 0.4370\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9680 - acc: 0.4734 - val_loss: 2.2018 - val_acc: 0.4442\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9635 - acc: 0.4748 - val_loss: 2.2095 - val_acc: 0.4429\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9710 - acc: 0.4723 - val_loss: 2.2066 - val_acc: 0.4438\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9643 - acc: 0.4748 - val_loss: 2.2130 - val_acc: 0.4410\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9634 - acc: 0.4747 - val_loss: 2.2274 - val_acc: 0.4414\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9800 - acc: 0.4710 - val_loss: 2.2106 - val_acc: 0.4413\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9658 - acc: 0.4725 - val_loss: 2.2004 - val_acc: 0.4432\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9550 - acc: 0.4774 - val_loss: 2.2080 - val_acc: 0.4443\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9683 - acc: 0.4734 - val_loss: 2.2115 - val_acc: 0.4427\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9532 - acc: 0.4770 - val_loss: 2.2048 - val_acc: 0.4439\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9462 - acc: 0.4795 - val_loss: 2.2052 - val_acc: 0.4447\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9602 - acc: 0.4759 - val_loss: 2.2263 - val_acc: 0.4394\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9526 - acc: 0.4780 - val_loss: 2.2066 - val_acc: 0.4433\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9477 - acc: 0.4790 - val_loss: 2.2069 - val_acc: 0.4421\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9430 - acc: 0.4817 - val_loss: 2.2019 - val_acc: 0.4428\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9414 - acc: 0.4819 - val_loss: 2.2140 - val_acc: 0.4419\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9405 - acc: 0.4778 - val_loss: 2.2125 - val_acc: 0.4401\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9496 - acc: 0.4791 - val_loss: 2.2159 - val_acc: 0.4416\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9398 - acc: 0.4818 - val_loss: 2.2256 - val_acc: 0.4405\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9731 - acc: 0.4743 - val_loss: 2.2446 - val_acc: 0.4378\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9450 - acc: 0.4811 - val_loss: 2.2122 - val_acc: 0.4413\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9407 - acc: 0.4803 - val_loss: 2.2138 - val_acc: 0.4405\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9392 - acc: 0.4808 - val_loss: 2.2078 - val_acc: 0.4429\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9441 - acc: 0.4787 - val_loss: 2.2069 - val_acc: 0.4423\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9416 - acc: 0.4816 - val_loss: 2.2136 - val_acc: 0.4414\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9350 - acc: 0.4821 - val_loss: 2.2048 - val_acc: 0.4440\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9416 - acc: 0.4813 - val_loss: 2.2057 - val_acc: 0.4419\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9281 - acc: 0.4828 - val_loss: 2.2067 - val_acc: 0.4432\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9365 - acc: 0.4814 - val_loss: 2.2207 - val_acc: 0.4408\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9468 - acc: 0.4791 - val_loss: 2.2009 - val_acc: 0.4439\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9328 - acc: 0.4818 - val_loss: 2.2259 - val_acc: 0.4406\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9337 - acc: 0.4813 - val_loss: 2.2001 - val_acc: 0.4423\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9262 - acc: 0.4833 - val_loss: 2.2276 - val_acc: 0.4432\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9489 - acc: 0.4793 - val_loss: 2.2115 - val_acc: 0.4409\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9333 - acc: 0.4808 - val_loss: 2.2278 - val_acc: 0.4413\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1731 - acc: 0.4318 - val_loss: 2.2138 - val_acc: 0.4389\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1468 - acc: 0.4367 - val_loss: 2.1996 - val_acc: 0.4410\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1312 - acc: 0.4387 - val_loss: 2.2128 - val_acc: 0.4397\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1213 - acc: 0.4418 - val_loss: 2.2037 - val_acc: 0.4409\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1172 - acc: 0.4436 - val_loss: 2.2050 - val_acc: 0.4408\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1093 - acc: 0.4426 - val_loss: 2.2059 - val_acc: 0.4403\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1019 - acc: 0.4432 - val_loss: 2.2208 - val_acc: 0.4384\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0932 - acc: 0.4472 - val_loss: 2.2161 - val_acc: 0.4397\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0937 - acc: 0.4463 - val_loss: 2.2430 - val_acc: 0.4361\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0928 - acc: 0.4459 - val_loss: 2.2134 - val_acc: 0.4414\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0839 - acc: 0.4481 - val_loss: 2.2178 - val_acc: 0.4364\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0740 - acc: 0.4514 - val_loss: 2.2126 - val_acc: 0.4417\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0814 - acc: 0.4484 - val_loss: 2.2204 - val_acc: 0.4378\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0693 - acc: 0.4522 - val_loss: 2.2089 - val_acc: 0.4391\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0636 - acc: 0.4533 - val_loss: 2.2170 - val_acc: 0.4395\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0635 - acc: 0.4522 - val_loss: 2.2327 - val_acc: 0.4398\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0974 - acc: 0.4468 - val_loss: 2.2491 - val_acc: 0.4337\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0747 - acc: 0.4520 - val_loss: 2.2202 - val_acc: 0.4401\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0534 - acc: 0.4542 - val_loss: 2.2384 - val_acc: 0.4358\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0522 - acc: 0.4549 - val_loss: 2.2295 - val_acc: 0.4395\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0502 - acc: 0.4555 - val_loss: 2.2137 - val_acc: 0.4390\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0400 - acc: 0.4570 - val_loss: 2.2243 - val_acc: 0.4397\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0440 - acc: 0.4559 - val_loss: 2.2153 - val_acc: 0.4414\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0539 - acc: 0.4535 - val_loss: 2.2185 - val_acc: 0.4383\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0340 - acc: 0.4581 - val_loss: 2.2035 - val_acc: 0.4419\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0224 - acc: 0.4595 - val_loss: 2.2341 - val_acc: 0.4398\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0668 - acc: 0.4524 - val_loss: 2.2497 - val_acc: 0.4320\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0546 - acc: 0.4564 - val_loss: 2.2273 - val_acc: 0.4386\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0346 - acc: 0.4603 - val_loss: 2.2136 - val_acc: 0.4416\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0214 - acc: 0.4623 - val_loss: 2.2226 - val_acc: 0.4388\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0210 - acc: 0.4616 - val_loss: 2.2490 - val_acc: 0.4377\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0330 - acc: 0.4605 - val_loss: 2.2169 - val_acc: 0.4392\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0149 - acc: 0.4631 - val_loss: 2.2132 - val_acc: 0.4420\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0106 - acc: 0.4638 - val_loss: 2.2020 - val_acc: 0.4403\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0012 - acc: 0.4659 - val_loss: 2.2065 - val_acc: 0.4425\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0005 - acc: 0.4662 - val_loss: 2.2084 - val_acc: 0.4403\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0000 - acc: 0.4654 - val_loss: 2.1980 - val_acc: 0.4435\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9908 - acc: 0.4694 - val_loss: 2.2112 - val_acc: 0.4412\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0005 - acc: 0.4673 - val_loss: 2.2021 - val_acc: 0.4420\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9935 - acc: 0.4661 - val_loss: 2.1985 - val_acc: 0.4423\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9885 - acc: 0.4673 - val_loss: 2.2046 - val_acc: 0.4425\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9862 - acc: 0.4678 - val_loss: 2.1936 - val_acc: 0.4445\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9850 - acc: 0.4704 - val_loss: 2.2120 - val_acc: 0.4404\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9829 - acc: 0.4700 - val_loss: 2.2240 - val_acc: 0.4381\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9953 - acc: 0.4698 - val_loss: 2.2302 - val_acc: 0.4373\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9814 - acc: 0.4708 - val_loss: 2.2020 - val_acc: 0.4437\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9901 - acc: 0.4698 - val_loss: 2.2202 - val_acc: 0.4401\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9808 - acc: 0.4702 - val_loss: 2.2146 - val_acc: 0.4420\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9803 - acc: 0.4713 - val_loss: 2.2283 - val_acc: 0.4385\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9891 - acc: 0.4681 - val_loss: 2.1981 - val_acc: 0.4424\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9788 - acc: 0.4712 - val_loss: 2.2186 - val_acc: 0.4404\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9770 - acc: 0.4719 - val_loss: 2.1927 - val_acc: 0.4445\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9752 - acc: 0.4700 - val_loss: 2.2051 - val_acc: 0.4418\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9696 - acc: 0.4745 - val_loss: 2.2018 - val_acc: 0.4424\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9667 - acc: 0.4732 - val_loss: 2.2056 - val_acc: 0.4408\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9725 - acc: 0.4719 - val_loss: 2.2239 - val_acc: 0.4400\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9749 - acc: 0.4696 - val_loss: 2.2056 - val_acc: 0.4434\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9625 - acc: 0.4744 - val_loss: 2.2042 - val_acc: 0.4424\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9543 - acc: 0.4774 - val_loss: 2.2015 - val_acc: 0.4435\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9577 - acc: 0.4754 - val_loss: 2.2220 - val_acc: 0.4406\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9676 - acc: 0.4739 - val_loss: 2.2155 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9747 - acc: 0.4720 - val_loss: 2.2033 - val_acc: 0.4440\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9508 - acc: 0.4781 - val_loss: 2.1979 - val_acc: 0.4441\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9583 - acc: 0.4738 - val_loss: 2.2336 - val_acc: 0.4380\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9673 - acc: 0.4737 - val_loss: 2.2027 - val_acc: 0.4424\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9515 - acc: 0.4772 - val_loss: 2.2211 - val_acc: 0.4414\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9512 - acc: 0.4786 - val_loss: 2.2118 - val_acc: 0.4419\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9563 - acc: 0.4747 - val_loss: 2.2477 - val_acc: 0.4403\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9544 - acc: 0.4784 - val_loss: 2.2075 - val_acc: 0.4431\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9466 - acc: 0.4793 - val_loss: 2.1998 - val_acc: 0.4436\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9391 - acc: 0.4805 - val_loss: 2.2026 - val_acc: 0.4440\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9347 - acc: 0.4814 - val_loss: 2.2212 - val_acc: 0.4423\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9447 - acc: 0.4807 - val_loss: 2.2072 - val_acc: 0.4420\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9372 - acc: 0.4812 - val_loss: 2.2162 - val_acc: 0.4405\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9437 - acc: 0.4790 - val_loss: 2.2303 - val_acc: 0.4395\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9482 - acc: 0.4779 - val_loss: 2.1988 - val_acc: 0.4439\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9358 - acc: 0.4793 - val_loss: 2.2098 - val_acc: 0.4428\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9367 - acc: 0.4801 - val_loss: 2.2194 - val_acc: 0.4425\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9452 - acc: 0.4779 - val_loss: 2.2232 - val_acc: 0.4407\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9257 - acc: 0.4840 - val_loss: 2.1968 - val_acc: 0.4432\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9279 - acc: 0.4787 - val_loss: 2.2397 - val_acc: 0.4340\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9410 - acc: 0.4799 - val_loss: 2.2402 - val_acc: 0.4374\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9291 - acc: 0.4831 - val_loss: 2.2161 - val_acc: 0.4421\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9218 - acc: 0.4843 - val_loss: 2.2253 - val_acc: 0.4393\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9234 - acc: 0.4826 - val_loss: 2.2080 - val_acc: 0.4434\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9194 - acc: 0.4856 - val_loss: 2.2006 - val_acc: 0.4429\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9121 - acc: 0.4866 - val_loss: 2.2045 - val_acc: 0.4416\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9183 - acc: 0.4840 - val_loss: 2.2139 - val_acc: 0.4416\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9173 - acc: 0.4850 - val_loss: 2.2162 - val_acc: 0.4422\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9189 - acc: 0.4848 - val_loss: 2.2176 - val_acc: 0.4426\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9222 - acc: 0.4828 - val_loss: 2.2218 - val_acc: 0.4408\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9180 - acc: 0.4841 - val_loss: 2.2164 - val_acc: 0.4420\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9192 - acc: 0.4834 - val_loss: 2.2623 - val_acc: 0.4385\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9350 - acc: 0.4810 - val_loss: 2.2235 - val_acc: 0.4405\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9173 - acc: 0.4871 - val_loss: 2.2301 - val_acc: 0.4396\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9179 - acc: 0.4851 - val_loss: 2.2292 - val_acc: 0.4395\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9144 - acc: 0.4849 - val_loss: 2.2351 - val_acc: 0.4401\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9108 - acc: 0.4880 - val_loss: 2.2415 - val_acc: 0.4366\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9160 - acc: 0.4837 - val_loss: 2.2157 - val_acc: 0.4442\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9045 - acc: 0.4884 - val_loss: 2.2112 - val_acc: 0.4405\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1584 - acc: 0.4337 - val_loss: 2.1837 - val_acc: 0.4428\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1390 - acc: 0.4393 - val_loss: 2.1922 - val_acc: 0.4409\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1219 - acc: 0.4423 - val_loss: 2.1946 - val_acc: 0.4408\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1135 - acc: 0.4450 - val_loss: 2.1957 - val_acc: 0.4392\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1055 - acc: 0.4470 - val_loss: 2.2160 - val_acc: 0.4359\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0971 - acc: 0.4480 - val_loss: 2.2017 - val_acc: 0.4398\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0931 - acc: 0.4490 - val_loss: 2.2004 - val_acc: 0.4380\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0852 - acc: 0.4507 - val_loss: 2.2059 - val_acc: 0.4363\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0784 - acc: 0.4513 - val_loss: 2.2001 - val_acc: 0.4385\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0707 - acc: 0.4531 - val_loss: 2.2068 - val_acc: 0.4384\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0650 - acc: 0.4552 - val_loss: 2.2164 - val_acc: 0.4372\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0639 - acc: 0.4546 - val_loss: 2.1998 - val_acc: 0.4389\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0582 - acc: 0.4562 - val_loss: 2.1955 - val_acc: 0.4375\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0544 - acc: 0.4570 - val_loss: 2.1971 - val_acc: 0.4384\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0526 - acc: 0.4547 - val_loss: 2.2036 - val_acc: 0.4385\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0465 - acc: 0.4592 - val_loss: 2.2064 - val_acc: 0.4383\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0494 - acc: 0.4559 - val_loss: 2.1972 - val_acc: 0.4409\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0388 - acc: 0.4584 - val_loss: 2.2012 - val_acc: 0.4395\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0361 - acc: 0.4590 - val_loss: 2.1974 - val_acc: 0.4401\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0351 - acc: 0.4596 - val_loss: 2.2013 - val_acc: 0.4418\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0279 - acc: 0.4617 - val_loss: 2.2034 - val_acc: 0.4366\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0278 - acc: 0.4631 - val_loss: 2.1917 - val_acc: 0.4394\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0223 - acc: 0.4621 - val_loss: 2.2079 - val_acc: 0.4388\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0210 - acc: 0.4644 - val_loss: 2.2007 - val_acc: 0.4406\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0162 - acc: 0.4651 - val_loss: 2.2023 - val_acc: 0.4377\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0137 - acc: 0.4669 - val_loss: 2.1930 - val_acc: 0.4410\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0124 - acc: 0.4660 - val_loss: 2.1941 - val_acc: 0.4398\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0104 - acc: 0.4653 - val_loss: 2.1969 - val_acc: 0.4396\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0032 - acc: 0.4694 - val_loss: 2.1976 - val_acc: 0.4417\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0037 - acc: 0.4674 - val_loss: 2.2060 - val_acc: 0.4397\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0100 - acc: 0.4655 - val_loss: 2.2091 - val_acc: 0.4372\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0020 - acc: 0.4682 - val_loss: 2.1967 - val_acc: 0.4399\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9941 - acc: 0.4706 - val_loss: 2.2111 - val_acc: 0.4377\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9909 - acc: 0.4710 - val_loss: 2.2011 - val_acc: 0.4414\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9921 - acc: 0.4700 - val_loss: 2.1897 - val_acc: 0.4436\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9865 - acc: 0.4736 - val_loss: 2.1860 - val_acc: 0.4442\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9849 - acc: 0.4715 - val_loss: 2.2066 - val_acc: 0.4379\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9857 - acc: 0.4714 - val_loss: 2.1986 - val_acc: 0.4402\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9879 - acc: 0.4731 - val_loss: 2.2217 - val_acc: 0.4363\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9828 - acc: 0.4719 - val_loss: 2.1978 - val_acc: 0.4416\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9843 - acc: 0.4725 - val_loss: 2.1959 - val_acc: 0.4427\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9822 - acc: 0.4715 - val_loss: 2.1960 - val_acc: 0.4418\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9760 - acc: 0.4735 - val_loss: 2.1956 - val_acc: 0.4428\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9754 - acc: 0.4746 - val_loss: 2.1991 - val_acc: 0.4400\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9752 - acc: 0.4738 - val_loss: 2.1930 - val_acc: 0.4415\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9693 - acc: 0.4759 - val_loss: 2.2195 - val_acc: 0.4384\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9745 - acc: 0.4751 - val_loss: 2.1857 - val_acc: 0.4446\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9651 - acc: 0.4763 - val_loss: 2.1959 - val_acc: 0.4387\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9590 - acc: 0.4796 - val_loss: 2.1917 - val_acc: 0.4433\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9649 - acc: 0.4762 - val_loss: 2.2288 - val_acc: 0.4374\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9642 - acc: 0.4777 - val_loss: 2.2088 - val_acc: 0.4400\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9587 - acc: 0.4788 - val_loss: 2.2002 - val_acc: 0.4426\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9615 - acc: 0.4784 - val_loss: 2.2054 - val_acc: 0.4413\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9620 - acc: 0.4759 - val_loss: 2.1962 - val_acc: 0.4419\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9560 - acc: 0.4784 - val_loss: 2.2347 - val_acc: 0.4372\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9592 - acc: 0.4782 - val_loss: 2.1948 - val_acc: 0.4452\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9578 - acc: 0.4782 - val_loss: 2.1939 - val_acc: 0.4394\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9497 - acc: 0.4795 - val_loss: 2.1876 - val_acc: 0.4417\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9447 - acc: 0.4797 - val_loss: 2.1874 - val_acc: 0.4429\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9369 - acc: 0.4836 - val_loss: 2.1892 - val_acc: 0.4423\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9361 - acc: 0.4830 - val_loss: 2.2016 - val_acc: 0.4448\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9377 - acc: 0.4837 - val_loss: 2.1963 - val_acc: 0.4426\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9469 - acc: 0.4798 - val_loss: 2.2215 - val_acc: 0.4392\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9702 - acc: 0.4776 - val_loss: 2.2010 - val_acc: 0.4388\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9495 - acc: 0.4789 - val_loss: 2.2338 - val_acc: 0.4370\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9548 - acc: 0.4795 - val_loss: 2.1867 - val_acc: 0.4424\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9331 - acc: 0.4846 - val_loss: 2.1891 - val_acc: 0.4424\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9267 - acc: 0.4858 - val_loss: 2.1888 - val_acc: 0.4419\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9261 - acc: 0.4855 - val_loss: 2.1867 - val_acc: 0.4420\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9248 - acc: 0.4865 - val_loss: 2.1898 - val_acc: 0.4437\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9210 - acc: 0.4862 - val_loss: 2.1922 - val_acc: 0.4425\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9254 - acc: 0.4845 - val_loss: 2.1994 - val_acc: 0.4404\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9288 - acc: 0.4851 - val_loss: 2.2019 - val_acc: 0.4416\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9337 - acc: 0.4830 - val_loss: 2.2016 - val_acc: 0.4411\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9376 - acc: 0.4835 - val_loss: 2.2203 - val_acc: 0.4385\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9365 - acc: 0.4812 - val_loss: 2.2058 - val_acc: 0.4397\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9223 - acc: 0.4857 - val_loss: 2.2095 - val_acc: 0.4421\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9277 - acc: 0.4848 - val_loss: 2.1911 - val_acc: 0.4434\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9194 - acc: 0.4867 - val_loss: 2.2114 - val_acc: 0.4406\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9314 - acc: 0.4845 - val_loss: 2.1930 - val_acc: 0.4418\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9199 - acc: 0.4872 - val_loss: 2.2073 - val_acc: 0.4385\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9130 - acc: 0.4885 - val_loss: 2.1973 - val_acc: 0.4426\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9189 - acc: 0.4888 - val_loss: 2.2027 - val_acc: 0.4408\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9187 - acc: 0.4859 - val_loss: 2.2028 - val_acc: 0.4390\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9240 - acc: 0.4870 - val_loss: 2.2092 - val_acc: 0.4385\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9238 - acc: 0.4852 - val_loss: 2.2069 - val_acc: 0.4388\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9199 - acc: 0.4869 - val_loss: 2.2205 - val_acc: 0.4366\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9078 - acc: 0.4891 - val_loss: 2.2033 - val_acc: 0.4400\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9042 - acc: 0.4901 - val_loss: 2.2078 - val_acc: 0.4379\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9004 - acc: 0.4902 - val_loss: 2.2098 - val_acc: 0.4375\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8998 - acc: 0.4912 - val_loss: 2.2087 - val_acc: 0.4382\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9020 - acc: 0.4902 - val_loss: 2.2041 - val_acc: 0.4410\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8929 - acc: 0.4923 - val_loss: 2.1931 - val_acc: 0.4412\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8915 - acc: 0.4923 - val_loss: 2.1943 - val_acc: 0.4401\n",
      "Epoch 95/100\n",
      "1s - loss: 1.8895 - acc: 0.4941 - val_loss: 2.2071 - val_acc: 0.4423\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8976 - acc: 0.4903 - val_loss: 2.1888 - val_acc: 0.4434\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8843 - acc: 0.4952 - val_loss: 2.2020 - val_acc: 0.4388\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8941 - acc: 0.4918 - val_loss: 2.2172 - val_acc: 0.4364\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9066 - acc: 0.4898 - val_loss: 2.2249 - val_acc: 0.4342\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8997 - acc: 0.4920 - val_loss: 2.2306 - val_acc: 0.4343\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2329 - acc: 0.4214 - val_loss: 2.1357 - val_acc: 0.4512\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1954 - acc: 0.4268 - val_loss: 2.1436 - val_acc: 0.4495\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1817 - acc: 0.4295 - val_loss: 2.1401 - val_acc: 0.4510\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1740 - acc: 0.4321 - val_loss: 2.1473 - val_acc: 0.4458\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1654 - acc: 0.4329 - val_loss: 2.1512 - val_acc: 0.4490\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1590 - acc: 0.4358 - val_loss: 2.1428 - val_acc: 0.4514\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1535 - acc: 0.4371 - val_loss: 2.1549 - val_acc: 0.4480\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1491 - acc: 0.4365 - val_loss: 2.1499 - val_acc: 0.4481\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1390 - acc: 0.4390 - val_loss: 2.1430 - val_acc: 0.4512\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1319 - acc: 0.4411 - val_loss: 2.1358 - val_acc: 0.4503\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1273 - acc: 0.4413 - val_loss: 2.1381 - val_acc: 0.4500\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1217 - acc: 0.4420 - val_loss: 2.1477 - val_acc: 0.4483\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1198 - acc: 0.4443 - val_loss: 2.1515 - val_acc: 0.4467\n",
      "Epoch 14/100\n",
      "1s - loss: 2.1157 - acc: 0.4431 - val_loss: 2.1462 - val_acc: 0.4468\n",
      "Epoch 15/100\n",
      "1s - loss: 2.1130 - acc: 0.4438 - val_loss: 2.1416 - val_acc: 0.4477\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1069 - acc: 0.4454 - val_loss: 2.1463 - val_acc: 0.4475\n",
      "Epoch 17/100\n",
      "1s - loss: 2.1141 - acc: 0.4460 - val_loss: 2.1394 - val_acc: 0.4494\n",
      "Epoch 18/100\n",
      "1s - loss: 2.1110 - acc: 0.4435 - val_loss: 2.1488 - val_acc: 0.4464\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0962 - acc: 0.4473 - val_loss: 2.1459 - val_acc: 0.4482\n",
      "Epoch 20/100\n",
      "1s - loss: 2.1035 - acc: 0.4479 - val_loss: 2.1383 - val_acc: 0.4508\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0930 - acc: 0.4484 - val_loss: 2.1278 - val_acc: 0.4522\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0815 - acc: 0.4513 - val_loss: 2.1326 - val_acc: 0.4496\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0884 - acc: 0.4495 - val_loss: 2.1424 - val_acc: 0.4485\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0831 - acc: 0.4503 - val_loss: 2.1293 - val_acc: 0.4522\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0788 - acc: 0.4507 - val_loss: 2.1283 - val_acc: 0.4519\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0791 - acc: 0.4521 - val_loss: 2.1300 - val_acc: 0.4496\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0783 - acc: 0.4518 - val_loss: 2.1321 - val_acc: 0.4476\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0717 - acc: 0.4508 - val_loss: 2.1299 - val_acc: 0.4534\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0697 - acc: 0.4524 - val_loss: 2.1256 - val_acc: 0.4508\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0679 - acc: 0.4538 - val_loss: 2.1244 - val_acc: 0.4519\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0671 - acc: 0.4540 - val_loss: 2.1284 - val_acc: 0.4507\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0638 - acc: 0.4540 - val_loss: 2.1235 - val_acc: 0.4549\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0647 - acc: 0.4543 - val_loss: 2.1338 - val_acc: 0.4523\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0669 - acc: 0.4532 - val_loss: 2.1362 - val_acc: 0.4495\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0624 - acc: 0.4549 - val_loss: 2.1177 - val_acc: 0.4527\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0516 - acc: 0.4576 - val_loss: 2.1306 - val_acc: 0.4519\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0580 - acc: 0.4550 - val_loss: 2.1344 - val_acc: 0.4507\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0559 - acc: 0.4538 - val_loss: 2.1237 - val_acc: 0.4523\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0464 - acc: 0.4585 - val_loss: 2.1161 - val_acc: 0.4553\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0400 - acc: 0.4595 - val_loss: 2.1298 - val_acc: 0.4530\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0447 - acc: 0.4574 - val_loss: 2.1232 - val_acc: 0.4519\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0364 - acc: 0.4607 - val_loss: 2.1197 - val_acc: 0.4515\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0332 - acc: 0.4617 - val_loss: 2.1191 - val_acc: 0.4548\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0310 - acc: 0.4625 - val_loss: 2.1241 - val_acc: 0.4528\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0323 - acc: 0.4625 - val_loss: 2.1159 - val_acc: 0.4544\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0350 - acc: 0.4603 - val_loss: 2.1204 - val_acc: 0.4534\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0266 - acc: 0.4620 - val_loss: 2.1133 - val_acc: 0.4564\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0204 - acc: 0.4649 - val_loss: 2.1164 - val_acc: 0.4552\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0202 - acc: 0.4639 - val_loss: 2.1141 - val_acc: 0.4549\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0184 - acc: 0.4642 - val_loss: 2.1211 - val_acc: 0.4509\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0276 - acc: 0.4616 - val_loss: 2.1249 - val_acc: 0.4550\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0237 - acc: 0.4645 - val_loss: 2.1165 - val_acc: 0.4539\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0291 - acc: 0.4614 - val_loss: 2.1292 - val_acc: 0.4528\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0259 - acc: 0.4627 - val_loss: 2.1305 - val_acc: 0.4523\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0214 - acc: 0.4643 - val_loss: 2.1261 - val_acc: 0.4524\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0267 - acc: 0.4626 - val_loss: 2.1219 - val_acc: 0.4525\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0134 - acc: 0.4655 - val_loss: 2.1347 - val_acc: 0.4525\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0228 - acc: 0.4636 - val_loss: 2.1289 - val_acc: 0.4513\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0173 - acc: 0.4646 - val_loss: 2.1419 - val_acc: 0.4509\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0111 - acc: 0.4680 - val_loss: 2.1296 - val_acc: 0.4535\n",
      "Epoch 61/100\n",
      "1s - loss: 2.0097 - acc: 0.4660 - val_loss: 2.1512 - val_acc: 0.4518\n",
      "Epoch 62/100\n",
      "1s - loss: 2.0334 - acc: 0.4607 - val_loss: 2.1313 - val_acc: 0.4529\n",
      "Epoch 63/100\n",
      "1s - loss: 2.0077 - acc: 0.4685 - val_loss: 2.1318 - val_acc: 0.4524\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0065 - acc: 0.4665 - val_loss: 2.1389 - val_acc: 0.4508\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0155 - acc: 0.4666 - val_loss: 2.1398 - val_acc: 0.4532\n",
      "Epoch 66/100\n",
      "1s - loss: 2.0050 - acc: 0.4658 - val_loss: 2.1366 - val_acc: 0.4510\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9999 - acc: 0.4682 - val_loss: 2.1382 - val_acc: 0.4530\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0134 - acc: 0.4660 - val_loss: 2.1418 - val_acc: 0.4503\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9987 - acc: 0.4693 - val_loss: 2.1166 - val_acc: 0.4553\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9896 - acc: 0.4705 - val_loss: 2.1244 - val_acc: 0.4524\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9945 - acc: 0.4683 - val_loss: 2.1213 - val_acc: 0.4561\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9873 - acc: 0.4700 - val_loss: 2.1271 - val_acc: 0.4535\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9832 - acc: 0.4708 - val_loss: 2.1286 - val_acc: 0.4529\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9885 - acc: 0.4715 - val_loss: 2.1324 - val_acc: 0.4539\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9980 - acc: 0.4672 - val_loss: 2.1344 - val_acc: 0.4539\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9951 - acc: 0.4692 - val_loss: 2.1331 - val_acc: 0.4533\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0007 - acc: 0.4684 - val_loss: 2.1373 - val_acc: 0.4525\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9943 - acc: 0.4698 - val_loss: 2.1281 - val_acc: 0.4536\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9786 - acc: 0.4752 - val_loss: 2.1498 - val_acc: 0.4497\n",
      "Epoch 80/100\n",
      "1s - loss: 2.0028 - acc: 0.4680 - val_loss: 2.1441 - val_acc: 0.4499\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9896 - acc: 0.4711 - val_loss: 2.1499 - val_acc: 0.4492\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9795 - acc: 0.4708 - val_loss: 2.1267 - val_acc: 0.4518\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9755 - acc: 0.4734 - val_loss: 2.1513 - val_acc: 0.4524\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9757 - acc: 0.4752 - val_loss: 2.1404 - val_acc: 0.4509\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9842 - acc: 0.4757 - val_loss: 2.1490 - val_acc: 0.4482\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9733 - acc: 0.4752 - val_loss: 2.1246 - val_acc: 0.4544\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9739 - acc: 0.4741 - val_loss: 2.1504 - val_acc: 0.4492\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9771 - acc: 0.4740 - val_loss: 2.1478 - val_acc: 0.4474\n",
      "Epoch 89/100\n",
      "1s - loss: 2.0127 - acc: 0.4674 - val_loss: 2.1839 - val_acc: 0.4400\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9925 - acc: 0.4696 - val_loss: 2.1413 - val_acc: 0.4476\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9808 - acc: 0.4749 - val_loss: 2.1505 - val_acc: 0.4511\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9763 - acc: 0.4730 - val_loss: 2.1289 - val_acc: 0.4503\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9631 - acc: 0.4784 - val_loss: 2.1296 - val_acc: 0.4515\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9595 - acc: 0.4767 - val_loss: 2.1286 - val_acc: 0.4527\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9506 - acc: 0.4794 - val_loss: 2.1293 - val_acc: 0.4547\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9573 - acc: 0.4786 - val_loss: 2.1528 - val_acc: 0.4494\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9606 - acc: 0.4766 - val_loss: 2.1376 - val_acc: 0.4556\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9563 - acc: 0.4779 - val_loss: 2.1589 - val_acc: 0.4491\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9649 - acc: 0.4769 - val_loss: 2.1416 - val_acc: 0.4512\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9568 - acc: 0.4784 - val_loss: 2.1635 - val_acc: 0.4482\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1948 - acc: 0.4279 - val_loss: 2.1737 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1671 - acc: 0.4334 - val_loss: 2.1760 - val_acc: 0.4432\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1575 - acc: 0.4368 - val_loss: 2.1741 - val_acc: 0.4444\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1448 - acc: 0.4400 - val_loss: 2.1780 - val_acc: 0.4431\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1340 - acc: 0.4406 - val_loss: 2.1768 - val_acc: 0.4445\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1286 - acc: 0.4430 - val_loss: 2.1867 - val_acc: 0.4452\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1269 - acc: 0.4436 - val_loss: 2.1773 - val_acc: 0.4434\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1155 - acc: 0.4446 - val_loss: 2.1902 - val_acc: 0.4435\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1080 - acc: 0.4446 - val_loss: 2.1916 - val_acc: 0.4409\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1064 - acc: 0.4478 - val_loss: 2.2064 - val_acc: 0.4403\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0985 - acc: 0.4486 - val_loss: 2.1914 - val_acc: 0.4430\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0975 - acc: 0.4487 - val_loss: 2.1928 - val_acc: 0.4436\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0909 - acc: 0.4494 - val_loss: 2.1954 - val_acc: 0.4409\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0881 - acc: 0.4518 - val_loss: 2.1916 - val_acc: 0.4436\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0832 - acc: 0.4531 - val_loss: 2.1851 - val_acc: 0.4432\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0807 - acc: 0.4510 - val_loss: 2.1943 - val_acc: 0.4414\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0762 - acc: 0.4540 - val_loss: 2.1911 - val_acc: 0.4398\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0748 - acc: 0.4538 - val_loss: 2.1935 - val_acc: 0.4410\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0664 - acc: 0.4562 - val_loss: 2.1914 - val_acc: 0.4417\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0627 - acc: 0.4563 - val_loss: 2.1867 - val_acc: 0.4412\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0665 - acc: 0.4553 - val_loss: 2.1932 - val_acc: 0.4415\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0580 - acc: 0.4573 - val_loss: 2.1893 - val_acc: 0.4393\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0563 - acc: 0.4568 - val_loss: 2.1937 - val_acc: 0.4402\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0533 - acc: 0.4587 - val_loss: 2.1791 - val_acc: 0.4435\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0519 - acc: 0.4588 - val_loss: 2.1876 - val_acc: 0.4423\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0488 - acc: 0.4595 - val_loss: 2.2445 - val_acc: 0.4407\n",
      "Epoch 27/100\n",
      "1s - loss: 2.1196 - acc: 0.4443 - val_loss: 2.2495 - val_acc: 0.4298\n",
      "Epoch 28/100\n",
      "1s - loss: 2.1097 - acc: 0.4466 - val_loss: 2.2352 - val_acc: 0.4355\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0873 - acc: 0.4515 - val_loss: 2.2039 - val_acc: 0.4380\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0608 - acc: 0.4550 - val_loss: 2.1840 - val_acc: 0.4430\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0336 - acc: 0.4644 - val_loss: 2.1783 - val_acc: 0.4442\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0319 - acc: 0.4629 - val_loss: 2.1866 - val_acc: 0.4414\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0375 - acc: 0.4617 - val_loss: 2.1773 - val_acc: 0.4443\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0341 - acc: 0.4629 - val_loss: 2.1760 - val_acc: 0.4446\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0246 - acc: 0.4648 - val_loss: 2.1696 - val_acc: 0.4428\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0184 - acc: 0.4657 - val_loss: 2.1800 - val_acc: 0.4422\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0222 - acc: 0.4669 - val_loss: 2.1712 - val_acc: 0.4427\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0200 - acc: 0.4666 - val_loss: 2.1891 - val_acc: 0.4421\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0154 - acc: 0.4666 - val_loss: 2.1782 - val_acc: 0.4446\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0178 - acc: 0.4670 - val_loss: 2.2108 - val_acc: 0.4368\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0245 - acc: 0.4633 - val_loss: 2.1863 - val_acc: 0.4415\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0144 - acc: 0.4667 - val_loss: 2.1847 - val_acc: 0.4424\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0128 - acc: 0.4661 - val_loss: 2.1779 - val_acc: 0.4411\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0106 - acc: 0.4665 - val_loss: 2.1845 - val_acc: 0.4420\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0054 - acc: 0.4700 - val_loss: 2.1860 - val_acc: 0.4419\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0048 - acc: 0.4696 - val_loss: 2.1917 - val_acc: 0.4423\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0090 - acc: 0.4685 - val_loss: 2.1879 - val_acc: 0.4401\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0074 - acc: 0.4682 - val_loss: 2.1882 - val_acc: 0.4416\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0004 - acc: 0.4696 - val_loss: 2.1818 - val_acc: 0.4447\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0010 - acc: 0.4679 - val_loss: 2.1981 - val_acc: 0.4431\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0026 - acc: 0.4684 - val_loss: 2.1792 - val_acc: 0.4462\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9928 - acc: 0.4725 - val_loss: 2.1836 - val_acc: 0.4444\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9880 - acc: 0.4732 - val_loss: 2.1869 - val_acc: 0.4438\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9891 - acc: 0.4736 - val_loss: 2.1868 - val_acc: 0.4443\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9897 - acc: 0.4734 - val_loss: 2.1808 - val_acc: 0.4442\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9867 - acc: 0.4745 - val_loss: 2.1841 - val_acc: 0.4410\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9838 - acc: 0.4741 - val_loss: 2.1734 - val_acc: 0.4462\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9763 - acc: 0.4760 - val_loss: 2.1783 - val_acc: 0.4431\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9731 - acc: 0.4742 - val_loss: 2.1767 - val_acc: 0.4456\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9812 - acc: 0.4738 - val_loss: 2.1840 - val_acc: 0.4431\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9792 - acc: 0.4739 - val_loss: 2.1824 - val_acc: 0.4444\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9780 - acc: 0.4744 - val_loss: 2.1823 - val_acc: 0.4446\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9704 - acc: 0.4781 - val_loss: 2.1830 - val_acc: 0.4423\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9705 - acc: 0.4772 - val_loss: 2.1958 - val_acc: 0.4424\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9849 - acc: 0.4722 - val_loss: 2.1815 - val_acc: 0.4437\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9828 - acc: 0.4739 - val_loss: 2.1950 - val_acc: 0.4414\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9779 - acc: 0.4755 - val_loss: 2.1799 - val_acc: 0.4452\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9688 - acc: 0.4774 - val_loss: 2.1755 - val_acc: 0.4433\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9612 - acc: 0.4789 - val_loss: 2.1810 - val_acc: 0.4452\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9728 - acc: 0.4747 - val_loss: 2.2101 - val_acc: 0.4399\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9756 - acc: 0.4751 - val_loss: 2.1886 - val_acc: 0.4428\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9809 - acc: 0.4737 - val_loss: 2.2239 - val_acc: 0.4369\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9688 - acc: 0.4769 - val_loss: 2.1815 - val_acc: 0.4473\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9599 - acc: 0.4785 - val_loss: 2.1909 - val_acc: 0.4431\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9543 - acc: 0.4822 - val_loss: 2.1870 - val_acc: 0.4434\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9653 - acc: 0.4782 - val_loss: 2.2117 - val_acc: 0.4388\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9666 - acc: 0.4774 - val_loss: 2.2136 - val_acc: 0.4405\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9622 - acc: 0.4786 - val_loss: 2.1929 - val_acc: 0.4428\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9506 - acc: 0.4821 - val_loss: 2.2033 - val_acc: 0.4401\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9483 - acc: 0.4824 - val_loss: 2.1851 - val_acc: 0.4447\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9459 - acc: 0.4843 - val_loss: 2.1937 - val_acc: 0.4423\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9373 - acc: 0.4836 - val_loss: 2.1796 - val_acc: 0.4461\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9341 - acc: 0.4856 - val_loss: 2.1814 - val_acc: 0.4433\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9409 - acc: 0.4838 - val_loss: 2.1889 - val_acc: 0.4435\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9401 - acc: 0.4850 - val_loss: 2.2320 - val_acc: 0.4388\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9626 - acc: 0.4779 - val_loss: 2.1926 - val_acc: 0.4435\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9477 - acc: 0.4807 - val_loss: 2.2177 - val_acc: 0.4394\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9588 - acc: 0.4789 - val_loss: 2.1932 - val_acc: 0.4425\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9480 - acc: 0.4799 - val_loss: 2.2131 - val_acc: 0.4399\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9452 - acc: 0.4815 - val_loss: 2.1821 - val_acc: 0.4443\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9359 - acc: 0.4848 - val_loss: 2.1919 - val_acc: 0.4415\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9344 - acc: 0.4860 - val_loss: 2.1864 - val_acc: 0.4439\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9337 - acc: 0.4841 - val_loss: 2.2147 - val_acc: 0.4388\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9371 - acc: 0.4852 - val_loss: 2.1931 - val_acc: 0.4428\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9331 - acc: 0.4834 - val_loss: 2.2109 - val_acc: 0.4425\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9303 - acc: 0.4858 - val_loss: 2.1953 - val_acc: 0.4404\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9318 - acc: 0.4864 - val_loss: 2.2283 - val_acc: 0.4389\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9318 - acc: 0.4881 - val_loss: 2.2079 - val_acc: 0.4410\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9359 - acc: 0.4854 - val_loss: 2.2210 - val_acc: 0.4410\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9288 - acc: 0.4855 - val_loss: 2.2059 - val_acc: 0.4453\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1867 - acc: 0.4289 - val_loss: 2.2024 - val_acc: 0.4396\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1574 - acc: 0.4323 - val_loss: 2.2115 - val_acc: 0.4382\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1465 - acc: 0.4348 - val_loss: 2.2204 - val_acc: 0.4356\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1408 - acc: 0.4352 - val_loss: 2.2352 - val_acc: 0.4352\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1285 - acc: 0.4381 - val_loss: 2.2205 - val_acc: 0.4352\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1229 - acc: 0.4381 - val_loss: 2.2362 - val_acc: 0.4356\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1149 - acc: 0.4400 - val_loss: 2.2397 - val_acc: 0.4327\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1078 - acc: 0.4424 - val_loss: 2.2383 - val_acc: 0.4329\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1016 - acc: 0.4432 - val_loss: 2.2353 - val_acc: 0.4339\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0983 - acc: 0.4459 - val_loss: 2.2400 - val_acc: 0.4323\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0910 - acc: 0.4460 - val_loss: 2.2495 - val_acc: 0.4348\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0866 - acc: 0.4482 - val_loss: 2.2387 - val_acc: 0.4357\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0800 - acc: 0.4486 - val_loss: 2.2420 - val_acc: 0.4335\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0779 - acc: 0.4501 - val_loss: 2.2337 - val_acc: 0.4360\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0750 - acc: 0.4483 - val_loss: 2.2450 - val_acc: 0.4321\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0744 - acc: 0.4505 - val_loss: 2.2411 - val_acc: 0.4315\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0646 - acc: 0.4523 - val_loss: 2.2462 - val_acc: 0.4328\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0628 - acc: 0.4529 - val_loss: 2.2318 - val_acc: 0.4320\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0592 - acc: 0.4517 - val_loss: 2.2396 - val_acc: 0.4343\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0547 - acc: 0.4523 - val_loss: 2.2412 - val_acc: 0.4343\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0598 - acc: 0.4531 - val_loss: 2.2352 - val_acc: 0.4329\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0468 - acc: 0.4535 - val_loss: 2.2341 - val_acc: 0.4328\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0459 - acc: 0.4554 - val_loss: 2.2314 - val_acc: 0.4343\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0447 - acc: 0.4553 - val_loss: 2.2426 - val_acc: 0.4327\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0544 - acc: 0.4529 - val_loss: 2.2270 - val_acc: 0.4370\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0395 - acc: 0.4565 - val_loss: 2.2309 - val_acc: 0.4346\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0390 - acc: 0.4555 - val_loss: 2.2442 - val_acc: 0.4316\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0369 - acc: 0.4570 - val_loss: 2.2337 - val_acc: 0.4315\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0265 - acc: 0.4591 - val_loss: 2.2420 - val_acc: 0.4332\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0231 - acc: 0.4590 - val_loss: 2.2269 - val_acc: 0.4329\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0230 - acc: 0.4605 - val_loss: 2.2361 - val_acc: 0.4314\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0301 - acc: 0.4581 - val_loss: 2.2263 - val_acc: 0.4334\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0212 - acc: 0.4613 - val_loss: 2.2278 - val_acc: 0.4350\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0200 - acc: 0.4601 - val_loss: 2.2271 - val_acc: 0.4314\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0129 - acc: 0.4632 - val_loss: 2.2262 - val_acc: 0.4350\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0088 - acc: 0.4630 - val_loss: 2.2206 - val_acc: 0.4386\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0156 - acc: 0.4617 - val_loss: 2.2267 - val_acc: 0.4371\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0100 - acc: 0.4627 - val_loss: 2.2267 - val_acc: 0.4339\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0013 - acc: 0.4636 - val_loss: 2.2245 - val_acc: 0.4346\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9992 - acc: 0.4650 - val_loss: 2.2179 - val_acc: 0.4368\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9992 - acc: 0.4655 - val_loss: 2.2441 - val_acc: 0.4313\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0031 - acc: 0.4645 - val_loss: 2.2170 - val_acc: 0.4360\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9964 - acc: 0.4674 - val_loss: 2.2245 - val_acc: 0.4359\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9937 - acc: 0.4662 - val_loss: 2.2241 - val_acc: 0.4348\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9958 - acc: 0.4664 - val_loss: 2.2466 - val_acc: 0.4331\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9985 - acc: 0.4652 - val_loss: 2.2185 - val_acc: 0.4377\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9954 - acc: 0.4666 - val_loss: 2.2265 - val_acc: 0.4355\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9944 - acc: 0.4674 - val_loss: 2.2324 - val_acc: 0.4353\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9953 - acc: 0.4656 - val_loss: 2.2378 - val_acc: 0.4363\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0132 - acc: 0.4620 - val_loss: 2.2364 - val_acc: 0.4313\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9857 - acc: 0.4677 - val_loss: 2.2317 - val_acc: 0.4349\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9797 - acc: 0.4699 - val_loss: 2.2229 - val_acc: 0.4366\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9746 - acc: 0.4717 - val_loss: 2.2239 - val_acc: 0.4360\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9719 - acc: 0.4715 - val_loss: 2.2204 - val_acc: 0.4365\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9745 - acc: 0.4725 - val_loss: 2.2359 - val_acc: 0.4355\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9718 - acc: 0.4727 - val_loss: 2.2316 - val_acc: 0.4344\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9734 - acc: 0.4710 - val_loss: 2.2322 - val_acc: 0.4358\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9861 - acc: 0.4688 - val_loss: 2.2675 - val_acc: 0.4310\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9825 - acc: 0.4687 - val_loss: 2.2333 - val_acc: 0.4350\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9774 - acc: 0.4690 - val_loss: 2.2482 - val_acc: 0.4337\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9747 - acc: 0.4694 - val_loss: 2.2201 - val_acc: 0.4375\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9662 - acc: 0.4724 - val_loss: 2.2182 - val_acc: 0.4372\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9590 - acc: 0.4722 - val_loss: 2.2234 - val_acc: 0.4385\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9606 - acc: 0.4725 - val_loss: 2.2321 - val_acc: 0.4368\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9618 - acc: 0.4727 - val_loss: 2.2340 - val_acc: 0.4313\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9649 - acc: 0.4706 - val_loss: 2.2263 - val_acc: 0.4364\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9565 - acc: 0.4757 - val_loss: 2.2192 - val_acc: 0.4352\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9573 - acc: 0.4759 - val_loss: 2.2238 - val_acc: 0.4363\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9564 - acc: 0.4746 - val_loss: 2.2150 - val_acc: 0.4379\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9562 - acc: 0.4751 - val_loss: 2.2353 - val_acc: 0.4341\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9628 - acc: 0.4737 - val_loss: 2.2143 - val_acc: 0.4363\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9425 - acc: 0.4787 - val_loss: 2.2159 - val_acc: 0.4374\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9408 - acc: 0.4790 - val_loss: 2.2148 - val_acc: 0.4388\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9426 - acc: 0.4771 - val_loss: 2.2154 - val_acc: 0.4350\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9398 - acc: 0.4788 - val_loss: 2.2337 - val_acc: 0.4340\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9392 - acc: 0.4796 - val_loss: 2.2166 - val_acc: 0.4375\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9364 - acc: 0.4820 - val_loss: 2.2239 - val_acc: 0.4362\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9363 - acc: 0.4809 - val_loss: 2.2211 - val_acc: 0.4365\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9394 - acc: 0.4787 - val_loss: 2.2225 - val_acc: 0.4360\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9317 - acc: 0.4806 - val_loss: 2.2201 - val_acc: 0.4364\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9339 - acc: 0.4804 - val_loss: 2.2406 - val_acc: 0.4341\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9433 - acc: 0.4779 - val_loss: 2.2111 - val_acc: 0.4385\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9335 - acc: 0.4811 - val_loss: 2.2426 - val_acc: 0.4349\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9513 - acc: 0.4758 - val_loss: 2.2226 - val_acc: 0.4356\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9475 - acc: 0.4767 - val_loss: 2.2435 - val_acc: 0.4345\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9482 - acc: 0.4764 - val_loss: 2.2226 - val_acc: 0.4355\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9387 - acc: 0.4783 - val_loss: 2.2401 - val_acc: 0.4340\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9346 - acc: 0.4791 - val_loss: 2.2365 - val_acc: 0.4357\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9308 - acc: 0.4798 - val_loss: 2.2467 - val_acc: 0.4329\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9290 - acc: 0.4796 - val_loss: 2.2208 - val_acc: 0.4382\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9239 - acc: 0.4817 - val_loss: 2.2537 - val_acc: 0.4320\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9380 - acc: 0.4798 - val_loss: 2.2481 - val_acc: 0.4338\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9303 - acc: 0.4812 - val_loss: 2.2324 - val_acc: 0.4347\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9238 - acc: 0.4822 - val_loss: 2.2338 - val_acc: 0.4371\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9263 - acc: 0.4812 - val_loss: 2.2568 - val_acc: 0.4311\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9281 - acc: 0.4826 - val_loss: 2.2608 - val_acc: 0.4326\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9207 - acc: 0.4811 - val_loss: 2.2288 - val_acc: 0.4374\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9182 - acc: 0.4830 - val_loss: 2.2277 - val_acc: 0.4359\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9074 - acc: 0.4872 - val_loss: 2.2212 - val_acc: 0.4367\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9053 - acc: 0.4854 - val_loss: 2.2210 - val_acc: 0.4392\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1999 - acc: 0.4261 - val_loss: 2.2208 - val_acc: 0.4380\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1735 - acc: 0.4316 - val_loss: 2.2140 - val_acc: 0.4404\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1614 - acc: 0.4336 - val_loss: 2.2217 - val_acc: 0.4381\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1502 - acc: 0.4368 - val_loss: 2.2196 - val_acc: 0.4376\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1412 - acc: 0.4395 - val_loss: 2.2332 - val_acc: 0.4337\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1302 - acc: 0.4417 - val_loss: 2.2396 - val_acc: 0.4336\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1299 - acc: 0.4396 - val_loss: 2.2543 - val_acc: 0.4316\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1195 - acc: 0.4427 - val_loss: 2.2389 - val_acc: 0.4340\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1190 - acc: 0.4426 - val_loss: 2.2413 - val_acc: 0.4314\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1078 - acc: 0.4463 - val_loss: 2.2422 - val_acc: 0.4314\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1006 - acc: 0.4466 - val_loss: 2.2372 - val_acc: 0.4329\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1008 - acc: 0.4470 - val_loss: 2.2358 - val_acc: 0.4326\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0910 - acc: 0.4500 - val_loss: 2.2382 - val_acc: 0.4324\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0910 - acc: 0.4482 - val_loss: 2.2356 - val_acc: 0.4310\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0916 - acc: 0.4483 - val_loss: 2.2417 - val_acc: 0.4348\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0989 - acc: 0.4489 - val_loss: 2.2503 - val_acc: 0.4303\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0814 - acc: 0.4501 - val_loss: 2.2472 - val_acc: 0.4312\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0887 - acc: 0.4508 - val_loss: 2.2591 - val_acc: 0.4265\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0949 - acc: 0.4465 - val_loss: 2.2592 - val_acc: 0.4290\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0827 - acc: 0.4493 - val_loss: 2.2489 - val_acc: 0.4284\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0758 - acc: 0.4516 - val_loss: 2.2596 - val_acc: 0.4270\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0695 - acc: 0.4528 - val_loss: 2.2434 - val_acc: 0.4280\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0578 - acc: 0.4553 - val_loss: 2.2557 - val_acc: 0.4275\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0629 - acc: 0.4546 - val_loss: 2.2392 - val_acc: 0.4307\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0503 - acc: 0.4566 - val_loss: 2.2431 - val_acc: 0.4273\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0519 - acc: 0.4573 - val_loss: 2.2454 - val_acc: 0.4289\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0530 - acc: 0.4571 - val_loss: 2.2454 - val_acc: 0.4264\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0533 - acc: 0.4562 - val_loss: 2.2309 - val_acc: 0.4306\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0414 - acc: 0.4601 - val_loss: 2.2393 - val_acc: 0.4267\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0464 - acc: 0.4575 - val_loss: 2.2629 - val_acc: 0.4261\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0482 - acc: 0.4568 - val_loss: 2.2455 - val_acc: 0.4287\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0501 - acc: 0.4576 - val_loss: 2.2410 - val_acc: 0.4297\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0469 - acc: 0.4577 - val_loss: 2.2367 - val_acc: 0.4298\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0390 - acc: 0.4591 - val_loss: 2.2176 - val_acc: 0.4334\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0449 - acc: 0.4586 - val_loss: 2.2200 - val_acc: 0.4320\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0234 - acc: 0.4627 - val_loss: 2.2252 - val_acc: 0.4339\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0221 - acc: 0.4627 - val_loss: 2.2327 - val_acc: 0.4309\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0225 - acc: 0.4622 - val_loss: 2.2274 - val_acc: 0.4306\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0221 - acc: 0.4646 - val_loss: 2.2185 - val_acc: 0.4335\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0124 - acc: 0.4644 - val_loss: 2.2320 - val_acc: 0.4316\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0218 - acc: 0.4626 - val_loss: 2.2306 - val_acc: 0.4295\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0394 - acc: 0.4586 - val_loss: 2.2363 - val_acc: 0.4307\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0213 - acc: 0.4633 - val_loss: 2.2518 - val_acc: 0.4289\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0402 - acc: 0.4608 - val_loss: 2.2298 - val_acc: 0.4345\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0469 - acc: 0.4587 - val_loss: 2.2532 - val_acc: 0.4284\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0224 - acc: 0.4630 - val_loss: 2.2230 - val_acc: 0.4345\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0070 - acc: 0.4665 - val_loss: 2.2294 - val_acc: 0.4336\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9995 - acc: 0.4694 - val_loss: 2.2196 - val_acc: 0.4348\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9977 - acc: 0.4690 - val_loss: 2.2225 - val_acc: 0.4316\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9978 - acc: 0.4698 - val_loss: 2.2270 - val_acc: 0.4353\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0060 - acc: 0.4675 - val_loss: 2.2361 - val_acc: 0.4303\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0118 - acc: 0.4658 - val_loss: 2.2308 - val_acc: 0.4331\n",
      "Epoch 53/100\n",
      "1s - loss: 2.0014 - acc: 0.4677 - val_loss: 2.2416 - val_acc: 0.4316\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0159 - acc: 0.4654 - val_loss: 2.2314 - val_acc: 0.4326\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0109 - acc: 0.4646 - val_loss: 2.2470 - val_acc: 0.4284\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0126 - acc: 0.4643 - val_loss: 2.2292 - val_acc: 0.4319\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0282 - acc: 0.4645 - val_loss: 2.2408 - val_acc: 0.4305\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0002 - acc: 0.4682 - val_loss: 2.2319 - val_acc: 0.4337\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9972 - acc: 0.4689 - val_loss: 2.2379 - val_acc: 0.4304\n",
      "Epoch 60/100\n",
      "1s - loss: 2.0146 - acc: 0.4645 - val_loss: 2.2259 - val_acc: 0.4338\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9860 - acc: 0.4726 - val_loss: 2.2238 - val_acc: 0.4301\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9906 - acc: 0.4720 - val_loss: 2.2193 - val_acc: 0.4333\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9795 - acc: 0.4723 - val_loss: 2.2233 - val_acc: 0.4341\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9757 - acc: 0.4754 - val_loss: 2.2157 - val_acc: 0.4346\n",
      "Epoch 65/100\n",
      "1s - loss: 2.0036 - acc: 0.4669 - val_loss: 2.2357 - val_acc: 0.4306\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9861 - acc: 0.4711 - val_loss: 2.2282 - val_acc: 0.4359\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9688 - acc: 0.4756 - val_loss: 2.2266 - val_acc: 0.4314\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9691 - acc: 0.4748 - val_loss: 2.2258 - val_acc: 0.4324\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9733 - acc: 0.4736 - val_loss: 2.2461 - val_acc: 0.4285\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9703 - acc: 0.4752 - val_loss: 2.2304 - val_acc: 0.4310\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9625 - acc: 0.4761 - val_loss: 2.2283 - val_acc: 0.4334\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9843 - acc: 0.4725 - val_loss: 2.2558 - val_acc: 0.4293\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9774 - acc: 0.4728 - val_loss: 2.2273 - val_acc: 0.4320\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9749 - acc: 0.4727 - val_loss: 2.2282 - val_acc: 0.4343\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9824 - acc: 0.4712 - val_loss: 2.2304 - val_acc: 0.4310\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9909 - acc: 0.4692 - val_loss: 2.2233 - val_acc: 0.4329\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9952 - acc: 0.4685 - val_loss: 2.2384 - val_acc: 0.4299\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9797 - acc: 0.4730 - val_loss: 2.2214 - val_acc: 0.4334\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9654 - acc: 0.4749 - val_loss: 2.2162 - val_acc: 0.4319\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9556 - acc: 0.4786 - val_loss: 2.2331 - val_acc: 0.4315\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9582 - acc: 0.4784 - val_loss: 2.2277 - val_acc: 0.4306\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9583 - acc: 0.4754 - val_loss: 2.2192 - val_acc: 0.4339\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9434 - acc: 0.4806 - val_loss: 2.2210 - val_acc: 0.4330\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9512 - acc: 0.4788 - val_loss: 2.2388 - val_acc: 0.4304\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9572 - acc: 0.4776 - val_loss: 2.2260 - val_acc: 0.4314\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9873 - acc: 0.4712 - val_loss: 2.2090 - val_acc: 0.4356\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9493 - acc: 0.4797 - val_loss: 2.2222 - val_acc: 0.4326\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9595 - acc: 0.4771 - val_loss: 2.2131 - val_acc: 0.4337\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9527 - acc: 0.4778 - val_loss: 2.2272 - val_acc: 0.4351\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9744 - acc: 0.4744 - val_loss: 2.2529 - val_acc: 0.4284\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9620 - acc: 0.4779 - val_loss: 2.2347 - val_acc: 0.4329\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9472 - acc: 0.4810 - val_loss: 2.2271 - val_acc: 0.4321\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9589 - acc: 0.4771 - val_loss: 2.2455 - val_acc: 0.4277\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9949 - acc: 0.4682 - val_loss: 2.2186 - val_acc: 0.4335\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9526 - acc: 0.4807 - val_loss: 2.2117 - val_acc: 0.4336\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9247 - acc: 0.4836 - val_loss: 2.2184 - val_acc: 0.4329\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9270 - acc: 0.4848 - val_loss: 2.2318 - val_acc: 0.4302\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9325 - acc: 0.4835 - val_loss: 2.2252 - val_acc: 0.4326\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9280 - acc: 0.4850 - val_loss: 2.2225 - val_acc: 0.4331\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9187 - acc: 0.4888 - val_loss: 2.2278 - val_acc: 0.4317\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1705 - acc: 0.4301 - val_loss: 2.1648 - val_acc: 0.4392\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1458 - acc: 0.4360 - val_loss: 2.1576 - val_acc: 0.4426\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1330 - acc: 0.4391 - val_loss: 2.1745 - val_acc: 0.4380\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1201 - acc: 0.4420 - val_loss: 2.1799 - val_acc: 0.4369\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1111 - acc: 0.4425 - val_loss: 2.1911 - val_acc: 0.4349\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1119 - acc: 0.4421 - val_loss: 2.1798 - val_acc: 0.4383\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0999 - acc: 0.4435 - val_loss: 2.1862 - val_acc: 0.4362\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0950 - acc: 0.4470 - val_loss: 2.2002 - val_acc: 0.4350\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0923 - acc: 0.4480 - val_loss: 2.1835 - val_acc: 0.4376\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0772 - acc: 0.4491 - val_loss: 2.1831 - val_acc: 0.4369\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0750 - acc: 0.4506 - val_loss: 2.1804 - val_acc: 0.4393\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0762 - acc: 0.4497 - val_loss: 2.1850 - val_acc: 0.4363\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0662 - acc: 0.4520 - val_loss: 2.1888 - val_acc: 0.4388\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0639 - acc: 0.4520 - val_loss: 2.1808 - val_acc: 0.4398\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0609 - acc: 0.4530 - val_loss: 2.1827 - val_acc: 0.4390\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0578 - acc: 0.4528 - val_loss: 2.1870 - val_acc: 0.4371\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0567 - acc: 0.4539 - val_loss: 2.1897 - val_acc: 0.4375\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0521 - acc: 0.4525 - val_loss: 2.1789 - val_acc: 0.4391\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0424 - acc: 0.4583 - val_loss: 2.1779 - val_acc: 0.4416\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0410 - acc: 0.4573 - val_loss: 2.1775 - val_acc: 0.4418\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0338 - acc: 0.4594 - val_loss: 2.1939 - val_acc: 0.4370\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0465 - acc: 0.4556 - val_loss: 2.2119 - val_acc: 0.4392\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0454 - acc: 0.4565 - val_loss: 2.1813 - val_acc: 0.4395\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0323 - acc: 0.4602 - val_loss: 2.1823 - val_acc: 0.4396\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0318 - acc: 0.4585 - val_loss: 2.1930 - val_acc: 0.4371\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0358 - acc: 0.4573 - val_loss: 2.2083 - val_acc: 0.4381\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0420 - acc: 0.4567 - val_loss: 2.1812 - val_acc: 0.4391\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0236 - acc: 0.4618 - val_loss: 2.1975 - val_acc: 0.4393\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0292 - acc: 0.4603 - val_loss: 2.1789 - val_acc: 0.4402\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0238 - acc: 0.4617 - val_loss: 2.1816 - val_acc: 0.4427\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0124 - acc: 0.4633 - val_loss: 2.1877 - val_acc: 0.4391\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0062 - acc: 0.4652 - val_loss: 2.1690 - val_acc: 0.4417\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0056 - acc: 0.4652 - val_loss: 2.1794 - val_acc: 0.4383\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0042 - acc: 0.4654 - val_loss: 2.1901 - val_acc: 0.4381\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0101 - acc: 0.4631 - val_loss: 2.1735 - val_acc: 0.4418\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0097 - acc: 0.4651 - val_loss: 2.1792 - val_acc: 0.4410\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0019 - acc: 0.4665 - val_loss: 2.1803 - val_acc: 0.4418\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0078 - acc: 0.4651 - val_loss: 2.1718 - val_acc: 0.4426\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9932 - acc: 0.4708 - val_loss: 2.1677 - val_acc: 0.4433\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9975 - acc: 0.4687 - val_loss: 2.1782 - val_acc: 0.4427\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9910 - acc: 0.4674 - val_loss: 2.1817 - val_acc: 0.4392\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9914 - acc: 0.4678 - val_loss: 2.1967 - val_acc: 0.4404\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9934 - acc: 0.4694 - val_loss: 2.1822 - val_acc: 0.4394\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9870 - acc: 0.4705 - val_loss: 2.1554 - val_acc: 0.4465\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9781 - acc: 0.4709 - val_loss: 2.1761 - val_acc: 0.4407\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9788 - acc: 0.4730 - val_loss: 2.1704 - val_acc: 0.4428\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9764 - acc: 0.4708 - val_loss: 2.1880 - val_acc: 0.4405\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9824 - acc: 0.4697 - val_loss: 2.1780 - val_acc: 0.4412\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9780 - acc: 0.4741 - val_loss: 2.1837 - val_acc: 0.4409\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9928 - acc: 0.4683 - val_loss: 2.1892 - val_acc: 0.4391\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9951 - acc: 0.4683 - val_loss: 2.1794 - val_acc: 0.4426\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9846 - acc: 0.4715 - val_loss: 2.1709 - val_acc: 0.4437\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9648 - acc: 0.4747 - val_loss: 2.1831 - val_acc: 0.4399\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9741 - acc: 0.4740 - val_loss: 2.1915 - val_acc: 0.4436\n",
      "Epoch 55/100\n",
      "1s - loss: 2.0002 - acc: 0.4690 - val_loss: 2.1774 - val_acc: 0.4434\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9769 - acc: 0.4730 - val_loss: 2.1655 - val_acc: 0.4457\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9552 - acc: 0.4755 - val_loss: 2.1614 - val_acc: 0.4433\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9640 - acc: 0.4768 - val_loss: 2.1699 - val_acc: 0.4417\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9703 - acc: 0.4722 - val_loss: 2.1645 - val_acc: 0.4452\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9644 - acc: 0.4750 - val_loss: 2.1660 - val_acc: 0.4446\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9622 - acc: 0.4764 - val_loss: 2.1701 - val_acc: 0.4439\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9704 - acc: 0.4758 - val_loss: 2.1929 - val_acc: 0.4388\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9672 - acc: 0.4742 - val_loss: 2.1803 - val_acc: 0.4389\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9593 - acc: 0.4777 - val_loss: 2.1857 - val_acc: 0.4413\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9424 - acc: 0.4807 - val_loss: 2.1641 - val_acc: 0.4411\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9395 - acc: 0.4798 - val_loss: 2.1681 - val_acc: 0.4452\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9400 - acc: 0.4809 - val_loss: 2.1656 - val_acc: 0.4459\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9363 - acc: 0.4812 - val_loss: 2.1725 - val_acc: 0.4440\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9536 - acc: 0.4754 - val_loss: 2.1838 - val_acc: 0.4424\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9602 - acc: 0.4752 - val_loss: 2.1836 - val_acc: 0.4422\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9451 - acc: 0.4820 - val_loss: 2.1853 - val_acc: 0.4403\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9462 - acc: 0.4779 - val_loss: 2.1753 - val_acc: 0.4430\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9434 - acc: 0.4796 - val_loss: 2.1669 - val_acc: 0.4434\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9383 - acc: 0.4812 - val_loss: 2.1702 - val_acc: 0.4436\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9342 - acc: 0.4822 - val_loss: 2.1759 - val_acc: 0.4451\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9483 - acc: 0.4783 - val_loss: 2.1926 - val_acc: 0.4404\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9381 - acc: 0.4811 - val_loss: 2.1708 - val_acc: 0.4426\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9451 - acc: 0.4791 - val_loss: 2.1807 - val_acc: 0.4428\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9504 - acc: 0.4784 - val_loss: 2.1743 - val_acc: 0.4425\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9354 - acc: 0.4829 - val_loss: 2.1718 - val_acc: 0.4430\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9284 - acc: 0.4833 - val_loss: 2.1595 - val_acc: 0.4471\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9204 - acc: 0.4846 - val_loss: 2.1736 - val_acc: 0.4410\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9208 - acc: 0.4846 - val_loss: 2.1750 - val_acc: 0.4448\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9167 - acc: 0.4871 - val_loss: 2.2067 - val_acc: 0.4380\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9255 - acc: 0.4840 - val_loss: 2.1812 - val_acc: 0.4444\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9396 - acc: 0.4815 - val_loss: 2.1772 - val_acc: 0.4423\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9221 - acc: 0.4834 - val_loss: 2.1742 - val_acc: 0.4436\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9252 - acc: 0.4833 - val_loss: 2.1950 - val_acc: 0.4404\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9283 - acc: 0.4831 - val_loss: 2.1702 - val_acc: 0.4423\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9200 - acc: 0.4840 - val_loss: 2.1992 - val_acc: 0.4396\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9216 - acc: 0.4840 - val_loss: 2.1724 - val_acc: 0.4402\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9198 - acc: 0.4841 - val_loss: 2.2244 - val_acc: 0.4316\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9242 - acc: 0.4829 - val_loss: 2.1763 - val_acc: 0.4422\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9088 - acc: 0.4865 - val_loss: 2.1950 - val_acc: 0.4390\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9081 - acc: 0.4858 - val_loss: 2.1832 - val_acc: 0.4434\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9338 - acc: 0.4819 - val_loss: 2.1636 - val_acc: 0.4433\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9038 - acc: 0.4908 - val_loss: 2.1589 - val_acc: 0.4448\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9030 - acc: 0.4905 - val_loss: 2.1718 - val_acc: 0.4442\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9133 - acc: 0.4874 - val_loss: 2.2119 - val_acc: 0.4420\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9534 - acc: 0.4761 - val_loss: 2.2106 - val_acc: 0.4366\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2103 - acc: 0.4253 - val_loss: 2.2023 - val_acc: 0.4373\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1662 - acc: 0.4320 - val_loss: 2.1953 - val_acc: 0.4370\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1504 - acc: 0.4357 - val_loss: 2.2009 - val_acc: 0.4355\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1356 - acc: 0.4381 - val_loss: 2.2008 - val_acc: 0.4350\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1295 - acc: 0.4400 - val_loss: 2.2226 - val_acc: 0.4348\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1326 - acc: 0.4390 - val_loss: 2.2125 - val_acc: 0.4351\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1146 - acc: 0.4424 - val_loss: 2.2202 - val_acc: 0.4321\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1145 - acc: 0.4430 - val_loss: 2.2070 - val_acc: 0.4346\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1042 - acc: 0.4462 - val_loss: 2.2129 - val_acc: 0.4329\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1002 - acc: 0.4471 - val_loss: 2.2279 - val_acc: 0.4337\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0961 - acc: 0.4453 - val_loss: 2.2182 - val_acc: 0.4318\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0894 - acc: 0.4467 - val_loss: 2.2093 - val_acc: 0.4356\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0839 - acc: 0.4484 - val_loss: 2.2058 - val_acc: 0.4377\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0806 - acc: 0.4491 - val_loss: 2.2144 - val_acc: 0.4340\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0763 - acc: 0.4506 - val_loss: 2.2257 - val_acc: 0.4328\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0765 - acc: 0.4494 - val_loss: 2.2162 - val_acc: 0.4370\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0776 - acc: 0.4495 - val_loss: 2.2085 - val_acc: 0.4344\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0630 - acc: 0.4517 - val_loss: 2.2121 - val_acc: 0.4386\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0700 - acc: 0.4538 - val_loss: 2.2130 - val_acc: 0.4346\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0607 - acc: 0.4544 - val_loss: 2.2054 - val_acc: 0.4347\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0539 - acc: 0.4555 - val_loss: 2.2192 - val_acc: 0.4353\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0606 - acc: 0.4534 - val_loss: 2.2159 - val_acc: 0.4343\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0526 - acc: 0.4557 - val_loss: 2.2134 - val_acc: 0.4381\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0634 - acc: 0.4530 - val_loss: 2.2047 - val_acc: 0.4351\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0417 - acc: 0.4570 - val_loss: 2.2168 - val_acc: 0.4344\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0415 - acc: 0.4573 - val_loss: 2.2203 - val_acc: 0.4375\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0479 - acc: 0.4571 - val_loss: 2.2157 - val_acc: 0.4385\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0377 - acc: 0.4580 - val_loss: 2.2053 - val_acc: 0.4392\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0316 - acc: 0.4599 - val_loss: 2.2090 - val_acc: 0.4398\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0290 - acc: 0.4607 - val_loss: 2.2047 - val_acc: 0.4389\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0230 - acc: 0.4619 - val_loss: 2.2095 - val_acc: 0.4386\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0291 - acc: 0.4596 - val_loss: 2.2043 - val_acc: 0.4398\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0220 - acc: 0.4622 - val_loss: 2.2072 - val_acc: 0.4381\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0200 - acc: 0.4613 - val_loss: 2.2304 - val_acc: 0.4383\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0419 - acc: 0.4600 - val_loss: 2.2044 - val_acc: 0.4380\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0157 - acc: 0.4621 - val_loss: 2.1985 - val_acc: 0.4406\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0078 - acc: 0.4642 - val_loss: 2.1989 - val_acc: 0.4409\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0080 - acc: 0.4646 - val_loss: 2.2131 - val_acc: 0.4387\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0067 - acc: 0.4648 - val_loss: 2.1917 - val_acc: 0.4419\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0024 - acc: 0.4671 - val_loss: 2.2333 - val_acc: 0.4371\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0055 - acc: 0.4666 - val_loss: 2.2028 - val_acc: 0.4398\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0002 - acc: 0.4663 - val_loss: 2.1952 - val_acc: 0.4392\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0004 - acc: 0.4653 - val_loss: 2.2233 - val_acc: 0.4327\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9951 - acc: 0.4681 - val_loss: 2.1994 - val_acc: 0.4388\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9930 - acc: 0.4680 - val_loss: 2.2008 - val_acc: 0.4397\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9934 - acc: 0.4692 - val_loss: 2.2081 - val_acc: 0.4396\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9924 - acc: 0.4663 - val_loss: 2.2249 - val_acc: 0.4384\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0067 - acc: 0.4657 - val_loss: 2.1946 - val_acc: 0.4436\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9898 - acc: 0.4660 - val_loss: 2.2251 - val_acc: 0.4369\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0035 - acc: 0.4690 - val_loss: 2.2011 - val_acc: 0.4393\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9822 - acc: 0.4724 - val_loss: 2.2070 - val_acc: 0.4369\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9838 - acc: 0.4692 - val_loss: 2.2082 - val_acc: 0.4379\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9931 - acc: 0.4678 - val_loss: 2.2303 - val_acc: 0.4346\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9824 - acc: 0.4717 - val_loss: 2.2090 - val_acc: 0.4399\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9746 - acc: 0.4731 - val_loss: 2.2232 - val_acc: 0.4394\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9716 - acc: 0.4741 - val_loss: 2.2017 - val_acc: 0.4419\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9761 - acc: 0.4721 - val_loss: 2.2180 - val_acc: 0.4379\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9664 - acc: 0.4745 - val_loss: 2.2064 - val_acc: 0.4388\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9836 - acc: 0.4707 - val_loss: 2.2285 - val_acc: 0.4364\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9748 - acc: 0.4716 - val_loss: 2.2011 - val_acc: 0.4397\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9711 - acc: 0.4730 - val_loss: 2.2293 - val_acc: 0.4387\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9959 - acc: 0.4689 - val_loss: 2.2243 - val_acc: 0.4336\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9769 - acc: 0.4711 - val_loss: 2.1987 - val_acc: 0.4410\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9672 - acc: 0.4747 - val_loss: 2.2183 - val_acc: 0.4383\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9748 - acc: 0.4731 - val_loss: 2.2058 - val_acc: 0.4381\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9640 - acc: 0.4750 - val_loss: 2.2134 - val_acc: 0.4382\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9583 - acc: 0.4774 - val_loss: 2.2157 - val_acc: 0.4380\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9568 - acc: 0.4775 - val_loss: 2.2253 - val_acc: 0.4354\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9554 - acc: 0.4792 - val_loss: 2.1958 - val_acc: 0.4414\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9545 - acc: 0.4769 - val_loss: 2.2033 - val_acc: 0.4375\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9527 - acc: 0.4757 - val_loss: 2.2089 - val_acc: 0.4394\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9648 - acc: 0.4755 - val_loss: 2.2152 - val_acc: 0.4370\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9503 - acc: 0.4777 - val_loss: 2.2145 - val_acc: 0.4399\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9619 - acc: 0.4753 - val_loss: 2.2224 - val_acc: 0.4362\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9472 - acc: 0.4770 - val_loss: 2.2199 - val_acc: 0.4359\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9538 - acc: 0.4771 - val_loss: 2.2135 - val_acc: 0.4357\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9507 - acc: 0.4771 - val_loss: 2.2066 - val_acc: 0.4366\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9481 - acc: 0.4801 - val_loss: 2.2499 - val_acc: 0.4320\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9469 - acc: 0.4790 - val_loss: 2.2022 - val_acc: 0.4406\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9386 - acc: 0.4799 - val_loss: 2.2207 - val_acc: 0.4376\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9391 - acc: 0.4795 - val_loss: 2.2175 - val_acc: 0.4384\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9436 - acc: 0.4790 - val_loss: 2.2169 - val_acc: 0.4352\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9360 - acc: 0.4808 - val_loss: 2.2118 - val_acc: 0.4376\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9425 - acc: 0.4787 - val_loss: 2.2050 - val_acc: 0.4377\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9301 - acc: 0.4819 - val_loss: 2.2050 - val_acc: 0.4374\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9272 - acc: 0.4820 - val_loss: 2.2051 - val_acc: 0.4364\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9295 - acc: 0.4824 - val_loss: 2.2088 - val_acc: 0.4383\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9219 - acc: 0.4846 - val_loss: 2.2070 - val_acc: 0.4375\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9290 - acc: 0.4827 - val_loss: 2.2034 - val_acc: 0.4390\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9227 - acc: 0.4852 - val_loss: 2.2128 - val_acc: 0.4394\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9390 - acc: 0.4822 - val_loss: 2.2297 - val_acc: 0.4365\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9388 - acc: 0.4801 - val_loss: 2.2522 - val_acc: 0.4323\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9234 - acc: 0.4850 - val_loss: 2.2118 - val_acc: 0.4372\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9256 - acc: 0.4845 - val_loss: 2.2322 - val_acc: 0.4380\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9184 - acc: 0.4838 - val_loss: 2.2196 - val_acc: 0.4367\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9244 - acc: 0.4861 - val_loss: 2.2332 - val_acc: 0.4358\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9192 - acc: 0.4861 - val_loss: 2.2202 - val_acc: 0.4357\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9284 - acc: 0.4842 - val_loss: 2.2442 - val_acc: 0.4327\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9213 - acc: 0.4843 - val_loss: 2.2054 - val_acc: 0.4375\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9075 - acc: 0.4877 - val_loss: 2.2055 - val_acc: 0.4406\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2017 - acc: 0.4276 - val_loss: 2.1885 - val_acc: 0.4450\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1794 - acc: 0.4325 - val_loss: 2.1725 - val_acc: 0.4492\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1616 - acc: 0.4340 - val_loss: 2.1864 - val_acc: 0.4479\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1708 - acc: 0.4322 - val_loss: 2.2007 - val_acc: 0.4433\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1478 - acc: 0.4369 - val_loss: 2.1943 - val_acc: 0.4449\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1330 - acc: 0.4394 - val_loss: 2.2456 - val_acc: 0.4345\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1619 - acc: 0.4351 - val_loss: 2.2111 - val_acc: 0.4435\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1239 - acc: 0.4410 - val_loss: 2.1958 - val_acc: 0.4427\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1188 - acc: 0.4423 - val_loss: 2.2054 - val_acc: 0.4404\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1124 - acc: 0.4438 - val_loss: 2.2238 - val_acc: 0.4401\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1054 - acc: 0.4462 - val_loss: 2.2168 - val_acc: 0.4400\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0990 - acc: 0.4460 - val_loss: 2.2087 - val_acc: 0.4426\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1006 - acc: 0.4459 - val_loss: 2.2263 - val_acc: 0.4385\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0936 - acc: 0.4462 - val_loss: 2.2244 - val_acc: 0.4398\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0886 - acc: 0.4482 - val_loss: 2.2250 - val_acc: 0.4400\n",
      "Epoch 16/100\n",
      "1s - loss: 2.1121 - acc: 0.4442 - val_loss: 2.2231 - val_acc: 0.4403\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0963 - acc: 0.4461 - val_loss: 2.2063 - val_acc: 0.4436\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0789 - acc: 0.4493 - val_loss: 2.2146 - val_acc: 0.4422\n",
      "Epoch 19/100\n",
      "1s - loss: 2.1011 - acc: 0.4461 - val_loss: 2.2031 - val_acc: 0.4433\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0775 - acc: 0.4508 - val_loss: 2.2183 - val_acc: 0.4430\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0709 - acc: 0.4524 - val_loss: 2.2304 - val_acc: 0.4381\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0875 - acc: 0.4489 - val_loss: 2.2008 - val_acc: 0.4421\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0692 - acc: 0.4536 - val_loss: 2.2241 - val_acc: 0.4407\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0635 - acc: 0.4550 - val_loss: 2.2072 - val_acc: 0.4411\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0693 - acc: 0.4521 - val_loss: 2.2075 - val_acc: 0.4426\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0540 - acc: 0.4553 - val_loss: 2.2166 - val_acc: 0.4398\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0500 - acc: 0.4583 - val_loss: 2.2352 - val_acc: 0.4369\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0758 - acc: 0.4510 - val_loss: 2.2086 - val_acc: 0.4406\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0516 - acc: 0.4570 - val_loss: 2.2195 - val_acc: 0.4419\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0415 - acc: 0.4585 - val_loss: 2.1907 - val_acc: 0.4433\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0371 - acc: 0.4610 - val_loss: 2.2210 - val_acc: 0.4412\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0547 - acc: 0.4551 - val_loss: 2.2040 - val_acc: 0.4405\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0390 - acc: 0.4598 - val_loss: 2.2642 - val_acc: 0.4418\n",
      "Epoch 34/100\n",
      "1s - loss: 2.1253 - acc: 0.4437 - val_loss: 2.2868 - val_acc: 0.4176\n",
      "Epoch 35/100\n",
      "1s - loss: 2.1374 - acc: 0.4382 - val_loss: 2.2808 - val_acc: 0.4280\n",
      "Epoch 36/100\n",
      "1s - loss: 2.1036 - acc: 0.4458 - val_loss: 2.2389 - val_acc: 0.4338\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0853 - acc: 0.4505 - val_loss: 2.2477 - val_acc: 0.4364\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0711 - acc: 0.4525 - val_loss: 2.2159 - val_acc: 0.4421\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0493 - acc: 0.4594 - val_loss: 2.1872 - val_acc: 0.4453\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0277 - acc: 0.4628 - val_loss: 2.1759 - val_acc: 0.4468\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0193 - acc: 0.4644 - val_loss: 2.1817 - val_acc: 0.4475\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0131 - acc: 0.4628 - val_loss: 2.1842 - val_acc: 0.4467\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0141 - acc: 0.4663 - val_loss: 2.2108 - val_acc: 0.4430\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0140 - acc: 0.4657 - val_loss: 2.1896 - val_acc: 0.4453\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0165 - acc: 0.4647 - val_loss: 2.1840 - val_acc: 0.4465\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0087 - acc: 0.4668 - val_loss: 2.1953 - val_acc: 0.4451\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0147 - acc: 0.4657 - val_loss: 2.2164 - val_acc: 0.4403\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0192 - acc: 0.4643 - val_loss: 2.1829 - val_acc: 0.4454\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0146 - acc: 0.4653 - val_loss: 2.2234 - val_acc: 0.4401\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0073 - acc: 0.4661 - val_loss: 2.1878 - val_acc: 0.4445\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0015 - acc: 0.4672 - val_loss: 2.2125 - val_acc: 0.4413\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0079 - acc: 0.4661 - val_loss: 2.1916 - val_acc: 0.4420\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9997 - acc: 0.4686 - val_loss: 2.2074 - val_acc: 0.4417\n",
      "Epoch 54/100\n",
      "1s - loss: 2.0056 - acc: 0.4666 - val_loss: 2.1792 - val_acc: 0.4471\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9944 - acc: 0.4697 - val_loss: 2.1913 - val_acc: 0.4447\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9903 - acc: 0.4696 - val_loss: 2.1770 - val_acc: 0.4462\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9991 - acc: 0.4687 - val_loss: 2.2226 - val_acc: 0.4402\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9965 - acc: 0.4661 - val_loss: 2.1785 - val_acc: 0.4441\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9923 - acc: 0.4700 - val_loss: 2.1880 - val_acc: 0.4448\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9868 - acc: 0.4675 - val_loss: 2.1937 - val_acc: 0.4441\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9919 - acc: 0.4698 - val_loss: 2.2098 - val_acc: 0.4419\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9823 - acc: 0.4719 - val_loss: 2.1890 - val_acc: 0.4425\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9815 - acc: 0.4741 - val_loss: 2.2184 - val_acc: 0.4424\n",
      "Epoch 64/100\n",
      "1s - loss: 2.0023 - acc: 0.4679 - val_loss: 2.2156 - val_acc: 0.4370\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9879 - acc: 0.4716 - val_loss: 2.1946 - val_acc: 0.4458\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9811 - acc: 0.4709 - val_loss: 2.2168 - val_acc: 0.4368\n",
      "Epoch 67/100\n",
      "1s - loss: 2.0147 - acc: 0.4666 - val_loss: 2.2338 - val_acc: 0.4376\n",
      "Epoch 68/100\n",
      "1s - loss: 2.0174 - acc: 0.4656 - val_loss: 2.1958 - val_acc: 0.4404\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9873 - acc: 0.4707 - val_loss: 2.2216 - val_acc: 0.4400\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9819 - acc: 0.4712 - val_loss: 2.1947 - val_acc: 0.4387\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9717 - acc: 0.4738 - val_loss: 2.1904 - val_acc: 0.4454\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9671 - acc: 0.4769 - val_loss: 2.2355 - val_acc: 0.4359\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9880 - acc: 0.4715 - val_loss: 2.2103 - val_acc: 0.4440\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9940 - acc: 0.4699 - val_loss: 2.2199 - val_acc: 0.4367\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9736 - acc: 0.4755 - val_loss: 2.2432 - val_acc: 0.4409\n",
      "Epoch 76/100\n",
      "1s - loss: 2.0316 - acc: 0.4650 - val_loss: 2.2678 - val_acc: 0.4262\n",
      "Epoch 77/100\n",
      "1s - loss: 2.0356 - acc: 0.4628 - val_loss: 2.2408 - val_acc: 0.4372\n",
      "Epoch 78/100\n",
      "1s - loss: 2.0120 - acc: 0.4678 - val_loss: 2.2139 - val_acc: 0.4398\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9857 - acc: 0.4730 - val_loss: 2.2054 - val_acc: 0.4420\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9576 - acc: 0.4780 - val_loss: 2.1845 - val_acc: 0.4428\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9597 - acc: 0.4778 - val_loss: 2.1958 - val_acc: 0.4432\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9599 - acc: 0.4773 - val_loss: 2.1701 - val_acc: 0.4447\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9484 - acc: 0.4801 - val_loss: 2.1913 - val_acc: 0.4443\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9537 - acc: 0.4789 - val_loss: 2.1847 - val_acc: 0.4418\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9575 - acc: 0.4775 - val_loss: 2.2314 - val_acc: 0.4387\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9662 - acc: 0.4779 - val_loss: 2.1837 - val_acc: 0.4438\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9510 - acc: 0.4785 - val_loss: 2.2177 - val_acc: 0.4393\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9680 - acc: 0.4756 - val_loss: 2.1888 - val_acc: 0.4444\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9459 - acc: 0.4808 - val_loss: 2.1833 - val_acc: 0.4478\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9514 - acc: 0.4792 - val_loss: 2.1939 - val_acc: 0.4434\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9526 - acc: 0.4803 - val_loss: 2.1881 - val_acc: 0.4450\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9425 - acc: 0.4812 - val_loss: 2.1837 - val_acc: 0.4442\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9413 - acc: 0.4808 - val_loss: 2.2017 - val_acc: 0.4406\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9395 - acc: 0.4810 - val_loss: 2.1972 - val_acc: 0.4420\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9454 - acc: 0.4802 - val_loss: 2.2311 - val_acc: 0.4344\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9456 - acc: 0.4804 - val_loss: 2.1933 - val_acc: 0.4424\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9349 - acc: 0.4835 - val_loss: 2.2082 - val_acc: 0.4402\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9332 - acc: 0.4811 - val_loss: 2.1878 - val_acc: 0.4444\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9403 - acc: 0.4819 - val_loss: 2.1758 - val_acc: 0.4450\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9226 - acc: 0.4868 - val_loss: 2.1672 - val_acc: 0.4448\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1882 - acc: 0.4272 - val_loss: 2.1821 - val_acc: 0.4401\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1695 - acc: 0.4303 - val_loss: 2.1861 - val_acc: 0.4414\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1524 - acc: 0.4334 - val_loss: 2.1828 - val_acc: 0.4414\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1447 - acc: 0.4346 - val_loss: 2.1805 - val_acc: 0.4387\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1365 - acc: 0.4358 - val_loss: 2.1931 - val_acc: 0.4393\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1294 - acc: 0.4366 - val_loss: 2.1957 - val_acc: 0.4370\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1239 - acc: 0.4361 - val_loss: 2.1926 - val_acc: 0.4390\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1238 - acc: 0.4414 - val_loss: 2.2070 - val_acc: 0.4364\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1085 - acc: 0.4407 - val_loss: 2.1979 - val_acc: 0.4405\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1070 - acc: 0.4404 - val_loss: 2.2060 - val_acc: 0.4358\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1048 - acc: 0.4433 - val_loss: 2.2162 - val_acc: 0.4344\n",
      "Epoch 12/100\n",
      "1s - loss: 2.1039 - acc: 0.4426 - val_loss: 2.1986 - val_acc: 0.4378\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0926 - acc: 0.4438 - val_loss: 2.1913 - val_acc: 0.4399\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0895 - acc: 0.4453 - val_loss: 2.1934 - val_acc: 0.4397\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0790 - acc: 0.4489 - val_loss: 2.1943 - val_acc: 0.4389\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0738 - acc: 0.4486 - val_loss: 2.2003 - val_acc: 0.4369\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0896 - acc: 0.4459 - val_loss: 2.1958 - val_acc: 0.4368\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0732 - acc: 0.4484 - val_loss: 2.1965 - val_acc: 0.4377\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0663 - acc: 0.4506 - val_loss: 2.1952 - val_acc: 0.4387\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0648 - acc: 0.4524 - val_loss: 2.1938 - val_acc: 0.4410\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0622 - acc: 0.4509 - val_loss: 2.1953 - val_acc: 0.4384\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0534 - acc: 0.4533 - val_loss: 2.1854 - val_acc: 0.4396\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0577 - acc: 0.4526 - val_loss: 2.1941 - val_acc: 0.4409\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0617 - acc: 0.4514 - val_loss: 2.1927 - val_acc: 0.4384\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0530 - acc: 0.4543 - val_loss: 2.1918 - val_acc: 0.4398\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0532 - acc: 0.4540 - val_loss: 2.2006 - val_acc: 0.4389\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0532 - acc: 0.4536 - val_loss: 2.1975 - val_acc: 0.4386\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0415 - acc: 0.4562 - val_loss: 2.1869 - val_acc: 0.4393\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0399 - acc: 0.4552 - val_loss: 2.1780 - val_acc: 0.4399\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0338 - acc: 0.4573 - val_loss: 2.1918 - val_acc: 0.4398\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0327 - acc: 0.4591 - val_loss: 2.1862 - val_acc: 0.4402\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0285 - acc: 0.4579 - val_loss: 2.1976 - val_acc: 0.4389\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0382 - acc: 0.4573 - val_loss: 2.1822 - val_acc: 0.4400\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0282 - acc: 0.4584 - val_loss: 2.1922 - val_acc: 0.4396\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0231 - acc: 0.4601 - val_loss: 2.1850 - val_acc: 0.4398\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0244 - acc: 0.4593 - val_loss: 2.1968 - val_acc: 0.4392\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0193 - acc: 0.4624 - val_loss: 2.1829 - val_acc: 0.4398\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0134 - acc: 0.4604 - val_loss: 2.1878 - val_acc: 0.4401\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0150 - acc: 0.4614 - val_loss: 2.1807 - val_acc: 0.4410\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0111 - acc: 0.4636 - val_loss: 2.1831 - val_acc: 0.4405\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0119 - acc: 0.4631 - val_loss: 2.1825 - val_acc: 0.4398\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0148 - acc: 0.4624 - val_loss: 2.1867 - val_acc: 0.4407\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0137 - acc: 0.4621 - val_loss: 2.1823 - val_acc: 0.4429\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0149 - acc: 0.4599 - val_loss: 2.1720 - val_acc: 0.4428\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0059 - acc: 0.4638 - val_loss: 2.1768 - val_acc: 0.4441\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0058 - acc: 0.4629 - val_loss: 2.1931 - val_acc: 0.4399\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0211 - acc: 0.4617 - val_loss: 2.1874 - val_acc: 0.4437\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0025 - acc: 0.4637 - val_loss: 2.1823 - val_acc: 0.4417\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0030 - acc: 0.4637 - val_loss: 2.1890 - val_acc: 0.4420\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0134 - acc: 0.4617 - val_loss: 2.2009 - val_acc: 0.4395\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0145 - acc: 0.4631 - val_loss: 2.1851 - val_acc: 0.4424\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0022 - acc: 0.4637 - val_loss: 2.1721 - val_acc: 0.4445\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9857 - acc: 0.4687 - val_loss: 2.2019 - val_acc: 0.4375\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9955 - acc: 0.4670 - val_loss: 2.1772 - val_acc: 0.4421\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9896 - acc: 0.4667 - val_loss: 2.1921 - val_acc: 0.4392\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9948 - acc: 0.4673 - val_loss: 2.1757 - val_acc: 0.4442\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9869 - acc: 0.4682 - val_loss: 2.1986 - val_acc: 0.4385\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0085 - acc: 0.4627 - val_loss: 2.1869 - val_acc: 0.4418\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9793 - acc: 0.4684 - val_loss: 2.1966 - val_acc: 0.4418\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9767 - acc: 0.4700 - val_loss: 2.1917 - val_acc: 0.4411\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9867 - acc: 0.4683 - val_loss: 2.1960 - val_acc: 0.4401\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9793 - acc: 0.4702 - val_loss: 2.1949 - val_acc: 0.4409\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9819 - acc: 0.4680 - val_loss: 2.2076 - val_acc: 0.4399\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9877 - acc: 0.4676 - val_loss: 2.2151 - val_acc: 0.4368\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9898 - acc: 0.4660 - val_loss: 2.1971 - val_acc: 0.4396\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9788 - acc: 0.4702 - val_loss: 2.1873 - val_acc: 0.4424\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9708 - acc: 0.4713 - val_loss: 2.2018 - val_acc: 0.4429\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9978 - acc: 0.4668 - val_loss: 2.2086 - val_acc: 0.4391\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9702 - acc: 0.4726 - val_loss: 2.1873 - val_acc: 0.4427\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9653 - acc: 0.4727 - val_loss: 2.1942 - val_acc: 0.4410\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9584 - acc: 0.4745 - val_loss: 2.1924 - val_acc: 0.4409\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9599 - acc: 0.4736 - val_loss: 2.1782 - val_acc: 0.4440\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9456 - acc: 0.4793 - val_loss: 2.1904 - val_acc: 0.4435\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9553 - acc: 0.4767 - val_loss: 2.2017 - val_acc: 0.4409\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9702 - acc: 0.4693 - val_loss: 2.2183 - val_acc: 0.4400\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9677 - acc: 0.4733 - val_loss: 2.2023 - val_acc: 0.4396\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9738 - acc: 0.4702 - val_loss: 2.1875 - val_acc: 0.4449\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9645 - acc: 0.4725 - val_loss: 2.2018 - val_acc: 0.4405\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9649 - acc: 0.4708 - val_loss: 2.1844 - val_acc: 0.4434\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9528 - acc: 0.4756 - val_loss: 2.1872 - val_acc: 0.4425\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9414 - acc: 0.4792 - val_loss: 2.1972 - val_acc: 0.4396\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9462 - acc: 0.4784 - val_loss: 2.1858 - val_acc: 0.4440\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9380 - acc: 0.4790 - val_loss: 2.1884 - val_acc: 0.4430\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9452 - acc: 0.4784 - val_loss: 2.1971 - val_acc: 0.4415\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9536 - acc: 0.4755 - val_loss: 2.1867 - val_acc: 0.4433\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9461 - acc: 0.4779 - val_loss: 2.1973 - val_acc: 0.4415\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9414 - acc: 0.4802 - val_loss: 2.1954 - val_acc: 0.4419\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9421 - acc: 0.4783 - val_loss: 2.2183 - val_acc: 0.4394\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9357 - acc: 0.4786 - val_loss: 2.2066 - val_acc: 0.4403\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9434 - acc: 0.4778 - val_loss: 2.2540 - val_acc: 0.4329\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9650 - acc: 0.4723 - val_loss: 2.2256 - val_acc: 0.4404\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9652 - acc: 0.4736 - val_loss: 2.2294 - val_acc: 0.4388\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9471 - acc: 0.4777 - val_loss: 2.2048 - val_acc: 0.4383\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9375 - acc: 0.4806 - val_loss: 2.2015 - val_acc: 0.4404\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9358 - acc: 0.4795 - val_loss: 2.2137 - val_acc: 0.4391\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9445 - acc: 0.4755 - val_loss: 2.1967 - val_acc: 0.4437\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9274 - acc: 0.4802 - val_loss: 2.1965 - val_acc: 0.4406\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9312 - acc: 0.4789 - val_loss: 2.2002 - val_acc: 0.4403\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9390 - acc: 0.4793 - val_loss: 2.2086 - val_acc: 0.4411\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9367 - acc: 0.4795 - val_loss: 2.2008 - val_acc: 0.4412\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1734 - acc: 0.4283 - val_loss: 2.1801 - val_acc: 0.4445\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1470 - acc: 0.4351 - val_loss: 2.1845 - val_acc: 0.4431\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1361 - acc: 0.4351 - val_loss: 2.1874 - val_acc: 0.4418\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1268 - acc: 0.4399 - val_loss: 2.1890 - val_acc: 0.4409\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1154 - acc: 0.4417 - val_loss: 2.2143 - val_acc: 0.4370\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1132 - acc: 0.4435 - val_loss: 2.2008 - val_acc: 0.4389\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1011 - acc: 0.4452 - val_loss: 2.2053 - val_acc: 0.4377\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0940 - acc: 0.4453 - val_loss: 2.2010 - val_acc: 0.4399\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0904 - acc: 0.4473 - val_loss: 2.2004 - val_acc: 0.4398\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0861 - acc: 0.4497 - val_loss: 2.1975 - val_acc: 0.4405\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0763 - acc: 0.4501 - val_loss: 2.2105 - val_acc: 0.4369\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0766 - acc: 0.4511 - val_loss: 2.2008 - val_acc: 0.4378\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0731 - acc: 0.4496 - val_loss: 2.2027 - val_acc: 0.4364\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0668 - acc: 0.4527 - val_loss: 2.2020 - val_acc: 0.4397\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0628 - acc: 0.4526 - val_loss: 2.2046 - val_acc: 0.4389\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0555 - acc: 0.4541 - val_loss: 2.2133 - val_acc: 0.4362\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0548 - acc: 0.4537 - val_loss: 2.1983 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0499 - acc: 0.4538 - val_loss: 2.1960 - val_acc: 0.4377\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0466 - acc: 0.4560 - val_loss: 2.1898 - val_acc: 0.4407\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0428 - acc: 0.4573 - val_loss: 2.2011 - val_acc: 0.4376\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0436 - acc: 0.4573 - val_loss: 2.1941 - val_acc: 0.4410\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0384 - acc: 0.4593 - val_loss: 2.1865 - val_acc: 0.4409\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0346 - acc: 0.4593 - val_loss: 2.1812 - val_acc: 0.4432\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0331 - acc: 0.4605 - val_loss: 2.1828 - val_acc: 0.4414\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0274 - acc: 0.4616 - val_loss: 2.1881 - val_acc: 0.4420\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0431 - acc: 0.4574 - val_loss: 2.1915 - val_acc: 0.4403\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0292 - acc: 0.4608 - val_loss: 2.1831 - val_acc: 0.4426\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0232 - acc: 0.4619 - val_loss: 2.1913 - val_acc: 0.4415\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0217 - acc: 0.4625 - val_loss: 2.1833 - val_acc: 0.4439\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0149 - acc: 0.4633 - val_loss: 2.1942 - val_acc: 0.4401\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0192 - acc: 0.4637 - val_loss: 2.1834 - val_acc: 0.4414\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0116 - acc: 0.4663 - val_loss: 2.1812 - val_acc: 0.4441\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0091 - acc: 0.4658 - val_loss: 2.1811 - val_acc: 0.4413\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0048 - acc: 0.4677 - val_loss: 2.1779 - val_acc: 0.4424\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0039 - acc: 0.4662 - val_loss: 2.2057 - val_acc: 0.4386\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0056 - acc: 0.4653 - val_loss: 2.1846 - val_acc: 0.4437\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9994 - acc: 0.4685 - val_loss: 2.1875 - val_acc: 0.4435\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0019 - acc: 0.4675 - val_loss: 2.1884 - val_acc: 0.4422\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0008 - acc: 0.4666 - val_loss: 2.1926 - val_acc: 0.4423\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0011 - acc: 0.4673 - val_loss: 2.2211 - val_acc: 0.4373\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9992 - acc: 0.4676 - val_loss: 2.1912 - val_acc: 0.4425\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9977 - acc: 0.4693 - val_loss: 2.1868 - val_acc: 0.4420\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9956 - acc: 0.4682 - val_loss: 2.1876 - val_acc: 0.4428\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9956 - acc: 0.4698 - val_loss: 2.1868 - val_acc: 0.4427\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9880 - acc: 0.4704 - val_loss: 2.1739 - val_acc: 0.4468\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9812 - acc: 0.4722 - val_loss: 2.1837 - val_acc: 0.4454\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9820 - acc: 0.4702 - val_loss: 2.1718 - val_acc: 0.4461\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9735 - acc: 0.4726 - val_loss: 2.1689 - val_acc: 0.4457\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9695 - acc: 0.4747 - val_loss: 2.1727 - val_acc: 0.4441\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9724 - acc: 0.4755 - val_loss: 2.1851 - val_acc: 0.4417\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9694 - acc: 0.4741 - val_loss: 2.1772 - val_acc: 0.4444\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9681 - acc: 0.4754 - val_loss: 2.1794 - val_acc: 0.4465\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9661 - acc: 0.4745 - val_loss: 2.1847 - val_acc: 0.4431\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9676 - acc: 0.4773 - val_loss: 2.1689 - val_acc: 0.4476\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9653 - acc: 0.4772 - val_loss: 2.1980 - val_acc: 0.4403\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9621 - acc: 0.4764 - val_loss: 2.1874 - val_acc: 0.4432\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9626 - acc: 0.4748 - val_loss: 2.2077 - val_acc: 0.4374\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9733 - acc: 0.4733 - val_loss: 2.2061 - val_acc: 0.4435\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9765 - acc: 0.4728 - val_loss: 2.1989 - val_acc: 0.4403\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9603 - acc: 0.4763 - val_loss: 2.1790 - val_acc: 0.4450\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9599 - acc: 0.4773 - val_loss: 2.2000 - val_acc: 0.4391\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9644 - acc: 0.4744 - val_loss: 2.1831 - val_acc: 0.4442\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9583 - acc: 0.4794 - val_loss: 2.1816 - val_acc: 0.4438\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9490 - acc: 0.4801 - val_loss: 2.1842 - val_acc: 0.4460\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9499 - acc: 0.4799 - val_loss: 2.1817 - val_acc: 0.4439\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9508 - acc: 0.4753 - val_loss: 2.1967 - val_acc: 0.4431\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9431 - acc: 0.4806 - val_loss: 2.1982 - val_acc: 0.4434\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9432 - acc: 0.4803 - val_loss: 2.1791 - val_acc: 0.4453\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9447 - acc: 0.4797 - val_loss: 2.1927 - val_acc: 0.4429\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9480 - acc: 0.4805 - val_loss: 2.1882 - val_acc: 0.4432\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9350 - acc: 0.4826 - val_loss: 2.1920 - val_acc: 0.4413\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9378 - acc: 0.4808 - val_loss: 2.1721 - val_acc: 0.4469\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9341 - acc: 0.4825 - val_loss: 2.1745 - val_acc: 0.4447\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9356 - acc: 0.4847 - val_loss: 2.1760 - val_acc: 0.4444\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9326 - acc: 0.4820 - val_loss: 2.2000 - val_acc: 0.4430\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9341 - acc: 0.4826 - val_loss: 2.1758 - val_acc: 0.4433\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9256 - acc: 0.4846 - val_loss: 2.1767 - val_acc: 0.4456\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9259 - acc: 0.4879 - val_loss: 2.1887 - val_acc: 0.4404\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9303 - acc: 0.4831 - val_loss: 2.1971 - val_acc: 0.4420\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9458 - acc: 0.4801 - val_loss: 2.2530 - val_acc: 0.4316\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9334 - acc: 0.4837 - val_loss: 2.1867 - val_acc: 0.4440\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9329 - acc: 0.4849 - val_loss: 2.2027 - val_acc: 0.4415\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9284 - acc: 0.4836 - val_loss: 2.2043 - val_acc: 0.4420\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9376 - acc: 0.4811 - val_loss: 2.2248 - val_acc: 0.4355\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9289 - acc: 0.4851 - val_loss: 2.2040 - val_acc: 0.4421\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9383 - acc: 0.4823 - val_loss: 2.2220 - val_acc: 0.4377\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9283 - acc: 0.4856 - val_loss: 2.1851 - val_acc: 0.4439\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9231 - acc: 0.4852 - val_loss: 2.2549 - val_acc: 0.4333\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9277 - acc: 0.4848 - val_loss: 2.1870 - val_acc: 0.4441\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9237 - acc: 0.4858 - val_loss: 2.2189 - val_acc: 0.4403\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9204 - acc: 0.4852 - val_loss: 2.2018 - val_acc: 0.4432\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9280 - acc: 0.4846 - val_loss: 2.2333 - val_acc: 0.4382\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9243 - acc: 0.4854 - val_loss: 2.1856 - val_acc: 0.4460\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9041 - acc: 0.4890 - val_loss: 2.2090 - val_acc: 0.4412\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9085 - acc: 0.4885 - val_loss: 2.1954 - val_acc: 0.4451\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9178 - acc: 0.4853 - val_loss: 2.1821 - val_acc: 0.4477\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9083 - acc: 0.4899 - val_loss: 2.1911 - val_acc: 0.4404\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9032 - acc: 0.4904 - val_loss: 2.1846 - val_acc: 0.4448\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8977 - acc: 0.4925 - val_loss: 2.1805 - val_acc: 0.4448\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9011 - acc: 0.4901 - val_loss: 2.1910 - val_acc: 0.4441\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1683 - acc: 0.4297 - val_loss: 2.2038 - val_acc: 0.4439\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1388 - acc: 0.4351 - val_loss: 2.2072 - val_acc: 0.4446\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1267 - acc: 0.4371 - val_loss: 2.2132 - val_acc: 0.4425\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1198 - acc: 0.4377 - val_loss: 2.2053 - val_acc: 0.4424\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1114 - acc: 0.4390 - val_loss: 2.2209 - val_acc: 0.4399\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1103 - acc: 0.4388 - val_loss: 2.2182 - val_acc: 0.4379\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0964 - acc: 0.4430 - val_loss: 2.2175 - val_acc: 0.4356\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0872 - acc: 0.4435 - val_loss: 2.2272 - val_acc: 0.4366\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0848 - acc: 0.4430 - val_loss: 2.2169 - val_acc: 0.4382\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0756 - acc: 0.4485 - val_loss: 2.2389 - val_acc: 0.4363\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0698 - acc: 0.4492 - val_loss: 2.2290 - val_acc: 0.4376\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0632 - acc: 0.4495 - val_loss: 2.2576 - val_acc: 0.4320\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0618 - acc: 0.4506 - val_loss: 2.2384 - val_acc: 0.4364\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0612 - acc: 0.4514 - val_loss: 2.2364 - val_acc: 0.4377\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0553 - acc: 0.4514 - val_loss: 2.2284 - val_acc: 0.4381\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0601 - acc: 0.4491 - val_loss: 2.2303 - val_acc: 0.4380\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0432 - acc: 0.4553 - val_loss: 2.2266 - val_acc: 0.4390\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0427 - acc: 0.4547 - val_loss: 2.2294 - val_acc: 0.4358\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0389 - acc: 0.4546 - val_loss: 2.2302 - val_acc: 0.4349\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0349 - acc: 0.4556 - val_loss: 2.2280 - val_acc: 0.4360\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0301 - acc: 0.4573 - val_loss: 2.2431 - val_acc: 0.4364\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0318 - acc: 0.4579 - val_loss: 2.2173 - val_acc: 0.4387\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0271 - acc: 0.4576 - val_loss: 2.2361 - val_acc: 0.4364\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0275 - acc: 0.4572 - val_loss: 2.2256 - val_acc: 0.4372\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0188 - acc: 0.4568 - val_loss: 2.2280 - val_acc: 0.4366\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0156 - acc: 0.4593 - val_loss: 2.2539 - val_acc: 0.4336\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0154 - acc: 0.4608 - val_loss: 2.2242 - val_acc: 0.4382\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0108 - acc: 0.4595 - val_loss: 2.2328 - val_acc: 0.4364\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0097 - acc: 0.4613 - val_loss: 2.2157 - val_acc: 0.4377\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0102 - acc: 0.4614 - val_loss: 2.2434 - val_acc: 0.4355\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0210 - acc: 0.4599 - val_loss: 2.2373 - val_acc: 0.4348\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0027 - acc: 0.4619 - val_loss: 2.2501 - val_acc: 0.4347\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0323 - acc: 0.4575 - val_loss: 2.2408 - val_acc: 0.4371\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0168 - acc: 0.4609 - val_loss: 2.2494 - val_acc: 0.4362\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0089 - acc: 0.4609 - val_loss: 2.2108 - val_acc: 0.4395\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0019 - acc: 0.4622 - val_loss: 2.2329 - val_acc: 0.4378\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9918 - acc: 0.4641 - val_loss: 2.2107 - val_acc: 0.4407\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9937 - acc: 0.4675 - val_loss: 2.2261 - val_acc: 0.4388\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0018 - acc: 0.4626 - val_loss: 2.2184 - val_acc: 0.4395\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9927 - acc: 0.4645 - val_loss: 2.2305 - val_acc: 0.4371\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9812 - acc: 0.4677 - val_loss: 2.2238 - val_acc: 0.4390\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9809 - acc: 0.4682 - val_loss: 2.2268 - val_acc: 0.4379\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9773 - acc: 0.4686 - val_loss: 2.2060 - val_acc: 0.4425\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9744 - acc: 0.4683 - val_loss: 2.2113 - val_acc: 0.4373\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9729 - acc: 0.4692 - val_loss: 2.2076 - val_acc: 0.4418\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9735 - acc: 0.4689 - val_loss: 2.2222 - val_acc: 0.4390\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9694 - acc: 0.4696 - val_loss: 2.2280 - val_acc: 0.4383\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9732 - acc: 0.4719 - val_loss: 2.2236 - val_acc: 0.4398\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9766 - acc: 0.4696 - val_loss: 2.2331 - val_acc: 0.4403\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9665 - acc: 0.4715 - val_loss: 2.2158 - val_acc: 0.4386\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9654 - acc: 0.4707 - val_loss: 2.2024 - val_acc: 0.4416\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9653 - acc: 0.4704 - val_loss: 2.2208 - val_acc: 0.4376\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9639 - acc: 0.4711 - val_loss: 2.1987 - val_acc: 0.4435\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9692 - acc: 0.4690 - val_loss: 2.2165 - val_acc: 0.4410\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9621 - acc: 0.4719 - val_loss: 2.2144 - val_acc: 0.4433\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9607 - acc: 0.4717 - val_loss: 2.2321 - val_acc: 0.4381\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9713 - acc: 0.4695 - val_loss: 2.2170 - val_acc: 0.4406\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9587 - acc: 0.4731 - val_loss: 2.2500 - val_acc: 0.4361\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9688 - acc: 0.4684 - val_loss: 2.2236 - val_acc: 0.4394\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9591 - acc: 0.4741 - val_loss: 2.2400 - val_acc: 0.4373\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9607 - acc: 0.4721 - val_loss: 2.2523 - val_acc: 0.4349\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9595 - acc: 0.4716 - val_loss: 2.2141 - val_acc: 0.4400\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9418 - acc: 0.4780 - val_loss: 2.2143 - val_acc: 0.4386\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9389 - acc: 0.4789 - val_loss: 2.2297 - val_acc: 0.4406\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9611 - acc: 0.4738 - val_loss: 2.2384 - val_acc: 0.4371\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9417 - acc: 0.4766 - val_loss: 2.2082 - val_acc: 0.4429\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9275 - acc: 0.4795 - val_loss: 2.2117 - val_acc: 0.4413\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9346 - acc: 0.4775 - val_loss: 2.2085 - val_acc: 0.4420\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9257 - acc: 0.4802 - val_loss: 2.2086 - val_acc: 0.4411\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9317 - acc: 0.4772 - val_loss: 2.2016 - val_acc: 0.4432\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9352 - acc: 0.4777 - val_loss: 2.2270 - val_acc: 0.4368\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9350 - acc: 0.4785 - val_loss: 2.2058 - val_acc: 0.4437\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9354 - acc: 0.4785 - val_loss: 2.2348 - val_acc: 0.4397\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9690 - acc: 0.4699 - val_loss: 2.2332 - val_acc: 0.4378\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9574 - acc: 0.4752 - val_loss: 2.2112 - val_acc: 0.4409\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9235 - acc: 0.4807 - val_loss: 2.2002 - val_acc: 0.4441\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9269 - acc: 0.4783 - val_loss: 2.2209 - val_acc: 0.4389\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9302 - acc: 0.4805 - val_loss: 2.2033 - val_acc: 0.4430\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9261 - acc: 0.4787 - val_loss: 2.2395 - val_acc: 0.4380\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9406 - acc: 0.4766 - val_loss: 2.2353 - val_acc: 0.4406\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9299 - acc: 0.4806 - val_loss: 2.2344 - val_acc: 0.4381\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9284 - acc: 0.4805 - val_loss: 2.2388 - val_acc: 0.4379\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9196 - acc: 0.4839 - val_loss: 2.2118 - val_acc: 0.4416\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9079 - acc: 0.4822 - val_loss: 2.2241 - val_acc: 0.4421\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9115 - acc: 0.4846 - val_loss: 2.2205 - val_acc: 0.4400\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9201 - acc: 0.4787 - val_loss: 2.2109 - val_acc: 0.4414\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9352 - acc: 0.4792 - val_loss: 2.2262 - val_acc: 0.4395\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9078 - acc: 0.4844 - val_loss: 2.2192 - val_acc: 0.4417\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9100 - acc: 0.4840 - val_loss: 2.2247 - val_acc: 0.4385\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9121 - acc: 0.4834 - val_loss: 2.2377 - val_acc: 0.4377\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9183 - acc: 0.4812 - val_loss: 2.2354 - val_acc: 0.4369\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9120 - acc: 0.4838 - val_loss: 2.2328 - val_acc: 0.4369\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9035 - acc: 0.4855 - val_loss: 2.2211 - val_acc: 0.4400\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9043 - acc: 0.4868 - val_loss: 2.2222 - val_acc: 0.4399\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9059 - acc: 0.4854 - val_loss: 2.2184 - val_acc: 0.4393\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8976 - acc: 0.4869 - val_loss: 2.2142 - val_acc: 0.4397\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9109 - acc: 0.4815 - val_loss: 2.2430 - val_acc: 0.4383\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8995 - acc: 0.4860 - val_loss: 2.2034 - val_acc: 0.4421\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8951 - acc: 0.4864 - val_loss: 2.2168 - val_acc: 0.4393\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8937 - acc: 0.4863 - val_loss: 2.2165 - val_acc: 0.4430\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1988 - acc: 0.4245 - val_loss: 2.1525 - val_acc: 0.4502\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1681 - acc: 0.4319 - val_loss: 2.1475 - val_acc: 0.4503\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1544 - acc: 0.4335 - val_loss: 2.1548 - val_acc: 0.4487\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1427 - acc: 0.4364 - val_loss: 2.1718 - val_acc: 0.4475\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1352 - acc: 0.4362 - val_loss: 2.1633 - val_acc: 0.4472\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1276 - acc: 0.4380 - val_loss: 2.1532 - val_acc: 0.4477\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1218 - acc: 0.4414 - val_loss: 2.1661 - val_acc: 0.4467\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1109 - acc: 0.4419 - val_loss: 2.1608 - val_acc: 0.4493\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1067 - acc: 0.4425 - val_loss: 2.1574 - val_acc: 0.4486\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1055 - acc: 0.4430 - val_loss: 2.1716 - val_acc: 0.4466\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1018 - acc: 0.4444 - val_loss: 2.1554 - val_acc: 0.4492\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0945 - acc: 0.4442 - val_loss: 2.1554 - val_acc: 0.4468\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0894 - acc: 0.4472 - val_loss: 2.1776 - val_acc: 0.4447\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0868 - acc: 0.4465 - val_loss: 2.1604 - val_acc: 0.4463\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0784 - acc: 0.4500 - val_loss: 2.1626 - val_acc: 0.4484\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0778 - acc: 0.4507 - val_loss: 2.1665 - val_acc: 0.4462\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0698 - acc: 0.4511 - val_loss: 2.1605 - val_acc: 0.4464\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0680 - acc: 0.4508 - val_loss: 2.1728 - val_acc: 0.4451\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0673 - acc: 0.4516 - val_loss: 2.1674 - val_acc: 0.4452\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0639 - acc: 0.4514 - val_loss: 2.1622 - val_acc: 0.4466\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0629 - acc: 0.4536 - val_loss: 2.1726 - val_acc: 0.4451\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0568 - acc: 0.4547 - val_loss: 2.1571 - val_acc: 0.4485\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0507 - acc: 0.4555 - val_loss: 2.1591 - val_acc: 0.4470\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0492 - acc: 0.4564 - val_loss: 2.1697 - val_acc: 0.4463\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0603 - acc: 0.4541 - val_loss: 2.1608 - val_acc: 0.4477\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0503 - acc: 0.4554 - val_loss: 2.1687 - val_acc: 0.4484\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0433 - acc: 0.4582 - val_loss: 2.1583 - val_acc: 0.4465\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0381 - acc: 0.4599 - val_loss: 2.1823 - val_acc: 0.4465\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0432 - acc: 0.4570 - val_loss: 2.1593 - val_acc: 0.4471\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0397 - acc: 0.4564 - val_loss: 2.1683 - val_acc: 0.4471\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0352 - acc: 0.4577 - val_loss: 2.1727 - val_acc: 0.4460\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0326 - acc: 0.4593 - val_loss: 2.1680 - val_acc: 0.4498\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0386 - acc: 0.4568 - val_loss: 2.1622 - val_acc: 0.4484\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0346 - acc: 0.4596 - val_loss: 2.1613 - val_acc: 0.4502\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0196 - acc: 0.4630 - val_loss: 2.1813 - val_acc: 0.4487\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0567 - acc: 0.4557 - val_loss: 2.2069 - val_acc: 0.4396\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0408 - acc: 0.4577 - val_loss: 2.1634 - val_acc: 0.4494\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0267 - acc: 0.4608 - val_loss: 2.1732 - val_acc: 0.4459\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0178 - acc: 0.4625 - val_loss: 2.1600 - val_acc: 0.4474\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0214 - acc: 0.4629 - val_loss: 2.1929 - val_acc: 0.4449\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0341 - acc: 0.4585 - val_loss: 2.1583 - val_acc: 0.4466\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0101 - acc: 0.4640 - val_loss: 2.1721 - val_acc: 0.4445\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0201 - acc: 0.4632 - val_loss: 2.1834 - val_acc: 0.4416\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0254 - acc: 0.4603 - val_loss: 2.1790 - val_acc: 0.4438\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0006 - acc: 0.4658 - val_loss: 2.1482 - val_acc: 0.4506\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0031 - acc: 0.4647 - val_loss: 2.1470 - val_acc: 0.4501\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9980 - acc: 0.4676 - val_loss: 2.1557 - val_acc: 0.4487\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9927 - acc: 0.4691 - val_loss: 2.1562 - val_acc: 0.4458\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0024 - acc: 0.4650 - val_loss: 2.1596 - val_acc: 0.4485\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9965 - acc: 0.4665 - val_loss: 2.1444 - val_acc: 0.4495\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0038 - acc: 0.4664 - val_loss: 2.1744 - val_acc: 0.4482\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0257 - acc: 0.4611 - val_loss: 2.1492 - val_acc: 0.4489\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9955 - acc: 0.4694 - val_loss: 2.1603 - val_acc: 0.4484\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9833 - acc: 0.4704 - val_loss: 2.1558 - val_acc: 0.4486\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9825 - acc: 0.4714 - val_loss: 2.1524 - val_acc: 0.4502\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9770 - acc: 0.4711 - val_loss: 2.1448 - val_acc: 0.4493\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9786 - acc: 0.4725 - val_loss: 2.1574 - val_acc: 0.4495\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9816 - acc: 0.4716 - val_loss: 2.1528 - val_acc: 0.4465\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9721 - acc: 0.4719 - val_loss: 2.1562 - val_acc: 0.4453\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9666 - acc: 0.4744 - val_loss: 2.1523 - val_acc: 0.4460\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9625 - acc: 0.4748 - val_loss: 2.1498 - val_acc: 0.4488\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9653 - acc: 0.4749 - val_loss: 2.1554 - val_acc: 0.4484\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9652 - acc: 0.4746 - val_loss: 2.1620 - val_acc: 0.4470\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9864 - acc: 0.4701 - val_loss: 2.1787 - val_acc: 0.4447\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9816 - acc: 0.4714 - val_loss: 2.1671 - val_acc: 0.4457\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9765 - acc: 0.4722 - val_loss: 2.1814 - val_acc: 0.4444\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9792 - acc: 0.4711 - val_loss: 2.1538 - val_acc: 0.4469\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9649 - acc: 0.4768 - val_loss: 2.1585 - val_acc: 0.4458\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9578 - acc: 0.4762 - val_loss: 2.1637 - val_acc: 0.4463\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9556 - acc: 0.4778 - val_loss: 2.1609 - val_acc: 0.4455\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9497 - acc: 0.4815 - val_loss: 2.1640 - val_acc: 0.4471\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9501 - acc: 0.4782 - val_loss: 2.1782 - val_acc: 0.4447\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9545 - acc: 0.4780 - val_loss: 2.1954 - val_acc: 0.4457\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9741 - acc: 0.4730 - val_loss: 2.1874 - val_acc: 0.4442\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9560 - acc: 0.4752 - val_loss: 2.1834 - val_acc: 0.4451\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9677 - acc: 0.4733 - val_loss: 2.1897 - val_acc: 0.4448\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9642 - acc: 0.4749 - val_loss: 2.1790 - val_acc: 0.4466\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9649 - acc: 0.4735 - val_loss: 2.1857 - val_acc: 0.4422\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9689 - acc: 0.4710 - val_loss: 2.1913 - val_acc: 0.4438\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9582 - acc: 0.4760 - val_loss: 2.1920 - val_acc: 0.4417\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9504 - acc: 0.4776 - val_loss: 2.1653 - val_acc: 0.4482\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9454 - acc: 0.4781 - val_loss: 2.1571 - val_acc: 0.4472\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9410 - acc: 0.4810 - val_loss: 2.1726 - val_acc: 0.4459\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9425 - acc: 0.4804 - val_loss: 2.1732 - val_acc: 0.4444\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9537 - acc: 0.4781 - val_loss: 2.1808 - val_acc: 0.4441\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9490 - acc: 0.4761 - val_loss: 2.1595 - val_acc: 0.4470\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9312 - acc: 0.4829 - val_loss: 2.1514 - val_acc: 0.4481\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9246 - acc: 0.4835 - val_loss: 2.1829 - val_acc: 0.4432\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9321 - acc: 0.4831 - val_loss: 2.1578 - val_acc: 0.4482\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9263 - acc: 0.4839 - val_loss: 2.1659 - val_acc: 0.4446\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9399 - acc: 0.4799 - val_loss: 2.1562 - val_acc: 0.4471\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9454 - acc: 0.4788 - val_loss: 2.1787 - val_acc: 0.4443\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9385 - acc: 0.4814 - val_loss: 2.1753 - val_acc: 0.4452\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9380 - acc: 0.4801 - val_loss: 2.1755 - val_acc: 0.4454\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9235 - acc: 0.4840 - val_loss: 2.1597 - val_acc: 0.4485\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9441 - acc: 0.4790 - val_loss: 2.1892 - val_acc: 0.4410\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9304 - acc: 0.4792 - val_loss: 2.1613 - val_acc: 0.4488\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9663 - acc: 0.4743 - val_loss: 2.2033 - val_acc: 0.4407\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9393 - acc: 0.4798 - val_loss: 2.1672 - val_acc: 0.4451\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9244 - acc: 0.4857 - val_loss: 2.1938 - val_acc: 0.4431\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2006 - acc: 0.4251 - val_loss: 2.1586 - val_acc: 0.4498\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1618 - acc: 0.4335 - val_loss: 2.1753 - val_acc: 0.4433\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1494 - acc: 0.4355 - val_loss: 2.1705 - val_acc: 0.4450\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1414 - acc: 0.4374 - val_loss: 2.1887 - val_acc: 0.4445\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1329 - acc: 0.4400 - val_loss: 2.1861 - val_acc: 0.4429\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1215 - acc: 0.4410 - val_loss: 2.1907 - val_acc: 0.4441\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1148 - acc: 0.4439 - val_loss: 2.1919 - val_acc: 0.4422\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1098 - acc: 0.4439 - val_loss: 2.1919 - val_acc: 0.4385\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0982 - acc: 0.4477 - val_loss: 2.1996 - val_acc: 0.4377\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0984 - acc: 0.4444 - val_loss: 2.2364 - val_acc: 0.4326\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1050 - acc: 0.4453 - val_loss: 2.2102 - val_acc: 0.4366\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0910 - acc: 0.4470 - val_loss: 2.2090 - val_acc: 0.4402\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0816 - acc: 0.4497 - val_loss: 2.2123 - val_acc: 0.4354\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0804 - acc: 0.4484 - val_loss: 2.2176 - val_acc: 0.4364\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0770 - acc: 0.4509 - val_loss: 2.2003 - val_acc: 0.4389\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0800 - acc: 0.4503 - val_loss: 2.2073 - val_acc: 0.4395\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0636 - acc: 0.4538 - val_loss: 2.2141 - val_acc: 0.4381\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0728 - acc: 0.4508 - val_loss: 2.1955 - val_acc: 0.4382\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0598 - acc: 0.4545 - val_loss: 2.1969 - val_acc: 0.4378\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0551 - acc: 0.4554 - val_loss: 2.1901 - val_acc: 0.4380\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0538 - acc: 0.4548 - val_loss: 2.2028 - val_acc: 0.4381\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0548 - acc: 0.4556 - val_loss: 2.1971 - val_acc: 0.4395\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0457 - acc: 0.4569 - val_loss: 2.1967 - val_acc: 0.4377\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0411 - acc: 0.4594 - val_loss: 2.2055 - val_acc: 0.4342\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0423 - acc: 0.4582 - val_loss: 2.1865 - val_acc: 0.4397\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0420 - acc: 0.4572 - val_loss: 2.1999 - val_acc: 0.4368\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0382 - acc: 0.4590 - val_loss: 2.1893 - val_acc: 0.4404\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0281 - acc: 0.4607 - val_loss: 2.2083 - val_acc: 0.4365\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0319 - acc: 0.4598 - val_loss: 2.1955 - val_acc: 0.4397\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0367 - acc: 0.4616 - val_loss: 2.1915 - val_acc: 0.4397\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0252 - acc: 0.4616 - val_loss: 2.2131 - val_acc: 0.4370\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0230 - acc: 0.4634 - val_loss: 2.1960 - val_acc: 0.4371\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0229 - acc: 0.4632 - val_loss: 2.2188 - val_acc: 0.4356\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0166 - acc: 0.4642 - val_loss: 2.1975 - val_acc: 0.4360\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0252 - acc: 0.4612 - val_loss: 2.1940 - val_acc: 0.4389\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0167 - acc: 0.4644 - val_loss: 2.1868 - val_acc: 0.4406\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0097 - acc: 0.4646 - val_loss: 2.1954 - val_acc: 0.4392\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0147 - acc: 0.4639 - val_loss: 2.1965 - val_acc: 0.4391\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0215 - acc: 0.4635 - val_loss: 2.1847 - val_acc: 0.4435\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0107 - acc: 0.4663 - val_loss: 2.1787 - val_acc: 0.4415\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0070 - acc: 0.4659 - val_loss: 2.1823 - val_acc: 0.4432\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0129 - acc: 0.4660 - val_loss: 2.1920 - val_acc: 0.4395\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0106 - acc: 0.4653 - val_loss: 2.1785 - val_acc: 0.4424\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9978 - acc: 0.4690 - val_loss: 2.1922 - val_acc: 0.4395\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0037 - acc: 0.4662 - val_loss: 2.1772 - val_acc: 0.4432\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9938 - acc: 0.4702 - val_loss: 2.2113 - val_acc: 0.4380\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0057 - acc: 0.4664 - val_loss: 2.1927 - val_acc: 0.4430\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0042 - acc: 0.4678 - val_loss: 2.2092 - val_acc: 0.4396\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0024 - acc: 0.4654 - val_loss: 2.1767 - val_acc: 0.4449\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9988 - acc: 0.4707 - val_loss: 2.2014 - val_acc: 0.4424\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9932 - acc: 0.4691 - val_loss: 2.1885 - val_acc: 0.4412\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9953 - acc: 0.4662 - val_loss: 2.1953 - val_acc: 0.4390\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9995 - acc: 0.4709 - val_loss: 2.1663 - val_acc: 0.4459\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9767 - acc: 0.4727 - val_loss: 2.1687 - val_acc: 0.4454\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9726 - acc: 0.4747 - val_loss: 2.1706 - val_acc: 0.4443\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9757 - acc: 0.4756 - val_loss: 2.1776 - val_acc: 0.4423\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9695 - acc: 0.4731 - val_loss: 2.1657 - val_acc: 0.4422\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9631 - acc: 0.4761 - val_loss: 2.1645 - val_acc: 0.4451\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9736 - acc: 0.4733 - val_loss: 2.1749 - val_acc: 0.4473\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9758 - acc: 0.4719 - val_loss: 2.1945 - val_acc: 0.4411\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9837 - acc: 0.4717 - val_loss: 2.1750 - val_acc: 0.4454\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9736 - acc: 0.4745 - val_loss: 2.1831 - val_acc: 0.4430\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9795 - acc: 0.4737 - val_loss: 2.1655 - val_acc: 0.4437\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9645 - acc: 0.4748 - val_loss: 2.2007 - val_acc: 0.4414\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9871 - acc: 0.4713 - val_loss: 2.1676 - val_acc: 0.4447\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9555 - acc: 0.4769 - val_loss: 2.1887 - val_acc: 0.4418\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9606 - acc: 0.4773 - val_loss: 2.1866 - val_acc: 0.4403\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9705 - acc: 0.4742 - val_loss: 2.1908 - val_acc: 0.4440\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9749 - acc: 0.4742 - val_loss: 2.1683 - val_acc: 0.4467\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9566 - acc: 0.4762 - val_loss: 2.1739 - val_acc: 0.4427\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9575 - acc: 0.4785 - val_loss: 2.1631 - val_acc: 0.4435\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9503 - acc: 0.4794 - val_loss: 2.1706 - val_acc: 0.4447\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9538 - acc: 0.4782 - val_loss: 2.1602 - val_acc: 0.4457\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9491 - acc: 0.4787 - val_loss: 2.1779 - val_acc: 0.4436\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9421 - acc: 0.4813 - val_loss: 2.1864 - val_acc: 0.4431\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9526 - acc: 0.4777 - val_loss: 2.1791 - val_acc: 0.4451\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9551 - acc: 0.4786 - val_loss: 2.1643 - val_acc: 0.4436\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9383 - acc: 0.4800 - val_loss: 2.1724 - val_acc: 0.4448\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9435 - acc: 0.4815 - val_loss: 2.1844 - val_acc: 0.4428\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9522 - acc: 0.4799 - val_loss: 2.1867 - val_acc: 0.4408\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9496 - acc: 0.4802 - val_loss: 2.1861 - val_acc: 0.4444\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9419 - acc: 0.4805 - val_loss: 2.1753 - val_acc: 0.4459\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9512 - acc: 0.4787 - val_loss: 2.1761 - val_acc: 0.4455\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9341 - acc: 0.4812 - val_loss: 2.1836 - val_acc: 0.4455\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9448 - acc: 0.4795 - val_loss: 2.1934 - val_acc: 0.4437\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9396 - acc: 0.4800 - val_loss: 2.1723 - val_acc: 0.4420\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9288 - acc: 0.4856 - val_loss: 2.1640 - val_acc: 0.4466\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9242 - acc: 0.4836 - val_loss: 2.1718 - val_acc: 0.4442\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9303 - acc: 0.4852 - val_loss: 2.1840 - val_acc: 0.4446\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9308 - acc: 0.4846 - val_loss: 2.1729 - val_acc: 0.4450\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9256 - acc: 0.4831 - val_loss: 2.1909 - val_acc: 0.4446\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9227 - acc: 0.4852 - val_loss: 2.1735 - val_acc: 0.4452\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9227 - acc: 0.4858 - val_loss: 2.1806 - val_acc: 0.4423\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9231 - acc: 0.4846 - val_loss: 2.1844 - val_acc: 0.4422\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9342 - acc: 0.4823 - val_loss: 2.2192 - val_acc: 0.4406\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9407 - acc: 0.4809 - val_loss: 2.1786 - val_acc: 0.4458\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9464 - acc: 0.4805 - val_loss: 2.1826 - val_acc: 0.4441\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9219 - acc: 0.4851 - val_loss: 2.1819 - val_acc: 0.4452\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9154 - acc: 0.4864 - val_loss: 2.2044 - val_acc: 0.4413\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9170 - acc: 0.4869 - val_loss: 2.1867 - val_acc: 0.4433\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1635 - acc: 0.4314 - val_loss: 2.1866 - val_acc: 0.4358\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1428 - acc: 0.4383 - val_loss: 2.1892 - val_acc: 0.4357\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1210 - acc: 0.4431 - val_loss: 2.2334 - val_acc: 0.4293\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1266 - acc: 0.4401 - val_loss: 2.2043 - val_acc: 0.4345\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1091 - acc: 0.4434 - val_loss: 2.2234 - val_acc: 0.4304\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1063 - acc: 0.4455 - val_loss: 2.2248 - val_acc: 0.4305\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1000 - acc: 0.4463 - val_loss: 2.2258 - val_acc: 0.4298\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0949 - acc: 0.4480 - val_loss: 2.2199 - val_acc: 0.4307\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0859 - acc: 0.4477 - val_loss: 2.2390 - val_acc: 0.4291\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0815 - acc: 0.4476 - val_loss: 2.2325 - val_acc: 0.4305\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0840 - acc: 0.4473 - val_loss: 2.2331 - val_acc: 0.4269\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0700 - acc: 0.4511 - val_loss: 2.2479 - val_acc: 0.4266\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0662 - acc: 0.4527 - val_loss: 2.2850 - val_acc: 0.4200\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0691 - acc: 0.4509 - val_loss: 2.2379 - val_acc: 0.4253\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0559 - acc: 0.4563 - val_loss: 2.2554 - val_acc: 0.4214\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0579 - acc: 0.4544 - val_loss: 2.2419 - val_acc: 0.4260\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0529 - acc: 0.4535 - val_loss: 2.2446 - val_acc: 0.4235\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0514 - acc: 0.4573 - val_loss: 2.2515 - val_acc: 0.4243\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0527 - acc: 0.4551 - val_loss: 2.2603 - val_acc: 0.4252\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0397 - acc: 0.4593 - val_loss: 2.2438 - val_acc: 0.4282\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0483 - acc: 0.4569 - val_loss: 2.2328 - val_acc: 0.4263\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0318 - acc: 0.4594 - val_loss: 2.2651 - val_acc: 0.4234\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0335 - acc: 0.4596 - val_loss: 2.2499 - val_acc: 0.4278\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0356 - acc: 0.4583 - val_loss: 2.2382 - val_acc: 0.4286\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0204 - acc: 0.4624 - val_loss: 2.2393 - val_acc: 0.4282\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0250 - acc: 0.4615 - val_loss: 2.2757 - val_acc: 0.4231\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0271 - acc: 0.4605 - val_loss: 2.2331 - val_acc: 0.4284\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0275 - acc: 0.4604 - val_loss: 2.2383 - val_acc: 0.4270\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0157 - acc: 0.4632 - val_loss: 2.2669 - val_acc: 0.4236\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0208 - acc: 0.4624 - val_loss: 2.2380 - val_acc: 0.4282\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0149 - acc: 0.4645 - val_loss: 2.2443 - val_acc: 0.4261\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0099 - acc: 0.4647 - val_loss: 2.2411 - val_acc: 0.4293\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0102 - acc: 0.4633 - val_loss: 2.2361 - val_acc: 0.4275\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0057 - acc: 0.4662 - val_loss: 2.2370 - val_acc: 0.4284\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9956 - acc: 0.4673 - val_loss: 2.2248 - val_acc: 0.4311\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9947 - acc: 0.4690 - val_loss: 2.2248 - val_acc: 0.4315\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9977 - acc: 0.4677 - val_loss: 2.2220 - val_acc: 0.4299\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0019 - acc: 0.4663 - val_loss: 2.2122 - val_acc: 0.4329\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0012 - acc: 0.4675 - val_loss: 2.2578 - val_acc: 0.4239\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0031 - acc: 0.4676 - val_loss: 2.2237 - val_acc: 0.4324\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9861 - acc: 0.4707 - val_loss: 2.2107 - val_acc: 0.4321\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9828 - acc: 0.4678 - val_loss: 2.2159 - val_acc: 0.4318\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9780 - acc: 0.4714 - val_loss: 2.2248 - val_acc: 0.4289\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9863 - acc: 0.4683 - val_loss: 2.2140 - val_acc: 0.4323\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9841 - acc: 0.4695 - val_loss: 2.2196 - val_acc: 0.4296\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9851 - acc: 0.4673 - val_loss: 2.2365 - val_acc: 0.4281\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9839 - acc: 0.4708 - val_loss: 2.2186 - val_acc: 0.4328\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9831 - acc: 0.4716 - val_loss: 2.2144 - val_acc: 0.4320\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9808 - acc: 0.4708 - val_loss: 2.1998 - val_acc: 0.4360\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9707 - acc: 0.4726 - val_loss: 2.2026 - val_acc: 0.4331\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9695 - acc: 0.4734 - val_loss: 2.2132 - val_acc: 0.4346\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9740 - acc: 0.4706 - val_loss: 2.2095 - val_acc: 0.4356\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9902 - acc: 0.4693 - val_loss: 2.2110 - val_acc: 0.4318\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9687 - acc: 0.4736 - val_loss: 2.2082 - val_acc: 0.4321\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9630 - acc: 0.4754 - val_loss: 2.1985 - val_acc: 0.4364\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9610 - acc: 0.4749 - val_loss: 2.1892 - val_acc: 0.4370\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9609 - acc: 0.4735 - val_loss: 2.1983 - val_acc: 0.4377\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9638 - acc: 0.4742 - val_loss: 2.1987 - val_acc: 0.4365\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9562 - acc: 0.4772 - val_loss: 2.1983 - val_acc: 0.4340\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9508 - acc: 0.4784 - val_loss: 2.2084 - val_acc: 0.4336\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9586 - acc: 0.4771 - val_loss: 2.1990 - val_acc: 0.4343\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9540 - acc: 0.4774 - val_loss: 2.1920 - val_acc: 0.4358\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9524 - acc: 0.4762 - val_loss: 2.2036 - val_acc: 0.4358\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9600 - acc: 0.4756 - val_loss: 2.2028 - val_acc: 0.4321\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9564 - acc: 0.4776 - val_loss: 2.2147 - val_acc: 0.4316\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9511 - acc: 0.4768 - val_loss: 2.1831 - val_acc: 0.4372\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9481 - acc: 0.4770 - val_loss: 2.2234 - val_acc: 0.4333\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9538 - acc: 0.4758 - val_loss: 2.2021 - val_acc: 0.4361\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9498 - acc: 0.4774 - val_loss: 2.2100 - val_acc: 0.4338\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9606 - acc: 0.4754 - val_loss: 2.2334 - val_acc: 0.4275\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9663 - acc: 0.4753 - val_loss: 2.1944 - val_acc: 0.4345\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9311 - acc: 0.4826 - val_loss: 2.1916 - val_acc: 0.4360\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9244 - acc: 0.4848 - val_loss: 2.1971 - val_acc: 0.4359\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9265 - acc: 0.4842 - val_loss: 2.1952 - val_acc: 0.4341\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9310 - acc: 0.4826 - val_loss: 2.1966 - val_acc: 0.4356\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9264 - acc: 0.4826 - val_loss: 2.2061 - val_acc: 0.4351\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9349 - acc: 0.4796 - val_loss: 2.2214 - val_acc: 0.4342\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9404 - acc: 0.4797 - val_loss: 2.2022 - val_acc: 0.4359\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9391 - acc: 0.4797 - val_loss: 2.2378 - val_acc: 0.4298\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9356 - acc: 0.4806 - val_loss: 2.2162 - val_acc: 0.4341\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9323 - acc: 0.4826 - val_loss: 2.2153 - val_acc: 0.4326\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9210 - acc: 0.4843 - val_loss: 2.1967 - val_acc: 0.4386\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9269 - acc: 0.4823 - val_loss: 2.2189 - val_acc: 0.4315\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9254 - acc: 0.4843 - val_loss: 2.1944 - val_acc: 0.4377\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9235 - acc: 0.4822 - val_loss: 2.2017 - val_acc: 0.4382\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9169 - acc: 0.4836 - val_loss: 2.1947 - val_acc: 0.4378\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9198 - acc: 0.4838 - val_loss: 2.2339 - val_acc: 0.4352\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9592 - acc: 0.4777 - val_loss: 2.2488 - val_acc: 0.4290\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9582 - acc: 0.4754 - val_loss: 2.2289 - val_acc: 0.4329\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9257 - acc: 0.4823 - val_loss: 2.1976 - val_acc: 0.4397\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9109 - acc: 0.4865 - val_loss: 2.2131 - val_acc: 0.4359\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9182 - acc: 0.4850 - val_loss: 2.2008 - val_acc: 0.4357\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9122 - acc: 0.4869 - val_loss: 2.2043 - val_acc: 0.4368\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9015 - acc: 0.4891 - val_loss: 2.1936 - val_acc: 0.4358\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9040 - acc: 0.4889 - val_loss: 2.1987 - val_acc: 0.4372\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9059 - acc: 0.4879 - val_loss: 2.2301 - val_acc: 0.4287\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9100 - acc: 0.4875 - val_loss: 2.2117 - val_acc: 0.4363\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9091 - acc: 0.4859 - val_loss: 2.2331 - val_acc: 0.4335\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9552 - acc: 0.4772 - val_loss: 2.2560 - val_acc: 0.4287\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9477 - acc: 0.4784 - val_loss: 2.2640 - val_acc: 0.4243\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2073 - acc: 0.4253 - val_loss: 2.2287 - val_acc: 0.4336\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1720 - acc: 0.4300 - val_loss: 2.2215 - val_acc: 0.4334\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1552 - acc: 0.4319 - val_loss: 2.2350 - val_acc: 0.4316\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1481 - acc: 0.4367 - val_loss: 2.2244 - val_acc: 0.4323\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1376 - acc: 0.4387 - val_loss: 2.2330 - val_acc: 0.4313\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1313 - acc: 0.4387 - val_loss: 2.2284 - val_acc: 0.4297\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1239 - acc: 0.4399 - val_loss: 2.2272 - val_acc: 0.4281\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1140 - acc: 0.4429 - val_loss: 2.2303 - val_acc: 0.4292\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1103 - acc: 0.4431 - val_loss: 2.2318 - val_acc: 0.4286\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1053 - acc: 0.4420 - val_loss: 2.2407 - val_acc: 0.4264\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1008 - acc: 0.4462 - val_loss: 2.2475 - val_acc: 0.4267\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0976 - acc: 0.4454 - val_loss: 2.2410 - val_acc: 0.4295\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0993 - acc: 0.4451 - val_loss: 2.2384 - val_acc: 0.4277\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0904 - acc: 0.4469 - val_loss: 2.2425 - val_acc: 0.4276\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0870 - acc: 0.4485 - val_loss: 2.2345 - val_acc: 0.4283\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0783 - acc: 0.4495 - val_loss: 2.2505 - val_acc: 0.4276\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0743 - acc: 0.4503 - val_loss: 2.2416 - val_acc: 0.4279\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0762 - acc: 0.4504 - val_loss: 2.2521 - val_acc: 0.4266\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0689 - acc: 0.4523 - val_loss: 2.2416 - val_acc: 0.4267\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0666 - acc: 0.4528 - val_loss: 2.2401 - val_acc: 0.4291\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0657 - acc: 0.4515 - val_loss: 2.2383 - val_acc: 0.4297\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0616 - acc: 0.4533 - val_loss: 2.2425 - val_acc: 0.4293\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0565 - acc: 0.4531 - val_loss: 2.2778 - val_acc: 0.4266\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0786 - acc: 0.4501 - val_loss: 2.2558 - val_acc: 0.4258\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0566 - acc: 0.4549 - val_loss: 2.2490 - val_acc: 0.4293\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0571 - acc: 0.4551 - val_loss: 2.2361 - val_acc: 0.4311\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0445 - acc: 0.4552 - val_loss: 2.2525 - val_acc: 0.4270\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0403 - acc: 0.4582 - val_loss: 2.2339 - val_acc: 0.4326\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0356 - acc: 0.4592 - val_loss: 2.2485 - val_acc: 0.4280\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0395 - acc: 0.4582 - val_loss: 2.2235 - val_acc: 0.4324\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0360 - acc: 0.4601 - val_loss: 2.2384 - val_acc: 0.4313\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0314 - acc: 0.4583 - val_loss: 2.2241 - val_acc: 0.4330\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0260 - acc: 0.4606 - val_loss: 2.2346 - val_acc: 0.4317\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0367 - acc: 0.4597 - val_loss: 2.2358 - val_acc: 0.4309\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0253 - acc: 0.4590 - val_loss: 2.2230 - val_acc: 0.4321\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0193 - acc: 0.4623 - val_loss: 2.2255 - val_acc: 0.4328\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0156 - acc: 0.4641 - val_loss: 2.2299 - val_acc: 0.4290\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0140 - acc: 0.4635 - val_loss: 2.2363 - val_acc: 0.4318\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0110 - acc: 0.4631 - val_loss: 2.2289 - val_acc: 0.4304\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0127 - acc: 0.4635 - val_loss: 2.2399 - val_acc: 0.4306\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0133 - acc: 0.4649 - val_loss: 2.2202 - val_acc: 0.4339\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0144 - acc: 0.4627 - val_loss: 2.2511 - val_acc: 0.4303\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0225 - acc: 0.4618 - val_loss: 2.2283 - val_acc: 0.4330\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0205 - acc: 0.4625 - val_loss: 2.2534 - val_acc: 0.4312\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0146 - acc: 0.4631 - val_loss: 2.2263 - val_acc: 0.4320\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0068 - acc: 0.4660 - val_loss: 2.2462 - val_acc: 0.4290\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0076 - acc: 0.4640 - val_loss: 2.2262 - val_acc: 0.4316\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0065 - acc: 0.4659 - val_loss: 2.2824 - val_acc: 0.4250\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0013 - acc: 0.4665 - val_loss: 2.2306 - val_acc: 0.4313\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9980 - acc: 0.4671 - val_loss: 2.2496 - val_acc: 0.4291\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9971 - acc: 0.4677 - val_loss: 2.2173 - val_acc: 0.4331\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9862 - acc: 0.4697 - val_loss: 2.2411 - val_acc: 0.4308\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9834 - acc: 0.4703 - val_loss: 2.2394 - val_acc: 0.4270\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9890 - acc: 0.4680 - val_loss: 2.2514 - val_acc: 0.4313\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9988 - acc: 0.4674 - val_loss: 2.2534 - val_acc: 0.4284\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9947 - acc: 0.4690 - val_loss: 2.2477 - val_acc: 0.4322\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9905 - acc: 0.4682 - val_loss: 2.2685 - val_acc: 0.4271\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9813 - acc: 0.4702 - val_loss: 2.2358 - val_acc: 0.4305\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9834 - acc: 0.4711 - val_loss: 2.2347 - val_acc: 0.4322\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9795 - acc: 0.4698 - val_loss: 2.2191 - val_acc: 0.4330\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9727 - acc: 0.4735 - val_loss: 2.2406 - val_acc: 0.4289\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9763 - acc: 0.4709 - val_loss: 2.2349 - val_acc: 0.4311\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9756 - acc: 0.4724 - val_loss: 2.2386 - val_acc: 0.4317\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9818 - acc: 0.4707 - val_loss: 2.2500 - val_acc: 0.4308\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9821 - acc: 0.4695 - val_loss: 2.2383 - val_acc: 0.4332\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9767 - acc: 0.4717 - val_loss: 2.2348 - val_acc: 0.4327\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9680 - acc: 0.4752 - val_loss: 2.2352 - val_acc: 0.4324\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9587 - acc: 0.4756 - val_loss: 2.2287 - val_acc: 0.4344\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9735 - acc: 0.4748 - val_loss: 2.2283 - val_acc: 0.4322\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9584 - acc: 0.4757 - val_loss: 2.2242 - val_acc: 0.4334\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9571 - acc: 0.4760 - val_loss: 2.2233 - val_acc: 0.4320\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9594 - acc: 0.4760 - val_loss: 2.2537 - val_acc: 0.4318\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9600 - acc: 0.4750 - val_loss: 2.2309 - val_acc: 0.4289\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9618 - acc: 0.4758 - val_loss: 2.2481 - val_acc: 0.4308\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9617 - acc: 0.4755 - val_loss: 2.2424 - val_acc: 0.4315\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9676 - acc: 0.4738 - val_loss: 2.2626 - val_acc: 0.4305\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9720 - acc: 0.4720 - val_loss: 2.2282 - val_acc: 0.4337\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9581 - acc: 0.4752 - val_loss: 2.2423 - val_acc: 0.4323\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9513 - acc: 0.4787 - val_loss: 2.2329 - val_acc: 0.4306\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9476 - acc: 0.4793 - val_loss: 2.2420 - val_acc: 0.4305\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9456 - acc: 0.4780 - val_loss: 2.2410 - val_acc: 0.4320\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9580 - acc: 0.4748 - val_loss: 2.2463 - val_acc: 0.4303\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9426 - acc: 0.4799 - val_loss: 2.2296 - val_acc: 0.4322\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9360 - acc: 0.4796 - val_loss: 2.2473 - val_acc: 0.4275\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9404 - acc: 0.4781 - val_loss: 2.2248 - val_acc: 0.4335\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9451 - acc: 0.4782 - val_loss: 2.2491 - val_acc: 0.4287\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9424 - acc: 0.4801 - val_loss: 2.2537 - val_acc: 0.4271\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9609 - acc: 0.4751 - val_loss: 2.2742 - val_acc: 0.4314\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9552 - acc: 0.4751 - val_loss: 2.2580 - val_acc: 0.4282\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9550 - acc: 0.4774 - val_loss: 2.2539 - val_acc: 0.4322\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9469 - acc: 0.4776 - val_loss: 2.2266 - val_acc: 0.4340\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9303 - acc: 0.4821 - val_loss: 2.2414 - val_acc: 0.4317\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9312 - acc: 0.4811 - val_loss: 2.2645 - val_acc: 0.4283\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9452 - acc: 0.4783 - val_loss: 2.2703 - val_acc: 0.4299\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9484 - acc: 0.4786 - val_loss: 2.2749 - val_acc: 0.4243\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9383 - acc: 0.4807 - val_loss: 2.2361 - val_acc: 0.4339\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9301 - acc: 0.4838 - val_loss: 2.2246 - val_acc: 0.4328\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9169 - acc: 0.4862 - val_loss: 2.2405 - val_acc: 0.4307\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9216 - acc: 0.4858 - val_loss: 2.2701 - val_acc: 0.4271\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9296 - acc: 0.4819 - val_loss: 2.2423 - val_acc: 0.4336\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1706 - acc: 0.4330 - val_loss: 2.1875 - val_acc: 0.4459\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1418 - acc: 0.4371 - val_loss: 2.1836 - val_acc: 0.4418\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1250 - acc: 0.4412 - val_loss: 2.1822 - val_acc: 0.4448\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1186 - acc: 0.4399 - val_loss: 2.1934 - val_acc: 0.4405\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1122 - acc: 0.4426 - val_loss: 2.2015 - val_acc: 0.4393\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1013 - acc: 0.4445 - val_loss: 2.1989 - val_acc: 0.4374\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0941 - acc: 0.4453 - val_loss: 2.2001 - val_acc: 0.4390\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0903 - acc: 0.4480 - val_loss: 2.2080 - val_acc: 0.4390\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0852 - acc: 0.4485 - val_loss: 2.1997 - val_acc: 0.4395\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0768 - acc: 0.4488 - val_loss: 2.1917 - val_acc: 0.4394\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0731 - acc: 0.4506 - val_loss: 2.1957 - val_acc: 0.4397\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0695 - acc: 0.4524 - val_loss: 2.1918 - val_acc: 0.4396\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0636 - acc: 0.4537 - val_loss: 2.1927 - val_acc: 0.4396\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0608 - acc: 0.4535 - val_loss: 2.1939 - val_acc: 0.4404\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0568 - acc: 0.4557 - val_loss: 2.1910 - val_acc: 0.4407\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0513 - acc: 0.4559 - val_loss: 2.1927 - val_acc: 0.4408\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0469 - acc: 0.4549 - val_loss: 2.1884 - val_acc: 0.4410\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0477 - acc: 0.4558 - val_loss: 2.2108 - val_acc: 0.4413\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0701 - acc: 0.4513 - val_loss: 2.2058 - val_acc: 0.4367\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0475 - acc: 0.4577 - val_loss: 2.1863 - val_acc: 0.4413\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0335 - acc: 0.4593 - val_loss: 2.1890 - val_acc: 0.4406\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0341 - acc: 0.4607 - val_loss: 2.1956 - val_acc: 0.4383\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0313 - acc: 0.4581 - val_loss: 2.2029 - val_acc: 0.4386\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0319 - acc: 0.4607 - val_loss: 2.1968 - val_acc: 0.4395\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0252 - acc: 0.4617 - val_loss: 2.1849 - val_acc: 0.4422\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0191 - acc: 0.4626 - val_loss: 2.1998 - val_acc: 0.4400\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0229 - acc: 0.4615 - val_loss: 2.1956 - val_acc: 0.4391\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0128 - acc: 0.4639 - val_loss: 2.1899 - val_acc: 0.4422\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0080 - acc: 0.4657 - val_loss: 2.1903 - val_acc: 0.4392\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0070 - acc: 0.4660 - val_loss: 2.1821 - val_acc: 0.4415\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0055 - acc: 0.4664 - val_loss: 2.1987 - val_acc: 0.4390\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0074 - acc: 0.4654 - val_loss: 2.1890 - val_acc: 0.4401\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0011 - acc: 0.4680 - val_loss: 2.1871 - val_acc: 0.4420\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0067 - acc: 0.4656 - val_loss: 2.1946 - val_acc: 0.4393\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9992 - acc: 0.4662 - val_loss: 2.1882 - val_acc: 0.4421\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9945 - acc: 0.4684 - val_loss: 2.1927 - val_acc: 0.4407\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9966 - acc: 0.4685 - val_loss: 2.1814 - val_acc: 0.4429\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9927 - acc: 0.4689 - val_loss: 2.1919 - val_acc: 0.4408\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9922 - acc: 0.4684 - val_loss: 2.1957 - val_acc: 0.4399\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9907 - acc: 0.4694 - val_loss: 2.1934 - val_acc: 0.4402\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9878 - acc: 0.4692 - val_loss: 2.1780 - val_acc: 0.4429\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9831 - acc: 0.4694 - val_loss: 2.1866 - val_acc: 0.4418\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9790 - acc: 0.4711 - val_loss: 2.1842 - val_acc: 0.4430\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9794 - acc: 0.4722 - val_loss: 2.1892 - val_acc: 0.4430\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9804 - acc: 0.4724 - val_loss: 2.1949 - val_acc: 0.4404\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9824 - acc: 0.4728 - val_loss: 2.1942 - val_acc: 0.4411\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9830 - acc: 0.4723 - val_loss: 2.1938 - val_acc: 0.4402\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9738 - acc: 0.4726 - val_loss: 2.2018 - val_acc: 0.4390\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9727 - acc: 0.4748 - val_loss: 2.1914 - val_acc: 0.4427\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9705 - acc: 0.4735 - val_loss: 2.1784 - val_acc: 0.4441\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9675 - acc: 0.4739 - val_loss: 2.2012 - val_acc: 0.4423\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9870 - acc: 0.4693 - val_loss: 2.1841 - val_acc: 0.4440\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9647 - acc: 0.4750 - val_loss: 2.1843 - val_acc: 0.4423\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9550 - acc: 0.4769 - val_loss: 2.1838 - val_acc: 0.4429\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9537 - acc: 0.4777 - val_loss: 2.1894 - val_acc: 0.4437\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9499 - acc: 0.4788 - val_loss: 2.1797 - val_acc: 0.4444\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9532 - acc: 0.4757 - val_loss: 2.1957 - val_acc: 0.4420\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9628 - acc: 0.4743 - val_loss: 2.2073 - val_acc: 0.4398\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9614 - acc: 0.4761 - val_loss: 2.2005 - val_acc: 0.4406\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9551 - acc: 0.4774 - val_loss: 2.1859 - val_acc: 0.4426\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9511 - acc: 0.4792 - val_loss: 2.1979 - val_acc: 0.4419\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9532 - acc: 0.4775 - val_loss: 2.1929 - val_acc: 0.4441\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9519 - acc: 0.4783 - val_loss: 2.1934 - val_acc: 0.4434\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9472 - acc: 0.4800 - val_loss: 2.2191 - val_acc: 0.4389\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9480 - acc: 0.4787 - val_loss: 2.1905 - val_acc: 0.4412\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9358 - acc: 0.4811 - val_loss: 2.2025 - val_acc: 0.4417\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9398 - acc: 0.4808 - val_loss: 2.1932 - val_acc: 0.4428\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9356 - acc: 0.4833 - val_loss: 2.2116 - val_acc: 0.4392\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9373 - acc: 0.4826 - val_loss: 2.2015 - val_acc: 0.4403\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9439 - acc: 0.4803 - val_loss: 2.2219 - val_acc: 0.4374\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9507 - acc: 0.4774 - val_loss: 2.1905 - val_acc: 0.4442\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9372 - acc: 0.4811 - val_loss: 2.2160 - val_acc: 0.4390\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9334 - acc: 0.4832 - val_loss: 2.1992 - val_acc: 0.4424\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9467 - acc: 0.4806 - val_loss: 2.2036 - val_acc: 0.4409\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9283 - acc: 0.4830 - val_loss: 2.2019 - val_acc: 0.4406\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9467 - acc: 0.4795 - val_loss: 2.1948 - val_acc: 0.4410\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9214 - acc: 0.4835 - val_loss: 2.1950 - val_acc: 0.4383\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9219 - acc: 0.4854 - val_loss: 2.1806 - val_acc: 0.4449\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9216 - acc: 0.4848 - val_loss: 2.2222 - val_acc: 0.4370\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9311 - acc: 0.4830 - val_loss: 2.2035 - val_acc: 0.4404\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9206 - acc: 0.4858 - val_loss: 2.2052 - val_acc: 0.4376\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9191 - acc: 0.4844 - val_loss: 2.1963 - val_acc: 0.4427\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9199 - acc: 0.4850 - val_loss: 2.2158 - val_acc: 0.4408\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9326 - acc: 0.4825 - val_loss: 2.1933 - val_acc: 0.4427\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9238 - acc: 0.4834 - val_loss: 2.2018 - val_acc: 0.4406\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9288 - acc: 0.4828 - val_loss: 2.2058 - val_acc: 0.4418\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9237 - acc: 0.4830 - val_loss: 2.2245 - val_acc: 0.4372\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9152 - acc: 0.4858 - val_loss: 2.2122 - val_acc: 0.4435\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9065 - acc: 0.4887 - val_loss: 2.1946 - val_acc: 0.4437\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9006 - acc: 0.4904 - val_loss: 2.2027 - val_acc: 0.4417\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9025 - acc: 0.4904 - val_loss: 2.1938 - val_acc: 0.4421\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8986 - acc: 0.4890 - val_loss: 2.1884 - val_acc: 0.4450\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9036 - acc: 0.4898 - val_loss: 2.2065 - val_acc: 0.4387\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9096 - acc: 0.4886 - val_loss: 2.2010 - val_acc: 0.4409\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9100 - acc: 0.4875 - val_loss: 2.1986 - val_acc: 0.4407\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9035 - acc: 0.4888 - val_loss: 2.2111 - val_acc: 0.4417\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9032 - acc: 0.4895 - val_loss: 2.2094 - val_acc: 0.4399\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9028 - acc: 0.4886 - val_loss: 2.2055 - val_acc: 0.4405\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8974 - acc: 0.4911 - val_loss: 2.2321 - val_acc: 0.4362\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8970 - acc: 0.4899 - val_loss: 2.2146 - val_acc: 0.4410\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1837 - acc: 0.4340 - val_loss: 2.1562 - val_acc: 0.4551\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1607 - acc: 0.4379 - val_loss: 2.1552 - val_acc: 0.4558\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1458 - acc: 0.4413 - val_loss: 2.1541 - val_acc: 0.4565\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1348 - acc: 0.4427 - val_loss: 2.1564 - val_acc: 0.4553\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1243 - acc: 0.4438 - val_loss: 2.1621 - val_acc: 0.4567\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1186 - acc: 0.4478 - val_loss: 2.1780 - val_acc: 0.4567\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1412 - acc: 0.4426 - val_loss: 2.1942 - val_acc: 0.4496\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1122 - acc: 0.4480 - val_loss: 2.1790 - val_acc: 0.4550\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1124 - acc: 0.4477 - val_loss: 2.1644 - val_acc: 0.4556\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0943 - acc: 0.4506 - val_loss: 2.1711 - val_acc: 0.4535\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0923 - acc: 0.4509 - val_loss: 2.1642 - val_acc: 0.4553\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0881 - acc: 0.4517 - val_loss: 2.1757 - val_acc: 0.4536\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0845 - acc: 0.4534 - val_loss: 2.1593 - val_acc: 0.4561\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0789 - acc: 0.4529 - val_loss: 2.1640 - val_acc: 0.4539\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0748 - acc: 0.4547 - val_loss: 2.1569 - val_acc: 0.4546\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0687 - acc: 0.4548 - val_loss: 2.1575 - val_acc: 0.4539\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0662 - acc: 0.4568 - val_loss: 2.1686 - val_acc: 0.4507\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0654 - acc: 0.4565 - val_loss: 2.1560 - val_acc: 0.4558\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0596 - acc: 0.4586 - val_loss: 2.1582 - val_acc: 0.4555\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0546 - acc: 0.4570 - val_loss: 2.1576 - val_acc: 0.4537\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0501 - acc: 0.4578 - val_loss: 2.1507 - val_acc: 0.4576\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0498 - acc: 0.4584 - val_loss: 2.1656 - val_acc: 0.4553\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0566 - acc: 0.4597 - val_loss: 2.1523 - val_acc: 0.4548\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0412 - acc: 0.4611 - val_loss: 2.1537 - val_acc: 0.4557\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0429 - acc: 0.4597 - val_loss: 2.1655 - val_acc: 0.4514\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0441 - acc: 0.4607 - val_loss: 2.1586 - val_acc: 0.4532\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0348 - acc: 0.4635 - val_loss: 2.1597 - val_acc: 0.4520\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0322 - acc: 0.4631 - val_loss: 2.1600 - val_acc: 0.4531\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0366 - acc: 0.4633 - val_loss: 2.1560 - val_acc: 0.4532\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0274 - acc: 0.4629 - val_loss: 2.1611 - val_acc: 0.4536\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0254 - acc: 0.4627 - val_loss: 2.1667 - val_acc: 0.4542\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0352 - acc: 0.4625 - val_loss: 2.1556 - val_acc: 0.4559\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0229 - acc: 0.4645 - val_loss: 2.1530 - val_acc: 0.4524\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0174 - acc: 0.4674 - val_loss: 2.1581 - val_acc: 0.4541\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0200 - acc: 0.4657 - val_loss: 2.1607 - val_acc: 0.4535\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0151 - acc: 0.4687 - val_loss: 2.1601 - val_acc: 0.4534\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0396 - acc: 0.4619 - val_loss: 2.1761 - val_acc: 0.4536\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0208 - acc: 0.4661 - val_loss: 2.1601 - val_acc: 0.4555\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0139 - acc: 0.4667 - val_loss: 2.1749 - val_acc: 0.4540\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0240 - acc: 0.4651 - val_loss: 2.1588 - val_acc: 0.4539\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0129 - acc: 0.4677 - val_loss: 2.1622 - val_acc: 0.4520\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0114 - acc: 0.4683 - val_loss: 2.1599 - val_acc: 0.4561\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0079 - acc: 0.4705 - val_loss: 2.1483 - val_acc: 0.4550\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9934 - acc: 0.4719 - val_loss: 2.1495 - val_acc: 0.4555\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9910 - acc: 0.4740 - val_loss: 2.1568 - val_acc: 0.4570\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0004 - acc: 0.4713 - val_loss: 2.1532 - val_acc: 0.4552\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9881 - acc: 0.4736 - val_loss: 2.1570 - val_acc: 0.4536\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9899 - acc: 0.4729 - val_loss: 2.1640 - val_acc: 0.4517\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9925 - acc: 0.4734 - val_loss: 2.1555 - val_acc: 0.4562\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9952 - acc: 0.4711 - val_loss: 2.1603 - val_acc: 0.4541\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9934 - acc: 0.4710 - val_loss: 2.1609 - val_acc: 0.4542\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9859 - acc: 0.4744 - val_loss: 2.1528 - val_acc: 0.4544\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9771 - acc: 0.4761 - val_loss: 2.1528 - val_acc: 0.4519\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9740 - acc: 0.4757 - val_loss: 2.1729 - val_acc: 0.4514\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9868 - acc: 0.4738 - val_loss: 2.1669 - val_acc: 0.4502\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9855 - acc: 0.4730 - val_loss: 2.1653 - val_acc: 0.4542\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9819 - acc: 0.4736 - val_loss: 2.1950 - val_acc: 0.4468\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9884 - acc: 0.4728 - val_loss: 2.1651 - val_acc: 0.4555\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9808 - acc: 0.4760 - val_loss: 2.1713 - val_acc: 0.4521\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9727 - acc: 0.4779 - val_loss: 2.1607 - val_acc: 0.4540\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9689 - acc: 0.4758 - val_loss: 2.1701 - val_acc: 0.4506\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9645 - acc: 0.4796 - val_loss: 2.1579 - val_acc: 0.4557\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9601 - acc: 0.4799 - val_loss: 2.1584 - val_acc: 0.4536\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9557 - acc: 0.4791 - val_loss: 2.1565 - val_acc: 0.4535\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9599 - acc: 0.4788 - val_loss: 2.1643 - val_acc: 0.4525\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9634 - acc: 0.4788 - val_loss: 2.1606 - val_acc: 0.4495\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9600 - acc: 0.4805 - val_loss: 2.1572 - val_acc: 0.4558\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9533 - acc: 0.4791 - val_loss: 2.1529 - val_acc: 0.4542\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9482 - acc: 0.4823 - val_loss: 2.1664 - val_acc: 0.4530\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9534 - acc: 0.4796 - val_loss: 2.1592 - val_acc: 0.4544\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9475 - acc: 0.4811 - val_loss: 2.1572 - val_acc: 0.4506\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9496 - acc: 0.4823 - val_loss: 2.1618 - val_acc: 0.4530\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9545 - acc: 0.4790 - val_loss: 2.1728 - val_acc: 0.4540\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9517 - acc: 0.4803 - val_loss: 2.1683 - val_acc: 0.4522\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9558 - acc: 0.4809 - val_loss: 2.1599 - val_acc: 0.4549\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9350 - acc: 0.4864 - val_loss: 2.1680 - val_acc: 0.4518\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9371 - acc: 0.4864 - val_loss: 2.1690 - val_acc: 0.4526\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9558 - acc: 0.4803 - val_loss: 2.1816 - val_acc: 0.4497\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9477 - acc: 0.4814 - val_loss: 2.1786 - val_acc: 0.4524\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9529 - acc: 0.4820 - val_loss: 2.1875 - val_acc: 0.4494\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9484 - acc: 0.4811 - val_loss: 2.1626 - val_acc: 0.4536\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9360 - acc: 0.4829 - val_loss: 2.1769 - val_acc: 0.4529\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9366 - acc: 0.4825 - val_loss: 2.1540 - val_acc: 0.4543\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9276 - acc: 0.4860 - val_loss: 2.1568 - val_acc: 0.4567\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9237 - acc: 0.4905 - val_loss: 2.1533 - val_acc: 0.4532\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9218 - acc: 0.4887 - val_loss: 2.1705 - val_acc: 0.4510\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9375 - acc: 0.4847 - val_loss: 2.1777 - val_acc: 0.4526\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9300 - acc: 0.4869 - val_loss: 2.1616 - val_acc: 0.4538\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9396 - acc: 0.4848 - val_loss: 2.1787 - val_acc: 0.4538\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9463 - acc: 0.4826 - val_loss: 2.1747 - val_acc: 0.4523\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9372 - acc: 0.4841 - val_loss: 2.2011 - val_acc: 0.4509\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9463 - acc: 0.4811 - val_loss: 2.1788 - val_acc: 0.4499\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9300 - acc: 0.4872 - val_loss: 2.1834 - val_acc: 0.4524\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9278 - acc: 0.4871 - val_loss: 2.2088 - val_acc: 0.4447\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9409 - acc: 0.4823 - val_loss: 2.1803 - val_acc: 0.4532\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9303 - acc: 0.4851 - val_loss: 2.1846 - val_acc: 0.4512\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9229 - acc: 0.4867 - val_loss: 2.1667 - val_acc: 0.4528\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9136 - acc: 0.4892 - val_loss: 2.1914 - val_acc: 0.4462\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9126 - acc: 0.4916 - val_loss: 2.1775 - val_acc: 0.4521\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9269 - acc: 0.4880 - val_loss: 2.1862 - val_acc: 0.4464\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1843 - acc: 0.4303 - val_loss: 2.1342 - val_acc: 0.4506\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1572 - acc: 0.4350 - val_loss: 2.1350 - val_acc: 0.4528\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1413 - acc: 0.4375 - val_loss: 2.1337 - val_acc: 0.4552\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1332 - acc: 0.4391 - val_loss: 2.1499 - val_acc: 0.4526\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1250 - acc: 0.4428 - val_loss: 2.1578 - val_acc: 0.4512\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1202 - acc: 0.4420 - val_loss: 2.1524 - val_acc: 0.4506\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1120 - acc: 0.4446 - val_loss: 2.1624 - val_acc: 0.4494\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1105 - acc: 0.4451 - val_loss: 2.1496 - val_acc: 0.4525\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0980 - acc: 0.4478 - val_loss: 2.1561 - val_acc: 0.4528\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0907 - acc: 0.4490 - val_loss: 2.1474 - val_acc: 0.4522\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0890 - acc: 0.4493 - val_loss: 2.1555 - val_acc: 0.4498\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0840 - acc: 0.4480 - val_loss: 2.1708 - val_acc: 0.4478\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0874 - acc: 0.4482 - val_loss: 2.1451 - val_acc: 0.4512\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0767 - acc: 0.4501 - val_loss: 2.1464 - val_acc: 0.4523\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0672 - acc: 0.4531 - val_loss: 2.1503 - val_acc: 0.4520\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0687 - acc: 0.4533 - val_loss: 2.1520 - val_acc: 0.4537\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0634 - acc: 0.4534 - val_loss: 2.1438 - val_acc: 0.4562\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0591 - acc: 0.4544 - val_loss: 2.1454 - val_acc: 0.4543\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0569 - acc: 0.4539 - val_loss: 2.1392 - val_acc: 0.4521\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0493 - acc: 0.4565 - val_loss: 2.1369 - val_acc: 0.4528\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0476 - acc: 0.4569 - val_loss: 2.1444 - val_acc: 0.4515\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0475 - acc: 0.4588 - val_loss: 2.1432 - val_acc: 0.4539\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0455 - acc: 0.4582 - val_loss: 2.1452 - val_acc: 0.4512\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0446 - acc: 0.4569 - val_loss: 2.1387 - val_acc: 0.4557\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0372 - acc: 0.4590 - val_loss: 2.1402 - val_acc: 0.4539\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0347 - acc: 0.4613 - val_loss: 2.1303 - val_acc: 0.4542\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0307 - acc: 0.4600 - val_loss: 2.1377 - val_acc: 0.4537\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0338 - acc: 0.4599 - val_loss: 2.1461 - val_acc: 0.4532\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0282 - acc: 0.4616 - val_loss: 2.1388 - val_acc: 0.4548\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0262 - acc: 0.4616 - val_loss: 2.1334 - val_acc: 0.4538\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0198 - acc: 0.4629 - val_loss: 2.1388 - val_acc: 0.4533\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0191 - acc: 0.4636 - val_loss: 2.1423 - val_acc: 0.4510\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0205 - acc: 0.4638 - val_loss: 2.1296 - val_acc: 0.4544\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0094 - acc: 0.4659 - val_loss: 2.1300 - val_acc: 0.4563\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0085 - acc: 0.4646 - val_loss: 2.1294 - val_acc: 0.4573\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0160 - acc: 0.4649 - val_loss: 2.1265 - val_acc: 0.4564\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0051 - acc: 0.4653 - val_loss: 2.1334 - val_acc: 0.4563\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0073 - acc: 0.4679 - val_loss: 2.1334 - val_acc: 0.4536\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0025 - acc: 0.4670 - val_loss: 2.1496 - val_acc: 0.4557\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0144 - acc: 0.4654 - val_loss: 2.1354 - val_acc: 0.4552\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9951 - acc: 0.4676 - val_loss: 2.1251 - val_acc: 0.4591\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9926 - acc: 0.4664 - val_loss: 2.1287 - val_acc: 0.4543\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9949 - acc: 0.4681 - val_loss: 2.1385 - val_acc: 0.4560\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9997 - acc: 0.4659 - val_loss: 2.1310 - val_acc: 0.4554\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9950 - acc: 0.4684 - val_loss: 2.1394 - val_acc: 0.4563\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9895 - acc: 0.4691 - val_loss: 2.1379 - val_acc: 0.4539\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9935 - acc: 0.4696 - val_loss: 2.1404 - val_acc: 0.4512\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9917 - acc: 0.4688 - val_loss: 2.1312 - val_acc: 0.4535\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9795 - acc: 0.4718 - val_loss: 2.1288 - val_acc: 0.4540\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9816 - acc: 0.4722 - val_loss: 2.1507 - val_acc: 0.4535\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9768 - acc: 0.4720 - val_loss: 2.1245 - val_acc: 0.4552\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9813 - acc: 0.4722 - val_loss: 2.1440 - val_acc: 0.4538\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9810 - acc: 0.4711 - val_loss: 2.1385 - val_acc: 0.4538\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9756 - acc: 0.4709 - val_loss: 2.1417 - val_acc: 0.4516\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9734 - acc: 0.4725 - val_loss: 2.1434 - val_acc: 0.4538\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9684 - acc: 0.4752 - val_loss: 2.1280 - val_acc: 0.4559\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9669 - acc: 0.4731 - val_loss: 2.1386 - val_acc: 0.4521\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9687 - acc: 0.4728 - val_loss: 2.1273 - val_acc: 0.4529\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9704 - acc: 0.4733 - val_loss: 2.1654 - val_acc: 0.4525\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9952 - acc: 0.4684 - val_loss: 2.1289 - val_acc: 0.4542\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9663 - acc: 0.4741 - val_loss: 2.1425 - val_acc: 0.4554\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9643 - acc: 0.4743 - val_loss: 2.1295 - val_acc: 0.4547\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9645 - acc: 0.4749 - val_loss: 2.1335 - val_acc: 0.4551\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9569 - acc: 0.4767 - val_loss: 2.1290 - val_acc: 0.4540\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9541 - acc: 0.4776 - val_loss: 2.1324 - val_acc: 0.4558\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9549 - acc: 0.4765 - val_loss: 2.1604 - val_acc: 0.4490\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9647 - acc: 0.4751 - val_loss: 2.1394 - val_acc: 0.4547\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9544 - acc: 0.4788 - val_loss: 2.1374 - val_acc: 0.4537\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9453 - acc: 0.4802 - val_loss: 2.1478 - val_acc: 0.4537\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9522 - acc: 0.4773 - val_loss: 2.1381 - val_acc: 0.4536\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9420 - acc: 0.4804 - val_loss: 2.1342 - val_acc: 0.4561\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9577 - acc: 0.4774 - val_loss: 2.1361 - val_acc: 0.4546\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9463 - acc: 0.4793 - val_loss: 2.1337 - val_acc: 0.4574\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9412 - acc: 0.4789 - val_loss: 2.1282 - val_acc: 0.4573\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9367 - acc: 0.4815 - val_loss: 2.1321 - val_acc: 0.4542\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9403 - acc: 0.4799 - val_loss: 2.1290 - val_acc: 0.4577\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9549 - acc: 0.4762 - val_loss: 2.1411 - val_acc: 0.4564\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9394 - acc: 0.4805 - val_loss: 2.1255 - val_acc: 0.4564\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9364 - acc: 0.4824 - val_loss: 2.1445 - val_acc: 0.4553\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9358 - acc: 0.4806 - val_loss: 2.1818 - val_acc: 0.4479\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9568 - acc: 0.4791 - val_loss: 2.1449 - val_acc: 0.4556\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9373 - acc: 0.4825 - val_loss: 2.1633 - val_acc: 0.4492\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9387 - acc: 0.4830 - val_loss: 2.1566 - val_acc: 0.4547\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9522 - acc: 0.4801 - val_loss: 2.1485 - val_acc: 0.4487\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9314 - acc: 0.4833 - val_loss: 2.1352 - val_acc: 0.4565\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9342 - acc: 0.4813 - val_loss: 2.1392 - val_acc: 0.4522\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9265 - acc: 0.4824 - val_loss: 2.1213 - val_acc: 0.4564\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9231 - acc: 0.4859 - val_loss: 2.1385 - val_acc: 0.4528\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9208 - acc: 0.4831 - val_loss: 2.1250 - val_acc: 0.4562\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9150 - acc: 0.4880 - val_loss: 2.1361 - val_acc: 0.4550\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9149 - acc: 0.4862 - val_loss: 2.1341 - val_acc: 0.4557\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9184 - acc: 0.4858 - val_loss: 2.1363 - val_acc: 0.4503\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9133 - acc: 0.4866 - val_loss: 2.1396 - val_acc: 0.4553\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9274 - acc: 0.4854 - val_loss: 2.1513 - val_acc: 0.4529\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9189 - acc: 0.4841 - val_loss: 2.1365 - val_acc: 0.4549\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9153 - acc: 0.4844 - val_loss: 2.1474 - val_acc: 0.4513\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9230 - acc: 0.4845 - val_loss: 2.1348 - val_acc: 0.4564\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9177 - acc: 0.4872 - val_loss: 2.1362 - val_acc: 0.4538\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9165 - acc: 0.4861 - val_loss: 2.1301 - val_acc: 0.4552\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9107 - acc: 0.4890 - val_loss: 2.1396 - val_acc: 0.4536\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1549 - acc: 0.4337 - val_loss: 2.1849 - val_acc: 0.4423\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1360 - acc: 0.4381 - val_loss: 2.1947 - val_acc: 0.4396\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1222 - acc: 0.4412 - val_loss: 2.1963 - val_acc: 0.4376\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1132 - acc: 0.4425 - val_loss: 2.2260 - val_acc: 0.4326\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1123 - acc: 0.4427 - val_loss: 2.2176 - val_acc: 0.4335\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0936 - acc: 0.4454 - val_loss: 2.2176 - val_acc: 0.4345\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0895 - acc: 0.4469 - val_loss: 2.2134 - val_acc: 0.4358\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0857 - acc: 0.4470 - val_loss: 2.2207 - val_acc: 0.4342\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0751 - acc: 0.4502 - val_loss: 2.2497 - val_acc: 0.4295\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0708 - acc: 0.4525 - val_loss: 2.2569 - val_acc: 0.4254\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0717 - acc: 0.4510 - val_loss: 2.2670 - val_acc: 0.4263\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0638 - acc: 0.4512 - val_loss: 2.2585 - val_acc: 0.4277\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0624 - acc: 0.4527 - val_loss: 2.2738 - val_acc: 0.4262\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0548 - acc: 0.4540 - val_loss: 2.2472 - val_acc: 0.4286\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0514 - acc: 0.4551 - val_loss: 2.2348 - val_acc: 0.4311\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0451 - acc: 0.4560 - val_loss: 2.2605 - val_acc: 0.4265\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0419 - acc: 0.4577 - val_loss: 2.2409 - val_acc: 0.4313\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0396 - acc: 0.4582 - val_loss: 2.2517 - val_acc: 0.4273\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0343 - acc: 0.4570 - val_loss: 2.2609 - val_acc: 0.4284\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0281 - acc: 0.4594 - val_loss: 2.2381 - val_acc: 0.4313\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0256 - acc: 0.4598 - val_loss: 2.2619 - val_acc: 0.4255\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0250 - acc: 0.4603 - val_loss: 2.2308 - val_acc: 0.4308\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0245 - acc: 0.4619 - val_loss: 2.2412 - val_acc: 0.4284\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0175 - acc: 0.4631 - val_loss: 2.2321 - val_acc: 0.4302\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0228 - acc: 0.4595 - val_loss: 2.2552 - val_acc: 0.4325\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0440 - acc: 0.4581 - val_loss: 2.2578 - val_acc: 0.4280\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0188 - acc: 0.4616 - val_loss: 2.2401 - val_acc: 0.4317\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0100 - acc: 0.4649 - val_loss: 2.2278 - val_acc: 0.4329\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0094 - acc: 0.4641 - val_loss: 2.2289 - val_acc: 0.4323\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0004 - acc: 0.4650 - val_loss: 2.2336 - val_acc: 0.4316\n",
      "Epoch 31/100\n",
      "1s - loss: 1.9975 - acc: 0.4649 - val_loss: 2.2213 - val_acc: 0.4343\n",
      "Epoch 32/100\n",
      "1s - loss: 1.9945 - acc: 0.4684 - val_loss: 2.1999 - val_acc: 0.4376\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9943 - acc: 0.4657 - val_loss: 2.2132 - val_acc: 0.4376\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9887 - acc: 0.4686 - val_loss: 2.2260 - val_acc: 0.4315\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9903 - acc: 0.4672 - val_loss: 2.2184 - val_acc: 0.4328\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9899 - acc: 0.4683 - val_loss: 2.2383 - val_acc: 0.4324\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9845 - acc: 0.4693 - val_loss: 2.2179 - val_acc: 0.4348\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9816 - acc: 0.4698 - val_loss: 2.2267 - val_acc: 0.4337\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9832 - acc: 0.4700 - val_loss: 2.2120 - val_acc: 0.4371\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9826 - acc: 0.4706 - val_loss: 2.2161 - val_acc: 0.4355\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9817 - acc: 0.4698 - val_loss: 2.2034 - val_acc: 0.4351\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9747 - acc: 0.4715 - val_loss: 2.2260 - val_acc: 0.4334\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9782 - acc: 0.4706 - val_loss: 2.2028 - val_acc: 0.4374\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9750 - acc: 0.4704 - val_loss: 2.2089 - val_acc: 0.4359\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9665 - acc: 0.4743 - val_loss: 2.2214 - val_acc: 0.4329\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9662 - acc: 0.4744 - val_loss: 2.2216 - val_acc: 0.4350\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9646 - acc: 0.4751 - val_loss: 2.2028 - val_acc: 0.4372\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9628 - acc: 0.4739 - val_loss: 2.2293 - val_acc: 0.4336\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9758 - acc: 0.4729 - val_loss: 2.2142 - val_acc: 0.4360\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9587 - acc: 0.4746 - val_loss: 2.2041 - val_acc: 0.4383\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9566 - acc: 0.4748 - val_loss: 2.2017 - val_acc: 0.4385\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9545 - acc: 0.4759 - val_loss: 2.1975 - val_acc: 0.4393\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9514 - acc: 0.4752 - val_loss: 2.2046 - val_acc: 0.4398\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9645 - acc: 0.4767 - val_loss: 2.2108 - val_acc: 0.4388\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9522 - acc: 0.4792 - val_loss: 2.2097 - val_acc: 0.4396\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9506 - acc: 0.4787 - val_loss: 2.2076 - val_acc: 0.4360\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9492 - acc: 0.4759 - val_loss: 2.2725 - val_acc: 0.4353\n",
      "Epoch 58/100\n",
      "1s - loss: 2.0138 - acc: 0.4666 - val_loss: 2.2759 - val_acc: 0.4214\n",
      "Epoch 59/100\n",
      "1s - loss: 2.0049 - acc: 0.4662 - val_loss: 2.2964 - val_acc: 0.4248\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9679 - acc: 0.4743 - val_loss: 2.2126 - val_acc: 0.4342\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9443 - acc: 0.4798 - val_loss: 2.2195 - val_acc: 0.4367\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9420 - acc: 0.4785 - val_loss: 2.2085 - val_acc: 0.4372\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9436 - acc: 0.4796 - val_loss: 2.2010 - val_acc: 0.4378\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9381 - acc: 0.4796 - val_loss: 2.2203 - val_acc: 0.4353\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9401 - acc: 0.4783 - val_loss: 2.2156 - val_acc: 0.4374\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9410 - acc: 0.4793 - val_loss: 2.2248 - val_acc: 0.4338\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9341 - acc: 0.4815 - val_loss: 2.1985 - val_acc: 0.4396\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9336 - acc: 0.4817 - val_loss: 2.2041 - val_acc: 0.4377\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9324 - acc: 0.4793 - val_loss: 2.2123 - val_acc: 0.4384\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9381 - acc: 0.4798 - val_loss: 2.2060 - val_acc: 0.4374\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9288 - acc: 0.4811 - val_loss: 2.1969 - val_acc: 0.4416\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9231 - acc: 0.4844 - val_loss: 2.1895 - val_acc: 0.4402\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9238 - acc: 0.4839 - val_loss: 2.2225 - val_acc: 0.4374\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9355 - acc: 0.4807 - val_loss: 2.1902 - val_acc: 0.4397\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9212 - acc: 0.4826 - val_loss: 2.2058 - val_acc: 0.4380\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9158 - acc: 0.4854 - val_loss: 2.2020 - val_acc: 0.4399\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9178 - acc: 0.4860 - val_loss: 2.1945 - val_acc: 0.4389\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9110 - acc: 0.4877 - val_loss: 2.1915 - val_acc: 0.4414\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9120 - acc: 0.4864 - val_loss: 2.2022 - val_acc: 0.4389\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9194 - acc: 0.4838 - val_loss: 2.2112 - val_acc: 0.4383\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9142 - acc: 0.4845 - val_loss: 2.2116 - val_acc: 0.4393\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9293 - acc: 0.4825 - val_loss: 2.2161 - val_acc: 0.4373\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9140 - acc: 0.4849 - val_loss: 2.2071 - val_acc: 0.4394\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9166 - acc: 0.4868 - val_loss: 2.2226 - val_acc: 0.4351\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9085 - acc: 0.4876 - val_loss: 2.1979 - val_acc: 0.4405\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9010 - acc: 0.4893 - val_loss: 2.2079 - val_acc: 0.4385\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9046 - acc: 0.4877 - val_loss: 2.1992 - val_acc: 0.4379\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9000 - acc: 0.4878 - val_loss: 2.1966 - val_acc: 0.4393\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9005 - acc: 0.4892 - val_loss: 2.1855 - val_acc: 0.4398\n",
      "Epoch 90/100\n",
      "1s - loss: 1.8997 - acc: 0.4889 - val_loss: 2.2010 - val_acc: 0.4385\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9021 - acc: 0.4871 - val_loss: 2.1903 - val_acc: 0.4395\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8950 - acc: 0.4897 - val_loss: 2.2028 - val_acc: 0.4384\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8975 - acc: 0.4877 - val_loss: 2.1894 - val_acc: 0.4409\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8963 - acc: 0.4894 - val_loss: 2.2442 - val_acc: 0.4322\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9032 - acc: 0.4883 - val_loss: 2.2036 - val_acc: 0.4379\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9015 - acc: 0.4873 - val_loss: 2.2121 - val_acc: 0.4364\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8930 - acc: 0.4923 - val_loss: 2.2169 - val_acc: 0.4359\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9043 - acc: 0.4888 - val_loss: 2.2115 - val_acc: 0.4365\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8969 - acc: 0.4907 - val_loss: 2.2065 - val_acc: 0.4391\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8957 - acc: 0.4895 - val_loss: 2.2162 - val_acc: 0.4363\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1941 - acc: 0.4258 - val_loss: 2.2093 - val_acc: 0.4383\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1673 - acc: 0.4315 - val_loss: 2.1982 - val_acc: 0.4396\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1534 - acc: 0.4343 - val_loss: 2.1962 - val_acc: 0.4409\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1444 - acc: 0.4373 - val_loss: 2.1966 - val_acc: 0.4393\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1365 - acc: 0.4375 - val_loss: 2.2039 - val_acc: 0.4399\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1261 - acc: 0.4401 - val_loss: 2.2133 - val_acc: 0.4374\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1212 - acc: 0.4411 - val_loss: 2.2207 - val_acc: 0.4372\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1206 - acc: 0.4402 - val_loss: 2.2124 - val_acc: 0.4378\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1066 - acc: 0.4438 - val_loss: 2.2166 - val_acc: 0.4377\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1056 - acc: 0.4445 - val_loss: 2.2184 - val_acc: 0.4385\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1005 - acc: 0.4442 - val_loss: 2.2144 - val_acc: 0.4396\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0969 - acc: 0.4481 - val_loss: 2.2204 - val_acc: 0.4374\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0882 - acc: 0.4475 - val_loss: 2.2046 - val_acc: 0.4419\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0858 - acc: 0.4457 - val_loss: 2.2061 - val_acc: 0.4388\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0838 - acc: 0.4481 - val_loss: 2.2059 - val_acc: 0.4403\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0773 - acc: 0.4504 - val_loss: 2.2082 - val_acc: 0.4404\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0715 - acc: 0.4506 - val_loss: 2.2094 - val_acc: 0.4398\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0713 - acc: 0.4484 - val_loss: 2.2112 - val_acc: 0.4406\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0676 - acc: 0.4524 - val_loss: 2.2256 - val_acc: 0.4381\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0649 - acc: 0.4527 - val_loss: 2.2104 - val_acc: 0.4429\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0595 - acc: 0.4532 - val_loss: 2.2118 - val_acc: 0.4423\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0550 - acc: 0.4544 - val_loss: 2.2362 - val_acc: 0.4345\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0595 - acc: 0.4517 - val_loss: 2.2122 - val_acc: 0.4394\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0495 - acc: 0.4564 - val_loss: 2.2109 - val_acc: 0.4405\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0466 - acc: 0.4552 - val_loss: 2.2143 - val_acc: 0.4387\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0441 - acc: 0.4566 - val_loss: 2.2071 - val_acc: 0.4413\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0420 - acc: 0.4569 - val_loss: 2.2170 - val_acc: 0.4419\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0605 - acc: 0.4532 - val_loss: 2.2158 - val_acc: 0.4392\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0393 - acc: 0.4593 - val_loss: 2.2118 - val_acc: 0.4421\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0314 - acc: 0.4597 - val_loss: 2.2000 - val_acc: 0.4410\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0317 - acc: 0.4588 - val_loss: 2.2068 - val_acc: 0.4414\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0255 - acc: 0.4613 - val_loss: 2.2073 - val_acc: 0.4383\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0256 - acc: 0.4589 - val_loss: 2.2071 - val_acc: 0.4420\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0251 - acc: 0.4603 - val_loss: 2.2148 - val_acc: 0.4385\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0237 - acc: 0.4593 - val_loss: 2.2076 - val_acc: 0.4416\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0183 - acc: 0.4610 - val_loss: 2.2145 - val_acc: 0.4401\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0267 - acc: 0.4617 - val_loss: 2.2155 - val_acc: 0.4395\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0189 - acc: 0.4620 - val_loss: 2.2085 - val_acc: 0.4392\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0111 - acc: 0.4633 - val_loss: 2.2253 - val_acc: 0.4386\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0263 - acc: 0.4595 - val_loss: 2.1975 - val_acc: 0.4422\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0176 - acc: 0.4627 - val_loss: 2.2126 - val_acc: 0.4418\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0095 - acc: 0.4647 - val_loss: 2.2027 - val_acc: 0.4427\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0017 - acc: 0.4651 - val_loss: 2.2075 - val_acc: 0.4433\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0075 - acc: 0.4653 - val_loss: 2.1973 - val_acc: 0.4427\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0034 - acc: 0.4663 - val_loss: 2.2116 - val_acc: 0.4405\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0050 - acc: 0.4655 - val_loss: 2.2148 - val_acc: 0.4399\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0054 - acc: 0.4667 - val_loss: 2.2278 - val_acc: 0.4406\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0072 - acc: 0.4658 - val_loss: 2.2073 - val_acc: 0.4411\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9960 - acc: 0.4689 - val_loss: 2.2117 - val_acc: 0.4415\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9925 - acc: 0.4705 - val_loss: 2.1983 - val_acc: 0.4402\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9862 - acc: 0.4695 - val_loss: 2.1949 - val_acc: 0.4430\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9820 - acc: 0.4718 - val_loss: 2.1981 - val_acc: 0.4432\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9788 - acc: 0.4714 - val_loss: 2.2105 - val_acc: 0.4411\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9860 - acc: 0.4686 - val_loss: 2.2047 - val_acc: 0.4403\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9835 - acc: 0.4704 - val_loss: 2.1992 - val_acc: 0.4429\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9822 - acc: 0.4668 - val_loss: 2.2055 - val_acc: 0.4418\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9845 - acc: 0.4696 - val_loss: 2.2102 - val_acc: 0.4410\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9908 - acc: 0.4680 - val_loss: 2.2124 - val_acc: 0.4399\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9894 - acc: 0.4681 - val_loss: 2.1920 - val_acc: 0.4421\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9815 - acc: 0.4702 - val_loss: 2.2230 - val_acc: 0.4413\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9866 - acc: 0.4709 - val_loss: 2.2110 - val_acc: 0.4407\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9814 - acc: 0.4704 - val_loss: 2.2077 - val_acc: 0.4419\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9756 - acc: 0.4718 - val_loss: 2.2026 - val_acc: 0.4396\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9730 - acc: 0.4732 - val_loss: 2.2050 - val_acc: 0.4406\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9738 - acc: 0.4745 - val_loss: 2.2008 - val_acc: 0.4402\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9634 - acc: 0.4748 - val_loss: 2.2018 - val_acc: 0.4395\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9611 - acc: 0.4758 - val_loss: 2.2019 - val_acc: 0.4417\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9614 - acc: 0.4770 - val_loss: 2.2025 - val_acc: 0.4401\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9691 - acc: 0.4731 - val_loss: 2.2009 - val_acc: 0.4409\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9672 - acc: 0.4741 - val_loss: 2.2305 - val_acc: 0.4386\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9722 - acc: 0.4726 - val_loss: 2.2177 - val_acc: 0.4385\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9672 - acc: 0.4728 - val_loss: 2.2189 - val_acc: 0.4398\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9558 - acc: 0.4767 - val_loss: 2.1934 - val_acc: 0.4417\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9477 - acc: 0.4787 - val_loss: 2.1989 - val_acc: 0.4405\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9522 - acc: 0.4780 - val_loss: 2.1952 - val_acc: 0.4412\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9506 - acc: 0.4767 - val_loss: 2.2216 - val_acc: 0.4407\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9737 - acc: 0.4717 - val_loss: 2.1950 - val_acc: 0.4397\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9485 - acc: 0.4773 - val_loss: 2.2136 - val_acc: 0.4393\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9527 - acc: 0.4758 - val_loss: 2.2000 - val_acc: 0.4418\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9459 - acc: 0.4792 - val_loss: 2.2239 - val_acc: 0.4370\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9518 - acc: 0.4779 - val_loss: 2.1982 - val_acc: 0.4400\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9436 - acc: 0.4792 - val_loss: 2.2269 - val_acc: 0.4368\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9530 - acc: 0.4771 - val_loss: 2.2112 - val_acc: 0.4384\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9557 - acc: 0.4754 - val_loss: 2.2231 - val_acc: 0.4368\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9478 - acc: 0.4783 - val_loss: 2.2117 - val_acc: 0.4401\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9555 - acc: 0.4761 - val_loss: 2.2167 - val_acc: 0.4373\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9384 - acc: 0.4794 - val_loss: 2.1972 - val_acc: 0.4415\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9353 - acc: 0.4815 - val_loss: 2.2161 - val_acc: 0.4409\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9514 - acc: 0.4793 - val_loss: 2.2079 - val_acc: 0.4400\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9238 - acc: 0.4854 - val_loss: 2.2066 - val_acc: 0.4403\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9261 - acc: 0.4832 - val_loss: 2.2112 - val_acc: 0.4398\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9284 - acc: 0.4853 - val_loss: 2.2045 - val_acc: 0.4420\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9218 - acc: 0.4855 - val_loss: 2.1970 - val_acc: 0.4419\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9207 - acc: 0.4842 - val_loss: 2.2032 - val_acc: 0.4393\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9197 - acc: 0.4845 - val_loss: 2.2043 - val_acc: 0.4445\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9218 - acc: 0.4874 - val_loss: 2.2292 - val_acc: 0.4348\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9220 - acc: 0.4834 - val_loss: 2.2135 - val_acc: 0.4416\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9213 - acc: 0.4870 - val_loss: 2.2293 - val_acc: 0.4375\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9273 - acc: 0.4842 - val_loss: 2.2492 - val_acc: 0.4403\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9682 - acc: 0.4752 - val_loss: 2.2774 - val_acc: 0.4302\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2169 - acc: 0.4224 - val_loss: 2.1730 - val_acc: 0.4426\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1644 - acc: 0.4329 - val_loss: 2.1725 - val_acc: 0.4446\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1571 - acc: 0.4321 - val_loss: 2.1884 - val_acc: 0.4413\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1443 - acc: 0.4342 - val_loss: 2.1809 - val_acc: 0.4421\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1371 - acc: 0.4372 - val_loss: 2.1773 - val_acc: 0.4427\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1287 - acc: 0.4401 - val_loss: 2.1901 - val_acc: 0.4419\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1198 - acc: 0.4421 - val_loss: 2.1774 - val_acc: 0.4414\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1137 - acc: 0.4429 - val_loss: 2.1923 - val_acc: 0.4441\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1118 - acc: 0.4420 - val_loss: 2.1880 - val_acc: 0.4435\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1046 - acc: 0.4440 - val_loss: 2.1768 - val_acc: 0.4443\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0995 - acc: 0.4444 - val_loss: 2.1860 - val_acc: 0.4427\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0938 - acc: 0.4460 - val_loss: 2.1850 - val_acc: 0.4427\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0855 - acc: 0.4497 - val_loss: 2.1944 - val_acc: 0.4412\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0904 - acc: 0.4470 - val_loss: 2.1776 - val_acc: 0.4435\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0801 - acc: 0.4487 - val_loss: 2.1997 - val_acc: 0.4375\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0801 - acc: 0.4482 - val_loss: 2.1812 - val_acc: 0.4428\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0753 - acc: 0.4524 - val_loss: 2.1861 - val_acc: 0.4409\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0741 - acc: 0.4505 - val_loss: 2.1999 - val_acc: 0.4390\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0710 - acc: 0.4523 - val_loss: 2.1833 - val_acc: 0.4434\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0639 - acc: 0.4520 - val_loss: 2.1791 - val_acc: 0.4428\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0590 - acc: 0.4553 - val_loss: 2.1710 - val_acc: 0.4432\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0569 - acc: 0.4565 - val_loss: 2.1665 - val_acc: 0.4459\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0613 - acc: 0.4543 - val_loss: 2.1783 - val_acc: 0.4432\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0504 - acc: 0.4567 - val_loss: 2.1768 - val_acc: 0.4443\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0513 - acc: 0.4557 - val_loss: 2.1672 - val_acc: 0.4452\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0451 - acc: 0.4570 - val_loss: 2.1707 - val_acc: 0.4457\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0506 - acc: 0.4552 - val_loss: 2.1786 - val_acc: 0.4430\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0350 - acc: 0.4582 - val_loss: 2.1626 - val_acc: 0.4462\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0388 - acc: 0.4587 - val_loss: 2.1742 - val_acc: 0.4439\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0384 - acc: 0.4609 - val_loss: 2.1628 - val_acc: 0.4471\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0297 - acc: 0.4606 - val_loss: 2.1752 - val_acc: 0.4444\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0306 - acc: 0.4593 - val_loss: 2.1691 - val_acc: 0.4475\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0271 - acc: 0.4611 - val_loss: 2.1855 - val_acc: 0.4431\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0310 - acc: 0.4608 - val_loss: 2.1809 - val_acc: 0.4446\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0279 - acc: 0.4601 - val_loss: 2.1717 - val_acc: 0.4443\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0215 - acc: 0.4624 - val_loss: 2.1746 - val_acc: 0.4473\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0259 - acc: 0.4624 - val_loss: 2.1807 - val_acc: 0.4447\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0277 - acc: 0.4628 - val_loss: 2.1748 - val_acc: 0.4478\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0250 - acc: 0.4626 - val_loss: 2.1734 - val_acc: 0.4439\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0170 - acc: 0.4637 - val_loss: 2.1734 - val_acc: 0.4459\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0202 - acc: 0.4631 - val_loss: 2.1682 - val_acc: 0.4443\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0165 - acc: 0.4649 - val_loss: 2.1622 - val_acc: 0.4482\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0083 - acc: 0.4663 - val_loss: 2.1677 - val_acc: 0.4464\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0093 - acc: 0.4642 - val_loss: 2.1593 - val_acc: 0.4496\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0031 - acc: 0.4676 - val_loss: 2.1759 - val_acc: 0.4423\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0142 - acc: 0.4648 - val_loss: 2.1715 - val_acc: 0.4485\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9998 - acc: 0.4667 - val_loss: 2.1801 - val_acc: 0.4436\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0064 - acc: 0.4655 - val_loss: 2.1705 - val_acc: 0.4463\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9988 - acc: 0.4677 - val_loss: 2.1594 - val_acc: 0.4493\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9961 - acc: 0.4697 - val_loss: 2.1726 - val_acc: 0.4485\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9955 - acc: 0.4683 - val_loss: 2.1811 - val_acc: 0.4433\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9960 - acc: 0.4684 - val_loss: 2.1625 - val_acc: 0.4480\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9837 - acc: 0.4711 - val_loss: 2.1681 - val_acc: 0.4468\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9829 - acc: 0.4728 - val_loss: 2.1646 - val_acc: 0.4505\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9879 - acc: 0.4702 - val_loss: 2.1788 - val_acc: 0.4446\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9805 - acc: 0.4726 - val_loss: 2.1658 - val_acc: 0.4497\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9827 - acc: 0.4702 - val_loss: 2.1780 - val_acc: 0.4443\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9869 - acc: 0.4709 - val_loss: 2.1810 - val_acc: 0.4470\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9999 - acc: 0.4673 - val_loss: 2.1738 - val_acc: 0.4447\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9762 - acc: 0.4727 - val_loss: 2.1586 - val_acc: 0.4476\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9690 - acc: 0.4758 - val_loss: 2.1740 - val_acc: 0.4439\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9744 - acc: 0.4739 - val_loss: 2.1651 - val_acc: 0.4487\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9662 - acc: 0.4752 - val_loss: 2.1712 - val_acc: 0.4466\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9683 - acc: 0.4734 - val_loss: 2.1653 - val_acc: 0.4475\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9675 - acc: 0.4761 - val_loss: 2.1970 - val_acc: 0.4422\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9682 - acc: 0.4741 - val_loss: 2.1606 - val_acc: 0.4479\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9683 - acc: 0.4748 - val_loss: 2.1990 - val_acc: 0.4437\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9814 - acc: 0.4741 - val_loss: 2.1707 - val_acc: 0.4465\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9761 - acc: 0.4725 - val_loss: 2.1904 - val_acc: 0.4446\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9704 - acc: 0.4742 - val_loss: 2.1821 - val_acc: 0.4465\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9573 - acc: 0.4778 - val_loss: 2.1747 - val_acc: 0.4474\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9639 - acc: 0.4764 - val_loss: 2.1701 - val_acc: 0.4461\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9580 - acc: 0.4763 - val_loss: 2.1743 - val_acc: 0.4457\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9604 - acc: 0.4757 - val_loss: 2.1808 - val_acc: 0.4446\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9540 - acc: 0.4769 - val_loss: 2.1762 - val_acc: 0.4458\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9518 - acc: 0.4788 - val_loss: 2.2148 - val_acc: 0.4402\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9568 - acc: 0.4791 - val_loss: 2.1795 - val_acc: 0.4474\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9539 - acc: 0.4779 - val_loss: 2.2026 - val_acc: 0.4417\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9592 - acc: 0.4787 - val_loss: 2.1845 - val_acc: 0.4471\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9626 - acc: 0.4786 - val_loss: 2.1739 - val_acc: 0.4459\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9483 - acc: 0.4784 - val_loss: 2.1940 - val_acc: 0.4449\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9670 - acc: 0.4760 - val_loss: 2.1750 - val_acc: 0.4446\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9436 - acc: 0.4798 - val_loss: 2.1733 - val_acc: 0.4493\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9424 - acc: 0.4819 - val_loss: 2.1657 - val_acc: 0.4483\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9422 - acc: 0.4824 - val_loss: 2.1786 - val_acc: 0.4458\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9317 - acc: 0.4823 - val_loss: 2.1617 - val_acc: 0.4508\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9266 - acc: 0.4847 - val_loss: 2.1693 - val_acc: 0.4464\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9266 - acc: 0.4853 - val_loss: 2.1609 - val_acc: 0.4488\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9291 - acc: 0.4841 - val_loss: 2.1687 - val_acc: 0.4487\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9287 - acc: 0.4846 - val_loss: 2.1678 - val_acc: 0.4460\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9428 - acc: 0.4798 - val_loss: 2.1827 - val_acc: 0.4444\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9414 - acc: 0.4784 - val_loss: 2.1726 - val_acc: 0.4460\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9409 - acc: 0.4799 - val_loss: 2.1776 - val_acc: 0.4466\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9297 - acc: 0.4820 - val_loss: 2.1739 - val_acc: 0.4447\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9369 - acc: 0.4812 - val_loss: 2.2036 - val_acc: 0.4440\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9327 - acc: 0.4834 - val_loss: 2.2000 - val_acc: 0.4424\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9350 - acc: 0.4824 - val_loss: 2.1809 - val_acc: 0.4492\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9266 - acc: 0.4834 - val_loss: 2.2203 - val_acc: 0.4376\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9320 - acc: 0.4845 - val_loss: 2.2130 - val_acc: 0.4432\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9474 - acc: 0.4824 - val_loss: 2.2185 - val_acc: 0.4378\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.2045 - acc: 0.4252 - val_loss: 2.1566 - val_acc: 0.4479\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1702 - acc: 0.4317 - val_loss: 2.1505 - val_acc: 0.4501\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1595 - acc: 0.4342 - val_loss: 2.1585 - val_acc: 0.4519\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1470 - acc: 0.4372 - val_loss: 2.1622 - val_acc: 0.4520\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1374 - acc: 0.4383 - val_loss: 2.1709 - val_acc: 0.4494\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1326 - acc: 0.4395 - val_loss: 2.1749 - val_acc: 0.4489\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1318 - acc: 0.4394 - val_loss: 2.1694 - val_acc: 0.4507\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1185 - acc: 0.4416 - val_loss: 2.1630 - val_acc: 0.4521\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1125 - acc: 0.4433 - val_loss: 2.1615 - val_acc: 0.4530\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1054 - acc: 0.4456 - val_loss: 2.1716 - val_acc: 0.4485\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1046 - acc: 0.4451 - val_loss: 2.1603 - val_acc: 0.4507\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0976 - acc: 0.4449 - val_loss: 2.1604 - val_acc: 0.4514\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0963 - acc: 0.4465 - val_loss: 2.1557 - val_acc: 0.4532\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0885 - acc: 0.4484 - val_loss: 2.1641 - val_acc: 0.4511\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0846 - acc: 0.4477 - val_loss: 2.1603 - val_acc: 0.4518\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0795 - acc: 0.4499 - val_loss: 2.1680 - val_acc: 0.4525\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0785 - acc: 0.4483 - val_loss: 2.1613 - val_acc: 0.4502\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0814 - acc: 0.4480 - val_loss: 2.1658 - val_acc: 0.4492\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0721 - acc: 0.4509 - val_loss: 2.1678 - val_acc: 0.4478\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0699 - acc: 0.4503 - val_loss: 2.1709 - val_acc: 0.4499\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0815 - acc: 0.4495 - val_loss: 2.1655 - val_acc: 0.4495\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0736 - acc: 0.4500 - val_loss: 2.1739 - val_acc: 0.4509\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0667 - acc: 0.4527 - val_loss: 2.1568 - val_acc: 0.4522\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0573 - acc: 0.4546 - val_loss: 2.1596 - val_acc: 0.4505\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0520 - acc: 0.4551 - val_loss: 2.1635 - val_acc: 0.4502\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0526 - acc: 0.4562 - val_loss: 2.1526 - val_acc: 0.4523\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0433 - acc: 0.4555 - val_loss: 2.1552 - val_acc: 0.4516\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0465 - acc: 0.4559 - val_loss: 2.1525 - val_acc: 0.4520\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0393 - acc: 0.4575 - val_loss: 2.1565 - val_acc: 0.4503\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0395 - acc: 0.4568 - val_loss: 2.1526 - val_acc: 0.4505\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0363 - acc: 0.4597 - val_loss: 2.1565 - val_acc: 0.4518\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0384 - acc: 0.4577 - val_loss: 2.1602 - val_acc: 0.4492\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0393 - acc: 0.4567 - val_loss: 2.1598 - val_acc: 0.4515\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0328 - acc: 0.4592 - val_loss: 2.1778 - val_acc: 0.4481\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0369 - acc: 0.4591 - val_loss: 2.1633 - val_acc: 0.4521\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0286 - acc: 0.4593 - val_loss: 2.1561 - val_acc: 0.4490\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0343 - acc: 0.4587 - val_loss: 2.1546 - val_acc: 0.4534\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0239 - acc: 0.4607 - val_loss: 2.1538 - val_acc: 0.4497\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0232 - acc: 0.4622 - val_loss: 2.1548 - val_acc: 0.4494\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0162 - acc: 0.4651 - val_loss: 2.1485 - val_acc: 0.4531\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0152 - acc: 0.4639 - val_loss: 2.1577 - val_acc: 0.4509\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0142 - acc: 0.4624 - val_loss: 2.1529 - val_acc: 0.4507\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0129 - acc: 0.4631 - val_loss: 2.1582 - val_acc: 0.4513\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0164 - acc: 0.4638 - val_loss: 2.2001 - val_acc: 0.4425\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0321 - acc: 0.4600 - val_loss: 2.1536 - val_acc: 0.4525\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0030 - acc: 0.4670 - val_loss: 2.1558 - val_acc: 0.4521\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0069 - acc: 0.4644 - val_loss: 2.1608 - val_acc: 0.4509\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0046 - acc: 0.4644 - val_loss: 2.1597 - val_acc: 0.4508\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0007 - acc: 0.4649 - val_loss: 2.1582 - val_acc: 0.4506\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9974 - acc: 0.4650 - val_loss: 2.1575 - val_acc: 0.4488\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9930 - acc: 0.4666 - val_loss: 2.1602 - val_acc: 0.4505\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9969 - acc: 0.4685 - val_loss: 2.1570 - val_acc: 0.4507\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9905 - acc: 0.4699 - val_loss: 2.1629 - val_acc: 0.4510\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9898 - acc: 0.4701 - val_loss: 2.1590 - val_acc: 0.4506\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9975 - acc: 0.4670 - val_loss: 2.1892 - val_acc: 0.4498\n",
      "Epoch 56/100\n",
      "1s - loss: 2.0473 - acc: 0.4581 - val_loss: 2.1999 - val_acc: 0.4397\n",
      "Epoch 57/100\n",
      "1s - loss: 2.0338 - acc: 0.4611 - val_loss: 2.1907 - val_acc: 0.4465\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9978 - acc: 0.4679 - val_loss: 2.1620 - val_acc: 0.4506\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9819 - acc: 0.4733 - val_loss: 2.1569 - val_acc: 0.4512\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9822 - acc: 0.4702 - val_loss: 2.1557 - val_acc: 0.4507\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9772 - acc: 0.4729 - val_loss: 2.1576 - val_acc: 0.4523\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9817 - acc: 0.4705 - val_loss: 2.1595 - val_acc: 0.4505\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9838 - acc: 0.4710 - val_loss: 2.1784 - val_acc: 0.4481\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9813 - acc: 0.4720 - val_loss: 2.1767 - val_acc: 0.4486\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9820 - acc: 0.4693 - val_loss: 2.1624 - val_acc: 0.4522\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9797 - acc: 0.4732 - val_loss: 2.1687 - val_acc: 0.4485\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9778 - acc: 0.4731 - val_loss: 2.1698 - val_acc: 0.4509\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9809 - acc: 0.4708 - val_loss: 2.1535 - val_acc: 0.4511\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9794 - acc: 0.4707 - val_loss: 2.1641 - val_acc: 0.4506\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9671 - acc: 0.4757 - val_loss: 2.1740 - val_acc: 0.4484\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9691 - acc: 0.4724 - val_loss: 2.1820 - val_acc: 0.4496\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9725 - acc: 0.4755 - val_loss: 2.1613 - val_acc: 0.4484\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9699 - acc: 0.4737 - val_loss: 2.1589 - val_acc: 0.4534\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9708 - acc: 0.4746 - val_loss: 2.1723 - val_acc: 0.4454\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9745 - acc: 0.4712 - val_loss: 2.1589 - val_acc: 0.4506\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9551 - acc: 0.4751 - val_loss: 2.1646 - val_acc: 0.4487\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9546 - acc: 0.4776 - val_loss: 2.1919 - val_acc: 0.4481\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9836 - acc: 0.4709 - val_loss: 2.1897 - val_acc: 0.4468\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9586 - acc: 0.4767 - val_loss: 2.1741 - val_acc: 0.4496\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9546 - acc: 0.4778 - val_loss: 2.1780 - val_acc: 0.4480\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9524 - acc: 0.4781 - val_loss: 2.1590 - val_acc: 0.4535\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9473 - acc: 0.4791 - val_loss: 2.1638 - val_acc: 0.4507\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9410 - acc: 0.4808 - val_loss: 2.1510 - val_acc: 0.4507\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9404 - acc: 0.4795 - val_loss: 2.1567 - val_acc: 0.4490\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9516 - acc: 0.4762 - val_loss: 2.1660 - val_acc: 0.4501\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9439 - acc: 0.4796 - val_loss: 2.1543 - val_acc: 0.4494\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9448 - acc: 0.4785 - val_loss: 2.1642 - val_acc: 0.4524\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9540 - acc: 0.4764 - val_loss: 2.1650 - val_acc: 0.4492\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9534 - acc: 0.4771 - val_loss: 2.1766 - val_acc: 0.4509\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9525 - acc: 0.4765 - val_loss: 2.1637 - val_acc: 0.4484\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9403 - acc: 0.4794 - val_loss: 2.1770 - val_acc: 0.4515\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9611 - acc: 0.4760 - val_loss: 2.1634 - val_acc: 0.4471\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9451 - acc: 0.4787 - val_loss: 2.1567 - val_acc: 0.4506\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9321 - acc: 0.4805 - val_loss: 2.1555 - val_acc: 0.4483\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9327 - acc: 0.4809 - val_loss: 2.1871 - val_acc: 0.4482\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9380 - acc: 0.4805 - val_loss: 2.1769 - val_acc: 0.4487\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9474 - acc: 0.4803 - val_loss: 2.1891 - val_acc: 0.4471\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9548 - acc: 0.4779 - val_loss: 2.1666 - val_acc: 0.4485\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9373 - acc: 0.4813 - val_loss: 2.1619 - val_acc: 0.4520\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9308 - acc: 0.4830 - val_loss: 2.1585 - val_acc: 0.4501\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1688 - acc: 0.4329 - val_loss: 2.2291 - val_acc: 0.4406\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1404 - acc: 0.4361 - val_loss: 2.2070 - val_acc: 0.4394\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1259 - acc: 0.4400 - val_loss: 2.2242 - val_acc: 0.4382\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1174 - acc: 0.4434 - val_loss: 2.2326 - val_acc: 0.4360\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1088 - acc: 0.4440 - val_loss: 2.2338 - val_acc: 0.4377\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0980 - acc: 0.4458 - val_loss: 2.2287 - val_acc: 0.4339\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0935 - acc: 0.4461 - val_loss: 2.2472 - val_acc: 0.4315\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0869 - acc: 0.4504 - val_loss: 2.2553 - val_acc: 0.4320\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0843 - acc: 0.4487 - val_loss: 2.2373 - val_acc: 0.4336\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0760 - acc: 0.4513 - val_loss: 2.2554 - val_acc: 0.4290\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0709 - acc: 0.4525 - val_loss: 2.2580 - val_acc: 0.4311\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0648 - acc: 0.4549 - val_loss: 2.2684 - val_acc: 0.4293\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0626 - acc: 0.4538 - val_loss: 2.2781 - val_acc: 0.4271\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0535 - acc: 0.4579 - val_loss: 2.2643 - val_acc: 0.4316\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0541 - acc: 0.4565 - val_loss: 2.2465 - val_acc: 0.4308\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0513 - acc: 0.4567 - val_loss: 2.2661 - val_acc: 0.4265\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0717 - acc: 0.4528 - val_loss: 2.3011 - val_acc: 0.4255\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0454 - acc: 0.4573 - val_loss: 2.2533 - val_acc: 0.4308\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0384 - acc: 0.4582 - val_loss: 2.2661 - val_acc: 0.4309\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0337 - acc: 0.4601 - val_loss: 2.2634 - val_acc: 0.4291\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0341 - acc: 0.4604 - val_loss: 2.2532 - val_acc: 0.4315\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0281 - acc: 0.4625 - val_loss: 2.2412 - val_acc: 0.4318\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0267 - acc: 0.4618 - val_loss: 2.2638 - val_acc: 0.4313\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0200 - acc: 0.4638 - val_loss: 2.2479 - val_acc: 0.4312\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0217 - acc: 0.4647 - val_loss: 2.2608 - val_acc: 0.4318\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0181 - acc: 0.4641 - val_loss: 2.2569 - val_acc: 0.4315\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0142 - acc: 0.4644 - val_loss: 2.2649 - val_acc: 0.4327\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0121 - acc: 0.4666 - val_loss: 2.2381 - val_acc: 0.4321\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0057 - acc: 0.4662 - val_loss: 2.2393 - val_acc: 0.4347\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0062 - acc: 0.4663 - val_loss: 2.2346 - val_acc: 0.4358\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0051 - acc: 0.4659 - val_loss: 2.2275 - val_acc: 0.4371\n",
      "Epoch 32/100\n",
      "1s - loss: 1.9987 - acc: 0.4683 - val_loss: 2.2415 - val_acc: 0.4336\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0031 - acc: 0.4663 - val_loss: 2.2111 - val_acc: 0.4396\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0018 - acc: 0.4660 - val_loss: 2.2444 - val_acc: 0.4372\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9930 - acc: 0.4690 - val_loss: 2.2112 - val_acc: 0.4389\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9982 - acc: 0.4682 - val_loss: 2.2468 - val_acc: 0.4340\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9954 - acc: 0.4683 - val_loss: 2.2171 - val_acc: 0.4373\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9932 - acc: 0.4685 - val_loss: 2.2189 - val_acc: 0.4396\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9829 - acc: 0.4708 - val_loss: 2.2078 - val_acc: 0.4408\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9824 - acc: 0.4738 - val_loss: 2.2319 - val_acc: 0.4374\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9831 - acc: 0.4713 - val_loss: 2.2178 - val_acc: 0.4359\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9855 - acc: 0.4701 - val_loss: 2.2462 - val_acc: 0.4345\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9756 - acc: 0.4728 - val_loss: 2.2353 - val_acc: 0.4339\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9860 - acc: 0.4738 - val_loss: 2.2307 - val_acc: 0.4385\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9744 - acc: 0.4733 - val_loss: 2.2049 - val_acc: 0.4405\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9752 - acc: 0.4703 - val_loss: 2.2168 - val_acc: 0.4421\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9686 - acc: 0.4765 - val_loss: 2.2057 - val_acc: 0.4399\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9624 - acc: 0.4751 - val_loss: 2.2019 - val_acc: 0.4406\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9609 - acc: 0.4763 - val_loss: 2.1981 - val_acc: 0.4409\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9625 - acc: 0.4765 - val_loss: 2.2007 - val_acc: 0.4419\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9563 - acc: 0.4775 - val_loss: 2.2008 - val_acc: 0.4414\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9566 - acc: 0.4768 - val_loss: 2.1974 - val_acc: 0.4409\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9585 - acc: 0.4749 - val_loss: 2.2216 - val_acc: 0.4388\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9566 - acc: 0.4772 - val_loss: 2.1997 - val_acc: 0.4405\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9534 - acc: 0.4795 - val_loss: 2.2328 - val_acc: 0.4380\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9580 - acc: 0.4792 - val_loss: 2.2056 - val_acc: 0.4412\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9595 - acc: 0.4779 - val_loss: 2.2277 - val_acc: 0.4384\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9545 - acc: 0.4783 - val_loss: 2.2046 - val_acc: 0.4401\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9597 - acc: 0.4771 - val_loss: 2.2308 - val_acc: 0.4373\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9574 - acc: 0.4779 - val_loss: 2.2164 - val_acc: 0.4360\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9538 - acc: 0.4787 - val_loss: 2.2221 - val_acc: 0.4398\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9493 - acc: 0.4793 - val_loss: 2.2209 - val_acc: 0.4367\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9556 - acc: 0.4793 - val_loss: 2.2014 - val_acc: 0.4418\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9387 - acc: 0.4822 - val_loss: 2.2013 - val_acc: 0.4418\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9356 - acc: 0.4820 - val_loss: 2.1977 - val_acc: 0.4413\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9337 - acc: 0.4825 - val_loss: 2.2098 - val_acc: 0.4419\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9317 - acc: 0.4820 - val_loss: 2.1880 - val_acc: 0.4446\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9359 - acc: 0.4826 - val_loss: 2.2074 - val_acc: 0.4398\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9288 - acc: 0.4837 - val_loss: 2.2026 - val_acc: 0.4411\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9303 - acc: 0.4824 - val_loss: 2.2175 - val_acc: 0.4368\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9266 - acc: 0.4856 - val_loss: 2.2008 - val_acc: 0.4434\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9277 - acc: 0.4849 - val_loss: 2.2116 - val_acc: 0.4381\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9222 - acc: 0.4865 - val_loss: 2.2263 - val_acc: 0.4408\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9332 - acc: 0.4836 - val_loss: 2.2238 - val_acc: 0.4384\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9353 - acc: 0.4837 - val_loss: 2.2503 - val_acc: 0.4362\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9438 - acc: 0.4810 - val_loss: 2.2140 - val_acc: 0.4391\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9311 - acc: 0.4830 - val_loss: 2.2280 - val_acc: 0.4376\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9331 - acc: 0.4819 - val_loss: 2.2235 - val_acc: 0.4355\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9237 - acc: 0.4875 - val_loss: 2.2241 - val_acc: 0.4383\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9250 - acc: 0.4854 - val_loss: 2.2150 - val_acc: 0.4389\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9164 - acc: 0.4871 - val_loss: 2.2135 - val_acc: 0.4411\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9181 - acc: 0.4865 - val_loss: 2.2049 - val_acc: 0.4397\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9156 - acc: 0.4868 - val_loss: 2.2164 - val_acc: 0.4419\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9176 - acc: 0.4862 - val_loss: 2.2154 - val_acc: 0.4351\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9170 - acc: 0.4863 - val_loss: 2.2250 - val_acc: 0.4392\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9037 - acc: 0.4906 - val_loss: 2.1946 - val_acc: 0.4413\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9047 - acc: 0.4904 - val_loss: 2.2365 - val_acc: 0.4401\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9097 - acc: 0.4861 - val_loss: 2.2194 - val_acc: 0.4369\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9132 - acc: 0.4871 - val_loss: 2.2287 - val_acc: 0.4402\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9245 - acc: 0.4865 - val_loss: 2.2255 - val_acc: 0.4336\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9079 - acc: 0.4892 - val_loss: 2.2155 - val_acc: 0.4415\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9064 - acc: 0.4911 - val_loss: 2.2360 - val_acc: 0.4340\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9020 - acc: 0.4896 - val_loss: 2.2173 - val_acc: 0.4402\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8924 - acc: 0.4939 - val_loss: 2.2246 - val_acc: 0.4386\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9050 - acc: 0.4898 - val_loss: 2.2118 - val_acc: 0.4404\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8974 - acc: 0.4911 - val_loss: 2.1961 - val_acc: 0.4411\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8934 - acc: 0.4896 - val_loss: 2.2130 - val_acc: 0.4411\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9071 - acc: 0.4884 - val_loss: 2.2107 - val_acc: 0.4417\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9036 - acc: 0.4895 - val_loss: 2.2379 - val_acc: 0.4380\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9062 - acc: 0.4889 - val_loss: 2.2073 - val_acc: 0.4403\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1971 - acc: 0.4263 - val_loss: 2.1832 - val_acc: 0.4407\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1753 - acc: 0.4310 - val_loss: 2.1870 - val_acc: 0.4415\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1597 - acc: 0.4319 - val_loss: 2.2043 - val_acc: 0.4374\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1494 - acc: 0.4348 - val_loss: 2.2039 - val_acc: 0.4374\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1420 - acc: 0.4382 - val_loss: 2.2136 - val_acc: 0.4343\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1349 - acc: 0.4392 - val_loss: 2.2023 - val_acc: 0.4377\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1281 - acc: 0.4391 - val_loss: 2.2152 - val_acc: 0.4365\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1162 - acc: 0.4444 - val_loss: 2.2306 - val_acc: 0.4324\n",
      "Epoch 9/100\n",
      "1s - loss: 2.1179 - acc: 0.4430 - val_loss: 2.2139 - val_acc: 0.4360\n",
      "Epoch 10/100\n",
      "1s - loss: 2.1069 - acc: 0.4441 - val_loss: 2.2056 - val_acc: 0.4361\n",
      "Epoch 11/100\n",
      "1s - loss: 2.1043 - acc: 0.4463 - val_loss: 2.2147 - val_acc: 0.4345\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0972 - acc: 0.4473 - val_loss: 2.2161 - val_acc: 0.4345\n",
      "Epoch 13/100\n",
      "1s - loss: 2.1008 - acc: 0.4462 - val_loss: 2.2184 - val_acc: 0.4322\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0895 - acc: 0.4484 - val_loss: 2.2098 - val_acc: 0.4343\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0851 - acc: 0.4505 - val_loss: 2.2057 - val_acc: 0.4347\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0809 - acc: 0.4495 - val_loss: 2.2009 - val_acc: 0.4377\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0768 - acc: 0.4506 - val_loss: 2.2096 - val_acc: 0.4339\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0720 - acc: 0.4506 - val_loss: 2.2051 - val_acc: 0.4363\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0748 - acc: 0.4516 - val_loss: 2.2295 - val_acc: 0.4345\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0923 - acc: 0.4486 - val_loss: 2.2189 - val_acc: 0.4306\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0677 - acc: 0.4533 - val_loss: 2.2156 - val_acc: 0.4330\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0770 - acc: 0.4486 - val_loss: 2.2238 - val_acc: 0.4308\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0599 - acc: 0.4541 - val_loss: 2.2216 - val_acc: 0.4349\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0547 - acc: 0.4552 - val_loss: 2.2043 - val_acc: 0.4365\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0525 - acc: 0.4567 - val_loss: 2.2008 - val_acc: 0.4372\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0484 - acc: 0.4570 - val_loss: 2.2180 - val_acc: 0.4335\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0481 - acc: 0.4560 - val_loss: 2.1964 - val_acc: 0.4368\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0452 - acc: 0.4563 - val_loss: 2.1904 - val_acc: 0.4384\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0406 - acc: 0.4572 - val_loss: 2.2002 - val_acc: 0.4354\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0349 - acc: 0.4591 - val_loss: 2.2064 - val_acc: 0.4347\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0388 - acc: 0.4581 - val_loss: 2.2034 - val_acc: 0.4353\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0413 - acc: 0.4584 - val_loss: 2.2159 - val_acc: 0.4324\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0401 - acc: 0.4594 - val_loss: 2.1987 - val_acc: 0.4358\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0316 - acc: 0.4607 - val_loss: 2.2112 - val_acc: 0.4341\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0373 - acc: 0.4594 - val_loss: 2.2040 - val_acc: 0.4346\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0239 - acc: 0.4623 - val_loss: 2.2039 - val_acc: 0.4361\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0194 - acc: 0.4619 - val_loss: 2.1930 - val_acc: 0.4378\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0279 - acc: 0.4610 - val_loss: 2.2006 - val_acc: 0.4373\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0309 - acc: 0.4608 - val_loss: 2.1988 - val_acc: 0.4374\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0215 - acc: 0.4635 - val_loss: 2.1894 - val_acc: 0.4373\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0126 - acc: 0.4651 - val_loss: 2.1864 - val_acc: 0.4383\n",
      "Epoch 42/100\n",
      "1s - loss: 2.0110 - acc: 0.4654 - val_loss: 2.2049 - val_acc: 0.4366\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0232 - acc: 0.4630 - val_loss: 2.1929 - val_acc: 0.4376\n",
      "Epoch 44/100\n",
      "1s - loss: 2.0134 - acc: 0.4625 - val_loss: 2.2207 - val_acc: 0.4359\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0637 - acc: 0.4542 - val_loss: 2.2507 - val_acc: 0.4262\n",
      "Epoch 46/100\n",
      "1s - loss: 2.0593 - acc: 0.4547 - val_loss: 2.2260 - val_acc: 0.4301\n",
      "Epoch 47/100\n",
      "1s - loss: 2.0224 - acc: 0.4633 - val_loss: 2.1960 - val_acc: 0.4380\n",
      "Epoch 48/100\n",
      "1s - loss: 2.0056 - acc: 0.4655 - val_loss: 2.2048 - val_acc: 0.4391\n",
      "Epoch 49/100\n",
      "1s - loss: 2.0023 - acc: 0.4668 - val_loss: 2.2185 - val_acc: 0.4350\n",
      "Epoch 50/100\n",
      "1s - loss: 2.0068 - acc: 0.4662 - val_loss: 2.2221 - val_acc: 0.4380\n",
      "Epoch 51/100\n",
      "1s - loss: 2.0279 - acc: 0.4647 - val_loss: 2.2406 - val_acc: 0.4294\n",
      "Epoch 52/100\n",
      "1s - loss: 2.0099 - acc: 0.4654 - val_loss: 2.1904 - val_acc: 0.4384\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9887 - acc: 0.4696 - val_loss: 2.2022 - val_acc: 0.4364\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9823 - acc: 0.4701 - val_loss: 2.1913 - val_acc: 0.4375\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9815 - acc: 0.4721 - val_loss: 2.1954 - val_acc: 0.4382\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9913 - acc: 0.4696 - val_loss: 2.2024 - val_acc: 0.4376\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9880 - acc: 0.4691 - val_loss: 2.2032 - val_acc: 0.4371\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9846 - acc: 0.4717 - val_loss: 2.1984 - val_acc: 0.4368\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9824 - acc: 0.4712 - val_loss: 2.1972 - val_acc: 0.4386\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9863 - acc: 0.4712 - val_loss: 2.2208 - val_acc: 0.4334\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9968 - acc: 0.4684 - val_loss: 2.2203 - val_acc: 0.4357\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9829 - acc: 0.4717 - val_loss: 2.2184 - val_acc: 0.4359\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9770 - acc: 0.4725 - val_loss: 2.1974 - val_acc: 0.4359\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9755 - acc: 0.4718 - val_loss: 2.2544 - val_acc: 0.4297\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9905 - acc: 0.4693 - val_loss: 2.2177 - val_acc: 0.4355\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9800 - acc: 0.4710 - val_loss: 2.2272 - val_acc: 0.4341\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9693 - acc: 0.4746 - val_loss: 2.2037 - val_acc: 0.4386\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9707 - acc: 0.4730 - val_loss: 2.2266 - val_acc: 0.4332\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9732 - acc: 0.4755 - val_loss: 2.2224 - val_acc: 0.4362\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9802 - acc: 0.4718 - val_loss: 2.2801 - val_acc: 0.4267\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9779 - acc: 0.4741 - val_loss: 2.2039 - val_acc: 0.4393\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9725 - acc: 0.4739 - val_loss: 2.2048 - val_acc: 0.4374\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9626 - acc: 0.4751 - val_loss: 2.2095 - val_acc: 0.4372\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9571 - acc: 0.4774 - val_loss: 2.2282 - val_acc: 0.4338\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9598 - acc: 0.4759 - val_loss: 2.2001 - val_acc: 0.4381\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9569 - acc: 0.4763 - val_loss: 2.2262 - val_acc: 0.4313\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9610 - acc: 0.4774 - val_loss: 2.2021 - val_acc: 0.4370\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9607 - acc: 0.4745 - val_loss: 2.1971 - val_acc: 0.4377\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9549 - acc: 0.4763 - val_loss: 2.2104 - val_acc: 0.4375\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9698 - acc: 0.4732 - val_loss: 2.2161 - val_acc: 0.4361\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9562 - acc: 0.4761 - val_loss: 2.2006 - val_acc: 0.4363\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9501 - acc: 0.4788 - val_loss: 2.2066 - val_acc: 0.4357\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9411 - acc: 0.4813 - val_loss: 2.2003 - val_acc: 0.4377\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9404 - acc: 0.4814 - val_loss: 2.1918 - val_acc: 0.4397\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9384 - acc: 0.4817 - val_loss: 2.2108 - val_acc: 0.4370\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9414 - acc: 0.4804 - val_loss: 2.2096 - val_acc: 0.4392\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9606 - acc: 0.4766 - val_loss: 2.2283 - val_acc: 0.4338\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9874 - acc: 0.4704 - val_loss: 2.2206 - val_acc: 0.4353\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9632 - acc: 0.4753 - val_loss: 2.2125 - val_acc: 0.4362\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9477 - acc: 0.4777 - val_loss: 2.2034 - val_acc: 0.4396\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9417 - acc: 0.4807 - val_loss: 2.2222 - val_acc: 0.4355\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9501 - acc: 0.4799 - val_loss: 2.2025 - val_acc: 0.4385\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9281 - acc: 0.4839 - val_loss: 2.2168 - val_acc: 0.4313\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9321 - acc: 0.4804 - val_loss: 2.2217 - val_acc: 0.4375\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9430 - acc: 0.4818 - val_loss: 2.2131 - val_acc: 0.4369\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9385 - acc: 0.4820 - val_loss: 2.2081 - val_acc: 0.4372\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9215 - acc: 0.4871 - val_loss: 2.2012 - val_acc: 0.4377\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9225 - acc: 0.4846 - val_loss: 2.2032 - val_acc: 0.4381\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9314 - acc: 0.4823 - val_loss: 2.2229 - val_acc: 0.4340\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9300 - acc: 0.4830 - val_loss: 2.2137 - val_acc: 0.4358\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1772 - acc: 0.4277 - val_loss: 2.1748 - val_acc: 0.4436\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1530 - acc: 0.4349 - val_loss: 2.1855 - val_acc: 0.4416\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1407 - acc: 0.4368 - val_loss: 2.1770 - val_acc: 0.4400\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1267 - acc: 0.4403 - val_loss: 2.1875 - val_acc: 0.4383\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1227 - acc: 0.4417 - val_loss: 2.1956 - val_acc: 0.4368\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1133 - acc: 0.4404 - val_loss: 2.2001 - val_acc: 0.4353\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1069 - acc: 0.4449 - val_loss: 2.2140 - val_acc: 0.4328\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1042 - acc: 0.4431 - val_loss: 2.2109 - val_acc: 0.4318\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0944 - acc: 0.4471 - val_loss: 2.2135 - val_acc: 0.4323\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0891 - acc: 0.4463 - val_loss: 2.2113 - val_acc: 0.4337\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0833 - acc: 0.4488 - val_loss: 2.2160 - val_acc: 0.4313\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0801 - acc: 0.4480 - val_loss: 2.2150 - val_acc: 0.4337\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0727 - acc: 0.4515 - val_loss: 2.2230 - val_acc: 0.4306\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0802 - acc: 0.4504 - val_loss: 2.2170 - val_acc: 0.4300\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0686 - acc: 0.4519 - val_loss: 2.2286 - val_acc: 0.4314\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0732 - acc: 0.4509 - val_loss: 2.2086 - val_acc: 0.4324\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0621 - acc: 0.4509 - val_loss: 2.2099 - val_acc: 0.4331\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0554 - acc: 0.4544 - val_loss: 2.2126 - val_acc: 0.4307\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0535 - acc: 0.4529 - val_loss: 2.2027 - val_acc: 0.4338\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0474 - acc: 0.4565 - val_loss: 2.2415 - val_acc: 0.4299\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0681 - acc: 0.4527 - val_loss: 2.2111 - val_acc: 0.4320\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0502 - acc: 0.4544 - val_loss: 2.2066 - val_acc: 0.4325\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0420 - acc: 0.4581 - val_loss: 2.2137 - val_acc: 0.4334\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0404 - acc: 0.4571 - val_loss: 2.2419 - val_acc: 0.4265\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0489 - acc: 0.4556 - val_loss: 2.2229 - val_acc: 0.4320\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0374 - acc: 0.4565 - val_loss: 2.2222 - val_acc: 0.4337\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0374 - acc: 0.4585 - val_loss: 2.1996 - val_acc: 0.4348\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0286 - acc: 0.4593 - val_loss: 2.2042 - val_acc: 0.4354\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0223 - acc: 0.4601 - val_loss: 2.2069 - val_acc: 0.4344\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0206 - acc: 0.4623 - val_loss: 2.1975 - val_acc: 0.4377\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0202 - acc: 0.4615 - val_loss: 2.1996 - val_acc: 0.4365\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0190 - acc: 0.4617 - val_loss: 2.2006 - val_acc: 0.4344\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0127 - acc: 0.4636 - val_loss: 2.1996 - val_acc: 0.4375\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0215 - acc: 0.4620 - val_loss: 2.2058 - val_acc: 0.4337\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0101 - acc: 0.4643 - val_loss: 2.1969 - val_acc: 0.4375\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0086 - acc: 0.4631 - val_loss: 2.1992 - val_acc: 0.4371\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0080 - acc: 0.4647 - val_loss: 2.2067 - val_acc: 0.4354\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0087 - acc: 0.4653 - val_loss: 2.1961 - val_acc: 0.4349\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0020 - acc: 0.4654 - val_loss: 2.2023 - val_acc: 0.4374\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0011 - acc: 0.4667 - val_loss: 2.1901 - val_acc: 0.4385\n",
      "Epoch 41/100\n",
      "1s - loss: 2.0044 - acc: 0.4661 - val_loss: 2.1866 - val_acc: 0.4412\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9963 - acc: 0.4665 - val_loss: 2.1905 - val_acc: 0.4410\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9952 - acc: 0.4665 - val_loss: 2.1839 - val_acc: 0.4405\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9904 - acc: 0.4670 - val_loss: 2.2044 - val_acc: 0.4373\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9978 - acc: 0.4667 - val_loss: 2.1925 - val_acc: 0.4392\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9863 - acc: 0.4678 - val_loss: 2.1852 - val_acc: 0.4404\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9820 - acc: 0.4706 - val_loss: 2.2078 - val_acc: 0.4372\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9937 - acc: 0.4686 - val_loss: 2.1954 - val_acc: 0.4393\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9859 - acc: 0.4698 - val_loss: 2.1880 - val_acc: 0.4399\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9816 - acc: 0.4720 - val_loss: 2.2051 - val_acc: 0.4374\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9851 - acc: 0.4687 - val_loss: 2.1831 - val_acc: 0.4417\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9770 - acc: 0.4716 - val_loss: 2.1877 - val_acc: 0.4403\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9683 - acc: 0.4727 - val_loss: 2.1832 - val_acc: 0.4420\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9643 - acc: 0.4736 - val_loss: 2.1863 - val_acc: 0.4411\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9616 - acc: 0.4768 - val_loss: 2.1879 - val_acc: 0.4405\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9624 - acc: 0.4750 - val_loss: 2.1810 - val_acc: 0.4413\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9625 - acc: 0.4746 - val_loss: 2.1821 - val_acc: 0.4390\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9601 - acc: 0.4751 - val_loss: 2.1774 - val_acc: 0.4428\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9601 - acc: 0.4767 - val_loss: 2.1764 - val_acc: 0.4418\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9586 - acc: 0.4751 - val_loss: 2.1997 - val_acc: 0.4383\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9626 - acc: 0.4768 - val_loss: 2.2018 - val_acc: 0.4401\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9820 - acc: 0.4737 - val_loss: 2.2093 - val_acc: 0.4357\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9653 - acc: 0.4739 - val_loss: 2.1863 - val_acc: 0.4402\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9569 - acc: 0.4758 - val_loss: 2.1859 - val_acc: 0.4418\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9667 - acc: 0.4742 - val_loss: 2.2105 - val_acc: 0.4345\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9602 - acc: 0.4758 - val_loss: 2.1865 - val_acc: 0.4416\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9600 - acc: 0.4746 - val_loss: 2.2079 - val_acc: 0.4377\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9599 - acc: 0.4750 - val_loss: 2.1825 - val_acc: 0.4392\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9521 - acc: 0.4751 - val_loss: 2.1904 - val_acc: 0.4391\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9467 - acc: 0.4795 - val_loss: 2.1724 - val_acc: 0.4437\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9445 - acc: 0.4796 - val_loss: 2.2049 - val_acc: 0.4373\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9410 - acc: 0.4806 - val_loss: 2.1730 - val_acc: 0.4437\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9339 - acc: 0.4820 - val_loss: 2.1852 - val_acc: 0.4399\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9364 - acc: 0.4812 - val_loss: 2.1846 - val_acc: 0.4434\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9431 - acc: 0.4790 - val_loss: 2.1931 - val_acc: 0.4392\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9372 - acc: 0.4792 - val_loss: 2.1795 - val_acc: 0.4424\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9312 - acc: 0.4813 - val_loss: 2.1779 - val_acc: 0.4405\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9241 - acc: 0.4838 - val_loss: 2.2061 - val_acc: 0.4366\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9367 - acc: 0.4806 - val_loss: 2.1922 - val_acc: 0.4386\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9258 - acc: 0.4841 - val_loss: 2.1821 - val_acc: 0.4399\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9180 - acc: 0.4849 - val_loss: 2.1911 - val_acc: 0.4387\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9235 - acc: 0.4844 - val_loss: 2.1830 - val_acc: 0.4411\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9271 - acc: 0.4810 - val_loss: 2.2001 - val_acc: 0.4394\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9265 - acc: 0.4835 - val_loss: 2.1874 - val_acc: 0.4403\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9367 - acc: 0.4820 - val_loss: 2.1983 - val_acc: 0.4411\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9351 - acc: 0.4809 - val_loss: 2.2142 - val_acc: 0.4390\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9427 - acc: 0.4819 - val_loss: 2.2418 - val_acc: 0.4355\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9424 - acc: 0.4819 - val_loss: 2.1976 - val_acc: 0.4389\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9215 - acc: 0.4857 - val_loss: 2.1914 - val_acc: 0.4412\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9189 - acc: 0.4841 - val_loss: 2.1993 - val_acc: 0.4379\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9211 - acc: 0.4839 - val_loss: 2.1961 - val_acc: 0.4391\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9256 - acc: 0.4823 - val_loss: 2.1761 - val_acc: 0.4419\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9118 - acc: 0.4843 - val_loss: 2.1932 - val_acc: 0.4418\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9119 - acc: 0.4864 - val_loss: 2.1829 - val_acc: 0.4431\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9060 - acc: 0.4866 - val_loss: 2.1971 - val_acc: 0.4380\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9028 - acc: 0.4894 - val_loss: 2.1860 - val_acc: 0.4400\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9145 - acc: 0.4856 - val_loss: 2.1956 - val_acc: 0.4407\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9066 - acc: 0.4887 - val_loss: 2.1950 - val_acc: 0.4397\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9182 - acc: 0.4850 - val_loss: 2.2119 - val_acc: 0.4356\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9119 - acc: 0.4875 - val_loss: 2.1813 - val_acc: 0.4407\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1645 - acc: 0.4326 - val_loss: 2.1782 - val_acc: 0.4428\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1436 - acc: 0.4371 - val_loss: 2.1723 - val_acc: 0.4419\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1452 - acc: 0.4359 - val_loss: 2.1828 - val_acc: 0.4382\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1182 - acc: 0.4414 - val_loss: 2.1825 - val_acc: 0.4382\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1144 - acc: 0.4414 - val_loss: 2.1772 - val_acc: 0.4393\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1075 - acc: 0.4430 - val_loss: 2.1799 - val_acc: 0.4383\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0971 - acc: 0.4446 - val_loss: 2.1796 - val_acc: 0.4387\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0904 - acc: 0.4473 - val_loss: 2.1844 - val_acc: 0.4398\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0844 - acc: 0.4480 - val_loss: 2.1823 - val_acc: 0.4383\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0784 - acc: 0.4495 - val_loss: 2.1787 - val_acc: 0.4401\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0746 - acc: 0.4501 - val_loss: 2.1925 - val_acc: 0.4389\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0906 - acc: 0.4467 - val_loss: 2.1938 - val_acc: 0.4364\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0701 - acc: 0.4518 - val_loss: 2.1914 - val_acc: 0.4372\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0634 - acc: 0.4533 - val_loss: 2.2053 - val_acc: 0.4379\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0637 - acc: 0.4526 - val_loss: 2.1876 - val_acc: 0.4369\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0555 - acc: 0.4545 - val_loss: 2.1908 - val_acc: 0.4392\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0503 - acc: 0.4571 - val_loss: 2.1950 - val_acc: 0.4396\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0475 - acc: 0.4559 - val_loss: 2.1871 - val_acc: 0.4391\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0396 - acc: 0.4571 - val_loss: 2.1830 - val_acc: 0.4389\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0379 - acc: 0.4569 - val_loss: 2.1820 - val_acc: 0.4416\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0360 - acc: 0.4586 - val_loss: 2.1816 - val_acc: 0.4399\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0281 - acc: 0.4606 - val_loss: 2.1894 - val_acc: 0.4398\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0266 - acc: 0.4630 - val_loss: 2.1823 - val_acc: 0.4410\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0245 - acc: 0.4608 - val_loss: 2.1862 - val_acc: 0.4391\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0262 - acc: 0.4622 - val_loss: 2.1860 - val_acc: 0.4406\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0217 - acc: 0.4620 - val_loss: 2.1844 - val_acc: 0.4379\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0219 - acc: 0.4625 - val_loss: 2.1792 - val_acc: 0.4409\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0167 - acc: 0.4634 - val_loss: 2.1933 - val_acc: 0.4381\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0086 - acc: 0.4634 - val_loss: 2.1781 - val_acc: 0.4413\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0102 - acc: 0.4646 - val_loss: 2.1798 - val_acc: 0.4399\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0102 - acc: 0.4657 - val_loss: 2.1707 - val_acc: 0.4420\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0037 - acc: 0.4664 - val_loss: 2.1743 - val_acc: 0.4402\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0037 - acc: 0.4673 - val_loss: 2.2019 - val_acc: 0.4404\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0206 - acc: 0.4633 - val_loss: 2.1853 - val_acc: 0.4391\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0084 - acc: 0.4636 - val_loss: 2.1917 - val_acc: 0.4402\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0145 - acc: 0.4653 - val_loss: 2.1739 - val_acc: 0.4429\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9970 - acc: 0.4689 - val_loss: 2.1795 - val_acc: 0.4435\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9954 - acc: 0.4691 - val_loss: 2.1740 - val_acc: 0.4426\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9907 - acc: 0.4679 - val_loss: 2.1724 - val_acc: 0.4420\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9924 - acc: 0.4684 - val_loss: 2.1735 - val_acc: 0.4435\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9864 - acc: 0.4715 - val_loss: 2.1683 - val_acc: 0.4436\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9884 - acc: 0.4701 - val_loss: 2.1717 - val_acc: 0.4447\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9881 - acc: 0.4689 - val_loss: 2.1690 - val_acc: 0.4444\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9750 - acc: 0.4731 - val_loss: 2.1752 - val_acc: 0.4433\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9982 - acc: 0.4667 - val_loss: 2.1827 - val_acc: 0.4407\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9765 - acc: 0.4719 - val_loss: 2.1688 - val_acc: 0.4441\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9751 - acc: 0.4727 - val_loss: 2.1782 - val_acc: 0.4425\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9751 - acc: 0.4752 - val_loss: 2.1788 - val_acc: 0.4421\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9793 - acc: 0.4719 - val_loss: 2.1886 - val_acc: 0.4414\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9781 - acc: 0.4724 - val_loss: 2.1776 - val_acc: 0.4441\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9711 - acc: 0.4716 - val_loss: 2.1645 - val_acc: 0.4463\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9602 - acc: 0.4766 - val_loss: 2.1645 - val_acc: 0.4451\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9621 - acc: 0.4755 - val_loss: 2.1723 - val_acc: 0.4432\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9608 - acc: 0.4760 - val_loss: 2.1715 - val_acc: 0.4436\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9574 - acc: 0.4782 - val_loss: 2.1738 - val_acc: 0.4454\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9594 - acc: 0.4774 - val_loss: 2.1588 - val_acc: 0.4451\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9531 - acc: 0.4781 - val_loss: 2.1629 - val_acc: 0.4434\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9491 - acc: 0.4773 - val_loss: 2.1690 - val_acc: 0.4404\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9469 - acc: 0.4785 - val_loss: 2.1699 - val_acc: 0.4423\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9508 - acc: 0.4787 - val_loss: 2.1867 - val_acc: 0.4407\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9516 - acc: 0.4784 - val_loss: 2.1909 - val_acc: 0.4390\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9475 - acc: 0.4795 - val_loss: 2.1671 - val_acc: 0.4443\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9511 - acc: 0.4778 - val_loss: 2.1613 - val_acc: 0.4441\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9455 - acc: 0.4802 - val_loss: 2.1753 - val_acc: 0.4407\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9479 - acc: 0.4781 - val_loss: 2.1724 - val_acc: 0.4414\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9444 - acc: 0.4788 - val_loss: 2.1820 - val_acc: 0.4414\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9492 - acc: 0.4782 - val_loss: 2.1838 - val_acc: 0.4397\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9541 - acc: 0.4765 - val_loss: 2.1913 - val_acc: 0.4402\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9490 - acc: 0.4775 - val_loss: 2.1742 - val_acc: 0.4404\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9341 - acc: 0.4821 - val_loss: 2.1865 - val_acc: 0.4406\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9450 - acc: 0.4800 - val_loss: 2.1733 - val_acc: 0.4444\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9296 - acc: 0.4831 - val_loss: 2.1794 - val_acc: 0.4413\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9329 - acc: 0.4823 - val_loss: 2.1723 - val_acc: 0.4424\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9329 - acc: 0.4819 - val_loss: 2.1763 - val_acc: 0.4416\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9318 - acc: 0.4812 - val_loss: 2.1698 - val_acc: 0.4447\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9337 - acc: 0.4807 - val_loss: 2.1712 - val_acc: 0.4437\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9237 - acc: 0.4836 - val_loss: 2.1684 - val_acc: 0.4453\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9241 - acc: 0.4847 - val_loss: 2.1791 - val_acc: 0.4415\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9349 - acc: 0.4817 - val_loss: 2.1828 - val_acc: 0.4429\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9421 - acc: 0.4816 - val_loss: 2.1825 - val_acc: 0.4409\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9324 - acc: 0.4821 - val_loss: 2.1756 - val_acc: 0.4420\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9342 - acc: 0.4816 - val_loss: 2.1799 - val_acc: 0.4423\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9291 - acc: 0.4819 - val_loss: 2.1699 - val_acc: 0.4426\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9192 - acc: 0.4865 - val_loss: 2.1734 - val_acc: 0.4440\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9060 - acc: 0.4879 - val_loss: 2.1661 - val_acc: 0.4451\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9065 - acc: 0.4877 - val_loss: 2.1743 - val_acc: 0.4427\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9086 - acc: 0.4869 - val_loss: 2.1685 - val_acc: 0.4432\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9092 - acc: 0.4898 - val_loss: 2.1875 - val_acc: 0.4429\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9177 - acc: 0.4844 - val_loss: 2.1764 - val_acc: 0.4427\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9174 - acc: 0.4876 - val_loss: 2.1959 - val_acc: 0.4391\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9141 - acc: 0.4857 - val_loss: 2.1738 - val_acc: 0.4414\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9114 - acc: 0.4873 - val_loss: 2.1744 - val_acc: 0.4440\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9004 - acc: 0.4900 - val_loss: 2.1739 - val_acc: 0.4431\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9061 - acc: 0.4897 - val_loss: 2.1881 - val_acc: 0.4430\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9093 - acc: 0.4887 - val_loss: 2.1752 - val_acc: 0.4416\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9066 - acc: 0.4886 - val_loss: 2.1841 - val_acc: 0.4399\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9068 - acc: 0.4891 - val_loss: 2.1829 - val_acc: 0.4416\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9027 - acc: 0.4901 - val_loss: 2.1776 - val_acc: 0.4401\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8925 - acc: 0.4915 - val_loss: 2.1749 - val_acc: 0.4436\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8964 - acc: 0.4909 - val_loss: 2.2013 - val_acc: 0.4394\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1734 - acc: 0.4293 - val_loss: 2.1286 - val_acc: 0.4513\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1503 - acc: 0.4318 - val_loss: 2.1361 - val_acc: 0.4489\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1332 - acc: 0.4353 - val_loss: 2.1349 - val_acc: 0.4492\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1208 - acc: 0.4381 - val_loss: 2.1344 - val_acc: 0.4462\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1173 - acc: 0.4370 - val_loss: 2.1555 - val_acc: 0.4457\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1066 - acc: 0.4408 - val_loss: 2.1511 - val_acc: 0.4464\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1010 - acc: 0.4415 - val_loss: 2.1474 - val_acc: 0.4460\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0907 - acc: 0.4440 - val_loss: 2.1547 - val_acc: 0.4428\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0860 - acc: 0.4449 - val_loss: 2.1549 - val_acc: 0.4428\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0841 - acc: 0.4453 - val_loss: 2.1553 - val_acc: 0.4440\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0806 - acc: 0.4462 - val_loss: 2.1648 - val_acc: 0.4423\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0896 - acc: 0.4450 - val_loss: 2.1464 - val_acc: 0.4468\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0724 - acc: 0.4504 - val_loss: 2.1510 - val_acc: 0.4426\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0680 - acc: 0.4486 - val_loss: 2.1479 - val_acc: 0.4450\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0621 - acc: 0.4484 - val_loss: 2.1384 - val_acc: 0.4463\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0608 - acc: 0.4513 - val_loss: 2.1447 - val_acc: 0.4444\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0569 - acc: 0.4509 - val_loss: 2.1434 - val_acc: 0.4466\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0484 - acc: 0.4535 - val_loss: 2.1522 - val_acc: 0.4441\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0454 - acc: 0.4520 - val_loss: 2.1486 - val_acc: 0.4447\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0449 - acc: 0.4546 - val_loss: 2.1494 - val_acc: 0.4470\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0408 - acc: 0.4555 - val_loss: 2.1515 - val_acc: 0.4441\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0353 - acc: 0.4533 - val_loss: 2.1582 - val_acc: 0.4461\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0350 - acc: 0.4557 - val_loss: 2.1512 - val_acc: 0.4467\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0534 - acc: 0.4513 - val_loss: 2.1615 - val_acc: 0.4450\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0312 - acc: 0.4562 - val_loss: 2.1470 - val_acc: 0.4488\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0243 - acc: 0.4566 - val_loss: 2.1431 - val_acc: 0.4452\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0231 - acc: 0.4595 - val_loss: 2.1382 - val_acc: 0.4479\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0198 - acc: 0.4585 - val_loss: 2.1477 - val_acc: 0.4446\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0231 - acc: 0.4587 - val_loss: 2.1418 - val_acc: 0.4453\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0198 - acc: 0.4586 - val_loss: 2.1453 - val_acc: 0.4473\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0238 - acc: 0.4572 - val_loss: 2.1538 - val_acc: 0.4451\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0175 - acc: 0.4603 - val_loss: 2.1409 - val_acc: 0.4478\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0090 - acc: 0.4606 - val_loss: 2.1517 - val_acc: 0.4450\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0030 - acc: 0.4618 - val_loss: 2.1364 - val_acc: 0.4476\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9974 - acc: 0.4648 - val_loss: 2.1353 - val_acc: 0.4493\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9990 - acc: 0.4636 - val_loss: 2.1332 - val_acc: 0.4479\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9981 - acc: 0.4638 - val_loss: 2.1377 - val_acc: 0.4478\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0024 - acc: 0.4628 - val_loss: 2.1352 - val_acc: 0.4479\n",
      "Epoch 39/100\n",
      "1s - loss: 2.0014 - acc: 0.4641 - val_loss: 2.1701 - val_acc: 0.4430\n",
      "Epoch 40/100\n",
      "1s - loss: 2.0098 - acc: 0.4620 - val_loss: 2.1420 - val_acc: 0.4471\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9964 - acc: 0.4623 - val_loss: 2.1373 - val_acc: 0.4461\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9997 - acc: 0.4631 - val_loss: 2.1450 - val_acc: 0.4471\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9878 - acc: 0.4663 - val_loss: 2.1401 - val_acc: 0.4468\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9823 - acc: 0.4674 - val_loss: 2.1345 - val_acc: 0.4478\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9832 - acc: 0.4671 - val_loss: 2.1282 - val_acc: 0.4478\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9796 - acc: 0.4683 - val_loss: 2.1450 - val_acc: 0.4462\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9788 - acc: 0.4698 - val_loss: 2.1280 - val_acc: 0.4475\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9714 - acc: 0.4718 - val_loss: 2.1371 - val_acc: 0.4487\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9798 - acc: 0.4691 - val_loss: 2.1302 - val_acc: 0.4475\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9725 - acc: 0.4701 - val_loss: 2.1326 - val_acc: 0.4470\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9690 - acc: 0.4703 - val_loss: 2.1261 - val_acc: 0.4488\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9699 - acc: 0.4706 - val_loss: 2.1248 - val_acc: 0.4491\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9658 - acc: 0.4717 - val_loss: 2.1224 - val_acc: 0.4499\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9653 - acc: 0.4711 - val_loss: 2.1272 - val_acc: 0.4503\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9611 - acc: 0.4739 - val_loss: 2.1323 - val_acc: 0.4457\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9576 - acc: 0.4725 - val_loss: 2.1307 - val_acc: 0.4489\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9677 - acc: 0.4713 - val_loss: 2.1239 - val_acc: 0.4497\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9581 - acc: 0.4728 - val_loss: 2.1195 - val_acc: 0.4507\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9497 - acc: 0.4742 - val_loss: 2.1273 - val_acc: 0.4484\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9552 - acc: 0.4724 - val_loss: 2.1296 - val_acc: 0.4483\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9530 - acc: 0.4752 - val_loss: 2.1219 - val_acc: 0.4492\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9570 - acc: 0.4750 - val_loss: 2.1412 - val_acc: 0.4470\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9590 - acc: 0.4744 - val_loss: 2.1370 - val_acc: 0.4470\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9574 - acc: 0.4734 - val_loss: 2.1321 - val_acc: 0.4479\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9481 - acc: 0.4757 - val_loss: 2.1479 - val_acc: 0.4469\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9513 - acc: 0.4754 - val_loss: 2.1391 - val_acc: 0.4453\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9620 - acc: 0.4727 - val_loss: 2.1579 - val_acc: 0.4456\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9592 - acc: 0.4738 - val_loss: 2.1312 - val_acc: 0.4485\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9432 - acc: 0.4758 - val_loss: 2.1259 - val_acc: 0.4484\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9354 - acc: 0.4773 - val_loss: 2.1233 - val_acc: 0.4507\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9273 - acc: 0.4789 - val_loss: 2.1172 - val_acc: 0.4506\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9284 - acc: 0.4790 - val_loss: 2.1299 - val_acc: 0.4475\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9351 - acc: 0.4789 - val_loss: 2.1419 - val_acc: 0.4484\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9429 - acc: 0.4760 - val_loss: 2.1601 - val_acc: 0.4443\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9451 - acc: 0.4767 - val_loss: 2.1372 - val_acc: 0.4494\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9299 - acc: 0.4803 - val_loss: 2.1486 - val_acc: 0.4462\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9330 - acc: 0.4789 - val_loss: 2.1512 - val_acc: 0.4453\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9355 - acc: 0.4795 - val_loss: 2.1687 - val_acc: 0.4430\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9352 - acc: 0.4800 - val_loss: 2.1450 - val_acc: 0.4457\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9300 - acc: 0.4801 - val_loss: 2.1523 - val_acc: 0.4475\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9307 - acc: 0.4789 - val_loss: 2.1383 - val_acc: 0.4449\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9265 - acc: 0.4802 - val_loss: 2.1355 - val_acc: 0.4468\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9226 - acc: 0.4807 - val_loss: 2.1304 - val_acc: 0.4476\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9255 - acc: 0.4775 - val_loss: 2.1338 - val_acc: 0.4480\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9209 - acc: 0.4819 - val_loss: 2.1383 - val_acc: 0.4491\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9300 - acc: 0.4790 - val_loss: 2.1292 - val_acc: 0.4474\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9258 - acc: 0.4812 - val_loss: 2.1358 - val_acc: 0.4483\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9129 - acc: 0.4839 - val_loss: 2.1311 - val_acc: 0.4498\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9093 - acc: 0.4830 - val_loss: 2.1270 - val_acc: 0.4492\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9109 - acc: 0.4825 - val_loss: 2.1366 - val_acc: 0.4488\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9036 - acc: 0.4858 - val_loss: 2.1290 - val_acc: 0.4486\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9054 - acc: 0.4841 - val_loss: 2.1233 - val_acc: 0.4504\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9028 - acc: 0.4851 - val_loss: 2.1431 - val_acc: 0.4467\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9111 - acc: 0.4844 - val_loss: 2.1347 - val_acc: 0.4470\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9079 - acc: 0.4857 - val_loss: 2.1696 - val_acc: 0.4430\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9199 - acc: 0.4814 - val_loss: 2.1471 - val_acc: 0.4476\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9189 - acc: 0.4826 - val_loss: 2.1394 - val_acc: 0.4490\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9113 - acc: 0.4839 - val_loss: 2.1344 - val_acc: 0.4493\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9021 - acc: 0.4856 - val_loss: 2.1396 - val_acc: 0.4485\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9190 - acc: 0.4823 - val_loss: 2.1277 - val_acc: 0.4498\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1788 - acc: 0.4332 - val_loss: 2.2153 - val_acc: 0.4405\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1541 - acc: 0.4348 - val_loss: 2.2187 - val_acc: 0.4404\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1392 - acc: 0.4405 - val_loss: 2.2213 - val_acc: 0.4398\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1268 - acc: 0.4418 - val_loss: 2.2312 - val_acc: 0.4397\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1190 - acc: 0.4421 - val_loss: 2.2343 - val_acc: 0.4384\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1236 - acc: 0.4440 - val_loss: 2.2477 - val_acc: 0.4352\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1083 - acc: 0.4448 - val_loss: 2.2238 - val_acc: 0.4377\n",
      "Epoch 8/100\n",
      "1s - loss: 2.1004 - acc: 0.4475 - val_loss: 2.2252 - val_acc: 0.4393\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0937 - acc: 0.4487 - val_loss: 2.2294 - val_acc: 0.4364\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0907 - acc: 0.4479 - val_loss: 2.2239 - val_acc: 0.4365\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0851 - acc: 0.4499 - val_loss: 2.2336 - val_acc: 0.4362\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0879 - acc: 0.4495 - val_loss: 2.2195 - val_acc: 0.4403\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0790 - acc: 0.4515 - val_loss: 2.2192 - val_acc: 0.4403\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0712 - acc: 0.4536 - val_loss: 2.2206 - val_acc: 0.4378\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0681 - acc: 0.4545 - val_loss: 2.2136 - val_acc: 0.4397\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0638 - acc: 0.4537 - val_loss: 2.2117 - val_acc: 0.4427\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0613 - acc: 0.4534 - val_loss: 2.2292 - val_acc: 0.4362\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0585 - acc: 0.4559 - val_loss: 2.2183 - val_acc: 0.4390\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0565 - acc: 0.4565 - val_loss: 2.2162 - val_acc: 0.4382\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0542 - acc: 0.4562 - val_loss: 2.2148 - val_acc: 0.4397\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0462 - acc: 0.4571 - val_loss: 2.2118 - val_acc: 0.4397\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0505 - acc: 0.4571 - val_loss: 2.2348 - val_acc: 0.4387\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0454 - acc: 0.4587 - val_loss: 2.2124 - val_acc: 0.4410\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0390 - acc: 0.4583 - val_loss: 2.2146 - val_acc: 0.4400\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0396 - acc: 0.4589 - val_loss: 2.2039 - val_acc: 0.4414\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0343 - acc: 0.4603 - val_loss: 2.2324 - val_acc: 0.4388\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0585 - acc: 0.4563 - val_loss: 2.2299 - val_acc: 0.4366\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0349 - acc: 0.4612 - val_loss: 2.2055 - val_acc: 0.4414\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0270 - acc: 0.4627 - val_loss: 2.2063 - val_acc: 0.4402\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0245 - acc: 0.4664 - val_loss: 2.2054 - val_acc: 0.4414\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0249 - acc: 0.4623 - val_loss: 2.2134 - val_acc: 0.4386\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0243 - acc: 0.4625 - val_loss: 2.2047 - val_acc: 0.4406\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0163 - acc: 0.4623 - val_loss: 2.2057 - val_acc: 0.4441\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0142 - acc: 0.4653 - val_loss: 2.1990 - val_acc: 0.4416\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0148 - acc: 0.4663 - val_loss: 2.1997 - val_acc: 0.4429\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0078 - acc: 0.4673 - val_loss: 2.2330 - val_acc: 0.4382\n",
      "Epoch 37/100\n",
      "1s - loss: 2.0176 - acc: 0.4645 - val_loss: 2.2063 - val_acc: 0.4425\n",
      "Epoch 38/100\n",
      "1s - loss: 2.0045 - acc: 0.4684 - val_loss: 2.2037 - val_acc: 0.4419\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9996 - acc: 0.4701 - val_loss: 2.2112 - val_acc: 0.4420\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9971 - acc: 0.4702 - val_loss: 2.2015 - val_acc: 0.4420\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9940 - acc: 0.4711 - val_loss: 2.2017 - val_acc: 0.4417\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9963 - acc: 0.4678 - val_loss: 2.1959 - val_acc: 0.4425\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9998 - acc: 0.4681 - val_loss: 2.2160 - val_acc: 0.4392\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9999 - acc: 0.4690 - val_loss: 2.2629 - val_acc: 0.4333\n",
      "Epoch 45/100\n",
      "1s - loss: 2.0178 - acc: 0.4665 - val_loss: 2.2112 - val_acc: 0.4408\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9909 - acc: 0.4697 - val_loss: 2.1994 - val_acc: 0.4416\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9916 - acc: 0.4688 - val_loss: 2.1974 - val_acc: 0.4413\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9805 - acc: 0.4737 - val_loss: 2.2012 - val_acc: 0.4417\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9952 - acc: 0.4702 - val_loss: 2.1988 - val_acc: 0.4422\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9845 - acc: 0.4729 - val_loss: 2.2065 - val_acc: 0.4410\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9879 - acc: 0.4710 - val_loss: 2.2082 - val_acc: 0.4419\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9756 - acc: 0.4726 - val_loss: 2.1916 - val_acc: 0.4454\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9772 - acc: 0.4744 - val_loss: 2.2089 - val_acc: 0.4406\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9737 - acc: 0.4751 - val_loss: 2.2167 - val_acc: 0.4390\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9800 - acc: 0.4736 - val_loss: 2.2151 - val_acc: 0.4403\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9790 - acc: 0.4731 - val_loss: 2.2371 - val_acc: 0.4361\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9772 - acc: 0.4748 - val_loss: 2.2056 - val_acc: 0.4418\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9928 - acc: 0.4701 - val_loss: 2.2060 - val_acc: 0.4398\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9760 - acc: 0.4727 - val_loss: 2.2006 - val_acc: 0.4405\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9630 - acc: 0.4764 - val_loss: 2.1943 - val_acc: 0.4432\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9675 - acc: 0.4751 - val_loss: 2.1981 - val_acc: 0.4428\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9671 - acc: 0.4741 - val_loss: 2.2086 - val_acc: 0.4420\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9686 - acc: 0.4752 - val_loss: 2.2071 - val_acc: 0.4411\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9625 - acc: 0.4768 - val_loss: 2.2203 - val_acc: 0.4383\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9626 - acc: 0.4772 - val_loss: 2.2269 - val_acc: 0.4399\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9745 - acc: 0.4752 - val_loss: 2.2054 - val_acc: 0.4422\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9547 - acc: 0.4797 - val_loss: 2.2055 - val_acc: 0.4410\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9524 - acc: 0.4792 - val_loss: 2.1989 - val_acc: 0.4424\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9543 - acc: 0.4780 - val_loss: 2.2035 - val_acc: 0.4418\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9479 - acc: 0.4794 - val_loss: 2.1950 - val_acc: 0.4407\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9444 - acc: 0.4811 - val_loss: 2.2133 - val_acc: 0.4382\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9485 - acc: 0.4785 - val_loss: 2.1944 - val_acc: 0.4435\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9404 - acc: 0.4820 - val_loss: 2.2035 - val_acc: 0.4411\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9369 - acc: 0.4830 - val_loss: 2.1934 - val_acc: 0.4422\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9438 - acc: 0.4826 - val_loss: 2.1947 - val_acc: 0.4423\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9399 - acc: 0.4818 - val_loss: 2.2118 - val_acc: 0.4397\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9461 - acc: 0.4807 - val_loss: 2.2186 - val_acc: 0.4369\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9452 - acc: 0.4808 - val_loss: 2.2289 - val_acc: 0.4366\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9500 - acc: 0.4797 - val_loss: 2.2197 - val_acc: 0.4390\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9335 - acc: 0.4819 - val_loss: 2.2078 - val_acc: 0.4410\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9328 - acc: 0.4832 - val_loss: 2.2032 - val_acc: 0.4404\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9269 - acc: 0.4837 - val_loss: 2.2084 - val_acc: 0.4422\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9428 - acc: 0.4818 - val_loss: 2.2047 - val_acc: 0.4387\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9330 - acc: 0.4841 - val_loss: 2.2088 - val_acc: 0.4403\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9372 - acc: 0.4831 - val_loss: 2.2116 - val_acc: 0.4390\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9343 - acc: 0.4836 - val_loss: 2.2082 - val_acc: 0.4396\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9372 - acc: 0.4796 - val_loss: 2.2149 - val_acc: 0.4389\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9297 - acc: 0.4840 - val_loss: 2.2133 - val_acc: 0.4411\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9225 - acc: 0.4865 - val_loss: 2.2648 - val_acc: 0.4312\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9432 - acc: 0.4824 - val_loss: 2.2095 - val_acc: 0.4429\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9214 - acc: 0.4855 - val_loss: 2.2362 - val_acc: 0.4369\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9315 - acc: 0.4846 - val_loss: 2.2138 - val_acc: 0.4412\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9316 - acc: 0.4841 - val_loss: 2.2143 - val_acc: 0.4372\n",
      "Epoch 94/100\n",
      "1s - loss: 1.9255 - acc: 0.4850 - val_loss: 2.2023 - val_acc: 0.4406\n",
      "Epoch 95/100\n",
      "1s - loss: 1.9231 - acc: 0.4856 - val_loss: 2.2136 - val_acc: 0.4395\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9158 - acc: 0.4876 - val_loss: 2.2369 - val_acc: 0.4364\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9226 - acc: 0.4859 - val_loss: 2.2390 - val_acc: 0.4374\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9371 - acc: 0.4827 - val_loss: 2.2334 - val_acc: 0.4345\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9181 - acc: 0.4857 - val_loss: 2.2246 - val_acc: 0.4380\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9151 - acc: 0.4863 - val_loss: 2.2464 - val_acc: 0.4350\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1657 - acc: 0.4307 - val_loss: 2.2129 - val_acc: 0.4392\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1363 - acc: 0.4336 - val_loss: 2.2189 - val_acc: 0.4382\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1302 - acc: 0.4376 - val_loss: 2.2094 - val_acc: 0.4383\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1156 - acc: 0.4414 - val_loss: 2.2387 - val_acc: 0.4351\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1066 - acc: 0.4436 - val_loss: 2.2447 - val_acc: 0.4336\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0987 - acc: 0.4425 - val_loss: 2.2596 - val_acc: 0.4307\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0934 - acc: 0.4451 - val_loss: 2.2515 - val_acc: 0.4324\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0885 - acc: 0.4477 - val_loss: 2.2520 - val_acc: 0.4318\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0823 - acc: 0.4473 - val_loss: 2.2659 - val_acc: 0.4263\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0754 - acc: 0.4492 - val_loss: 2.2555 - val_acc: 0.4286\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0710 - acc: 0.4516 - val_loss: 2.2515 - val_acc: 0.4301\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0651 - acc: 0.4505 - val_loss: 2.2620 - val_acc: 0.4271\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0707 - acc: 0.4503 - val_loss: 2.2919 - val_acc: 0.4243\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0647 - acc: 0.4522 - val_loss: 2.2519 - val_acc: 0.4296\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0563 - acc: 0.4514 - val_loss: 2.2411 - val_acc: 0.4318\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0566 - acc: 0.4528 - val_loss: 2.2562 - val_acc: 0.4326\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0507 - acc: 0.4543 - val_loss: 2.2561 - val_acc: 0.4300\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0401 - acc: 0.4563 - val_loss: 2.2431 - val_acc: 0.4333\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0438 - acc: 0.4543 - val_loss: 2.2387 - val_acc: 0.4311\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0375 - acc: 0.4545 - val_loss: 2.2221 - val_acc: 0.4344\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0349 - acc: 0.4568 - val_loss: 2.2351 - val_acc: 0.4326\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0276 - acc: 0.4583 - val_loss: 2.2432 - val_acc: 0.4307\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0289 - acc: 0.4592 - val_loss: 2.2313 - val_acc: 0.4346\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0272 - acc: 0.4593 - val_loss: 2.2380 - val_acc: 0.4327\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0225 - acc: 0.4618 - val_loss: 2.2441 - val_acc: 0.4333\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0278 - acc: 0.4595 - val_loss: 2.2337 - val_acc: 0.4333\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0349 - acc: 0.4559 - val_loss: 2.2367 - val_acc: 0.4360\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0127 - acc: 0.4610 - val_loss: 2.2201 - val_acc: 0.4341\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0095 - acc: 0.4642 - val_loss: 2.2376 - val_acc: 0.4332\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0129 - acc: 0.4616 - val_loss: 2.2226 - val_acc: 0.4349\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0162 - acc: 0.4618 - val_loss: 2.2417 - val_acc: 0.4329\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0194 - acc: 0.4601 - val_loss: 2.2279 - val_acc: 0.4340\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0120 - acc: 0.4628 - val_loss: 2.2280 - val_acc: 0.4344\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0185 - acc: 0.4613 - val_loss: 2.2204 - val_acc: 0.4362\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0030 - acc: 0.4640 - val_loss: 2.2485 - val_acc: 0.4339\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0048 - acc: 0.4634 - val_loss: 2.2133 - val_acc: 0.4360\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9932 - acc: 0.4638 - val_loss: 2.2059 - val_acc: 0.4382\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9897 - acc: 0.4671 - val_loss: 2.2107 - val_acc: 0.4389\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9885 - acc: 0.4670 - val_loss: 2.2147 - val_acc: 0.4355\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9977 - acc: 0.4639 - val_loss: 2.2219 - val_acc: 0.4361\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9882 - acc: 0.4679 - val_loss: 2.2181 - val_acc: 0.4367\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9911 - acc: 0.4658 - val_loss: 2.2130 - val_acc: 0.4375\n",
      "Epoch 43/100\n",
      "1s - loss: 2.0106 - acc: 0.4620 - val_loss: 2.2140 - val_acc: 0.4391\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9860 - acc: 0.4671 - val_loss: 2.2178 - val_acc: 0.4379\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9879 - acc: 0.4676 - val_loss: 2.2203 - val_acc: 0.4387\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9800 - acc: 0.4693 - val_loss: 2.2117 - val_acc: 0.4394\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9794 - acc: 0.4685 - val_loss: 2.2218 - val_acc: 0.4382\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9780 - acc: 0.4678 - val_loss: 2.2018 - val_acc: 0.4389\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9713 - acc: 0.4717 - val_loss: 2.2114 - val_acc: 0.4369\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9635 - acc: 0.4720 - val_loss: 2.1984 - val_acc: 0.4395\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9610 - acc: 0.4732 - val_loss: 2.2273 - val_acc: 0.4374\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9711 - acc: 0.4713 - val_loss: 2.2151 - val_acc: 0.4365\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9734 - acc: 0.4704 - val_loss: 2.2433 - val_acc: 0.4343\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9780 - acc: 0.4711 - val_loss: 2.2515 - val_acc: 0.4330\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9903 - acc: 0.4685 - val_loss: 2.2380 - val_acc: 0.4363\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9703 - acc: 0.4729 - val_loss: 2.2118 - val_acc: 0.4370\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9622 - acc: 0.4730 - val_loss: 2.2530 - val_acc: 0.4305\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9652 - acc: 0.4725 - val_loss: 2.2107 - val_acc: 0.4406\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9560 - acc: 0.4750 - val_loss: 2.2318 - val_acc: 0.4359\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9525 - acc: 0.4756 - val_loss: 2.2052 - val_acc: 0.4396\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9465 - acc: 0.4772 - val_loss: 2.2111 - val_acc: 0.4395\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9439 - acc: 0.4795 - val_loss: 2.2125 - val_acc: 0.4387\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9516 - acc: 0.4753 - val_loss: 2.2619 - val_acc: 0.4310\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9598 - acc: 0.4746 - val_loss: 2.2259 - val_acc: 0.4378\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9517 - acc: 0.4752 - val_loss: 2.2567 - val_acc: 0.4331\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9442 - acc: 0.4784 - val_loss: 2.2253 - val_acc: 0.4375\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9534 - acc: 0.4778 - val_loss: 2.2234 - val_acc: 0.4362\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9429 - acc: 0.4783 - val_loss: 2.2291 - val_acc: 0.4374\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9458 - acc: 0.4770 - val_loss: 2.2424 - val_acc: 0.4358\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9424 - acc: 0.4796 - val_loss: 2.2275 - val_acc: 0.4386\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9500 - acc: 0.4761 - val_loss: 2.2236 - val_acc: 0.4394\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9481 - acc: 0.4760 - val_loss: 2.2338 - val_acc: 0.4351\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9519 - acc: 0.4753 - val_loss: 2.2088 - val_acc: 0.4385\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9350 - acc: 0.4776 - val_loss: 2.2170 - val_acc: 0.4372\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9333 - acc: 0.4803 - val_loss: 2.1996 - val_acc: 0.4392\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9227 - acc: 0.4810 - val_loss: 2.2060 - val_acc: 0.4399\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9409 - acc: 0.4777 - val_loss: 2.2112 - val_acc: 0.4391\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9302 - acc: 0.4815 - val_loss: 2.2380 - val_acc: 0.4366\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9329 - acc: 0.4795 - val_loss: 2.2193 - val_acc: 0.4367\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9329 - acc: 0.4786 - val_loss: 2.2195 - val_acc: 0.4374\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9281 - acc: 0.4810 - val_loss: 2.2186 - val_acc: 0.4375\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9196 - acc: 0.4835 - val_loss: 2.2008 - val_acc: 0.4393\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9205 - acc: 0.4827 - val_loss: 2.2125 - val_acc: 0.4365\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9134 - acc: 0.4876 - val_loss: 2.2150 - val_acc: 0.4372\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9176 - acc: 0.4846 - val_loss: 2.2205 - val_acc: 0.4365\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9275 - acc: 0.4813 - val_loss: 2.2502 - val_acc: 0.4321\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9286 - acc: 0.4803 - val_loss: 2.2189 - val_acc: 0.4367\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9195 - acc: 0.4825 - val_loss: 2.2364 - val_acc: 0.4345\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9133 - acc: 0.4849 - val_loss: 2.2216 - val_acc: 0.4378\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9213 - acc: 0.4832 - val_loss: 2.2435 - val_acc: 0.4336\n",
      "Epoch 91/100\n",
      "1s - loss: 1.9251 - acc: 0.4829 - val_loss: 2.2372 - val_acc: 0.4344\n",
      "Epoch 92/100\n",
      "1s - loss: 1.9225 - acc: 0.4835 - val_loss: 2.2445 - val_acc: 0.4339\n",
      "Epoch 93/100\n",
      "1s - loss: 1.9142 - acc: 0.4847 - val_loss: 2.2124 - val_acc: 0.4370\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8971 - acc: 0.4893 - val_loss: 2.2193 - val_acc: 0.4359\n",
      "Epoch 95/100\n",
      "1s - loss: 1.8986 - acc: 0.4896 - val_loss: 2.2317 - val_acc: 0.4370\n",
      "Epoch 96/100\n",
      "1s - loss: 1.9081 - acc: 0.4862 - val_loss: 2.2316 - val_acc: 0.4354\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9105 - acc: 0.4867 - val_loss: 2.2181 - val_acc: 0.4375\n",
      "Epoch 98/100\n",
      "1s - loss: 1.9093 - acc: 0.4851 - val_loss: 2.2129 - val_acc: 0.4372\n",
      "Epoch 99/100\n",
      "1s - loss: 1.9038 - acc: 0.4869 - val_loss: 2.2244 - val_acc: 0.4371\n",
      "Epoch 100/100\n",
      "1s - loss: 1.9093 - acc: 0.4843 - val_loss: 2.2205 - val_acc: 0.4360\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1642 - acc: 0.4334 - val_loss: 2.1722 - val_acc: 0.4550\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1397 - acc: 0.4402 - val_loss: 2.1776 - val_acc: 0.4528\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1240 - acc: 0.4422 - val_loss: 2.1832 - val_acc: 0.4490\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1123 - acc: 0.4457 - val_loss: 2.1935 - val_acc: 0.4482\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1084 - acc: 0.4469 - val_loss: 2.2014 - val_acc: 0.4464\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1049 - acc: 0.4480 - val_loss: 2.1987 - val_acc: 0.4460\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0900 - acc: 0.4499 - val_loss: 2.2006 - val_acc: 0.4465\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0819 - acc: 0.4514 - val_loss: 2.1973 - val_acc: 0.4473\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0760 - acc: 0.4538 - val_loss: 2.1961 - val_acc: 0.4465\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0730 - acc: 0.4540 - val_loss: 2.1931 - val_acc: 0.4452\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0716 - acc: 0.4548 - val_loss: 2.2021 - val_acc: 0.4428\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0619 - acc: 0.4559 - val_loss: 2.1975 - val_acc: 0.4456\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0619 - acc: 0.4565 - val_loss: 2.2085 - val_acc: 0.4443\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0562 - acc: 0.4591 - val_loss: 2.2020 - val_acc: 0.4437\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0503 - acc: 0.4589 - val_loss: 2.2053 - val_acc: 0.4463\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0461 - acc: 0.4578 - val_loss: 2.1987 - val_acc: 0.4456\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0505 - acc: 0.4574 - val_loss: 2.2251 - val_acc: 0.4402\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0457 - acc: 0.4572 - val_loss: 2.2126 - val_acc: 0.4448\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0355 - acc: 0.4606 - val_loss: 2.2060 - val_acc: 0.4429\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0334 - acc: 0.4604 - val_loss: 2.1950 - val_acc: 0.4471\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0280 - acc: 0.4630 - val_loss: 2.2128 - val_acc: 0.4446\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0277 - acc: 0.4635 - val_loss: 2.1969 - val_acc: 0.4471\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0264 - acc: 0.4631 - val_loss: 2.2049 - val_acc: 0.4452\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0217 - acc: 0.4650 - val_loss: 2.1978 - val_acc: 0.4449\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0160 - acc: 0.4657 - val_loss: 2.1940 - val_acc: 0.4471\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0125 - acc: 0.4660 - val_loss: 2.2080 - val_acc: 0.4467\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0138 - acc: 0.4665 - val_loss: 2.1947 - val_acc: 0.4453\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0118 - acc: 0.4667 - val_loss: 2.1984 - val_acc: 0.4469\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0102 - acc: 0.4669 - val_loss: 2.2002 - val_acc: 0.4458\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0043 - acc: 0.4696 - val_loss: 2.2000 - val_acc: 0.4469\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0035 - acc: 0.4686 - val_loss: 2.1998 - val_acc: 0.4439\n",
      "Epoch 32/100\n",
      "1s - loss: 1.9964 - acc: 0.4698 - val_loss: 2.2200 - val_acc: 0.4442\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9967 - acc: 0.4676 - val_loss: 2.2158 - val_acc: 0.4421\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9941 - acc: 0.4718 - val_loss: 2.1971 - val_acc: 0.4458\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9904 - acc: 0.4714 - val_loss: 2.2136 - val_acc: 0.4428\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9893 - acc: 0.4732 - val_loss: 2.2016 - val_acc: 0.4456\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9866 - acc: 0.4724 - val_loss: 2.2145 - val_acc: 0.4435\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9835 - acc: 0.4738 - val_loss: 2.1968 - val_acc: 0.4452\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9893 - acc: 0.4714 - val_loss: 2.2441 - val_acc: 0.4399\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9857 - acc: 0.4730 - val_loss: 2.1945 - val_acc: 0.4457\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9757 - acc: 0.4731 - val_loss: 2.2164 - val_acc: 0.4408\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9739 - acc: 0.4733 - val_loss: 2.2176 - val_acc: 0.4447\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9847 - acc: 0.4713 - val_loss: 2.1927 - val_acc: 0.4469\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9814 - acc: 0.4719 - val_loss: 2.2045 - val_acc: 0.4437\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9725 - acc: 0.4739 - val_loss: 2.2018 - val_acc: 0.4448\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9654 - acc: 0.4755 - val_loss: 2.1850 - val_acc: 0.4462\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9648 - acc: 0.4776 - val_loss: 2.2421 - val_acc: 0.4354\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9734 - acc: 0.4762 - val_loss: 2.2007 - val_acc: 0.4459\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9657 - acc: 0.4780 - val_loss: 2.2115 - val_acc: 0.4435\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9669 - acc: 0.4763 - val_loss: 2.2088 - val_acc: 0.4453\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9723 - acc: 0.4743 - val_loss: 2.2332 - val_acc: 0.4397\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9638 - acc: 0.4756 - val_loss: 2.1981 - val_acc: 0.4461\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9585 - acc: 0.4772 - val_loss: 2.1937 - val_acc: 0.4468\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9558 - acc: 0.4783 - val_loss: 2.1817 - val_acc: 0.4484\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9480 - acc: 0.4793 - val_loss: 2.1773 - val_acc: 0.4490\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9455 - acc: 0.4826 - val_loss: 2.1909 - val_acc: 0.4492\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9407 - acc: 0.4824 - val_loss: 2.1916 - val_acc: 0.4485\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9555 - acc: 0.4804 - val_loss: 2.1827 - val_acc: 0.4483\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9400 - acc: 0.4834 - val_loss: 2.1720 - val_acc: 0.4524\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9421 - acc: 0.4829 - val_loss: 2.1975 - val_acc: 0.4462\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9413 - acc: 0.4808 - val_loss: 2.1754 - val_acc: 0.4514\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9362 - acc: 0.4826 - val_loss: 2.1829 - val_acc: 0.4502\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9394 - acc: 0.4831 - val_loss: 2.1726 - val_acc: 0.4516\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9424 - acc: 0.4812 - val_loss: 2.2066 - val_acc: 0.4461\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9400 - acc: 0.4842 - val_loss: 2.1863 - val_acc: 0.4472\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9492 - acc: 0.4797 - val_loss: 2.1954 - val_acc: 0.4472\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9365 - acc: 0.4839 - val_loss: 2.1976 - val_acc: 0.4478\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9498 - acc: 0.4817 - val_loss: 2.1909 - val_acc: 0.4470\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9340 - acc: 0.4857 - val_loss: 2.1751 - val_acc: 0.4511\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9300 - acc: 0.4829 - val_loss: 2.1916 - val_acc: 0.4479\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9270 - acc: 0.4836 - val_loss: 2.1778 - val_acc: 0.4518\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9207 - acc: 0.4869 - val_loss: 2.1883 - val_acc: 0.4479\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9190 - acc: 0.4877 - val_loss: 2.1776 - val_acc: 0.4501\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9182 - acc: 0.4878 - val_loss: 2.1835 - val_acc: 0.4491\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9185 - acc: 0.4877 - val_loss: 2.1832 - val_acc: 0.4500\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9203 - acc: 0.4863 - val_loss: 2.2294 - val_acc: 0.4425\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9361 - acc: 0.4819 - val_loss: 2.2231 - val_acc: 0.4428\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9350 - acc: 0.4831 - val_loss: 2.2455 - val_acc: 0.4396\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9462 - acc: 0.4805 - val_loss: 2.2040 - val_acc: 0.4477\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9293 - acc: 0.4836 - val_loss: 2.2157 - val_acc: 0.4463\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9278 - acc: 0.4838 - val_loss: 2.1984 - val_acc: 0.4500\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9265 - acc: 0.4862 - val_loss: 2.1968 - val_acc: 0.4476\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9186 - acc: 0.4857 - val_loss: 2.2087 - val_acc: 0.4463\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9238 - acc: 0.4857 - val_loss: 2.1909 - val_acc: 0.4501\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9096 - acc: 0.4878 - val_loss: 2.1861 - val_acc: 0.4482\n",
      "Epoch 86/100\n",
      "1s - loss: 1.8999 - acc: 0.4909 - val_loss: 2.1973 - val_acc: 0.4463\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9027 - acc: 0.4912 - val_loss: 2.1836 - val_acc: 0.4519\n",
      "Epoch 88/100\n",
      "1s - loss: 1.8939 - acc: 0.4934 - val_loss: 2.1824 - val_acc: 0.4493\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9004 - acc: 0.4918 - val_loss: 2.1800 - val_acc: 0.4479\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9009 - acc: 0.4916 - val_loss: 2.1885 - val_acc: 0.4482\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8938 - acc: 0.4916 - val_loss: 2.1867 - val_acc: 0.4455\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8996 - acc: 0.4910 - val_loss: 2.1974 - val_acc: 0.4457\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8988 - acc: 0.4937 - val_loss: 2.1798 - val_acc: 0.4490\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8905 - acc: 0.4944 - val_loss: 2.2209 - val_acc: 0.4418\n",
      "Epoch 95/100\n",
      "1s - loss: 1.8980 - acc: 0.4922 - val_loss: 2.1777 - val_acc: 0.4512\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8890 - acc: 0.4963 - val_loss: 2.1930 - val_acc: 0.4471\n",
      "Epoch 97/100\n",
      "1s - loss: 1.8849 - acc: 0.4953 - val_loss: 2.1857 - val_acc: 0.4478\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8913 - acc: 0.4944 - val_loss: 2.2103 - val_acc: 0.4452\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8980 - acc: 0.4918 - val_loss: 2.2030 - val_acc: 0.4485\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8992 - acc: 0.4908 - val_loss: 2.2169 - val_acc: 0.4436\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1565 - acc: 0.4321 - val_loss: 2.1696 - val_acc: 0.4410\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1276 - acc: 0.4372 - val_loss: 2.1761 - val_acc: 0.4411\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1203 - acc: 0.4386 - val_loss: 2.1786 - val_acc: 0.4431\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1098 - acc: 0.4421 - val_loss: 2.1728 - val_acc: 0.4416\n",
      "Epoch 5/100\n",
      "1s - loss: 2.0982 - acc: 0.4424 - val_loss: 2.1837 - val_acc: 0.4383\n",
      "Epoch 6/100\n",
      "1s - loss: 2.0923 - acc: 0.4440 - val_loss: 2.1941 - val_acc: 0.4375\n",
      "Epoch 7/100\n",
      "1s - loss: 2.0844 - acc: 0.4470 - val_loss: 2.1843 - val_acc: 0.4405\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0762 - acc: 0.4496 - val_loss: 2.1910 - val_acc: 0.4392\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0711 - acc: 0.4489 - val_loss: 2.1937 - val_acc: 0.4375\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0667 - acc: 0.4503 - val_loss: 2.1874 - val_acc: 0.4399\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0614 - acc: 0.4504 - val_loss: 2.1837 - val_acc: 0.4409\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0564 - acc: 0.4527 - val_loss: 2.1846 - val_acc: 0.4406\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0537 - acc: 0.4508 - val_loss: 2.1985 - val_acc: 0.4390\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0476 - acc: 0.4536 - val_loss: 2.1937 - val_acc: 0.4374\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0454 - acc: 0.4535 - val_loss: 2.1898 - val_acc: 0.4386\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0416 - acc: 0.4542 - val_loss: 2.1900 - val_acc: 0.4376\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0375 - acc: 0.4545 - val_loss: 2.1859 - val_acc: 0.4435\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0353 - acc: 0.4555 - val_loss: 2.1742 - val_acc: 0.4425\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0319 - acc: 0.4567 - val_loss: 2.1853 - val_acc: 0.4400\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0283 - acc: 0.4556 - val_loss: 2.1892 - val_acc: 0.4407\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0249 - acc: 0.4582 - val_loss: 2.1786 - val_acc: 0.4405\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0201 - acc: 0.4607 - val_loss: 2.1789 - val_acc: 0.4417\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0199 - acc: 0.4601 - val_loss: 2.1901 - val_acc: 0.4423\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0169 - acc: 0.4591 - val_loss: 2.1732 - val_acc: 0.4420\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0115 - acc: 0.4602 - val_loss: 2.1793 - val_acc: 0.4399\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0077 - acc: 0.4621 - val_loss: 2.1974 - val_acc: 0.4379\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0114 - acc: 0.4610 - val_loss: 2.1856 - val_acc: 0.4404\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0024 - acc: 0.4649 - val_loss: 2.1989 - val_acc: 0.4413\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0450 - acc: 0.4555 - val_loss: 2.2252 - val_acc: 0.4312\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0297 - acc: 0.4577 - val_loss: 2.1981 - val_acc: 0.4370\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0032 - acc: 0.4614 - val_loss: 2.1849 - val_acc: 0.4410\n",
      "Epoch 32/100\n",
      "1s - loss: 1.9901 - acc: 0.4663 - val_loss: 2.1717 - val_acc: 0.4432\n",
      "Epoch 33/100\n",
      "1s - loss: 1.9919 - acc: 0.4673 - val_loss: 2.1917 - val_acc: 0.4408\n",
      "Epoch 34/100\n",
      "1s - loss: 1.9863 - acc: 0.4658 - val_loss: 2.1727 - val_acc: 0.4408\n",
      "Epoch 35/100\n",
      "1s - loss: 1.9896 - acc: 0.4647 - val_loss: 2.1786 - val_acc: 0.4428\n",
      "Epoch 36/100\n",
      "1s - loss: 1.9873 - acc: 0.4650 - val_loss: 2.1695 - val_acc: 0.4429\n",
      "Epoch 37/100\n",
      "1s - loss: 1.9863 - acc: 0.4673 - val_loss: 2.1661 - val_acc: 0.4458\n",
      "Epoch 38/100\n",
      "1s - loss: 1.9776 - acc: 0.4697 - val_loss: 2.1824 - val_acc: 0.4412\n",
      "Epoch 39/100\n",
      "1s - loss: 1.9835 - acc: 0.4673 - val_loss: 2.1677 - val_acc: 0.4451\n",
      "Epoch 40/100\n",
      "1s - loss: 1.9748 - acc: 0.4698 - val_loss: 2.1755 - val_acc: 0.4473\n",
      "Epoch 41/100\n",
      "1s - loss: 1.9869 - acc: 0.4671 - val_loss: 2.1667 - val_acc: 0.4435\n",
      "Epoch 42/100\n",
      "1s - loss: 1.9724 - acc: 0.4700 - val_loss: 2.1943 - val_acc: 0.4404\n",
      "Epoch 43/100\n",
      "1s - loss: 1.9734 - acc: 0.4706 - val_loss: 2.1745 - val_acc: 0.4435\n",
      "Epoch 44/100\n",
      "1s - loss: 1.9766 - acc: 0.4687 - val_loss: 2.1814 - val_acc: 0.4460\n",
      "Epoch 45/100\n",
      "1s - loss: 1.9775 - acc: 0.4699 - val_loss: 2.1762 - val_acc: 0.4419\n",
      "Epoch 46/100\n",
      "1s - loss: 1.9640 - acc: 0.4721 - val_loss: 2.1821 - val_acc: 0.4424\n",
      "Epoch 47/100\n",
      "1s - loss: 1.9639 - acc: 0.4730 - val_loss: 2.1780 - val_acc: 0.4392\n",
      "Epoch 48/100\n",
      "1s - loss: 1.9592 - acc: 0.4727 - val_loss: 2.1687 - val_acc: 0.4450\n",
      "Epoch 49/100\n",
      "1s - loss: 1.9566 - acc: 0.4740 - val_loss: 2.1680 - val_acc: 0.4440\n",
      "Epoch 50/100\n",
      "1s - loss: 1.9580 - acc: 0.4738 - val_loss: 2.1615 - val_acc: 0.4435\n",
      "Epoch 51/100\n",
      "1s - loss: 1.9549 - acc: 0.4742 - val_loss: 2.1824 - val_acc: 0.4439\n",
      "Epoch 52/100\n",
      "1s - loss: 1.9602 - acc: 0.4736 - val_loss: 2.1745 - val_acc: 0.4414\n",
      "Epoch 53/100\n",
      "1s - loss: 1.9549 - acc: 0.4744 - val_loss: 2.1664 - val_acc: 0.4475\n",
      "Epoch 54/100\n",
      "1s - loss: 1.9532 - acc: 0.4752 - val_loss: 2.1773 - val_acc: 0.4404\n",
      "Epoch 55/100\n",
      "1s - loss: 1.9547 - acc: 0.4761 - val_loss: 2.1757 - val_acc: 0.4451\n",
      "Epoch 56/100\n",
      "1s - loss: 1.9488 - acc: 0.4747 - val_loss: 2.1742 - val_acc: 0.4424\n",
      "Epoch 57/100\n",
      "1s - loss: 1.9499 - acc: 0.4738 - val_loss: 2.1706 - val_acc: 0.4473\n",
      "Epoch 58/100\n",
      "1s - loss: 1.9441 - acc: 0.4763 - val_loss: 2.1721 - val_acc: 0.4430\n",
      "Epoch 59/100\n",
      "1s - loss: 1.9489 - acc: 0.4736 - val_loss: 2.1737 - val_acc: 0.4466\n",
      "Epoch 60/100\n",
      "1s - loss: 1.9541 - acc: 0.4745 - val_loss: 2.1607 - val_acc: 0.4435\n",
      "Epoch 61/100\n",
      "1s - loss: 1.9362 - acc: 0.4783 - val_loss: 2.1912 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1s - loss: 1.9388 - acc: 0.4778 - val_loss: 2.1717 - val_acc: 0.4424\n",
      "Epoch 63/100\n",
      "1s - loss: 1.9355 - acc: 0.4792 - val_loss: 2.1748 - val_acc: 0.4430\n",
      "Epoch 64/100\n",
      "1s - loss: 1.9318 - acc: 0.4796 - val_loss: 2.1899 - val_acc: 0.4406\n",
      "Epoch 65/100\n",
      "1s - loss: 1.9368 - acc: 0.4782 - val_loss: 2.1639 - val_acc: 0.4447\n",
      "Epoch 66/100\n",
      "1s - loss: 1.9265 - acc: 0.4804 - val_loss: 2.1710 - val_acc: 0.4425\n",
      "Epoch 67/100\n",
      "1s - loss: 1.9238 - acc: 0.4805 - val_loss: 2.1657 - val_acc: 0.4437\n",
      "Epoch 68/100\n",
      "1s - loss: 1.9192 - acc: 0.4822 - val_loss: 2.1643 - val_acc: 0.4463\n",
      "Epoch 69/100\n",
      "1s - loss: 1.9241 - acc: 0.4817 - val_loss: 2.1574 - val_acc: 0.4461\n",
      "Epoch 70/100\n",
      "1s - loss: 1.9202 - acc: 0.4835 - val_loss: 2.1674 - val_acc: 0.4439\n",
      "Epoch 71/100\n",
      "1s - loss: 1.9198 - acc: 0.4826 - val_loss: 2.1570 - val_acc: 0.4449\n",
      "Epoch 72/100\n",
      "1s - loss: 1.9201 - acc: 0.4811 - val_loss: 2.1736 - val_acc: 0.4432\n",
      "Epoch 73/100\n",
      "1s - loss: 1.9233 - acc: 0.4812 - val_loss: 2.1656 - val_acc: 0.4454\n",
      "Epoch 74/100\n",
      "1s - loss: 1.9270 - acc: 0.4792 - val_loss: 2.1647 - val_acc: 0.4447\n",
      "Epoch 75/100\n",
      "1s - loss: 1.9198 - acc: 0.4807 - val_loss: 2.1586 - val_acc: 0.4461\n",
      "Epoch 76/100\n",
      "1s - loss: 1.9149 - acc: 0.4842 - val_loss: 2.1721 - val_acc: 0.4428\n",
      "Epoch 77/100\n",
      "1s - loss: 1.9100 - acc: 0.4839 - val_loss: 2.1672 - val_acc: 0.4448\n",
      "Epoch 78/100\n",
      "1s - loss: 1.9095 - acc: 0.4833 - val_loss: 2.1670 - val_acc: 0.4446\n",
      "Epoch 79/100\n",
      "1s - loss: 1.9040 - acc: 0.4849 - val_loss: 2.1598 - val_acc: 0.4460\n",
      "Epoch 80/100\n",
      "1s - loss: 1.9035 - acc: 0.4862 - val_loss: 2.1697 - val_acc: 0.4455\n",
      "Epoch 81/100\n",
      "1s - loss: 1.9054 - acc: 0.4841 - val_loss: 2.1633 - val_acc: 0.4438\n",
      "Epoch 82/100\n",
      "1s - loss: 1.9079 - acc: 0.4863 - val_loss: 2.1931 - val_acc: 0.4469\n",
      "Epoch 83/100\n",
      "1s - loss: 1.9321 - acc: 0.4803 - val_loss: 2.1874 - val_acc: 0.4423\n",
      "Epoch 84/100\n",
      "1s - loss: 1.9211 - acc: 0.4824 - val_loss: 2.1786 - val_acc: 0.4456\n",
      "Epoch 85/100\n",
      "1s - loss: 1.9061 - acc: 0.4872 - val_loss: 2.1855 - val_acc: 0.4409\n",
      "Epoch 86/100\n",
      "1s - loss: 1.9195 - acc: 0.4820 - val_loss: 2.1826 - val_acc: 0.4438\n",
      "Epoch 87/100\n",
      "1s - loss: 1.9156 - acc: 0.4831 - val_loss: 2.1874 - val_acc: 0.4436\n",
      "Epoch 88/100\n",
      "1s - loss: 1.9107 - acc: 0.4839 - val_loss: 2.1934 - val_acc: 0.4433\n",
      "Epoch 89/100\n",
      "1s - loss: 1.9087 - acc: 0.4841 - val_loss: 2.1900 - val_acc: 0.4419\n",
      "Epoch 90/100\n",
      "1s - loss: 1.9056 - acc: 0.4863 - val_loss: 2.1696 - val_acc: 0.4470\n",
      "Epoch 91/100\n",
      "1s - loss: 1.8906 - acc: 0.4878 - val_loss: 2.1823 - val_acc: 0.4431\n",
      "Epoch 92/100\n",
      "1s - loss: 1.8996 - acc: 0.4856 - val_loss: 2.1876 - val_acc: 0.4437\n",
      "Epoch 93/100\n",
      "1s - loss: 1.8978 - acc: 0.4882 - val_loss: 2.1669 - val_acc: 0.4439\n",
      "Epoch 94/100\n",
      "1s - loss: 1.8946 - acc: 0.4888 - val_loss: 2.1791 - val_acc: 0.4442\n",
      "Epoch 95/100\n",
      "1s - loss: 1.8981 - acc: 0.4841 - val_loss: 2.1622 - val_acc: 0.4436\n",
      "Epoch 96/100\n",
      "1s - loss: 1.8794 - acc: 0.4922 - val_loss: 2.1862 - val_acc: 0.4444\n",
      "Epoch 97/100\n",
      "1s - loss: 1.9077 - acc: 0.4873 - val_loss: 2.2022 - val_acc: 0.4392\n",
      "Epoch 98/100\n",
      "1s - loss: 1.8944 - acc: 0.4884 - val_loss: 2.1734 - val_acc: 0.4459\n",
      "Epoch 99/100\n",
      "1s - loss: 1.8815 - acc: 0.4909 - val_loss: 2.1796 - val_acc: 0.4456\n",
      "Epoch 100/100\n",
      "1s - loss: 1.8833 - acc: 0.4924 - val_loss: 2.1841 - val_acc: 0.4434\n",
      "Train on 46309 samples, validate on 19846 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.1714 - acc: 0.4314 - val_loss: 2.1878 - val_acc: 0.4399\n",
      "Epoch 2/100\n",
      "1s - loss: 2.1511 - acc: 0.4363 - val_loss: 2.2016 - val_acc: 0.4388\n",
      "Epoch 3/100\n",
      "1s - loss: 2.1370 - acc: 0.4370 - val_loss: 2.1887 - val_acc: 0.4424\n",
      "Epoch 4/100\n",
      "1s - loss: 2.1243 - acc: 0.4399 - val_loss: 2.1924 - val_acc: 0.4405\n",
      "Epoch 5/100\n",
      "1s - loss: 2.1165 - acc: 0.4419 - val_loss: 2.1918 - val_acc: 0.4409\n",
      "Epoch 6/100\n",
      "1s - loss: 2.1078 - acc: 0.4431 - val_loss: 2.2043 - val_acc: 0.4386\n",
      "Epoch 7/100\n",
      "1s - loss: 2.1033 - acc: 0.4454 - val_loss: 2.1953 - val_acc: 0.4421\n",
      "Epoch 8/100\n",
      "1s - loss: 2.0949 - acc: 0.4464 - val_loss: 2.1936 - val_acc: 0.4391\n",
      "Epoch 9/100\n",
      "1s - loss: 2.0856 - acc: 0.4486 - val_loss: 2.2237 - val_acc: 0.4365\n",
      "Epoch 10/100\n",
      "1s - loss: 2.0967 - acc: 0.4460 - val_loss: 2.2095 - val_acc: 0.4379\n",
      "Epoch 11/100\n",
      "1s - loss: 2.0774 - acc: 0.4517 - val_loss: 2.2029 - val_acc: 0.4393\n",
      "Epoch 12/100\n",
      "1s - loss: 2.0739 - acc: 0.4513 - val_loss: 2.2007 - val_acc: 0.4383\n",
      "Epoch 13/100\n",
      "1s - loss: 2.0706 - acc: 0.4516 - val_loss: 2.2097 - val_acc: 0.4370\n",
      "Epoch 14/100\n",
      "1s - loss: 2.0643 - acc: 0.4533 - val_loss: 2.2003 - val_acc: 0.4406\n",
      "Epoch 15/100\n",
      "1s - loss: 2.0613 - acc: 0.4534 - val_loss: 2.2068 - val_acc: 0.4387\n",
      "Epoch 16/100\n",
      "1s - loss: 2.0539 - acc: 0.4552 - val_loss: 2.1976 - val_acc: 0.4403\n",
      "Epoch 17/100\n",
      "1s - loss: 2.0508 - acc: 0.4557 - val_loss: 2.2176 - val_acc: 0.4360\n",
      "Epoch 18/100\n",
      "1s - loss: 2.0494 - acc: 0.4558 - val_loss: 2.1975 - val_acc: 0.4378\n",
      "Epoch 19/100\n",
      "1s - loss: 2.0472 - acc: 0.4554 - val_loss: 2.2048 - val_acc: 0.4377\n",
      "Epoch 20/100\n",
      "1s - loss: 2.0423 - acc: 0.4573 - val_loss: 2.1991 - val_acc: 0.4377\n",
      "Epoch 21/100\n",
      "1s - loss: 2.0396 - acc: 0.4566 - val_loss: 2.1919 - val_acc: 0.4406\n",
      "Epoch 22/100\n",
      "1s - loss: 2.0344 - acc: 0.4568 - val_loss: 2.2093 - val_acc: 0.4408\n",
      "Epoch 23/100\n",
      "1s - loss: 2.0395 - acc: 0.4593 - val_loss: 2.1980 - val_acc: 0.4427\n",
      "Epoch 24/100\n",
      "1s - loss: 2.0284 - acc: 0.4605 - val_loss: 2.1950 - val_acc: 0.4410\n",
      "Epoch 25/100\n",
      "1s - loss: 2.0266 - acc: 0.4610 - val_loss: 2.1994 - val_acc: 0.4393\n",
      "Epoch 26/100\n",
      "1s - loss: 2.0228 - acc: 0.4604 - val_loss: 2.2171 - val_acc: 0.4359\n",
      "Epoch 27/100\n",
      "1s - loss: 2.0202 - acc: 0.4614 - val_loss: 2.1977 - val_acc: 0.4388\n",
      "Epoch 28/100\n",
      "1s - loss: 2.0175 - acc: 0.4626 - val_loss: 2.2063 - val_acc: 0.4385\n",
      "Epoch 29/100\n",
      "1s - loss: 2.0195 - acc: 0.4618 - val_loss: 2.1947 - val_acc: 0.4382\n",
      "Epoch 30/100\n",
      "1s - loss: 2.0158 - acc: 0.4639 - val_loss: 2.2086 - val_acc: 0.4372\n",
      "Epoch 31/100\n",
      "1s - loss: 2.0174 - acc: 0.4615 - val_loss: 2.1967 - val_acc: 0.4394\n",
      "Epoch 32/100\n",
      "1s - loss: 2.0139 - acc: 0.4638 - val_loss: 2.2192 - val_acc: 0.4382\n",
      "Epoch 33/100\n",
      "1s - loss: 2.0136 - acc: 0.4621 - val_loss: 2.1955 - val_acc: 0.4394\n",
      "Epoch 34/100\n",
      "1s - loss: 2.0033 - acc: 0.4647 - val_loss: 2.1954 - val_acc: 0.4388\n",
      "Epoch 35/100\n",
      "1s - loss: 2.0023 - acc: 0.4644 - val_loss: 2.2030 - val_acc: 0.4373\n",
      "Epoch 36/100\n",
      "1s - loss: 2.0035 - acc: 0.4656 - val_loss: 2.2032 - val_acc: 0.4400\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2491f2bb0e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     model_policy.fit(neural_games, neural_ans, batch_size=20096, epochs=100, verbose=2,\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_saver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#TQDMNotebookCallback()],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      validation_data = next(data_gen(test_data, test=False)))\n\u001b[0m",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    neural_games, neural_ans = next(data_gen(data))\n",
    "    model_policy.fit(neural_games, neural_ans, batch_size=20096, epochs=100, verbose=2,\n",
    "                    callbacks=[model_saver, tensorboard], #TQDMNotebookCallback()],\n",
    "                     validation_data = next(data_gen(test_data, test=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback, TQDMCallback\n",
    "# keras, model definition...\n",
    "model_saver = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.6f}.hdf5\",\n",
    "                                              monitor='val_acc', verbose=0,\n",
    "                                              save_best_only=True, save_weights_only=False,\n",
    "                                              mode='auto')\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                          histogram_freq=0, \n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.save('zeros_policy_44')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n\nCaused by op 'Variable_15/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-965bb7b58cb9>\", line 5, in <module>\n    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\", line 1097, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1774, in fit_generator\n    self._make_train_function()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1001, in _make_train_function\n    self.total_loss)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in <listcomp>\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 520, in zeros\n    dtype, name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 286, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 305, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-965bb7b58cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n\u001b[0m",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1803\u001b[0m         callbacks.set_params({\n\u001b[1;32m   1804\u001b[0m             \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n\nCaused by op 'Variable_15/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-965bb7b58cb9>\", line 5, in <module>\n    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\", line 1097, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1774, in fit_generator\n    self._make_train_function()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1001, in _make_train_function\n    self.total_loss)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in <listcomp>\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 520, in zeros\n    dtype, name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 286, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 305, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n"
     ]
    }
   ],
   "source": [
    "model_policy.fit_generator(data_gen(data, True),\n",
    "        steps_per_epoch=500, epochs=350, verbose=0,\n",
    "                   validation_data = data_gen(test_data, test=True), validation_steps=2,\n",
    "                   callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_policy.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_check_env = load_model(\"ReshapedModelsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model, save_model\n",
    "\n",
    "#save_model(model, \"med_modelv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_policy.load_weights(\"weights.04-0.400417.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"weights.11-0.283181.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_policy = load_model(\"ReshapedModel0.46-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"Med5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.18-0.381543.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#clf = joblib.load('LinearClass')\n",
    "\n",
    "#model = load_model('zeros_policy_44')\n",
    "\n",
    "class RenjuTEST(object): \n",
    "    def __init__(self, player, mode):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = 1\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        self.mode = mode\n",
    "        self.obl_action = None\n",
    "        self.win_action = None\n",
    "        \n",
    "        \n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        self.moves_done += 1\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "            self.lr_pos[action] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "            self.lr_pos[action] = 2\n",
    "            \n",
    "            \n",
    "            \n",
    "        w = (action % 15) + 5\n",
    "        h = (action // 15) + 5\n",
    "        \n",
    "        self.ext_pos[w][h] = self.cur_player\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0 or self.cur_pos[i][j][1] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.cur_pos[:, :, 2:] = self.cur_pos.sum(2).reshape((15,15,1))\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        rdiag = [0, 0, 0]\n",
    "        rdiaginv = [0, 0, 0]\n",
    "        ldiag = [0, 0, 0]\n",
    "        ldiaginv = [0, 0, 0]\n",
    "        rrow = [0, 0, 0]\n",
    "        lrow = [0, 0, 0]\n",
    "        rcol = [0, 0, 0]\n",
    "        lcol = [0, 0, 0]\n",
    "        \n",
    "        broken = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            if self.ext_pos[w + i][h + i]:\n",
    "                if not broken[0]:\n",
    "                    rdiag[int(self.ext_pos[w + i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[0] = 1\n",
    "                \n",
    "            if self.ext_pos[w][h + i]:\n",
    "                if not broken[1]:\n",
    "                    rcol[int(self.ext_pos[w][h + i])] += 1\n",
    "                else:\n",
    "                    broken[1] = 1\n",
    "                \n",
    "            if self.ext_pos[w - i][h + i]:\n",
    "                if not broken[2]:\n",
    "                    ldiag[int(self.ext_pos[w - i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[2] = 1\n",
    "                \n",
    "            \n",
    "            if self.ext_pos[w - i][h]:\n",
    "                if not broken[3]:\n",
    "                    lrow[int(self.ext_pos[w - i][h])] += 1\n",
    "                else:\n",
    "                    broken[3] = 1\n",
    "            \n",
    "            if self.ext_pos[w - i][h - i]:\n",
    "                if not broken[4]:\n",
    "                    rdiaginv[int(self.ext_pos[w - i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[4] = 1\n",
    "            \n",
    "            if self.ext_pos[w][h - i]:\n",
    "                if not broken[5]:\n",
    "                    lcol[int(self.ext_pos[w][h - i])] += 1\n",
    "                else:\n",
    "                    broken[5] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h - i]:\n",
    "                if not broken[6]:\n",
    "                    ldiaginv[int(self.ext_pos[w + i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[6] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h]:\n",
    "                if not broken[7]:\n",
    "                    rrow[int(self.ext_pos[w + i][h])] += 1\n",
    "                else:\n",
    "                    broken[7] = 1\n",
    "                    \n",
    "        \n",
    "        self.obl_action = None\n",
    "        first_obl_action = None\n",
    "        second_obl_action = None\n",
    "        \n",
    "        rightdiag = rdiag[self.cur_player] + rdiaginv[self.cur_player]\n",
    "        \n",
    "        if rightdiag >= 4:\n",
    "            if rightdiag >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w + rdiag[self.cur_player]][h + rdiag[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 + rdiag[self.cur_player] < 15 and 0 <= h - 5 + rdiag[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 + rdiag[self.cur_player]) * 15 + w - 5 + rdiag[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w - rdiaginv[self.cur_player]][h - rdiaginv[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 - rdiaginv[self.cur_player] < 15 and 0 <= h - 5 - rdiaginv[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 - rdiaginv[self.cur_player]) * 15 + w - 5 - rdiaginv[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if rightdiag == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "        leftdiag = ldiag[self.cur_player] + ldiaginv[self.cur_player]\n",
    "        \n",
    "        if leftdiag >= 4:\n",
    "            if leftdiag >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w - ldiag[self.cur_player]][h + ldiag[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 - ldiag[self.cur_player] < 15 and 0 <= h - 5 + ldiag[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 + ldiag[self.cur_player]) * 15 + w - 5 - ldiag[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w + ldiaginv[self.cur_player]][h - ldiaginv[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 + ldiaginv[self.cur_player] < 15 and 0 <= h - 5 - ldiaginv[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 - ldiaginv[self.cur_player]) * 15 + w - 5 + ldiaginv[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if leftdiag == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "        \n",
    "        row = rrow[self.cur_player] + lrow[self.cur_player]\n",
    "        \n",
    "        if row >= 4:\n",
    "            if row >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w - lrow[self.cur_player]][h] == 0 and \n",
    "                 0 <= w - 5 - lrow[self.cur_player] < 15):\n",
    "                    first_obl_action = (h - 5) * 15 + w - 5 - lrow[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w + rrow[self.cur_player]][h] == 0 and \n",
    "                 0 <= w - 5 + rrow[self.cur_player] < 15):\n",
    "                    second_obl_action = (h - 5) * 15 + w - 5 + rrow[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if row == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "        \n",
    "        \n",
    "        col = lcol[self.cur_player] + rcol[self.cur_player]\n",
    "        \n",
    "        if col >= 4:\n",
    "            if col >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w][h - lcol[self.cur_player]] == 0 and \n",
    "                 0 <= h - 5 - lcol[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 - lcol[self.cur_player]) * 15 + w - 5\n",
    "                \n",
    "                if (self.ext_pos[w][h + rcol[self.cur_player]] == 0 and \n",
    "                 0 <= h - 5 + rcol[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 + rcol[self.cur_player]) * 15 + w - 5\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if col == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                        \n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "                    \n",
    "        #if self.win_action != None:\n",
    "        #    print(\"WARNING, WIN POSITION:\", self.win_action)\n",
    "            \n",
    "        #print(\"pure\", first_obl_action, second_obl_action)\n",
    "        #print(self.win_action)\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        done = True if (self.moves_done == 225 or reward != 0) else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if self.moves_done == 225:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def net_ans(self, model, mode = 'all'):\n",
    "        s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        if mode == 'one':\n",
    "            return np.argmax(s)\n",
    "        else:\n",
    "            #return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "            return np.argsort(s)\n",
    "    \n",
    "    def simulation(self, model, mode):\n",
    "        fake_env = RenjuTEST(1, 'neural')\n",
    "        fake_env.cur_pos = np.copy(self.cur_pos)\n",
    "        fake_env.cur_player = self.cur_player\n",
    "        fake_env.player = self.player\n",
    "        fake_env.action_space = self.action_space = 225\n",
    "        fake_env.moves_done = self.moves_done\n",
    "        fake_env.ext_pos = np.copy(self.ext_pos)\n",
    "        fake_env.lr_pos = np.copy(self.lr_pos)\n",
    "        fake_env.mode = self.mode\n",
    "        reward = 0\n",
    "        while reward == 0:\n",
    "            \"\"\"\n",
    "            action = random.randint(0,224)\n",
    "            while fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = random.randint(0,224)\n",
    "                \n",
    "            \"\"\" \n",
    "            #if mode == 'kn':\n",
    "            s = clf.predict_proba([fake_env.lr_pos])[0]\n",
    "            #else:\n",
    "            #    s = model.predict(np.array([[fake_env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if fake_env.cur_pos[act % 15][act // 15][0] == 0 and fake_env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "            #\"\"\"\n",
    "            cur_pos, reward, done, info = fake_env.in_step(action)\n",
    "            \n",
    "        if reward == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def step(self, action, mode = 'kn'):\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        \n",
    "        if self.win_action != None:\n",
    "            #print(\"WIN ACTION\")\n",
    "            if self.win_action[0]:\n",
    "                if self.ext_pos[(self.win_action[0] % 15) + 5][(self.win_action[0] // 15) + 5] == 0:\n",
    "                    action = self.win_action[0]\n",
    "                    cur_pos, reward, done, info = self.in_step(self.win_action[0])\n",
    "                    return cur_pos, reward, done, action\n",
    "            if self.win_action[1]:\n",
    "                if self.ext_pos[(self.win_action[1] % 15) + 5][(self.win_action[1] // 15) + 5] == 0:\n",
    "                    action = self.win_action[1]\n",
    "                    cur_pos, reward, done, info = self.in_step(self.win_action[1])\n",
    "                    return cur_pos, reward, done, action\n",
    "            \n",
    "        \n",
    "        if self.obl_action:\n",
    "            action = self.obl_action\n",
    "            cur_pos, reward, done, info = self.in_step(self.obl_action)\n",
    "            return cur_pos, reward, done, action\n",
    "        \n",
    "        if reward != 0:\n",
    "            #self.render()\n",
    "            if done:\n",
    "                self.reset()\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            if self.mode == 'neural':\n",
    "                s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "            else:\n",
    "                s = clf.predict_proba([self.lr_pos])[0]\n",
    "            #plt.figure()\n",
    "            #k = s.reshape((15,15))\n",
    "            #plt.imshow(k, cmap='hot', interpolation='nearest')\n",
    "            #plt.show()\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                \n",
    "                action = 0\n",
    "                #print(net_move)\n",
    "                for act in net_move:\n",
    "                    if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "                #print(\"Net:\", action)\n",
    "\n",
    "            rnd = np.random.randint(1, 100)\n",
    "            if rnd < -10:\n",
    "                action = np.random.randint(0, 224)\n",
    "                while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                    action = np.random.randint(0, 224)\n",
    "            print('Net action:', action)\n",
    "            cur_pos, reward, done, info = self.in_step(action)\n",
    "            return cur_pos, reward, done, action\n",
    "    \n",
    "    def learning(self, opponent):\n",
    "        if self.moves_done % 2 == 0:\n",
    "            s = opponent.predict(np.array([[self.cur_pos]]))[0]\n",
    "        else:\n",
    "            s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        action = np.argmax(s)\n",
    "        if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "            net_move = np.argsort(s)[::-1]\n",
    "\n",
    "            action = 0\n",
    "            #print(net_move)\n",
    "            for act in net_move:\n",
    "                if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                    action = act\n",
    "                    break\n",
    "            #print(\"Net:\", action)\n",
    "        rnd = np.random.randint(1, 100)\n",
    "        if rnd < 5:\n",
    "            action = np.random.randint(0, 224)\n",
    "            while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = np.random.randint(0, 224)\n",
    "\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        return cur_pos, reward, done, action\n",
    "            \n",
    "            \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in reversed(range(15)):\n",
    "                for i in range(15):\n",
    "                    flag = 0\n",
    "                    if self.cur_pos[i][j][0] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.cur_pos[i][j][1] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        if mode == 'debug':\n",
    "            for j in reversed(range(25)):\n",
    "                for i in range(25):\n",
    "                    flag = 0\n",
    "                    if self.ext_pos[i][j] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.ext_pos[i][j] == 2:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "\n",
    "        return self.cur_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opponent = load_model('cross_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class MCSTNode():\n",
    "    def __init__(self, env, len_nodes = 0):\n",
    "        self.wins = 0\n",
    "        self.all_games = 0\n",
    "        self.childs = []\n",
    "        self.env = env\n",
    "        self.parent = None\n",
    "        self.action = None\n",
    "        self.index = len_nodes\n",
    "        self.child_actions = []\n",
    "        \n",
    "\n",
    "class UCT():\n",
    "    def __init__(self, env, model, mode):        \n",
    "        self.root = MCSTNode(env)\n",
    "        self.all_games = 0\n",
    "        self.constant = 1.1\n",
    "        self.cur_node = self.root\n",
    "        self.model = model\n",
    "        self.cur_pos = self.root\n",
    "        self.mode = mode\n",
    "        self.len_nodes = 1\n",
    "        self.edges = []\n",
    "        \n",
    "    \n",
    "    def get_child_UCT_list(self, node):\n",
    "        return list(map(self.get_UCT_stat, node.childs))\n",
    "        \n",
    "    def explore(self):\n",
    "        self.cur_node = self.root\n",
    "        UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "        while len(UCT_list) > 0 and  max(UCT_list) > self.get_UCT_stat(self.cur_node):\n",
    "            #print(UCT_list, \"I am at \", self.cur_node.index)\n",
    "            #print(UCT_list)\n",
    "            best_node = np.argmax(UCT_list)\n",
    "            self.cur_node = self.cur_node.childs[best_node]\n",
    "            UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "    \n",
    "    def expand(self):\n",
    "        if self.cur_node.all_games > 1:\n",
    "            #if self.mode == 'kn':\n",
    "            #    s = clf.predict_proba([self.cur_node.env.lr_pos])[0]\n",
    "            #else:\n",
    "            s = model.predict(np.array([[self.cur_node.env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_node.env.cur_pos[action % 15][action // 15][0] != 0 or self.cur_node.env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if act not in self.cur_node.child_actions and self.cur_node.env.cur_pos[act % 15][act // 15][0] == 0 and self.cur_node.env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break        \n",
    "\n",
    "            new_node_action = action\n",
    "            #print('New Node!')\n",
    "\n",
    "            new_env = deepcopy(self.cur_node.env)\n",
    "            new_env.in_step(new_node_action)\n",
    "\n",
    "            new_node = MCSTNode(new_env, self.len_nodes)\n",
    "            new_node.parent = self.cur_node\n",
    "            new_node.action = new_node_action\n",
    "            self.cur_node.child_actions.append(new_node_action)\n",
    "            self.len_nodes += 1\n",
    "            \n",
    "            self.edges.append((self.cur_node.index, new_node.index))\n",
    "\n",
    "            run = new_node.env.simulation(self.model, self.mode)\n",
    "            self.all_games += 1\n",
    "\n",
    "            self.cur_node.childs.append(new_node)\n",
    "            self.cur_node = new_node\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "        else:\n",
    "            run = self.cur_node.env.simulation(self.model, self.model)\n",
    "            self.all_games += 1\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "            \n",
    "            \n",
    "    def search(self, time_limit):\n",
    "        begin = time()\n",
    "        while (time() - begin) < time_limit * 0.95:\n",
    "            self.explore()\n",
    "            self.expand()\n",
    "        root_values = list(map(self.get_stat, self.root.childs))\n",
    "        #print(root_UCT_values, len(self.root.childs))\n",
    "        if len(root_values) != 0:\n",
    "            best_child = np.argmax(root_values)\n",
    "        else:\n",
    "            self.root = self.root.childs[0]\n",
    "            return self.root.action\n",
    "        self.root = self.root.childs[best_child]\n",
    "        return self.root.action\n",
    "                    \n",
    "    def get_UCT_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games) + self.constant * math.sqrt(math.log(self.all_games / node.all_games))\n",
    "    \n",
    "    def get_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Example program to illustrate my issue with dynamic buttons.\n",
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "\n",
    "class my_app(Frame):\n",
    "    \"\"\"Basic Frame\"\"\"\n",
    "    def __init__(self, master, mode, time_for_search):\n",
    "        self.buttons = []\n",
    "        \"\"\"Init the Frame\"\"\"\n",
    "        Frame.__init__(self,master)\n",
    "        self.grid()\n",
    "        self.Create_Widgets()\n",
    "        self.tester = RenjuTEST(2 if mode == 'me' else 1, mode)\n",
    "        self.label = tkinter.Label(self, text='')\n",
    "        self.label.grid(column=7, row=16, sticky=W);   #creates label for image on window \n",
    "        self.tree = UCT(self.tester, model, mode)\n",
    "        self.mode = mode\n",
    "        self.time_for_search = time_for_search\n",
    "\n",
    "\n",
    "    def Create_Widgets(self):\n",
    "        for i in range(15):\n",
    "            for j in range(15): #Start creating buttons\n",
    "\n",
    "                button_id = i * 15 + j \n",
    "                #print(self.button_id)\n",
    "\n",
    "                self.newmessage = Button(self, #I want to bind the self.button_id to each button, so that it prints its number when clicked.\n",
    "                                         text = '',\n",
    "                                         anchor = W, command = lambda button_id=button_id: self.access(button_id))#Run the method\n",
    "\n",
    "                #Placing\n",
    "                self.newmessage.config(height = 1, width = 1)\n",
    "                self.newmessage.grid(row = 15 - i, column = j, sticky = NW)\n",
    "                self.buttons.append(self.newmessage)\n",
    "        \n",
    "    def access(self, b_id): #This is one of the areas where I need help. I want this to return the number of the button clicked.\n",
    "        self.b_id = b_id\n",
    "        print(b_id)\n",
    "        self.buttons[b_id].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.step(b_id)\n",
    "        self.tester.render(mode='human')\n",
    "        \n",
    "        self.buttons[info].config(text = 'X' if self.tester.cur_player == 2 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.in_step(b_id)\n",
    "        tree_act_right = None\n",
    "        if self.tester.obl_action:\n",
    "            tree_act_right = self.tester.obl_action\n",
    "            print(\"WARNING\")\n",
    "            \n",
    "        if self.tester.win_action != None:\n",
    "            print('Nyyyak')\n",
    "            if self.tester.win_action[0]:\n",
    "                if self.tester.ext_pos[(self.tester.win_action[0] % 15) + 5][(self.tester.win_action[0] // 15) + 5] == 0:\n",
    "                    tree_act_right = self.tester.win_action[0]\n",
    "            if self.tester.win_action[1]:\n",
    "                if self.tester.ext_pos[(self.tester.win_action[1] % 15) + 5][(self.tester.win_action[1] // 15) + 5] == 0:\n",
    "                    tree_act_right = self.tester.win_action[1]\n",
    "\n",
    "        \n",
    "        self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        \n",
    "        if b_id in self.tree.root.child_actions:\n",
    "            for node in self.tree.root.childs:\n",
    "                if node.action == b_id:\n",
    "                    self.tree.root = node\n",
    "                    break\n",
    "        else:\n",
    "            self.tree = UCT(self.tester, model, self.mode)\n",
    "        #tree = UCT(self.tester, model, 'kn' if random.randint(0,1) == 0 else 'neuron')\n",
    "        tree_act = self.tree.search(self.time_for_search)\n",
    "        print(self.tree.root.all_games, tree_act)\n",
    "        #print(tree_act, tree.root.all_games)\n",
    "        if tree_act_right:\n",
    "            tree_act = tree_act_right\n",
    "        self.buttons[tree_act].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        cur_pos, reward, done, info = self.tester.in_step(tree_act)\n",
    "        self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "71 110\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "125\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "71 141\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "126\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "65 65\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "80\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "72 109\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "124\n",
      "WARNING\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "101 108\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "129\n",
      "WARNING\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O _ _ O _ _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "112 108\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O _ _ O X _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "127\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O O _ O X _ _ _ _ \n",
      "_ _ _ _ X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "122 108\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O O _ O X _ _ _ _ \n",
      "_ _ _ X X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "128\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O O O O X _ _ _ _ \n",
      "_ _ _ X X X _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "121 111\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ X _ _ _ _ _ _ _ _ \n",
      "_ _ _ X O O O O O O X _ _ _ _ \n",
      "_ _ _ X X X X _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "opponent = 'neural'\n",
    "first_move = 'opp'\n",
    "time_for_search = 1\n",
    "\n",
    "edges = []\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Tic-Tac-Toe\")\n",
    "root.geometry(\"542x500\")\n",
    "app = my_app(root, first_move, time_for_search)\n",
    "\n",
    "\n",
    "if first_move != 'me':\n",
    "    model_policy = load_model('cross_40v4')\n",
    "    tree_act = app.tree.search(time_for_search)\n",
    "    #print(tree_act, tree.root.all_games)\n",
    "    app.buttons[tree_act].config(text = 'X' if app.tester.cur_player == 1 else 'O')\n",
    "    cur_pos, reward, done, info = app.tester.in_step(tree_act)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAFlsAAAY7CAYAAAAxUc1xAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n6E/8gIErBTMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from IPython.display import Image\n",
    "\n",
    "g=nx.DiGraph()\n",
    "g.add_edges_from(app.tree.edges)\n",
    "p=nx.drawing.nx_pydot.to_pydot(g)\n",
    "p.write_png('example.png')\n",
    "\n",
    "Image(filename='example.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-81eddafc915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "clf = joblib.load('LinearClass')\n",
    "\n",
    "model = load_model('zeros_policy_44')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('LinearClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "4111 127\n",
      "142\n",
      "3906 129\n",
      "157\n",
      "4381 126\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import numba\n",
    "\n",
    "\n",
    "model = load_model('zeros_policy_44')\n",
    "clf = joblib.load('LinearClass')\n",
    "\n",
    "class RenjuTEST(): \n",
    "    def __init__(self, player, mode):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = 1\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        self.mode = mode\n",
    "        \n",
    "\n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        self.moves_done += 1\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "            self.lr_pos[action] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "            self.lr_pos[action] = 2\n",
    "            \n",
    "            \n",
    "        w = (action % 15) + 5\n",
    "        h = (action // 15) + 5\n",
    "        \n",
    "        self.ext_pos[w][h] = self.cur_player\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0 or self.cur_pos[i][j][1] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "        \"\"\"\n",
    "        self.cur_pos[:, :, 2:] = self.cur_pos.sum(2).reshape((15,15,1))\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        rdiag = [0, 0, 0]\n",
    "        rdiaginv = [0, 0, 0]\n",
    "        ldiag = [0, 0, 0]\n",
    "        ldiaginv = [0, 0, 0]\n",
    "        rrow = [0, 0, 0]\n",
    "        lrow = [0, 0, 0]\n",
    "        rcol = [0, 0, 0]\n",
    "        lcol = [0, 0, 0]\n",
    "        \n",
    "        broken = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            if self.ext_pos[w + i][h + i]:\n",
    "                if not broken[0]:\n",
    "                    rdiag[int(self.ext_pos[w + i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[0] = 1\n",
    "                \n",
    "            if self.ext_pos[w][h + i]:\n",
    "                if not broken[1]:\n",
    "                    rcol[int(self.ext_pos[w][h + i])] += 1\n",
    "                else:\n",
    "                    broken[1] = 1\n",
    "                \n",
    "            if self.ext_pos[w - i][h + i]:\n",
    "                if not broken[2]:\n",
    "                    ldiag[int(self.ext_pos[w - i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[2] = 1\n",
    "                \n",
    "            \n",
    "            if self.ext_pos[w - i][h]:\n",
    "                if not broken[3]:\n",
    "                    lrow[int(self.ext_pos[w - i][h])] += 1\n",
    "                else:\n",
    "                    broken[3] = 1\n",
    "            \n",
    "            if self.ext_pos[w - i][h - i]:\n",
    "                if not broken[4]:\n",
    "                    rdiaginv[int(self.ext_pos[w - i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[4] = 1\n",
    "            \n",
    "            if self.ext_pos[w][h - i]:\n",
    "                if not broken[5]:\n",
    "                    lcol[int(self.ext_pos[w][h - i])] += 1\n",
    "                else:\n",
    "                    broken[5] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h - i]:\n",
    "                if not broken[6]:\n",
    "                    ldiaginv[int(self.ext_pos[w + i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[6] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h]:\n",
    "                if not broken[7]:\n",
    "                    rrow[int(self.ext_pos[w + i][h])] += 1\n",
    "                else:\n",
    "                    broken[7] = 1\n",
    "        \n",
    "        \n",
    "        if rdiag[self.cur_player] + rdiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if ldiag[self.cur_player] + ldiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \n",
    "        if rrow[self.cur_player] + lrow[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if lcol[self.cur_player] + rcol[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \"\"\"\n",
    "        print('rdiag', rdiag)\n",
    "        print('rdiaginv', rdiaginv)\n",
    "        print('ldiag', ldiag)\n",
    "        print('ldiaginv', ldiaginv)\n",
    "        print('rrow', rrow)\n",
    "        print('lrow', lrow)\n",
    "        print('rcol', rcol)\n",
    "        print('lcol', lcol)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        done = True if (self.moves_done == 225 or reward != 0) else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if self.moves_done == 225:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def net_ans(self, model, mode = 'all'):\n",
    "        s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        if mode == 'one':\n",
    "            return np.argmax(s)\n",
    "        else:\n",
    "            #return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "            return np.argsort(s)\n",
    "    \n",
    "    def simulation(self, model, mode):\n",
    "        fake_env = RenjuTEST(1, 'neural')\n",
    "        fake_env.cur_pos = np.copy(self.cur_pos)\n",
    "        fake_env.cur_player = self.cur_player\n",
    "        fake_env.player = self.player\n",
    "        fake_env.action_space = self.action_space = 225\n",
    "        fake_env.moves_done = self.moves_done\n",
    "        fake_env.ext_pos = np.copy(self.ext_pos)\n",
    "        fake_env.lr_pos = np.copy(self.lr_pos)\n",
    "        fake_env.mode = self.mode\n",
    "        reward = 0\n",
    "        while reward == 0:\n",
    "            \n",
    "            action = random.randint(0,224)\n",
    "            while fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = random.randint(0,224)\n",
    "                \n",
    "            \"\"\" \n",
    "            #if mode == 'kn':\n",
    "            s = clf.predict_proba([fake_env.lr_pos])[0]\n",
    "            #else:\n",
    "            #    s = model.predict(np.array([[fake_env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if fake_env.cur_pos[act % 15][act // 15][0] == 0 and fake_env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "            \"\"\"\n",
    "            cur_pos, reward, done, info = fake_env.in_step(action)\n",
    "            \n",
    "        if reward == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action, mode = 'kn'):\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "\n",
    "        if reward != 0:\n",
    "            #self.render()\n",
    "            if done:\n",
    "                self.reset()\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            if self.mode == 'neural':\n",
    "                s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "            else:\n",
    "                s = clf.predict_proba([self.lr_pos])[0]\n",
    "            #plt.figure()\n",
    "            #k = s.reshape((15,15))\n",
    "            #plt.imshow(k, cmap='hot', interpolation='nearest')\n",
    "            #plt.show()\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                \n",
    "                action = 0\n",
    "                #print(net_move)\n",
    "                for act in net_move:\n",
    "                    if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "                #print(\"Net:\", action)\n",
    "\n",
    "            rnd = np.random.randint(1, 100)\n",
    "            if rnd < -10:\n",
    "                action = np.random.randint(0, 224)\n",
    "                while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                    action = np.random.randint(0, 224)\n",
    "            print('Net action:', action)\n",
    "            cur_pos, reward, done, info = self.in_step(action)\n",
    "            return cur_pos, reward, done, action\n",
    "    \n",
    "    def learning(self, opponent):\n",
    "        if self.moves_done % 2 == 0:\n",
    "            s = opponent.predict(np.array([[self.cur_pos]]))[0]\n",
    "        else:\n",
    "            s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        action = np.argmax(s)\n",
    "        if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "            net_move = np.argsort(s)[::-1]\n",
    "\n",
    "            action = 0\n",
    "            #print(net_move)\n",
    "            for act in net_move:\n",
    "                if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                    action = act\n",
    "                    break\n",
    "            #print(\"Net:\", action)\n",
    "        rnd = np.random.randint(1, 100)\n",
    "        if rnd < 5:\n",
    "            action = np.random.randint(0, 224)\n",
    "            while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = np.random.randint(0, 224)\n",
    "\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        return cur_pos, reward, done, action\n",
    "            \n",
    "            \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in reversed(range(15)):\n",
    "                for i in range(15):\n",
    "                    flag = 0\n",
    "                    if self.cur_pos[i][j][0] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.cur_pos[i][j][1] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        if mode == 'debug':\n",
    "            for j in reversed(range(25)):\n",
    "                for i in range(25):\n",
    "                    flag = 0\n",
    "                    if self.ext_pos[i][j] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.ext_pos[i][j] == 2:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        \n",
    "     \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        return self.cur_pos\n",
    "\n",
    "\n",
    "class MCSTNode():\n",
    "    def __init__(self, env):\n",
    "        self.wins = 0\n",
    "        self.all_games = 0\n",
    "        self.childs = []\n",
    "        self.env = env\n",
    "        self.parent = None\n",
    "        self.action = None\n",
    "        self.index = np.random.randint(0, 10000000)\n",
    "        self.child_actions = []\n",
    "       \n",
    "\n",
    "class UCT():\n",
    "    def __init__(self, env, model, mode):        \n",
    "        self.root = MCSTNode(env)\n",
    "        self.all_games = 0\n",
    "        self.constant = math.sqrt(2)\n",
    "        self.cur_node = self.root\n",
    "        self.model = model\n",
    "        self.cur_pos = self.root\n",
    "        self.mode = mode\n",
    "\n",
    "    \n",
    "    def get_child_UCT_list(self, node):\n",
    "        return list(map(self.get_UCT_stat, node.childs))\n",
    "       \n",
    "    def explore(self):\n",
    "        self.cur_node = self.root\n",
    "        UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "        while len(UCT_list) > 0 and  max(UCT_list) > self.get_UCT_stat(self.cur_node):\n",
    "            #print(UCT_list, \"I am at \", self.cur_node.index)\n",
    "            #print(UCT_list)\n",
    "            best_node = np.argmax(UCT_list)\n",
    "            self.cur_node = self.cur_node.childs[best_node]\n",
    "            UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "    \n",
    "    def expand(self):\n",
    "        if self.cur_node.all_games > 1:\n",
    "            #if self.mode == 'kn':\n",
    "                #s = clf.predict_proba([self.cur_node.env.lr_pos])[0]\n",
    "            #else:\n",
    "            s = model.predict(np.array([[self.cur_node.env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_node.env.cur_pos[action % 15][action // 15][0] != 0 or self.cur_node.env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if act not in self.cur_node.child_actions and self.cur_node.env.cur_pos[act % 15][act // 15][0] == 0 and self.cur_node.env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break        \n",
    "\n",
    "            new_node_action = action\n",
    "            #print('New Node!')\n",
    "\n",
    "            new_env = deepcopy(self.cur_node.env)\n",
    "            new_env.in_step(new_node_action)\n",
    "\n",
    "            new_node = MCSTNode(new_env)\n",
    "            new_node.parent = self.cur_node\n",
    "            new_node.action = new_node_action\n",
    "            self.cur_node.child_actions.append(new_node_action)\n",
    "\n",
    "            run = new_node.env.simulation(self.model, self.mode)\n",
    "            self.all_games += 1\n",
    "\n",
    "            self.cur_node.childs.append(new_node)\n",
    "            self.cur_node = new_node\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "        else:\n",
    "            run = self.cur_node.env.simulation(self.model, self.model)\n",
    "            self.all_games += 1\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "            \n",
    "            \n",
    "    def search(self, time_limit):\n",
    "        begin = time()\n",
    "        while (time() - begin) < time_limit * 0.95:\n",
    "            self.explore()\n",
    "            self.expand()\n",
    "        root_values = list(map(self.get_stat, self.root.childs))\n",
    "        #print(root_UCT_values, len(self.root.childs))\n",
    "        if len(root_values) != 0:\n",
    "            best_child = np.argmax(root_values)\n",
    "        else:\n",
    "            self.root = self.root.childs[0]\n",
    "            return self.root.action\n",
    "        self.root = self.root.childs[best_child]\n",
    "        return self.root.action\n",
    "                    \n",
    "    def get_UCT_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games) + self.constant * math.sqrt(math.log(self.all_games / node.all_games))\n",
    "    \n",
    "    def get_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games)\n",
    "\n",
    "\n",
    "class my_app(Frame):\n",
    "    \"\"\"Basic Frame\"\"\"\n",
    "    def __init__(self, master, mode, time_for_search):\n",
    "        self.buttons = []\n",
    "        \"\"\"Init the Frame\"\"\"\n",
    "        Frame.__init__(self,master)\n",
    "        self.grid()\n",
    "        self.Create_Widgets()\n",
    "        self.tester = RenjuTEST(2 if mode == 'me' else 1, mode)\n",
    "        self.label = tkinter.Label(self, text='')\n",
    "        self.label.grid(column=7, row=16, sticky=W);   #creates label for image on window \n",
    "        self.tree = UCT(self.tester, model, mode)\n",
    "        self.mode = mode\n",
    "        self.time_for_search = time_for_search\n",
    "\n",
    "    \n",
    "    def Create_Widgets(self):\n",
    "        for i in range(15):\n",
    "            for j in range(15): #Start creating buttons\n",
    "\n",
    "                button_id = i * 15 + j \n",
    "                #print(self.button_id)\n",
    "\n",
    "                self.newmessage = Button(self, #I want to bind the self.button_id to each button, so that it prints its number when clicked.\n",
    "                                         text = '',\n",
    "                                         anchor = W, command = lambda button_id=button_id: self.access(button_id))#Run the method\n",
    "\n",
    "                #Placing\n",
    "                self.newmessage.config(height = 1, width = 1)\n",
    "                self.newmessage.grid(row = 15 - i, column = j, sticky = NW)\n",
    "                self.buttons.append(self.newmessage)\n",
    "      \n",
    "    def access(self, b_id): #This is one of the areas where I need help. I want this to return the number of the button clicked.\n",
    "        self.b_id = b_id\n",
    "        print(b_id)\n",
    "        self.buttons[b_id].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.step(b_id)\n",
    "        self.tester.render(mode='human')\n",
    "        \n",
    "        self.buttons[info].config(text = 'X' if self.tester.cur_player == 2 else 'O')\n",
    "        \n",
    "        #\"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.in_step(b_id)\n",
    "        \n",
    "        #self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        \n",
    "        if b_id in self.tree.root.child_actions:\n",
    "            for node in self.tree.root.childs:\n",
    "                if node.action == b_id:\n",
    "                    self.tree.root = node\n",
    "                    break\n",
    "        else:\n",
    "            self.tree = UCT(self.tester, model, self.mode)\n",
    "        #tree = UCT(self.tester, model, 'kn' if random.randint(0,1) == 0 else 'neuron')\n",
    "        tree_act = self.tree.search(self.time_for_search)\n",
    "        print(self.tree.root.all_games, tree_act)\n",
    "        #print(tree_act, tree.root.all_games)\n",
    "        self.buttons[tree_act].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        cur_pos, reward, done, info = self.tester.in_step(tree_act)\n",
    "        #self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        #\"\"\"\n",
    "        \n",
    "\n",
    "opponent = 'neural'\n",
    "first_move = 'me'\n",
    "time_for_search = 10\n",
    "\n",
    "\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Tic-Tac-Toe\")\n",
    "root.geometry(\"542x500\")\n",
    "app = my_app(root, opponent, time_for_search)\n",
    "\n",
    "\n",
    "if first_move != 'me':\n",
    "    tree_act = app.tree.search(time_for_search)\n",
    "    #print(tree_act, tree.root.all_games)\n",
    "    app.buttons[tree_act].config(text = 'X' if app.tester.cur_player == 1 else 'O')\n",
    "    cur_pos, reward, done, info = app.tester.in_step(tree_act)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 µs ± 15.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = clf.predict([tester_1.lr_pos])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([95]),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(tester_1.lr_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 ms ± 184 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = model.predict(np.array([[tester_1.cur_pos]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ec29a02d0db1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtree_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcur_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.UCT.search (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:10679)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.UCT.expand (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:10080)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.RenjuTEST.simulation (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:5176)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.RenjuTEST.in_step (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:2618)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tester_1 = RenjuTEST(1, 'neural')\n",
    "tester_2 = RenjuTEST(2, 'kn')\n",
    "\n",
    "win = 0\n",
    "loose = 0\n",
    "all_game = 0\n",
    "\n",
    "mode_1 = 'kn'\n",
    "mode_2 = 'neuron'\n",
    "\n",
    "tree_1 = UCT(tester_1, model, mode_1)\n",
    "tree_2 = UCT(tester_2, model, mode_2)\n",
    "\n",
    "for i in tqdm(range(150)):\n",
    "    \n",
    "    tree_act = tree_1.search(5)\n",
    "    print(tree_act, tree_1.root.all_games)\n",
    "    cur_pos, reward, done, info = tester_1.in_step(tree_act)\n",
    "    tester_2.in_step(tree_act)\n",
    "    \n",
    "    if tree_act in tree_2.root.child_actions:\n",
    "        for node in tree_2.root.childs:\n",
    "            if node.action == tree_act:\n",
    "                tree_2.root = node\n",
    "                break\n",
    "    else:\n",
    "        tree_2 = UCT(tester_2, model, mode_2)\n",
    "        \n",
    "        \n",
    "    if reward == 1:\n",
    "        win += 1\n",
    "        all_game += 1\n",
    "    if reward == -1:\n",
    "        loose += 1\n",
    "        all_game += 1\n",
    "        \n",
    "    \n",
    "    tree_act = tree_2.search(5)\n",
    "    print(tree_act, tree_2.root.all_games)\n",
    "    cur_pos, reward, done, info = tester_2.in_step(tree_act)\n",
    "    tester_1.in_step(tree_act)\n",
    "    if tree_act in tree_1.root.child_actions:\n",
    "        for node in tree_1.root.childs:\n",
    "            if node.action == tree_act:\n",
    "                tree_1.root = node\n",
    "                break\n",
    "    else:\n",
    "        tree_1 = UCT(tester_1, model, mode_1)\n",
    "        \n",
    "    if reward == -1:\n",
    "        win += 1\n",
    "        all_game += 1\n",
    "    if reward == 1:\n",
    "        loose += 1\n",
    "        all_game += 1\n",
    "        \n",
    "    #tester.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 7 loose: 12 all rate: 0.3684210526315789\n"
     ]
    }
   ],
   "source": [
    "print(\"win:\", win, \"loose:\", loose, \"all rate:\", win / all_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f  tester.step tester.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_check_env.predict(np.array([[env.cur_pos]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205,\n",
       " 157,\n",
       " 79,\n",
       " 191,\n",
       " 63,\n",
       " 202,\n",
       " 22,\n",
       " 175,\n",
       " 111,\n",
       " 91,\n",
       " 189,\n",
       " 152,\n",
       " 142,\n",
       " 34,\n",
       " 25,\n",
       " 160,\n",
       " 72,\n",
       " 33,\n",
       " 204,\n",
       " 36,\n",
       " 176,\n",
       " 19,\n",
       " 57,\n",
       " 48,\n",
       " 137,\n",
       " 178,\n",
       " 133,\n",
       " 52,\n",
       " 136,\n",
       " 106,\n",
       " 151,\n",
       " 132,\n",
       " 77,\n",
       " 187,\n",
       " 102,\n",
       " 103,\n",
       " 92,\n",
       " 26,\n",
       " 118,\n",
       " 88,\n",
       " 20,\n",
       " 42,\n",
       " 109,\n",
       " 35,\n",
       " 67,\n",
       " 172,\n",
       " 121,\n",
       " 126,\n",
       " 78,\n",
       " 93,\n",
       " 119,\n",
       " 169,\n",
       " 61,\n",
       " 39,\n",
       " 177,\n",
       " 153,\n",
       " 71,\n",
       " 24,\n",
       " 148,\n",
       " 59,\n",
       " 201,\n",
       " 18,\n",
       " 162,\n",
       " 107,\n",
       " 161,\n",
       " 104,\n",
       " 139,\n",
       " 122,\n",
       " 146,\n",
       " 64,\n",
       " 62,\n",
       " 190,\n",
       " 127,\n",
       " 186,\n",
       " 194,\n",
       " 199,\n",
       " 41,\n",
       " 167,\n",
       " 198,\n",
       " 74,\n",
       " 166,\n",
       " 56,\n",
       " 124,\n",
       " 40,\n",
       " 215,\n",
       " 131,\n",
       " 134,\n",
       " 174,\n",
       " 159,\n",
       " 164,\n",
       " 51,\n",
       " 182,\n",
       " 87,\n",
       " 117,\n",
       " 37,\n",
       " 140,\n",
       " 110,\n",
       " 155,\n",
       " 185,\n",
       " 168,\n",
       " 76,\n",
       " 54,\n",
       " 171,\n",
       " 154,\n",
       " 7,\n",
       " 206,\n",
       " 49,\n",
       " 193,\n",
       " 149,\n",
       " 73,\n",
       " 90,\n",
       " 123,\n",
       " 163,\n",
       " 31,\n",
       " 147,\n",
       " 183,\n",
       " 192,\n",
       " 138,\n",
       " 108,\n",
       " 58,\n",
       " 89,\n",
       " 4,\n",
       " 170,\n",
       " 65,\n",
       " 69,\n",
       " 116,\n",
       " 70,\n",
       " 94,\n",
       " 145,\n",
       " 66,\n",
       " 47,\n",
       " 50,\n",
       " 184,\n",
       " 200,\n",
       " 44,\n",
       " 21,\n",
       " 5,\n",
       " 2,\n",
       " 27,\n",
       " 86,\n",
       " 125,\n",
       " 80,\n",
       " 105,\n",
       " 97,\n",
       " 101,\n",
       " 209,\n",
       " 141,\n",
       " 156,\n",
       " 220,\n",
       " 179,\n",
       " 181,\n",
       " 55,\n",
       " 17,\n",
       " 75,\n",
       " 208,\n",
       " 43,\n",
       " 219,\n",
       " 216,\n",
       " 30,\n",
       " 14,\n",
       " 96,\n",
       " 6,\n",
       " 223,\n",
       " 46,\n",
       " 120,\n",
       " 150,\n",
       " 32,\n",
       " 12,\n",
       " 213,\n",
       " 135,\n",
       " 212,\n",
       " 217,\n",
       " 95,\n",
       " 195,\n",
       " 222,\n",
       " 15,\n",
       " 3,\n",
       " 196,\n",
       " 10,\n",
       " 214,\n",
       " 13,\n",
       " 144,\n",
       " 130,\n",
       " 221,\n",
       " 1,\n",
       " 28,\n",
       " 81,\n",
       " 197,\n",
       " 100,\n",
       " 115,\n",
       " 84,\n",
       " 11,\n",
       " 45,\n",
       " 207,\n",
       " 9,\n",
       " 82,\n",
       " 180,\n",
       " 165,\n",
       " 60,\n",
       " 211,\n",
       " 29,\n",
       " 224,\n",
       " 85,\n",
       " 99,\n",
       " 128,\n",
       " 16,\n",
       " 129,\n",
       " 0,\n",
       " 38,\n",
       " 114,\n",
       " 210,\n",
       " 53,\n",
       " 173,\n",
       " 83,\n",
       " 112,\n",
       " 188,\n",
       " 218,\n",
       " 68,\n",
       " 98,\n",
       " 143,\n",
       " 23,\n",
       " 158,\n",
       " 8,\n",
       " 203,\n",
       " 113]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def net_ans(env):\n",
    "    s = model_check_env.predict(np.array([[env.cur_pos]]))[0]\n",
    "    return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "\n",
    "net_ans(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_normal.layers)):\n",
    "    model.layers[i + 1].set_weights(model_normal.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 15, 15, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 8)         224       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 550)               744150    \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 225)               123975    \n",
      "=================================================================\n",
      "Total params: 868,349.0\n",
      "Trainable params: 868,349.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "model_policy = Sequential()\n",
    "\n",
    "model_policy.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model_policy.add(Convolution2D(8, (3,3), input_shape=(15, 15, 4), activation='relu'))\n",
    "\n",
    "model_policy.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "model_policy.add(Dense(550, activation='relu', name='fc1'))\n",
    "\n",
    "model_policy.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model_policy.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.agents.sarsa import SarsaAgent\n",
    "\n",
    "\n",
    "ENV_NAME = 'Renju'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = RenjuTEST(1, 'kn')\n",
    "np.random.seed(123)\n",
    "nb_actions = 225\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=1)\n",
    "\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=10000)\n",
    "\n",
    "\n",
    "dqn = DQNAgent(model=model_policy, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "                nb_steps_warmup=5000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 175000 steps ...\n",
      "     56/175000: episode: 1, duration: 0.512s, episode steps: 56, steps per second: 109, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.411 [3.000, 222.000], mean observation: 1.602 [0.000, 112.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     87/175000: episode: 2, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 129.194 [1.000, 219.000], mean observation: 0.564 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    124/175000: episode: 3, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 109.162 [3.000, 224.000], mean observation: 0.784 [0.000, 73.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    152/175000: episode: 4, duration: 0.213s, episode steps: 28, steps per second: 132, episode reward: 1.000, mean reward: 0.036 [0.000, 1.000], mean action: 110.179 [1.000, 224.000], mean observation: 0.461 [0.000, 55.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    176/175000: episode: 5, duration: 0.201s, episode steps: 24, steps per second: 120, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.083 [3.000, 223.000], mean observation: 0.358 [0.000, 48.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    222/175000: episode: 6, duration: 0.376s, episode steps: 46, steps per second: 122, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.826 [8.000, 211.000], mean observation: 1.061 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    255/175000: episode: 7, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 115.818 [0.000, 213.000], mean observation: 0.596 [0.000, 65.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    288/175000: episode: 8, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 136.303 [2.000, 217.000], mean observation: 0.626 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    316/175000: episode: 9, duration: 0.120s, episode steps: 28, steps per second: 233, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 100.321 [8.000, 223.000], mean observation: 0.440 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    347/175000: episode: 10, duration: 0.191s, episode steps: 31, steps per second: 162, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 113.000 [0.000, 216.000], mean observation: 0.527 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    373/175000: episode: 11, duration: 0.118s, episode steps: 26, steps per second: 220, episode reward: 1.000, mean reward: 0.038 [0.000, 1.000], mean action: 95.615 [12.000, 219.000], mean observation: 0.412 [0.000, 51.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    402/175000: episode: 12, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 86.207 [5.000, 204.000], mean observation: 0.496 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    448/175000: episode: 13, duration: 0.361s, episode steps: 46, steps per second: 128, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 117.043 [4.000, 216.000], mean observation: 1.177 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    479/175000: episode: 14, duration: 0.230s, episode steps: 31, steps per second: 135, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 116.516 [7.000, 219.000], mean observation: 0.567 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    499/175000: episode: 15, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 120.050 [38.000, 187.000], mean observation: 0.224 [0.000, 40.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    530/175000: episode: 16, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 126.548 [8.000, 215.000], mean observation: 0.533 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    565/175000: episode: 17, duration: 0.147s, episode steps: 35, steps per second: 237, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.086 [14.000, 217.000], mean observation: 0.651 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    610/175000: episode: 18, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 126.400 [5.000, 224.000], mean observation: 1.022 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    638/175000: episode: 19, duration: 0.132s, episode steps: 28, steps per second: 213, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 111.536 [7.000, 221.000], mean observation: 0.467 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    669/175000: episode: 20, duration: 0.178s, episode steps: 31, steps per second: 175, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 113.194 [0.000, 215.000], mean observation: 0.548 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    701/175000: episode: 21, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.031 [1.000, 213.000], mean observation: 0.575 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    733/175000: episode: 22, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.844 [6.000, 219.000], mean observation: 0.584 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    768/175000: episode: 23, duration: 0.150s, episode steps: 35, steps per second: 233, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.629 [2.000, 218.000], mean observation: 0.709 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    819/175000: episode: 24, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.000 [2.000, 218.000], mean observation: 1.285 [0.000, 102.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    855/175000: episode: 25, duration: 0.162s, episode steps: 36, steps per second: 223, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 110.417 [0.000, 217.000], mean observation: 0.684 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    892/175000: episode: 26, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.324 [12.000, 219.000], mean observation: 0.764 [0.000, 74.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    927/175000: episode: 27, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 94.029 [7.000, 209.000], mean observation: 0.634 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    959/175000: episode: 28, duration: 0.302s, episode steps: 32, steps per second: 106, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 93.938 [3.000, 210.000], mean observation: 0.575 [0.000, 63.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    991/175000: episode: 29, duration: 0.262s, episode steps: 32, steps per second: 122, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 100.156 [1.000, 216.000], mean observation: 0.575 [0.000, 63.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1027/175000: episode: 30, duration: 0.155s, episode steps: 36, steps per second: 232, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 94.361 [5.000, 181.000], mean observation: 0.686 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1058/175000: episode: 31, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.968 [7.000, 210.000], mean observation: 0.555 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1082/175000: episode: 32, duration: 0.102s, episode steps: 24, steps per second: 235, episode reward: 1.000, mean reward: 0.042 [0.000, 1.000], mean action: 112.000 [3.000, 219.000], mean observation: 0.340 [0.000, 47.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1117/175000: episode: 33, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.971 [0.000, 219.000], mean observation: 0.689 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1160/175000: episode: 34, duration: 0.397s, episode steps: 43, steps per second: 108, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.744 [11.000, 218.000], mean observation: 0.874 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1197/175000: episode: 35, duration: 0.166s, episode steps: 37, steps per second: 224, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 111.838 [11.000, 221.000], mean observation: 0.777 [0.000, 73.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1247/175000: episode: 36, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.740 [8.000, 221.000], mean observation: 1.243 [0.000, 100.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1296/175000: episode: 37, duration: 0.253s, episode steps: 49, steps per second: 193, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 123.653 [4.000, 221.000], mean observation: 1.210 [0.000, 97.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1339/175000: episode: 38, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 115.930 [2.000, 217.000], mean observation: 0.956 [0.000, 85.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1369/175000: episode: 39, duration: 0.126s, episode steps: 30, steps per second: 238, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 131.400 [13.000, 217.000], mean observation: 0.481 [0.000, 60.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1395/175000: episode: 40, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 120.769 [15.000, 220.000], mean observation: 0.409 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1431/175000: episode: 41, duration: 0.157s, episode steps: 36, steps per second: 230, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 119.056 [1.000, 217.000], mean observation: 0.720 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1470/175000: episode: 42, duration: 0.323s, episode steps: 39, steps per second: 121, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.282 [1.000, 221.000], mean observation: 0.821 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1514/175000: episode: 43, duration: 0.384s, episode steps: 44, steps per second: 115, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.068 [5.000, 210.000], mean observation: 0.891 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1560/175000: episode: 44, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 113.717 [0.000, 224.000], mean observation: 1.100 [0.000, 91.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1600/175000: episode: 45, duration: 0.295s, episode steps: 40, steps per second: 136, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.725 [7.000, 223.000], mean observation: 0.829 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1643/175000: episode: 46, duration: 0.377s, episode steps: 43, steps per second: 114, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.326 [1.000, 224.000], mean observation: 0.888 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1684/175000: episode: 47, duration: 0.173s, episode steps: 41, steps per second: 237, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.805 [10.000, 223.000], mean observation: 0.851 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1713/175000: episode: 48, duration: 0.283s, episode steps: 29, steps per second: 102, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.517 [7.000, 222.000], mean observation: 0.503 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1754/175000: episode: 49, duration: 0.295s, episode steps: 41, steps per second: 139, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.537 [6.000, 218.000], mean observation: 0.885 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1793/175000: episode: 50, duration: 0.162s, episode steps: 39, steps per second: 240, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.590 [2.000, 221.000], mean observation: 0.820 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1841/175000: episode: 51, duration: 0.361s, episode steps: 48, steps per second: 133, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.708 [5.000, 216.000], mean observation: 1.187 [0.000, 96.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1883/175000: episode: 52, duration: 0.321s, episode steps: 42, steps per second: 131, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.071 [3.000, 205.000], mean observation: 0.960 [0.000, 84.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1912/175000: episode: 53, duration: 0.128s, episode steps: 29, steps per second: 227, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 103.138 [20.000, 189.000], mean observation: 0.389 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1934/175000: episode: 54, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 106.091 [8.000, 197.000], mean observation: 0.236 [0.000, 44.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1963/175000: episode: 55, duration: 0.222s, episode steps: 29, steps per second: 131, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.897 [9.000, 219.000], mean observation: 0.477 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1996/175000: episode: 56, duration: 0.300s, episode steps: 33, steps per second: 110, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 122.333 [5.000, 216.000], mean observation: 0.540 [0.000, 65.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2043/175000: episode: 57, duration: 0.274s, episode steps: 47, steps per second: 172, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.851 [0.000, 224.000], mean observation: 1.198 [0.000, 94.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2082/175000: episode: 58, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 106.256 [2.000, 220.000], mean observation: 0.811 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2118/175000: episode: 59, duration: 0.345s, episode steps: 36, steps per second: 104, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 112.361 [1.000, 224.000], mean observation: 0.697 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2154/175000: episode: 60, duration: 0.236s, episode steps: 36, steps per second: 152, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 90.556 [3.000, 213.000], mean observation: 0.497 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2219/175000: episode: 61, duration: 0.322s, episode steps: 65, steps per second: 202, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 124.369 [16.000, 224.000], mean observation: 2.050 [0.000, 130.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2266/175000: episode: 62, duration: 0.355s, episode steps: 47, steps per second: 132, episode reward: 1.000, mean reward: 0.021 [0.000, 1.000], mean action: 108.468 [1.000, 216.000], mean observation: 1.147 [0.000, 93.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2296/175000: episode: 63, duration: 0.247s, episode steps: 30, steps per second: 121, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 118.233 [4.000, 222.000], mean observation: 0.471 [0.000, 60.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2351/175000: episode: 64, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 1.000, mean reward: 0.018 [0.000, 1.000], mean action: 102.855 [21.000, 223.000], mean observation: 1.342 [0.000, 109.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2401/175000: episode: 65, duration: 0.435s, episode steps: 50, steps per second: 115, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.380 [2.000, 219.000], mean observation: 1.062 [0.000, 100.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2444/175000: episode: 66, duration: 0.199s, episode steps: 43, steps per second: 216, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 110.419 [0.000, 223.000], mean observation: 0.835 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2478/175000: episode: 67, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.353 [10.000, 224.000], mean observation: 0.567 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2518/175000: episode: 68, duration: 0.312s, episode steps: 40, steps per second: 128, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 101.975 [2.000, 219.000], mean observation: 0.645 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2549/175000: episode: 69, duration: 0.288s, episode steps: 31, steps per second: 108, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.935 [5.000, 211.000], mean observation: 0.514 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2576/175000: episode: 70, duration: 0.117s, episode steps: 27, steps per second: 231, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 124.296 [11.000, 224.000], mean observation: 0.366 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2609/175000: episode: 71, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.455 [1.000, 210.000], mean observation: 0.560 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2652/175000: episode: 72, duration: 0.428s, episode steps: 43, steps per second: 100, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.767 [5.000, 224.000], mean observation: 0.976 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2684/175000: episode: 73, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 125.250 [1.000, 214.000], mean observation: 0.559 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2727/175000: episode: 74, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.837 [11.000, 222.000], mean observation: 0.978 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2753/175000: episode: 75, duration: 0.212s, episode steps: 26, steps per second: 122, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 111.077 [4.000, 221.000], mean observation: 0.413 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2785/175000: episode: 76, duration: 0.282s, episode steps: 32, steps per second: 114, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.781 [25.000, 210.000], mean observation: 0.420 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2812/175000: episode: 77, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 111.704 [18.000, 212.000], mean observation: 0.410 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2850/175000: episode: 78, duration: 0.166s, episode steps: 38, steps per second: 229, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.316 [17.000, 223.000], mean observation: 0.712 [0.000, 76.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2904/175000: episode: 79, duration: 0.416s, episode steps: 54, steps per second: 130, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 109.556 [5.000, 213.000], mean observation: 1.302 [0.000, 108.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2950/175000: episode: 80, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 135.304 [22.000, 223.000], mean observation: 1.164 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2976/175000: episode: 81, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 1.000, mean reward: 0.038 [0.000, 1.000], mean action: 101.308 [0.000, 222.000], mean observation: 0.369 [0.000, 51.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3016/175000: episode: 82, duration: 0.311s, episode steps: 40, steps per second: 128, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 126.650 [18.000, 221.000], mean observation: 0.717 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3052/175000: episode: 83, duration: 0.219s, episode steps: 36, steps per second: 165, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.306 [7.000, 216.000], mean observation: 0.617 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3087/175000: episode: 84, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.086 [8.000, 223.000], mean observation: 0.643 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3144/175000: episode: 85, duration: 0.501s, episode steps: 57, steps per second: 114, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.772 [0.000, 223.000], mean observation: 1.341 [0.000, 114.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3188/175000: episode: 86, duration: 0.151s, episode steps: 44, steps per second: 291, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.955 [0.000, 215.000], mean observation: 1.005 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3223/175000: episode: 87, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.057 [1.000, 223.000], mean observation: 0.632 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3267/175000: episode: 88, duration: 0.232s, episode steps: 44, steps per second: 189, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.432 [0.000, 216.000], mean observation: 0.984 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3306/175000: episode: 89, duration: 0.323s, episode steps: 39, steps per second: 121, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 102.000 [9.000, 210.000], mean observation: 0.764 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3357/175000: episode: 90, duration: 0.310s, episode steps: 51, steps per second: 165, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.510 [2.000, 211.000], mean observation: 1.252 [0.000, 102.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3398/175000: episode: 91, duration: 0.195s, episode steps: 41, steps per second: 211, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.049 [25.000, 210.000], mean observation: 0.774 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3439/175000: episode: 92, duration: 0.373s, episode steps: 41, steps per second: 110, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.878 [15.000, 223.000], mean observation: 0.821 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3473/175000: episode: 93, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.500 [2.000, 224.000], mean observation: 0.524 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3509/175000: episode: 94, duration: 0.217s, episode steps: 36, steps per second: 166, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.194 [37.000, 224.000], mean observation: 0.653 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3558/175000: episode: 95, duration: 0.228s, episode steps: 49, steps per second: 215, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 125.878 [14.000, 222.000], mean observation: 0.914 [0.000, 97.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3586/175000: episode: 96, duration: 0.222s, episode steps: 28, steps per second: 126, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 111.321 [11.000, 215.000], mean observation: 0.319 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3631/175000: episode: 97, duration: 0.351s, episode steps: 45, steps per second: 128, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.622 [0.000, 210.000], mean observation: 0.944 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3658/175000: episode: 98, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 99.556 [19.000, 210.000], mean observation: 0.428 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3701/175000: episode: 99, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.558 [0.000, 219.000], mean observation: 0.769 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3723/175000: episode: 100, duration: 0.098s, episode steps: 22, steps per second: 226, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.909 [2.000, 216.000], mean observation: 0.282 [0.000, 44.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3748/175000: episode: 101, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 111.880 [12.000, 210.000], mean observation: 0.295 [0.000, 50.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3779/175000: episode: 102, duration: 0.229s, episode steps: 31, steps per second: 135, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 113.065 [24.000, 224.000], mean observation: 0.467 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3809/175000: episode: 103, duration: 0.225s, episode steps: 30, steps per second: 133, episode reward: 1.000, mean reward: 0.033 [0.000, 1.000], mean action: 107.067 [10.000, 216.000], mean observation: 0.253 [0.000, 59.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3854/175000: episode: 104, duration: 0.281s, episode steps: 45, steps per second: 160, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 101.311 [2.000, 216.000], mean observation: 1.073 [0.000, 89.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3897/175000: episode: 105, duration: 0.188s, episode steps: 43, steps per second: 229, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.116 [0.000, 222.000], mean observation: 0.789 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3938/175000: episode: 106, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 99.146 [0.000, 220.000], mean observation: 0.736 [0.000, 81.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3969/175000: episode: 107, duration: 0.132s, episode steps: 31, steps per second: 234, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.677 [20.000, 217.000], mean observation: 0.382 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3985/175000: episode: 108, duration: 0.069s, episode steps: 16, steps per second: 232, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 105.562 [11.000, 216.000], mean observation: 0.106 [0.000, 32.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4023/175000: episode: 109, duration: 0.248s, episode steps: 38, steps per second: 153, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.342 [3.000, 212.000], mean observation: 0.694 [0.000, 76.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4054/175000: episode: 110, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 128.355 [34.000, 217.000], mean observation: 0.307 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4098/175000: episode: 111, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.773 [11.000, 215.000], mean observation: 0.823 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4124/175000: episode: 112, duration: 0.113s, episode steps: 26, steps per second: 231, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 102.115 [24.000, 167.000], mean observation: 0.223 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4160/175000: episode: 113, duration: 0.222s, episode steps: 36, steps per second: 162, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.639 [10.000, 221.000], mean observation: 0.621 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4212/175000: episode: 114, duration: 0.439s, episode steps: 52, steps per second: 119, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 113.865 [9.000, 214.000], mean observation: 1.134 [0.000, 104.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4248/175000: episode: 115, duration: 0.165s, episode steps: 36, steps per second: 219, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 107.056 [9.000, 212.000], mean observation: 0.476 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4283/175000: episode: 116, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.829 [2.000, 223.000], mean observation: 0.416 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4335/175000: episode: 117, duration: 0.259s, episode steps: 52, steps per second: 201, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 124.692 [8.000, 224.000], mean observation: 1.229 [0.000, 104.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4358/175000: episode: 118, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 110.783 [7.000, 223.000], mean observation: 0.217 [0.000, 46.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4391/175000: episode: 119, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.939 [16.000, 210.000], mean observation: 0.406 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4427/175000: episode: 120, duration: 0.158s, episode steps: 36, steps per second: 228, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 119.306 [3.000, 210.000], mean observation: 0.508 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4473/175000: episode: 121, duration: 0.245s, episode steps: 46, steps per second: 188, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.696 [2.000, 224.000], mean observation: 0.832 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4514/175000: episode: 122, duration: 0.177s, episode steps: 41, steps per second: 232, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 107.366 [7.000, 215.000], mean observation: 0.668 [0.000, 81.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4562/175000: episode: 123, duration: 0.228s, episode steps: 48, steps per second: 211, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 101.729 [15.000, 219.000], mean observation: 0.858 [0.000, 96.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4579/175000: episode: 124, duration: 0.204s, episode steps: 17, steps per second: 83, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 98.588 [24.000, 206.000], mean observation: 0.142 [0.000, 34.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4620/175000: episode: 125, duration: 0.377s, episode steps: 41, steps per second: 109, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 110.049 [10.000, 224.000], mean observation: 0.784 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4651/175000: episode: 126, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 112.226 [8.000, 210.000], mean observation: 0.372 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4684/175000: episode: 127, duration: 0.255s, episode steps: 33, steps per second: 129, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 109.576 [9.000, 210.000], mean observation: 0.393 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4698/175000: episode: 128, duration: 0.145s, episode steps: 14, steps per second: 97, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 110.071 [12.000, 208.000], mean observation: 0.106 [0.000, 28.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4732/175000: episode: 129, duration: 0.216s, episode steps: 34, steps per second: 158, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.971 [10.000, 215.000], mean observation: 0.481 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4757/175000: episode: 130, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 108.760 [14.000, 210.000], mean observation: 0.234 [0.000, 50.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4796/175000: episode: 131, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 108.615 [4.000, 215.000], mean observation: 0.781 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4835/175000: episode: 132, duration: 0.354s, episode steps: 39, steps per second: 110, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 115.487 [25.000, 210.000], mean observation: 0.466 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4876/175000: episode: 133, duration: 0.190s, episode steps: 41, steps per second: 216, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.561 [13.000, 216.000], mean observation: 0.510 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4912/175000: episode: 134, duration: 0.214s, episode steps: 36, steps per second: 168, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.778 [10.000, 216.000], mean observation: 0.594 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4957/175000: episode: 135, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.289 [4.000, 214.000], mean observation: 0.845 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4984/175000: episode: 136, duration: 0.218s, episode steps: 27, steps per second: 124, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 109.296 [15.000, 214.000], mean observation: 0.307 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   5026/175000: episode: 137, duration: 2.573s, episode steps: 42, steps per second: 16, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.595 [14.000, 223.000], mean observation: 0.652 [0.000, 84.000], loss: 0.443550, mean_absolute_error: 0.319872, mean_q: 1.193915, mean_eps: 0.548740\n",
      "   5087/175000: episode: 138, duration: 1.166s, episode steps: 61, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 103.148 [4.000, 220.000], mean observation: 1.491 [0.000, 122.000], loss: 0.335851, mean_absolute_error: 0.373059, mean_q: 1.247759, mean_eps: 0.544960\n",
      "   5112/175000: episode: 139, duration: 0.535s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 84.920 [1.000, 212.000], mean observation: 0.245 [0.000, 50.000], loss: 0.320287, mean_absolute_error: 0.449429, mean_q: 1.432621, mean_eps: 0.541180\n",
      "   5164/175000: episode: 140, duration: 1.052s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.654 [2.000, 222.000], mean observation: 1.003 [0.000, 104.000], loss: 0.353149, mean_absolute_error: 0.447526, mean_q: 1.287315, mean_eps: 0.537760\n",
      "   5219/175000: episode: 141, duration: 1.097s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 104.964 [7.000, 212.000], mean observation: 0.990 [0.000, 110.000], loss: 0.191706, mean_absolute_error: 0.433508, mean_q: 1.263952, mean_eps: 0.532900\n",
      "   5246/175000: episode: 142, duration: 0.550s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 92.815 [35.000, 212.000], mean observation: 0.186 [0.000, 54.000], loss: 0.200162, mean_absolute_error: 0.388345, mean_q: 1.237132, mean_eps: 0.529120\n",
      "   5266/175000: episode: 143, duration: 0.376s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 129.600 [19.000, 212.000], mean observation: 0.184 [0.000, 40.000], loss: 0.187205, mean_absolute_error: 0.367172, mean_q: 1.193088, mean_eps: 0.526960\n",
      "   5292/175000: episode: 144, duration: 0.506s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 107.808 [33.000, 223.000], mean observation: 0.348 [0.000, 52.000], loss: 0.210597, mean_absolute_error: 0.324375, mean_q: 1.053521, mean_eps: 0.524980\n",
      "   5332/175000: episode: 145, duration: 0.756s, episode steps: 40, steps per second: 53, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 103.700 [1.000, 222.000], mean observation: 0.655 [0.000, 79.000], loss: 0.117854, mean_absolute_error: 0.314486, mean_q: 1.089087, mean_eps: 0.522100\n",
      "   5373/175000: episode: 146, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.829 [1.000, 220.000], mean observation: 0.588 [0.000, 82.000], loss: 0.185525, mean_absolute_error: 0.299029, mean_q: 1.053195, mean_eps: 0.518320\n",
      "   5393/175000: episode: 147, duration: 0.344s, episode steps: 20, steps per second: 58, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 74.800 [0.000, 181.000], mean observation: 0.144 [0.000, 40.000], loss: 0.130733, mean_absolute_error: 0.306731, mean_q: 1.151301, mean_eps: 0.515440\n",
      "   5419/175000: episode: 148, duration: 0.445s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 68.808 [1.000, 196.000], mean observation: 0.337 [0.000, 52.000], loss: 0.172016, mean_absolute_error: 0.306390, mean_q: 1.142769, mean_eps: 0.513460\n",
      "   5453/175000: episode: 149, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.412 [8.000, 221.000], mean observation: 0.414 [0.000, 68.000], loss: 0.126104, mean_absolute_error: 0.295012, mean_q: 1.122403, mean_eps: 0.510760\n",
      "   5504/175000: episode: 150, duration: 0.978s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.392 [5.000, 221.000], mean observation: 1.237 [0.000, 102.000], loss: 0.123636, mean_absolute_error: 0.293767, mean_q: 1.200138, mean_eps: 0.506980\n",
      "   5532/175000: episode: 151, duration: 0.714s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 115.714 [26.000, 222.000], mean observation: 0.348 [0.000, 56.000], loss: 0.136955, mean_absolute_error: 0.262875, mean_q: 0.986355, mean_eps: 0.503560\n",
      "   5570/175000: episode: 152, duration: 0.933s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.500 [12.000, 214.000], mean observation: 0.587 [0.000, 76.000], loss: 0.118528, mean_absolute_error: 0.266896, mean_q: 1.022014, mean_eps: 0.500500\n",
      "   5631/175000: episode: 153, duration: 1.355s, episode steps: 61, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 113.426 [6.000, 215.000], mean observation: 1.349 [0.000, 122.000], loss: 0.095236, mean_absolute_error: 0.288875, mean_q: 1.092171, mean_eps: 0.496000\n",
      "   5655/175000: episode: 154, duration: 0.538s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 156.333 [36.000, 214.000], mean observation: 0.251 [0.000, 48.000], loss: 0.160988, mean_absolute_error: 0.310668, mean_q: 1.174143, mean_eps: 0.492220\n",
      "   5679/175000: episode: 155, duration: 0.612s, episode steps: 24, steps per second: 39, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 125.458 [17.000, 193.000], mean observation: 0.253 [0.000, 48.000], loss: 0.135548, mean_absolute_error: 0.290457, mean_q: 1.112301, mean_eps: 0.490060\n",
      "   5711/175000: episode: 156, duration: 0.630s, episode steps: 32, steps per second: 51, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 121.562 [4.000, 223.000], mean observation: 0.527 [0.000, 63.000], loss: 0.169889, mean_absolute_error: 0.284844, mean_q: 1.190255, mean_eps: 0.487540\n",
      "   5736/175000: episode: 157, duration: 0.584s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 108.960 [2.000, 201.000], mean observation: 0.341 [0.000, 50.000], loss: 0.114169, mean_absolute_error: 0.264324, mean_q: 1.148090, mean_eps: 0.485020\n",
      "   5789/175000: episode: 158, duration: 0.989s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 141.717 [17.000, 223.000], mean observation: 1.121 [0.000, 106.000], loss: 0.135553, mean_absolute_error: 0.272364, mean_q: 1.049968, mean_eps: 0.481420\n",
      "   5828/175000: episode: 159, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 108.513 [23.000, 221.000], mean observation: 0.503 [0.000, 78.000], loss: 0.091180, mean_absolute_error: 0.260359, mean_q: 0.889761, mean_eps: 0.477280\n",
      "   5852/175000: episode: 160, duration: 0.474s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 94.583 [19.000, 215.000], mean observation: 0.195 [0.000, 48.000], loss: 0.099804, mean_absolute_error: 0.246749, mean_q: 0.892359, mean_eps: 0.474580\n",
      "   5875/175000: episode: 161, duration: 0.445s, episode steps: 23, steps per second: 52, episode reward: 1.000, mean reward: 0.043 [0.000, 1.000], mean action: 101.478 [12.000, 219.000], mean observation: 0.234 [0.000, 45.000], loss: 0.149405, mean_absolute_error: 0.253427, mean_q: 0.905617, mean_eps: 0.472420\n",
      "   5897/175000: episode: 162, duration: 0.445s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 157.182 [13.000, 215.000], mean observation: 0.224 [0.000, 44.000], loss: 0.112518, mean_absolute_error: 0.255132, mean_q: 0.914953, mean_eps: 0.470260\n",
      "   5932/175000: episode: 163, duration: 0.674s, episode steps: 35, steps per second: 52, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 150.743 [0.000, 223.000], mean observation: 0.479 [0.000, 69.000], loss: 0.081262, mean_absolute_error: 0.239708, mean_q: 0.854257, mean_eps: 0.467740\n",
      "   5981/175000: episode: 164, duration: 1.173s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.918 [8.000, 221.000], mean observation: 0.771 [0.000, 98.000], loss: 0.091262, mean_absolute_error: 0.240236, mean_q: 0.801649, mean_eps: 0.463960\n",
      "   6005/175000: episode: 165, duration: 0.561s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 96.375 [11.000, 188.000], mean observation: 0.198 [0.000, 48.000], loss: 0.112897, mean_absolute_error: 0.232586, mean_q: 0.758231, mean_eps: 0.460540\n",
      "   6056/175000: episode: 166, duration: 1.307s, episode steps: 51, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.510 [10.000, 223.000], mean observation: 0.774 [0.000, 102.000], loss: 0.175853, mean_absolute_error: 0.240680, mean_q: 0.819738, mean_eps: 0.457300\n",
      "   6081/175000: episode: 167, duration: 0.600s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 146.600 [14.000, 221.000], mean observation: 0.276 [0.000, 50.000], loss: 0.137409, mean_absolute_error: 0.266731, mean_q: 0.953706, mean_eps: 0.453880\n",
      "   6129/175000: episode: 168, duration: 0.931s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 142.646 [4.000, 221.000], mean observation: 0.669 [0.000, 96.000], loss: 0.153055, mean_absolute_error: 0.269667, mean_q: 0.905342, mean_eps: 0.450460\n",
      "   6153/175000: episode: 169, duration: 0.534s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 153.375 [20.000, 199.000], mean observation: 0.164 [0.000, 48.000], loss: 0.185325, mean_absolute_error: 0.304252, mean_q: 1.037824, mean_eps: 0.447220\n",
      "   6170/175000: episode: 170, duration: 0.299s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 159.000 [8.000, 192.000], mean observation: 0.100 [0.000, 34.000], loss: 0.291013, mean_absolute_error: 0.288244, mean_q: 0.980248, mean_eps: 0.445420\n",
      "   6200/175000: episode: 171, duration: 0.635s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.133 [33.000, 218.000], mean observation: 0.383 [0.000, 60.000], loss: 0.288639, mean_absolute_error: 0.283883, mean_q: 1.037884, mean_eps: 0.443440\n",
      "   6228/175000: episode: 172, duration: 0.724s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 104.036 [19.000, 218.000], mean observation: 0.316 [0.000, 56.000], loss: 0.281411, mean_absolute_error: 0.303226, mean_q: 1.267328, mean_eps: 0.440920\n",
      "   6255/175000: episode: 173, duration: 0.687s, episode steps: 27, steps per second: 39, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.000 [6.000, 180.000], mean observation: 0.313 [0.000, 54.000], loss: 0.256985, mean_absolute_error: 0.381513, mean_q: 1.269541, mean_eps: 0.438400\n",
      "   6300/175000: episode: 174, duration: 0.948s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.844 [0.000, 224.000], mean observation: 0.655 [0.000, 90.000], loss: 0.299728, mean_absolute_error: 0.414544, mean_q: 1.151420, mean_eps: 0.435160\n",
      "   6322/175000: episode: 175, duration: 0.449s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.409 [3.000, 190.000], mean observation: 0.131 [0.000, 44.000], loss: 0.312578, mean_absolute_error: 0.407707, mean_q: 1.219926, mean_eps: 0.432100\n",
      "   6345/175000: episode: 176, duration: 0.427s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 43.652 [1.000, 207.000], mean observation: 0.195 [0.000, 46.000], loss: 0.261672, mean_absolute_error: 0.395509, mean_q: 1.257832, mean_eps: 0.429940\n",
      "   6391/175000: episode: 177, duration: 0.794s, episode steps: 46, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 105.478 [1.000, 221.000], mean observation: 0.754 [0.000, 92.000], loss: 0.192967, mean_absolute_error: 0.359960, mean_q: 1.084343, mean_eps: 0.426880\n",
      "   6425/175000: episode: 178, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 59.559 [1.000, 213.000], mean observation: 0.298 [0.000, 68.000], loss: 0.159912, mean_absolute_error: 0.359342, mean_q: 1.120786, mean_eps: 0.423280\n",
      "   6453/175000: episode: 179, duration: 0.505s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 81.893 [1.000, 224.000], mean observation: 0.268 [0.000, 56.000], loss: 0.112122, mean_absolute_error: 0.360923, mean_q: 1.087797, mean_eps: 0.420400\n",
      "   6504/175000: episode: 180, duration: 1.001s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 100.941 [1.000, 201.000], mean observation: 0.760 [0.000, 102.000], loss: 0.184638, mean_absolute_error: 0.372163, mean_q: 1.037890, mean_eps: 0.416980\n",
      "   6545/175000: episode: 181, duration: 0.888s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.146 [8.000, 224.000], mean observation: 0.719 [0.000, 82.000], loss: 0.173612, mean_absolute_error: 0.339382, mean_q: 1.009316, mean_eps: 0.412840\n",
      "   6570/175000: episode: 182, duration: 0.559s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 100.000 [4.000, 219.000], mean observation: 0.222 [0.000, 50.000], loss: 0.181901, mean_absolute_error: 0.308106, mean_q: 1.069818, mean_eps: 0.409780\n",
      "   6591/175000: episode: 183, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 100.667 [3.000, 171.000], mean observation: 0.118 [0.000, 42.000], loss: 0.174335, mean_absolute_error: 0.286439, mean_q: 1.106847, mean_eps: 0.407800\n",
      "   6617/175000: episode: 184, duration: 0.671s, episode steps: 26, steps per second: 39, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.500 [11.000, 222.000], mean observation: 0.235 [0.000, 52.000], loss: 0.209713, mean_absolute_error: 0.286569, mean_q: 1.148111, mean_eps: 0.405640\n",
      "   6651/175000: episode: 185, duration: 0.740s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.676 [16.000, 219.000], mean observation: 0.275 [0.000, 68.000], loss: 0.200950, mean_absolute_error: 0.298268, mean_q: 1.181644, mean_eps: 0.402940\n",
      "   6676/175000: episode: 186, duration: 0.609s, episode steps: 25, steps per second: 41, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 145.120 [54.000, 219.000], mean observation: 0.224 [0.000, 50.000], loss: 0.187008, mean_absolute_error: 0.306083, mean_q: 1.142547, mean_eps: 0.400420\n",
      "   6715/175000: episode: 187, duration: 0.782s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.436 [25.000, 219.000], mean observation: 0.410 [0.000, 78.000], loss: 0.162892, mean_absolute_error: 0.295234, mean_q: 0.976413, mean_eps: 0.397540\n",
      "   6726/175000: episode: 188, duration: 0.247s, episode steps: 11, steps per second: 44, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 146.727 [11.000, 213.000], mean observation: 0.065 [0.000, 22.000], loss: 0.196278, mean_absolute_error: 0.291309, mean_q: 0.949550, mean_eps: 0.395200\n",
      "   6787/175000: episode: 189, duration: 1.357s, episode steps: 61, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 144.984 [41.000, 219.000], mean observation: 0.810 [0.000, 122.000], loss: 0.134164, mean_absolute_error: 0.280306, mean_q: 0.935903, mean_eps: 0.391960\n",
      "   6817/175000: episode: 190, duration: 0.694s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 141.233 [13.000, 218.000], mean observation: 0.304 [0.000, 60.000], loss: 0.162682, mean_absolute_error: 0.266051, mean_q: 0.903618, mean_eps: 0.387820\n",
      "   6833/175000: episode: 191, duration: 0.288s, episode steps: 16, steps per second: 56, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 168.875 [58.000, 218.000], mean observation: 0.084 [0.000, 32.000], loss: 0.172154, mean_absolute_error: 0.262160, mean_q: 0.912572, mean_eps: 0.385660\n",
      "   6864/175000: episode: 192, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 111.516 [13.000, 202.000], mean observation: 0.329 [0.000, 62.000], loss: 0.159295, mean_absolute_error: 0.263464, mean_q: 0.924406, mean_eps: 0.383680\n",
      "   6890/175000: episode: 193, duration: 0.584s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.769 [13.000, 207.000], mean observation: 0.287 [0.000, 52.000], loss: 0.190581, mean_absolute_error: 0.254990, mean_q: 0.930271, mean_eps: 0.381160\n",
      "   6918/175000: episode: 194, duration: 0.611s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 91.857 [16.000, 219.000], mean observation: 0.186 [0.000, 56.000], loss: 0.163445, mean_absolute_error: 0.275532, mean_q: 1.131693, mean_eps: 0.378640\n",
      "   6956/175000: episode: 195, duration: 0.849s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 98.395 [13.000, 215.000], mean observation: 0.510 [0.000, 76.000], loss: 0.129758, mean_absolute_error: 0.270789, mean_q: 1.142558, mean_eps: 0.375760\n",
      "   6994/175000: episode: 196, duration: 0.912s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 127.868 [3.000, 216.000], mean observation: 0.393 [0.000, 76.000], loss: 0.192595, mean_absolute_error: 0.261106, mean_q: 1.143386, mean_eps: 0.372340\n",
      "   7035/175000: episode: 197, duration: 0.842s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.341 [13.000, 204.000], mean observation: 0.549 [0.000, 82.000], loss: 0.109564, mean_absolute_error: 0.245955, mean_q: 1.029922, mean_eps: 0.368740\n",
      "   7074/175000: episode: 198, duration: 0.871s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.513 [8.000, 205.000], mean observation: 0.504 [0.000, 78.000], loss: 0.157885, mean_absolute_error: 0.247806, mean_q: 0.920912, mean_eps: 0.365140\n",
      "   7102/175000: episode: 199, duration: 0.655s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.286 [8.000, 193.000], mean observation: 0.244 [0.000, 56.000], loss: 0.127396, mean_absolute_error: 0.251976, mean_q: 0.889514, mean_eps: 0.362080\n",
      "   7141/175000: episode: 200, duration: 0.796s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.590 [10.000, 224.000], mean observation: 0.411 [0.000, 78.000], loss: 0.130220, mean_absolute_error: 0.273926, mean_q: 0.866171, mean_eps: 0.359020\n",
      "   7191/175000: episode: 201, duration: 0.937s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.780 [9.000, 224.000], mean observation: 0.709 [0.000, 100.000], loss: 0.102803, mean_absolute_error: 0.276815, mean_q: 0.976539, mean_eps: 0.355060\n",
      "   7233/175000: episode: 202, duration: 0.995s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.262 [1.000, 210.000], mean observation: 0.713 [0.000, 84.000], loss: 0.173555, mean_absolute_error: 0.271609, mean_q: 1.058298, mean_eps: 0.350920\n",
      "   7279/175000: episode: 203, duration: 0.976s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.565 [6.000, 194.000], mean observation: 0.617 [0.000, 92.000], loss: 0.118471, mean_absolute_error: 0.254431, mean_q: 1.012366, mean_eps: 0.346960\n",
      "   7297/175000: episode: 204, duration: 0.395s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 120.056 [15.000, 208.000], mean observation: 0.131 [0.000, 36.000], loss: 0.097084, mean_absolute_error: 0.249931, mean_q: 0.973721, mean_eps: 0.344080\n",
      "   7336/175000: episode: 205, duration: 0.810s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.359 [16.000, 219.000], mean observation: 0.397 [0.000, 78.000], loss: 0.140767, mean_absolute_error: 0.278126, mean_q: 1.073120, mean_eps: 0.341560\n",
      "   7356/175000: episode: 206, duration: 0.548s, episode steps: 20, steps per second: 36, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 53.200 [35.000, 208.000], mean observation: 0.099 [0.000, 40.000], loss: 0.176212, mean_absolute_error: 0.251836, mean_q: 0.984148, mean_eps: 0.339040\n",
      "   7378/175000: episode: 207, duration: 0.536s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 65.455 [0.000, 219.000], mean observation: 0.194 [0.000, 44.000], loss: 0.113747, mean_absolute_error: 0.265094, mean_q: 0.932902, mean_eps: 0.337060\n",
      "   7418/175000: episode: 208, duration: 0.817s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.925 [5.000, 218.000], mean observation: 0.459 [0.000, 80.000], loss: 0.132454, mean_absolute_error: 0.250189, mean_q: 0.906028, mean_eps: 0.334180\n",
      "   7451/175000: episode: 209, duration: 0.690s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.545 [5.000, 200.000], mean observation: 0.296 [0.000, 66.000], loss: 0.117431, mean_absolute_error: 0.265944, mean_q: 1.001228, mean_eps: 0.330940\n",
      "   7471/175000: episode: 210, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 169.200 [102.000, 196.000], mean observation: 0.059 [0.000, 40.000], loss: 0.188718, mean_absolute_error: 0.279645, mean_q: 1.004620, mean_eps: 0.328600\n",
      "   7522/175000: episode: 211, duration: 1.134s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.255 [4.000, 224.000], mean observation: 0.997 [0.000, 102.000], loss: 0.142229, mean_absolute_error: 0.265131, mean_q: 0.901242, mean_eps: 0.325360\n",
      "   7565/175000: episode: 212, duration: 1.040s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.488 [5.000, 217.000], mean observation: 0.473 [0.000, 86.000], loss: 0.194123, mean_absolute_error: 0.268260, mean_q: 0.945093, mean_eps: 0.321040\n",
      "   7603/175000: episode: 213, duration: 0.921s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.605 [5.000, 219.000], mean observation: 0.375 [0.000, 76.000], loss: 0.153758, mean_absolute_error: 0.294260, mean_q: 1.009258, mean_eps: 0.317440\n",
      "   7637/175000: episode: 214, duration: 0.827s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 162.441 [0.000, 223.000], mean observation: 0.261 [0.000, 68.000], loss: 0.194759, mean_absolute_error: 0.323260, mean_q: 1.066288, mean_eps: 0.314200\n",
      "   7679/175000: episode: 215, duration: 0.870s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 94.786 [5.000, 210.000], mean observation: 0.346 [0.000, 84.000], loss: 0.201748, mean_absolute_error: 0.311263, mean_q: 1.074490, mean_eps: 0.310780\n",
      "   7693/175000: episode: 216, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 41.929 [11.000, 161.000], mean observation: 0.089 [0.000, 28.000], loss: 0.187729, mean_absolute_error: 0.317419, mean_q: 1.097854, mean_eps: 0.308260\n",
      "   7748/175000: episode: 217, duration: 1.187s, episode steps: 55, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 112.673 [5.000, 215.000], mean observation: 0.640 [0.000, 110.000], loss: 0.124410, mean_absolute_error: 0.304749, mean_q: 1.084975, mean_eps: 0.305200\n",
      "   7786/175000: episode: 218, duration: 0.856s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.211 [6.000, 203.000], mean observation: 0.248 [0.000, 76.000], loss: 0.144748, mean_absolute_error: 0.283058, mean_q: 0.958887, mean_eps: 0.301060\n",
      "   7831/175000: episode: 219, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.711 [6.000, 223.000], mean observation: 0.377 [0.000, 90.000], loss: 0.164704, mean_absolute_error: 0.260764, mean_q: 0.924505, mean_eps: 0.297280\n",
      "   7862/175000: episode: 220, duration: 0.637s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 62.484 [6.000, 211.000], mean observation: 0.270 [0.000, 62.000], loss: 0.170170, mean_absolute_error: 0.267249, mean_q: 1.001953, mean_eps: 0.293860\n",
      "   7905/175000: episode: 221, duration: 1.039s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.023 [2.000, 223.000], mean observation: 0.628 [0.000, 86.000], loss: 0.116276, mean_absolute_error: 0.269948, mean_q: 0.928175, mean_eps: 0.290440\n",
      "   7939/175000: episode: 222, duration: 0.686s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.941 [22.000, 221.000], mean observation: 0.212 [0.000, 68.000], loss: 0.179496, mean_absolute_error: 0.287189, mean_q: 0.957501, mean_eps: 0.287020\n",
      "   7980/175000: episode: 223, duration: 0.994s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.195 [2.000, 204.000], mean observation: 0.374 [0.000, 82.000], loss: 0.131892, mean_absolute_error: 0.270718, mean_q: 0.897746, mean_eps: 0.283780\n",
      "   8016/175000: episode: 224, duration: 0.810s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.861 [20.000, 191.000], mean observation: 0.381 [0.000, 72.000], loss: 0.108474, mean_absolute_error: 0.282691, mean_q: 0.955576, mean_eps: 0.280360\n",
      "   8063/175000: episode: 225, duration: 0.993s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 127.234 [6.000, 223.000], mean observation: 0.581 [0.000, 94.000], loss: 0.180948, mean_absolute_error: 0.277318, mean_q: 1.010722, mean_eps: 0.276580\n",
      "   8098/175000: episode: 226, duration: 0.803s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.171 [3.000, 192.000], mean observation: 0.254 [0.000, 70.000], loss: 0.295070, mean_absolute_error: 0.277714, mean_q: 1.049309, mean_eps: 0.272800\n",
      "   8131/175000: episode: 227, duration: 0.610s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 146.576 [8.000, 223.000], mean observation: 0.258 [0.000, 66.000], loss: 0.158416, mean_absolute_error: 0.312738, mean_q: 1.138158, mean_eps: 0.269740\n",
      "   8166/175000: episode: 228, duration: 0.743s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.600 [2.000, 223.000], mean observation: 0.311 [0.000, 70.000], loss: 0.154123, mean_absolute_error: 0.326897, mean_q: 1.176337, mean_eps: 0.266680\n",
      "   8207/175000: episode: 229, duration: 0.807s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.951 [1.000, 223.000], mean observation: 0.428 [0.000, 82.000], loss: 0.134960, mean_absolute_error: 0.339843, mean_q: 1.218082, mean_eps: 0.263260\n",
      "   8256/175000: episode: 230, duration: 1.039s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.735 [14.000, 215.000], mean observation: 0.484 [0.000, 98.000], loss: 0.134159, mean_absolute_error: 0.313701, mean_q: 1.186722, mean_eps: 0.259300\n",
      "   8265/175000: episode: 231, duration: 0.197s, episode steps: 9, steps per second: 46, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 60.556 [34.000, 209.000], mean observation: 0.032 [0.000, 18.000], loss: 0.127386, mean_absolute_error: 0.296933, mean_q: 1.194912, mean_eps: 0.256600\n",
      "   8294/175000: episode: 232, duration: 0.633s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 52.345 [34.000, 208.000], mean observation: 0.195 [0.000, 58.000], loss: 0.099185, mean_absolute_error: 0.304766, mean_q: 1.290602, mean_eps: 0.254800\n",
      "   8327/175000: episode: 233, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 49.939 [19.000, 220.000], mean observation: 0.193 [0.000, 66.000], loss: 0.222265, mean_absolute_error: 0.289992, mean_q: 1.251227, mean_eps: 0.252100\n",
      "   8369/175000: episode: 234, duration: 0.958s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.048 [2.000, 211.000], mean observation: 0.482 [0.000, 84.000], loss: 0.185608, mean_absolute_error: 0.301938, mean_q: 1.197379, mean_eps: 0.248680\n",
      "   8419/175000: episode: 235, duration: 0.912s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.520 [14.000, 205.000], mean observation: 0.586 [0.000, 100.000], loss: 0.179241, mean_absolute_error: 0.275627, mean_q: 1.081553, mean_eps: 0.244540\n",
      "   8459/175000: episode: 236, duration: 0.951s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 133.175 [23.000, 202.000], mean observation: 0.487 [0.000, 80.000], loss: 0.193300, mean_absolute_error: 0.293368, mean_q: 1.029658, mean_eps: 0.240580\n",
      "   8483/175000: episode: 237, duration: 0.574s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 133.333 [64.000, 197.000], mean observation: 0.166 [0.000, 48.000], loss: 0.157011, mean_absolute_error: 0.310715, mean_q: 1.061909, mean_eps: 0.237700\n",
      "   8499/175000: episode: 238, duration: 0.406s, episode steps: 16, steps per second: 39, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 189.812 [118.000, 223.000], mean observation: 0.086 [0.000, 32.000], loss: 0.249088, mean_absolute_error: 0.301915, mean_q: 1.065089, mean_eps: 0.235900\n",
      "   8531/175000: episode: 239, duration: 0.725s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 146.906 [8.000, 217.000], mean observation: 0.319 [0.000, 64.000], loss: 0.216053, mean_absolute_error: 0.303154, mean_q: 1.076979, mean_eps: 0.233740\n",
      "   8574/175000: episode: 240, duration: 0.864s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.349 [26.000, 197.000], mean observation: 0.516 [0.000, 86.000], loss: 0.157383, mean_absolute_error: 0.313493, mean_q: 1.009709, mean_eps: 0.230320\n",
      "   8624/175000: episode: 241, duration: 1.105s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 137.780 [16.000, 218.000], mean observation: 0.590 [0.000, 100.000], loss: 0.254332, mean_absolute_error: 0.308965, mean_q: 1.004328, mean_eps: 0.226180\n",
      "   8654/175000: episode: 242, duration: 0.695s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 100.367 [23.000, 223.000], mean observation: 0.288 [0.000, 60.000], loss: 0.147099, mean_absolute_error: 0.309080, mean_q: 1.019996, mean_eps: 0.222580\n",
      "   8684/175000: episode: 243, duration: 0.791s, episode steps: 30, steps per second: 38, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 81.967 [4.000, 157.000], mean observation: 0.288 [0.000, 60.000], loss: 0.199655, mean_absolute_error: 0.288057, mean_q: 0.972589, mean_eps: 0.219880\n",
      "   8724/175000: episode: 244, duration: 0.935s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 79.400 [14.000, 208.000], mean observation: 0.427 [0.000, 80.000], loss: 0.174847, mean_absolute_error: 0.310995, mean_q: 1.106217, mean_eps: 0.216820\n",
      "   8775/175000: episode: 245, duration: 1.161s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.471 [4.000, 210.000], mean observation: 0.596 [0.000, 102.000], loss: 0.227587, mean_absolute_error: 0.313625, mean_q: 1.104249, mean_eps: 0.212680\n",
      "   8798/175000: episode: 246, duration: 0.539s, episode steps: 23, steps per second: 43, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 127.391 [12.000, 199.000], mean observation: 0.134 [0.000, 46.000], loss: 0.227294, mean_absolute_error: 0.341414, mean_q: 1.203568, mean_eps: 0.209260\n",
      "   8840/175000: episode: 247, duration: 1.075s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.190 [58.000, 205.000], mean observation: 0.369 [0.000, 84.000], loss: 0.144351, mean_absolute_error: 0.307742, mean_q: 1.053599, mean_eps: 0.206380\n",
      "   8861/175000: episode: 248, duration: 0.549s, episode steps: 21, steps per second: 38, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.286 [33.000, 204.000], mean observation: 0.130 [0.000, 42.000], loss: 0.155186, mean_absolute_error: 0.299245, mean_q: 0.997489, mean_eps: 0.203500\n",
      "   8906/175000: episode: 249, duration: 0.962s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.178 [53.000, 218.000], mean observation: 0.411 [0.000, 90.000], loss: 0.144108, mean_absolute_error: 0.310033, mean_q: 1.066221, mean_eps: 0.200440\n",
      "   8917/175000: episode: 250, duration: 0.199s, episode steps: 11, steps per second: 55, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 85.818 [37.000, 118.000], mean observation: 0.044 [0.000, 22.000], loss: 0.220395, mean_absolute_error: 0.299558, mean_q: 1.046247, mean_eps: 0.197920\n",
      "   8948/175000: episode: 251, duration: 0.745s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.194 [0.000, 198.000], mean observation: 0.230 [0.000, 62.000], loss: 0.120045, mean_absolute_error: 0.288340, mean_q: 1.004611, mean_eps: 0.196120\n",
      "   8993/175000: episode: 252, duration: 0.961s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 111.511 [12.000, 205.000], mean observation: 0.509 [0.000, 90.000], loss: 0.166506, mean_absolute_error: 0.307789, mean_q: 1.036176, mean_eps: 0.192700\n",
      "   9026/175000: episode: 253, duration: 0.764s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.515 [22.000, 166.000], mean observation: 0.162 [0.000, 66.000], loss: 0.253380, mean_absolute_error: 0.288915, mean_q: 1.052505, mean_eps: 0.189100\n",
      "   9062/175000: episode: 254, duration: 0.673s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.278 [36.000, 224.000], mean observation: 0.348 [0.000, 72.000], loss: 0.219202, mean_absolute_error: 0.310682, mean_q: 1.145633, mean_eps: 0.186040\n",
      "   9105/175000: episode: 255, duration: 1.038s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.814 [11.000, 171.000], mean observation: 0.413 [0.000, 86.000], loss: 0.175335, mean_absolute_error: 0.291706, mean_q: 1.039622, mean_eps: 0.182440\n",
      "   9131/175000: episode: 256, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.538 [17.000, 224.000], mean observation: 0.084 [0.000, 52.000], loss: 0.124898, mean_absolute_error: 0.279509, mean_q: 0.973697, mean_eps: 0.179380\n",
      "   9177/175000: episode: 257, duration: 1.086s, episode steps: 46, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.739 [27.000, 224.000], mean observation: 0.389 [0.000, 92.000], loss: 0.151061, mean_absolute_error: 0.287170, mean_q: 1.037016, mean_eps: 0.176140\n",
      "   9215/175000: episode: 258, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.921 [25.000, 224.000], mean observation: 0.204 [0.000, 76.000], loss: 0.149756, mean_absolute_error: 0.277656, mean_q: 1.019909, mean_eps: 0.172360\n",
      "   9252/175000: episode: 259, duration: 0.891s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.541 [7.000, 224.000], mean observation: 0.408 [0.000, 74.000], loss: 0.231850, mean_absolute_error: 0.300715, mean_q: 1.118582, mean_eps: 0.169120\n",
      "   9290/175000: episode: 260, duration: 0.806s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.395 [7.000, 224.000], mean observation: 0.307 [0.000, 76.000], loss: 0.139407, mean_absolute_error: 0.296097, mean_q: 1.036399, mean_eps: 0.165700\n",
      "   9336/175000: episode: 261, duration: 0.882s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 171.783 [3.000, 221.000], mean observation: 0.310 [0.000, 92.000], loss: 0.243416, mean_absolute_error: 0.322668, mean_q: 1.105279, mean_eps: 0.161920\n",
      "   9370/175000: episode: 262, duration: 0.738s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.794 [2.000, 221.000], mean observation: 0.317 [0.000, 68.000], loss: 0.230377, mean_absolute_error: 0.314605, mean_q: 1.165094, mean_eps: 0.158320\n",
      "   9404/175000: episode: 263, duration: 0.673s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 149.353 [71.000, 221.000], mean observation: 0.247 [0.000, 68.000], loss: 0.218448, mean_absolute_error: 0.324238, mean_q: 1.218250, mean_eps: 0.155260\n",
      "   9421/175000: episode: 264, duration: 0.456s, episode steps: 17, steps per second: 37, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 177.118 [119.000, 203.000], mean observation: 0.072 [0.000, 34.000], loss: 0.171108, mean_absolute_error: 0.284586, mean_q: 1.052221, mean_eps: 0.152920\n",
      "   9457/175000: episode: 265, duration: 0.850s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.167 [11.000, 221.000], mean observation: 0.305 [0.000, 72.000], loss: 0.168585, mean_absolute_error: 0.314525, mean_q: 1.174939, mean_eps: 0.150400\n",
      "   9478/175000: episode: 266, duration: 0.515s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 123.429 [6.000, 221.000], mean observation: 0.180 [0.000, 42.000], loss: 0.209352, mean_absolute_error: 0.303974, mean_q: 1.055472, mean_eps: 0.147880\n",
      "   9526/175000: episode: 267, duration: 1.080s, episode steps: 48, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 28.312 [6.000, 221.000], mean observation: 0.257 [0.000, 96.000], loss: 0.261936, mean_absolute_error: 0.300726, mean_q: 1.027675, mean_eps: 0.144820\n",
      "   9571/175000: episode: 268, duration: 0.973s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 149.844 [6.000, 182.000], mean observation: 0.340 [0.000, 90.000], loss: 0.172336, mean_absolute_error: 0.321132, mean_q: 1.032094, mean_eps: 0.140680\n",
      "   9610/175000: episode: 269, duration: 0.790s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.821 [12.000, 210.000], mean observation: 0.389 [0.000, 78.000], loss: 0.151602, mean_absolute_error: 0.318360, mean_q: 1.085466, mean_eps: 0.136900\n",
      "   9641/175000: episode: 270, duration: 0.711s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 84.032 [33.000, 208.000], mean observation: 0.194 [0.000, 62.000], loss: 0.160345, mean_absolute_error: 0.297438, mean_q: 1.129116, mean_eps: 0.133660\n",
      "   9680/175000: episode: 271, duration: 0.821s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.949 [15.000, 175.000], mean observation: 0.359 [0.000, 78.000], loss: 0.145844, mean_absolute_error: 0.281658, mean_q: 0.990258, mean_eps: 0.130600\n",
      "   9726/175000: episode: 272, duration: 0.958s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.717 [27.000, 204.000], mean observation: 0.306 [0.000, 92.000], loss: 0.183389, mean_absolute_error: 0.264470, mean_q: 0.960034, mean_eps: 0.126820\n",
      "   9749/175000: episode: 273, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 119.609 [21.000, 217.000], mean observation: 0.177 [0.000, 46.000], loss: 0.261066, mean_absolute_error: 0.279306, mean_q: 1.075439, mean_eps: 0.123580\n",
      "   9786/175000: episode: 274, duration: 0.690s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 98.054 [27.000, 184.000], mean observation: 0.236 [0.000, 74.000], loss: 0.160478, mean_absolute_error: 0.256274, mean_q: 0.892033, mean_eps: 0.120880\n",
      "   9820/175000: episode: 275, duration: 0.774s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.088 [17.000, 209.000], mean observation: 0.133 [0.000, 68.000], loss: 0.200970, mean_absolute_error: 0.250138, mean_q: 0.813890, mean_eps: 0.117820\n",
      "   9859/175000: episode: 276, duration: 0.827s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 71.949 [27.000, 206.000], mean observation: 0.302 [0.000, 78.000], loss: 0.136371, mean_absolute_error: 0.268528, mean_q: 0.886586, mean_eps: 0.114580\n",
      "   9883/175000: episode: 277, duration: 0.472s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 73.667 [8.000, 202.000], mean observation: 0.085 [0.000, 48.000], loss: 0.164800, mean_absolute_error: 0.255217, mean_q: 0.880753, mean_eps: 0.111700\n",
      "   9915/175000: episode: 278, duration: 0.707s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.438 [6.000, 206.000], mean observation: 0.206 [0.000, 64.000], loss: 0.094525, mean_absolute_error: 0.259850, mean_q: 0.932823, mean_eps: 0.109180\n",
      "   9958/175000: episode: 279, duration: 0.918s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.837 [10.000, 215.000], mean observation: 0.308 [0.000, 86.000], loss: 0.122813, mean_absolute_error: 0.253576, mean_q: 0.871526, mean_eps: 0.105760\n",
      "   9996/175000: episode: 280, duration: 0.857s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.579 [10.000, 203.000], mean observation: 0.176 [0.000, 76.000], loss: 0.094657, mean_absolute_error: 0.261543, mean_q: 0.926039, mean_eps: 0.102160\n",
      "  10056/175000: episode: 281, duration: 1.421s, episode steps: 60, steps per second: 42, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 83.067 [0.000, 214.000], mean observation: 0.724 [0.000, 120.000], loss: 0.591978, mean_absolute_error: 0.261225, mean_q: 0.867033, mean_eps: 0.100024\n",
      "  10086/175000: episode: 282, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 105.233 [27.000, 162.000], mean observation: 0.095 [0.000, 60.000], loss: 0.447050, mean_absolute_error: 0.288863, mean_q: 0.958709, mean_eps: 0.100000\n",
      "  10106/175000: episode: 283, duration: 0.480s, episode steps: 20, steps per second: 42, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 101.250 [40.000, 162.000], mean observation: 0.066 [0.000, 40.000], loss: 0.214927, mean_absolute_error: 0.302144, mean_q: 0.986950, mean_eps: 0.100000\n",
      "  10143/175000: episode: 284, duration: 0.804s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 126.811 [34.000, 203.000], mean observation: 0.274 [0.000, 74.000], loss: 0.415060, mean_absolute_error: 0.307224, mean_q: 1.078963, mean_eps: 0.100000\n",
      "  10202/175000: episode: 285, duration: 1.339s, episode steps: 59, steps per second: 44, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 53.356 [27.000, 203.000], mean observation: 0.545 [0.000, 118.000], loss: 0.302851, mean_absolute_error: 0.292722, mean_q: 1.111227, mean_eps: 0.100000\n",
      "  10239/175000: episode: 286, duration: 0.742s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 45.081 [30.000, 219.000], mean observation: 0.190 [0.000, 74.000], loss: 0.303554, mean_absolute_error: 0.280105, mean_q: 1.136334, mean_eps: 0.100000\n",
      "  10269/175000: episode: 287, duration: 0.609s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 48.467 [44.000, 134.000], mean observation: 0.106 [0.000, 60.000], loss: 0.204598, mean_absolute_error: 0.248804, mean_q: 1.071238, mean_eps: 0.100000\n",
      "  10318/175000: episode: 288, duration: 0.916s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 72.653 [18.000, 137.000], mean observation: 0.208 [0.000, 98.000], loss: 0.154382, mean_absolute_error: 0.253875, mean_q: 1.158982, mean_eps: 0.100000\n",
      "  10346/175000: episode: 289, duration: 0.560s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 67.893 [27.000, 203.000], mean observation: 0.142 [0.000, 56.000], loss: 0.156721, mean_absolute_error: 0.253903, mean_q: 1.096212, mean_eps: 0.100000\n",
      "  10400/175000: episode: 290, duration: 1.241s, episode steps: 54, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.389 [78.000, 209.000], mean observation: 0.390 [0.000, 108.000], loss: 0.176180, mean_absolute_error: 0.258059, mean_q: 1.139706, mean_eps: 0.100000\n",
      "  10425/175000: episode: 291, duration: 0.558s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 48.600 [9.000, 199.000], mean observation: 0.102 [0.000, 50.000], loss: 0.133372, mean_absolute_error: 0.267475, mean_q: 1.322829, mean_eps: 0.100000\n",
      "  10451/175000: episode: 292, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 9.000 [9.000, 9.000], mean observation: 0.061 [0.000, 52.000], loss: 0.149868, mean_absolute_error: 0.271516, mean_q: 1.373930, mean_eps: 0.100000\n",
      "  10499/175000: episode: 293, duration: 1.054s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.979 [9.000, 205.000], mean observation: 0.175 [0.000, 96.000], loss: 0.138450, mean_absolute_error: 0.281499, mean_q: 1.337101, mean_eps: 0.100000\n",
      "  10546/175000: episode: 294, duration: 1.153s, episode steps: 47, steps per second: 41, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 83.447 [4.000, 209.000], mean observation: 0.322 [0.000, 94.000], loss: 0.132217, mean_absolute_error: 0.261495, mean_q: 1.191576, mean_eps: 0.100000\n",
      "  10590/175000: episode: 295, duration: 1.004s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.273 [19.000, 205.000], mean observation: 0.302 [0.000, 88.000], loss: 0.108647, mean_absolute_error: 0.252781, mean_q: 1.069751, mean_eps: 0.100000\n",
      "  10627/175000: episode: 296, duration: 0.787s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 81.946 [33.000, 130.000], mean observation: 0.205 [0.000, 74.000], loss: 0.058878, mean_absolute_error: 0.248261, mean_q: 1.078348, mean_eps: 0.100000\n",
      "  10654/175000: episode: 297, duration: 0.640s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 134.037 [9.000, 216.000], mean observation: 0.080 [0.000, 54.000], loss: 0.125071, mean_absolute_error: 0.242984, mean_q: 1.011419, mean_eps: 0.100000\n",
      "  10691/175000: episode: 298, duration: 0.863s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 82.189 [9.000, 216.000], mean observation: 0.207 [0.000, 74.000], loss: 0.060359, mean_absolute_error: 0.246788, mean_q: 1.024306, mean_eps: 0.100000\n",
      "  10718/175000: episode: 299, duration: 0.600s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.889 [9.000, 114.000], mean observation: 0.076 [0.000, 54.000], loss: 0.132397, mean_absolute_error: 0.233276, mean_q: 0.979701, mean_eps: 0.100000\n",
      "  10758/175000: episode: 300, duration: 0.906s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 123.650 [36.000, 222.000], mean observation: 0.247 [0.000, 80.000], loss: 0.122418, mean_absolute_error: 0.228583, mean_q: 1.029897, mean_eps: 0.100000\n",
      "  10787/175000: episode: 301, duration: 0.805s, episode steps: 29, steps per second: 36, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.345 [22.000, 198.000], mean observation: 0.151 [0.000, 58.000], loss: 0.146358, mean_absolute_error: 0.246408, mean_q: 1.142318, mean_eps: 0.100000\n",
      "  10826/175000: episode: 302, duration: 0.974s, episode steps: 39, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 136.077 [54.000, 222.000], mean observation: 0.270 [0.000, 78.000], loss: 0.146318, mean_absolute_error: 0.248330, mean_q: 1.040984, mean_eps: 0.100000\n",
      "  10875/175000: episode: 303, duration: 1.171s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 163.837 [29.000, 222.000], mean observation: 0.312 [0.000, 98.000], loss: 0.117388, mean_absolute_error: 0.269144, mean_q: 1.020811, mean_eps: 0.100000\n",
      "  10899/175000: episode: 304, duration: 0.538s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 79.250 [65.000, 222.000], mean observation: 0.139 [0.000, 48.000], loss: 0.075121, mean_absolute_error: 0.298480, mean_q: 1.117725, mean_eps: 0.100000\n",
      "  10947/175000: episode: 305, duration: 0.939s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.979 [3.000, 222.000], mean observation: 0.260 [0.000, 96.000], loss: 0.114696, mean_absolute_error: 0.302900, mean_q: 1.063863, mean_eps: 0.100000\n",
      "  10978/175000: episode: 306, duration: 0.669s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 106.161 [27.000, 210.000], mean observation: 0.149 [0.000, 62.000], loss: 0.131573, mean_absolute_error: 0.295028, mean_q: 0.994473, mean_eps: 0.100000\n",
      "  11003/175000: episode: 307, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 172.400 [96.000, 180.000], mean observation: 0.071 [0.000, 50.000], loss: 0.090437, mean_absolute_error: 0.290807, mean_q: 0.994804, mean_eps: 0.100000\n",
      "  11046/175000: episode: 308, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.419 [3.000, 205.000], mean observation: 0.317 [0.000, 86.000], loss: 0.101893, mean_absolute_error: 0.278423, mean_q: 0.970685, mean_eps: 0.100000\n",
      "  11087/175000: episode: 309, duration: 0.866s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.512 [3.000, 224.000], mean observation: 0.286 [0.000, 82.000], loss: 0.109473, mean_absolute_error: 0.257960, mean_q: 0.867641, mean_eps: 0.100000\n",
      "  11132/175000: episode: 310, duration: 1.042s, episode steps: 45, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 170.511 [29.000, 214.000], mean observation: 0.319 [0.000, 90.000], loss: 0.070617, mean_absolute_error: 0.253014, mean_q: 0.847118, mean_eps: 0.100000\n",
      "  11159/175000: episode: 311, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 175.630 [7.000, 200.000], mean observation: 0.180 [0.000, 54.000], loss: 0.064717, mean_absolute_error: 0.256763, mean_q: 0.947570, mean_eps: 0.100000\n",
      "  11174/175000: episode: 312, duration: 0.287s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 183.667 [86.000, 197.000], mean observation: 0.078 [0.000, 30.000], loss: 0.080185, mean_absolute_error: 0.241568, mean_q: 0.923067, mean_eps: 0.100000\n",
      "  11202/175000: episode: 313, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 192.500 [118.000, 210.000], mean observation: 0.087 [0.000, 56.000], loss: 0.097698, mean_absolute_error: 0.266827, mean_q: 1.018215, mean_eps: 0.100000\n",
      "  11236/175000: episode: 314, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 189.029 [42.000, 198.000], mean observation: 0.159 [0.000, 68.000], loss: 0.157746, mean_absolute_error: 0.245719, mean_q: 0.987873, mean_eps: 0.100000\n",
      "  11279/175000: episode: 315, duration: 0.867s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 166.651 [12.000, 200.000], mean observation: 0.226 [0.000, 86.000], loss: 0.157778, mean_absolute_error: 0.286396, mean_q: 1.041141, mean_eps: 0.100000\n",
      "  11325/175000: episode: 316, duration: 0.993s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.674 [3.000, 198.000], mean observation: 0.411 [0.000, 92.000], loss: 0.130091, mean_absolute_error: 0.337886, mean_q: 1.055946, mean_eps: 0.100000\n",
      "  11358/175000: episode: 317, duration: 0.669s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.939 [60.000, 168.000], mean observation: 0.101 [0.000, 66.000], loss: 0.111758, mean_absolute_error: 0.334182, mean_q: 1.034944, mean_eps: 0.100000\n",
      "  11379/175000: episode: 318, duration: 0.451s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 71.619 [4.000, 223.000], mean observation: 0.110 [0.000, 42.000], loss: 0.145209, mean_absolute_error: 0.324382, mean_q: 1.029811, mean_eps: 0.100000\n",
      "  11407/175000: episode: 319, duration: 0.560s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.679 [30.000, 139.000], mean observation: 0.151 [0.000, 56.000], loss: 0.118677, mean_absolute_error: 0.282059, mean_q: 0.910269, mean_eps: 0.100000\n",
      "  11461/175000: episode: 320, duration: 1.116s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.019 [23.000, 214.000], mean observation: 0.402 [0.000, 108.000], loss: 0.144175, mean_absolute_error: 0.290859, mean_q: 1.025345, mean_eps: 0.100000\n",
      "  11499/175000: episode: 321, duration: 0.847s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.026 [30.000, 218.000], mean observation: 0.335 [0.000, 76.000], loss: 0.117987, mean_absolute_error: 0.267239, mean_q: 1.001749, mean_eps: 0.100000\n",
      "  11539/175000: episode: 322, duration: 0.796s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 80.300 [7.000, 110.000], mean observation: 0.195 [0.000, 80.000], loss: 0.149490, mean_absolute_error: 0.265163, mean_q: 0.992374, mean_eps: 0.100000\n",
      "  11564/175000: episode: 323, duration: 0.512s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 123.600 [9.000, 156.000], mean observation: 0.089 [0.000, 50.000], loss: 0.242910, mean_absolute_error: 0.246519, mean_q: 0.901165, mean_eps: 0.100000\n",
      "  11604/175000: episode: 324, duration: 0.812s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 75.100 [7.000, 182.000], mean observation: 0.263 [0.000, 80.000], loss: 0.211537, mean_absolute_error: 0.247018, mean_q: 0.969406, mean_eps: 0.100000\n",
      "  11651/175000: episode: 325, duration: 0.896s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 79.255 [7.000, 205.000], mean observation: 0.214 [0.000, 94.000], loss: 0.180865, mean_absolute_error: 0.253533, mean_q: 1.068143, mean_eps: 0.100000\n",
      "  11675/175000: episode: 326, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 159.667 [117.000, 205.000], mean observation: 0.068 [0.000, 48.000], loss: 0.173545, mean_absolute_error: 0.294092, mean_q: 1.127890, mean_eps: 0.100000\n",
      "  11703/175000: episode: 327, duration: 0.527s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.786 [54.000, 205.000], mean observation: 0.150 [0.000, 56.000], loss: 0.187224, mean_absolute_error: 0.258076, mean_q: 0.909547, mean_eps: 0.100000\n",
      "  11736/175000: episode: 328, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 87.273 [8.000, 172.000], mean observation: 0.151 [0.000, 66.000], loss: 0.140535, mean_absolute_error: 0.249463, mean_q: 0.861931, mean_eps: 0.100000\n",
      "  11771/175000: episode: 329, duration: 0.768s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 90.686 [43.000, 145.000], mean observation: 0.133 [0.000, 70.000], loss: 0.190291, mean_absolute_error: 0.236412, mean_q: 0.867105, mean_eps: 0.100000\n",
      "  11806/175000: episode: 330, duration: 0.748s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.057 [43.000, 126.000], mean observation: 0.178 [0.000, 70.000], loss: 0.196167, mean_absolute_error: 0.270510, mean_q: 0.995933, mean_eps: 0.100000\n",
      "  11835/175000: episode: 331, duration: 0.654s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 138.034 [31.000, 149.000], mean observation: 0.131 [0.000, 58.000], loss: 0.153542, mean_absolute_error: 0.250594, mean_q: 0.955302, mean_eps: 0.100000\n",
      "  11887/175000: episode: 332, duration: 1.151s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.385 [29.000, 149.000], mean observation: 0.336 [0.000, 104.000], loss: 0.099841, mean_absolute_error: 0.247171, mean_q: 1.002131, mean_eps: 0.100000\n",
      "  11939/175000: episode: 333, duration: 0.967s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.096 [20.000, 200.000], mean observation: 0.272 [0.000, 104.000], loss: 0.088796, mean_absolute_error: 0.242219, mean_q: 0.863584, mean_eps: 0.100000\n",
      "  11955/175000: episode: 334, duration: 0.308s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 103.312 [17.000, 110.000], mean observation: 0.047 [0.000, 32.000], loss: 0.179644, mean_absolute_error: 0.275896, mean_q: 1.049858, mean_eps: 0.100000\n",
      "  12005/175000: episode: 335, duration: 0.996s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.500 [29.000, 133.000], mean observation: 0.209 [0.000, 100.000], loss: 0.141139, mean_absolute_error: 0.254752, mean_q: 1.095563, mean_eps: 0.100000\n",
      "  12046/175000: episode: 336, duration: 0.727s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 205.805 [198.000, 218.000], mean observation: 0.119 [0.000, 82.000], loss: 0.155018, mean_absolute_error: 0.262224, mean_q: 1.056698, mean_eps: 0.100000\n",
      "  12094/175000: episode: 337, duration: 0.889s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 159.125 [37.000, 218.000], mean observation: 0.433 [0.000, 96.000], loss: 0.160288, mean_absolute_error: 0.310173, mean_q: 1.195976, mean_eps: 0.100000\n",
      "  12123/175000: episode: 338, duration: 0.549s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 37.000 [37.000, 37.000], mean observation: 0.068 [0.000, 58.000], loss: 0.282825, mean_absolute_error: 0.307999, mean_q: 1.206933, mean_eps: 0.100000\n",
      "  12153/175000: episode: 339, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.633 [37.000, 218.000], mean observation: 0.150 [0.000, 60.000], loss: 0.167892, mean_absolute_error: 0.285643, mean_q: 1.131370, mean_eps: 0.100000\n",
      "  12193/175000: episode: 340, duration: 0.715s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 52.300 [37.000, 221.000], mean observation: 0.150 [0.000, 80.000], loss: 0.309218, mean_absolute_error: 0.318690, mean_q: 1.278199, mean_eps: 0.100000\n",
      "  12225/175000: episode: 341, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 81.625 [37.000, 134.000], mean observation: 0.115 [0.000, 64.000], loss: 0.278833, mean_absolute_error: 0.325743, mean_q: 1.117375, mean_eps: 0.100000\n",
      "  12241/175000: episode: 342, duration: 0.292s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 157.312 [21.000, 178.000], mean observation: 0.087 [0.000, 32.000], loss: 0.147036, mean_absolute_error: 0.327826, mean_q: 1.138951, mean_eps: 0.100000\n",
      "  12263/175000: episode: 343, duration: 0.382s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 125.545 [37.000, 176.000], mean observation: 0.113 [0.000, 44.000], loss: 0.315796, mean_absolute_error: 0.329928, mean_q: 1.257687, mean_eps: 0.100000\n",
      "  12289/175000: episode: 344, duration: 0.537s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 175.077 [164.000, 176.000], mean observation: 0.062 [0.000, 52.000], loss: 0.205363, mean_absolute_error: 0.293764, mean_q: 1.298121, mean_eps: 0.100000\n",
      "  12330/175000: episode: 345, duration: 0.722s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.780 [37.000, 199.000], mean observation: 0.165 [0.000, 82.000], loss: 0.305183, mean_absolute_error: 0.300083, mean_q: 1.397478, mean_eps: 0.100000\n",
      "  12356/175000: episode: 346, duration: 0.576s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 155.115 [29.000, 164.000], mean observation: 0.097 [0.000, 52.000], loss: 0.380995, mean_absolute_error: 0.294557, mean_q: 1.208987, mean_eps: 0.100000\n",
      "  12401/175000: episode: 347, duration: 0.853s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 154.533 [38.000, 199.000], mean observation: 0.395 [0.000, 90.000], loss: 0.174477, mean_absolute_error: 0.271035, mean_q: 1.011896, mean_eps: 0.100000\n",
      "  12441/175000: episode: 348, duration: 0.916s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 162.175 [5.000, 203.000], mean observation: 0.334 [0.000, 80.000], loss: 0.167172, mean_absolute_error: 0.277632, mean_q: 1.089150, mean_eps: 0.100000\n",
      "  12462/175000: episode: 349, duration: 0.481s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 148.190 [88.000, 164.000], mean observation: 0.085 [0.000, 42.000], loss: 0.478757, mean_absolute_error: 0.272463, mean_q: 1.145521, mean_eps: 0.100000\n",
      "  12482/175000: episode: 350, duration: 0.438s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 145.350 [59.000, 191.000], mean observation: 0.090 [0.000, 40.000], loss: 0.267177, mean_absolute_error: 0.245920, mean_q: 1.038725, mean_eps: 0.100000\n",
      "  12512/175000: episode: 351, duration: 0.732s, episode steps: 30, steps per second: 41, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 85.367 [37.000, 208.000], mean observation: 0.162 [0.000, 60.000], loss: 0.218246, mean_absolute_error: 0.274317, mean_q: 1.063617, mean_eps: 0.100000\n",
      "  12555/175000: episode: 352, duration: 0.948s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.721 [2.000, 182.000], mean observation: 0.262 [0.000, 86.000], loss: 0.206090, mean_absolute_error: 0.270936, mean_q: 1.072049, mean_eps: 0.100000\n",
      "  12604/175000: episode: 353, duration: 1.037s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 155.469 [1.000, 219.000], mean observation: 0.369 [0.000, 98.000], loss: 0.221700, mean_absolute_error: 0.283842, mean_q: 1.151397, mean_eps: 0.100000\n",
      "  12625/175000: episode: 354, duration: 0.568s, episode steps: 21, steps per second: 37, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 90.190 [19.000, 210.000], mean observation: 0.138 [0.000, 42.000], loss: 0.243806, mean_absolute_error: 0.287741, mean_q: 1.114314, mean_eps: 0.100000\n",
      "  12664/175000: episode: 355, duration: 0.796s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.128 [10.000, 178.000], mean observation: 0.192 [0.000, 78.000], loss: 0.191899, mean_absolute_error: 0.308387, mean_q: 1.153073, mean_eps: 0.100000\n",
      "  12695/175000: episode: 356, duration: 0.704s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 61.839 [10.000, 178.000], mean observation: 0.160 [0.000, 62.000], loss: 0.288527, mean_absolute_error: 0.297116, mean_q: 1.067363, mean_eps: 0.100000\n",
      "  12734/175000: episode: 357, duration: 0.866s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 46.103 [10.000, 178.000], mean observation: 0.191 [0.000, 78.000], loss: 0.324677, mean_absolute_error: 0.317379, mean_q: 1.182537, mean_eps: 0.100000\n",
      "  12756/175000: episode: 358, duration: 0.490s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 44.409 [10.000, 216.000], mean observation: 0.140 [0.000, 44.000], loss: 0.373289, mean_absolute_error: 0.314916, mean_q: 1.080853, mean_eps: 0.100000\n",
      "  12815/175000: episode: 359, duration: 1.163s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 85.881 [9.000, 165.000], mean observation: 0.308 [0.000, 118.000], loss: 0.377734, mean_absolute_error: 0.360409, mean_q: 1.174454, mean_eps: 0.100000\n",
      "  12867/175000: episode: 360, duration: 1.015s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.635 [11.000, 203.000], mean observation: 0.382 [0.000, 104.000], loss: 0.277614, mean_absolute_error: 0.361302, mean_q: 1.252862, mean_eps: 0.100000\n",
      "  12917/175000: episode: 361, duration: 0.932s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 143.300 [54.000, 223.000], mean observation: 0.326 [0.000, 100.000], loss: 0.241632, mean_absolute_error: 0.331285, mean_q: 1.110418, mean_eps: 0.100000\n",
      "  12961/175000: episode: 362, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 177.523 [47.000, 213.000], mean observation: 0.272 [0.000, 88.000], loss: 0.170155, mean_absolute_error: 0.344261, mean_q: 1.291608, mean_eps: 0.100000\n",
      "  12987/175000: episode: 363, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 210.385 [145.000, 213.000], mean observation: 0.065 [0.000, 52.000], loss: 0.167614, mean_absolute_error: 0.356569, mean_q: 1.275940, mean_eps: 0.100000\n",
      "  13016/175000: episode: 364, duration: 0.662s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 105.103 [35.000, 219.000], mean observation: 0.229 [0.000, 58.000], loss: 0.123374, mean_absolute_error: 0.333794, mean_q: 1.162972, mean_eps: 0.100000\n",
      "  13057/175000: episode: 365, duration: 0.834s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.415 [15.000, 139.000], mean observation: 0.204 [0.000, 82.000], loss: 0.166560, mean_absolute_error: 0.335943, mean_q: 1.170158, mean_eps: 0.100000\n",
      "  13092/175000: episode: 366, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.886 [19.000, 171.000], mean observation: 0.280 [0.000, 70.000], loss: 0.126287, mean_absolute_error: 0.318394, mean_q: 1.096325, mean_eps: 0.100000\n",
      "  13117/175000: episode: 367, duration: 0.592s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 154.440 [28.000, 171.000], mean observation: 0.075 [0.000, 50.000], loss: 0.165427, mean_absolute_error: 0.301540, mean_q: 1.084662, mean_eps: 0.100000\n",
      "  13170/175000: episode: 368, duration: 1.184s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 144.264 [25.000, 171.000], mean observation: 0.342 [0.000, 106.000], loss: 0.176557, mean_absolute_error: 0.321059, mean_q: 1.063383, mean_eps: 0.100000\n",
      "  13197/175000: episode: 369, duration: 0.557s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 76.852 [4.000, 188.000], mean observation: 0.117 [0.000, 54.000], loss: 0.132056, mean_absolute_error: 0.304170, mean_q: 0.949231, mean_eps: 0.100000\n",
      "  13224/175000: episode: 370, duration: 0.542s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 163.111 [70.000, 184.000], mean observation: 0.092 [0.000, 54.000], loss: 0.173725, mean_absolute_error: 0.301344, mean_q: 0.946284, mean_eps: 0.100000\n",
      "  13269/175000: episode: 371, duration: 0.910s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 159.889 [23.000, 163.000], mean observation: 0.123 [0.000, 90.000], loss: 0.129879, mean_absolute_error: 0.298610, mean_q: 1.017200, mean_eps: 0.100000\n",
      "  13305/175000: episode: 372, duration: 0.673s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.806 [4.000, 221.000], mean observation: 0.208 [0.000, 72.000], loss: 0.117579, mean_absolute_error: 0.276863, mean_q: 1.078631, mean_eps: 0.100000\n",
      "  13336/175000: episode: 373, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 104.806 [4.000, 206.000], mean observation: 0.133 [0.000, 62.000], loss: 0.196843, mean_absolute_error: 0.294261, mean_q: 1.112841, mean_eps: 0.100000\n",
      "  13372/175000: episode: 374, duration: 0.712s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 71.444 [4.000, 206.000], mean observation: 0.254 [0.000, 72.000], loss: 0.283921, mean_absolute_error: 0.319296, mean_q: 1.142717, mean_eps: 0.100000\n",
      "  13402/175000: episode: 375, duration: 0.630s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 40.267 [20.000, 186.000], mean observation: 0.141 [0.000, 60.000], loss: 0.223058, mean_absolute_error: 0.365308, mean_q: 1.138513, mean_eps: 0.100000\n",
      "  13425/175000: episode: 376, duration: 0.465s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.348 [19.000, 103.000], mean observation: 0.066 [0.000, 46.000], loss: 0.158353, mean_absolute_error: 0.364541, mean_q: 1.104974, mean_eps: 0.100000\n",
      "  13460/175000: episode: 377, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.000 [67.000, 212.000], mean observation: 0.247 [0.000, 70.000], loss: 0.215036, mean_absolute_error: 0.380901, mean_q: 1.211195, mean_eps: 0.100000\n",
      "  13500/175000: episode: 378, duration: 0.789s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 157.650 [64.000, 212.000], mean observation: 0.277 [0.000, 80.000], loss: 0.268109, mean_absolute_error: 0.342270, mean_q: 1.103623, mean_eps: 0.100000\n",
      "  13525/175000: episode: 379, duration: 0.509s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 8.960 [4.000, 128.000], mean observation: 0.066 [0.000, 50.000], loss: 0.327863, mean_absolute_error: 0.337725, mean_q: 1.209783, mean_eps: 0.100000\n",
      "  13568/175000: episode: 380, duration: 0.856s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 26.907 [2.000, 151.000], mean observation: 0.189 [0.000, 86.000], loss: 0.222592, mean_absolute_error: 0.327294, mean_q: 1.139334, mean_eps: 0.100000\n",
      "  13600/175000: episode: 381, duration: 0.681s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.750 [2.000, 204.000], mean observation: 0.209 [0.000, 64.000], loss: 0.306626, mean_absolute_error: 0.329119, mean_q: 1.269778, mean_eps: 0.100000\n",
      "  13645/175000: episode: 382, duration: 0.888s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.689 [3.000, 204.000], mean observation: 0.324 [0.000, 90.000], loss: 0.278883, mean_absolute_error: 0.297589, mean_q: 1.214664, mean_eps: 0.100000\n",
      "  13672/175000: episode: 383, duration: 0.542s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.259 [25.000, 217.000], mean observation: 0.151 [0.000, 54.000], loss: 0.310873, mean_absolute_error: 0.319748, mean_q: 1.313991, mean_eps: 0.100000\n",
      "  13725/175000: episode: 384, duration: 1.260s, episode steps: 53, steps per second: 42, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 157.264 [48.000, 185.000], mean observation: 0.359 [0.000, 106.000], loss: 0.238647, mean_absolute_error: 0.298088, mean_q: 1.264202, mean_eps: 0.100000\n",
      "  13748/175000: episode: 385, duration: 0.453s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 91.391 [12.000, 154.000], mean observation: 0.072 [0.000, 46.000], loss: 0.324566, mean_absolute_error: 0.269052, mean_q: 1.016295, mean_eps: 0.100000\n",
      "  13796/175000: episode: 386, duration: 0.947s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 22.000 [7.000, 224.000], mean observation: 0.189 [0.000, 96.000], loss: 0.273829, mean_absolute_error: 0.281492, mean_q: 1.044806, mean_eps: 0.100000\n",
      "  13834/175000: episode: 387, duration: 0.971s, episode steps: 38, steps per second: 39, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 47.211 [7.000, 172.000], mean observation: 0.184 [0.000, 76.000], loss: 0.159358, mean_absolute_error: 0.262748, mean_q: 1.066921, mean_eps: 0.100000\n",
      "  13861/175000: episode: 388, duration: 0.585s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 119.185 [7.000, 158.000], mean observation: 0.093 [0.000, 54.000], loss: 0.236766, mean_absolute_error: 0.268134, mean_q: 1.026931, mean_eps: 0.100000\n",
      "  13898/175000: episode: 389, duration: 0.769s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 19.946 [10.000, 63.000], mean observation: 0.110 [0.000, 74.000], loss: 0.240818, mean_absolute_error: 0.272483, mean_q: 1.091190, mean_eps: 0.100000\n",
      "  13948/175000: episode: 390, duration: 1.125s, episode steps: 50, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 22.300 [7.000, 204.000], mean observation: 0.338 [0.000, 100.000], loss: 0.383812, mean_absolute_error: 0.257405, mean_q: 1.049064, mean_eps: 0.100000\n",
      "  13995/175000: episode: 391, duration: 0.961s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.574 [15.000, 206.000], mean observation: 0.201 [0.000, 94.000], loss: 0.205813, mean_absolute_error: 0.268604, mean_q: 1.090655, mean_eps: 0.100000\n",
      "  14023/175000: episode: 392, duration: 0.603s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 58.857 [7.000, 198.000], mean observation: 0.165 [0.000, 56.000], loss: 0.185287, mean_absolute_error: 0.265832, mean_q: 0.976675, mean_eps: 0.100000\n",
      "  14075/175000: episode: 393, duration: 1.104s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 66.038 [7.000, 208.000], mean observation: 0.293 [0.000, 104.000], loss: 0.245817, mean_absolute_error: 0.278248, mean_q: 1.012257, mean_eps: 0.100000\n",
      "  14127/175000: episode: 394, duration: 0.931s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 116.250 [7.000, 200.000], mean observation: 0.303 [0.000, 104.000], loss: 0.153571, mean_absolute_error: 0.294504, mean_q: 1.303818, mean_eps: 0.100000\n",
      "  14156/175000: episode: 395, duration: 0.762s, episode steps: 29, steps per second: 38, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 83.069 [60.000, 188.000], mean observation: 0.080 [0.000, 58.000], loss: 0.092362, mean_absolute_error: 0.320779, mean_q: 1.449899, mean_eps: 0.100000\n",
      "  14182/175000: episode: 396, duration: 0.657s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.385 [77.000, 174.000], mean observation: 0.067 [0.000, 52.000], loss: 0.194907, mean_absolute_error: 0.303281, mean_q: 1.296679, mean_eps: 0.100000\n",
      "  14231/175000: episode: 397, duration: 0.926s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 147.102 [8.000, 216.000], mean observation: 0.466 [0.000, 98.000], loss: 0.185844, mean_absolute_error: 0.307080, mean_q: 1.098395, mean_eps: 0.100000\n",
      "  14273/175000: episode: 398, duration: 0.840s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 22.357 [5.000, 223.000], mean observation: 0.173 [0.000, 84.000], loss: 0.187280, mean_absolute_error: 0.309457, mean_q: 1.097479, mean_eps: 0.100000\n",
      "  14313/175000: episode: 399, duration: 0.925s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 84.625 [5.000, 214.000], mean observation: 0.377 [0.000, 80.000], loss: 0.183415, mean_absolute_error: 0.300820, mean_q: 1.104501, mean_eps: 0.100000\n",
      "  14326/175000: episode: 400, duration: 0.243s, episode steps: 13, steps per second: 53, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 50.154 [5.000, 209.000], mean observation: 0.053 [0.000, 26.000], loss: 0.110521, mean_absolute_error: 0.260943, mean_q: 0.959559, mean_eps: 0.100000\n",
      "  14354/175000: episode: 401, duration: 0.564s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.464 [66.000, 196.000], mean observation: 0.108 [0.000, 56.000], loss: 0.156893, mean_absolute_error: 0.275403, mean_q: 1.031654, mean_eps: 0.100000\n",
      "  14390/175000: episode: 402, duration: 0.649s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.167 [8.000, 217.000], mean observation: 0.319 [0.000, 72.000], loss: 0.270396, mean_absolute_error: 0.285789, mean_q: 1.021196, mean_eps: 0.100000\n",
      "  14419/175000: episode: 403, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 213.172 [206.000, 214.000], mean observation: 0.105 [0.000, 58.000], loss: 0.371132, mean_absolute_error: 0.312631, mean_q: 1.030130, mean_eps: 0.100000\n",
      "  14451/175000: episode: 404, duration: 0.708s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 184.312 [5.000, 214.000], mean observation: 0.112 [0.000, 64.000], loss: 0.318439, mean_absolute_error: 0.325346, mean_q: 1.009655, mean_eps: 0.100000\n",
      "  14477/175000: episode: 405, duration: 0.649s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 157.077 [36.000, 218.000], mean observation: 0.123 [0.000, 52.000], loss: 0.284791, mean_absolute_error: 0.333708, mean_q: 1.038886, mean_eps: 0.100000\n",
      "  14517/175000: episode: 406, duration: 0.901s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.675 [7.000, 214.000], mean observation: 0.247 [0.000, 80.000], loss: 0.292890, mean_absolute_error: 0.328398, mean_q: 1.001587, mean_eps: 0.100000\n",
      "  14547/175000: episode: 407, duration: 0.660s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 19.100 [7.000, 177.000], mean observation: 0.134 [0.000, 60.000], loss: 0.166923, mean_absolute_error: 0.325821, mean_q: 1.050458, mean_eps: 0.100000\n",
      "  14574/175000: episode: 408, duration: 0.626s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 55.148 [19.000, 202.000], mean observation: 0.114 [0.000, 54.000], loss: 0.194963, mean_absolute_error: 0.320094, mean_q: 1.087207, mean_eps: 0.100000\n",
      "  14599/175000: episode: 409, duration: 0.471s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.480 [1.000, 216.000], mean observation: 0.156 [0.000, 50.000], loss: 0.136648, mean_absolute_error: 0.295593, mean_q: 0.984699, mean_eps: 0.100000\n",
      "  14657/175000: episode: 410, duration: 1.150s, episode steps: 58, steps per second: 50, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 65.828 [1.000, 206.000], mean observation: 0.468 [0.000, 116.000], loss: 0.261710, mean_absolute_error: 0.297274, mean_q: 0.948125, mean_eps: 0.100000\n",
      "  14683/175000: episode: 411, duration: 0.569s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 9.346 [1.000, 193.000], mean observation: 0.065 [0.000, 52.000], loss: 0.160765, mean_absolute_error: 0.277265, mean_q: 0.947666, mean_eps: 0.100000\n",
      "  14702/175000: episode: 412, duration: 0.387s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 41.316 [1.000, 137.000], mean observation: 0.056 [0.000, 38.000], loss: 0.156814, mean_absolute_error: 0.297949, mean_q: 1.177012, mean_eps: 0.100000\n",
      "  14730/175000: episode: 413, duration: 0.544s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.321 [84.000, 211.000], mean observation: 0.094 [0.000, 56.000], loss: 0.157794, mean_absolute_error: 0.268422, mean_q: 1.072977, mean_eps: 0.100000\n",
      "  14769/175000: episode: 414, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.538 [20.000, 189.000], mean observation: 0.342 [0.000, 78.000], loss: 0.081975, mean_absolute_error: 0.263237, mean_q: 1.028248, mean_eps: 0.100000\n",
      "  14803/175000: episode: 415, duration: 0.650s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 150.647 [45.000, 218.000], mean observation: 0.234 [0.000, 68.000], loss: 0.115433, mean_absolute_error: 0.264994, mean_q: 1.205273, mean_eps: 0.100000\n",
      "  14840/175000: episode: 416, duration: 0.767s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 145.541 [38.000, 207.000], mean observation: 0.265 [0.000, 74.000], loss: 0.101563, mean_absolute_error: 0.270772, mean_q: 1.243899, mean_eps: 0.100000\n",
      "  14886/175000: episode: 417, duration: 0.871s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.761 [29.000, 216.000], mean observation: 0.369 [0.000, 92.000], loss: 0.106499, mean_absolute_error: 0.260772, mean_q: 1.139226, mean_eps: 0.100000\n",
      "  14912/175000: episode: 418, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.308 [16.000, 215.000], mean observation: 0.192 [0.000, 52.000], loss: 0.197572, mean_absolute_error: 0.259056, mean_q: 1.117681, mean_eps: 0.100000\n",
      "  14939/175000: episode: 419, duration: 0.519s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 142.630 [80.000, 154.000], mean observation: 0.090 [0.000, 54.000], loss: 0.148015, mean_absolute_error: 0.265869, mean_q: 0.992045, mean_eps: 0.100000\n",
      "  14964/175000: episode: 420, duration: 0.499s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 144.600 [42.000, 192.000], mean observation: 0.067 [0.000, 50.000], loss: 0.146599, mean_absolute_error: 0.237200, mean_q: 0.902295, mean_eps: 0.100000\n",
      "  15009/175000: episode: 421, duration: 0.966s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 143.311 [36.000, 148.000], mean observation: 0.133 [0.000, 90.000], loss: 0.103980, mean_absolute_error: 0.246818, mean_q: 0.927032, mean_eps: 0.100000\n",
      "  15043/175000: episode: 422, duration: 0.723s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.382 [52.000, 209.000], mean observation: 0.239 [0.000, 68.000], loss: 0.168491, mean_absolute_error: 0.253386, mean_q: 0.988389, mean_eps: 0.100000\n",
      "  15077/175000: episode: 423, duration: 0.729s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.029 [5.000, 179.000], mean observation: 0.203 [0.000, 68.000], loss: 0.121435, mean_absolute_error: 0.262526, mean_q: 1.033601, mean_eps: 0.100000\n",
      "  15119/175000: episode: 424, duration: 0.818s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.357 [21.000, 199.000], mean observation: 0.223 [0.000, 84.000], loss: 0.107344, mean_absolute_error: 0.251628, mean_q: 0.960889, mean_eps: 0.100000\n",
      "  15146/175000: episode: 425, duration: 0.576s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 123.926 [5.000, 189.000], mean observation: 0.124 [0.000, 54.000], loss: 0.129339, mean_absolute_error: 0.261033, mean_q: 0.941922, mean_eps: 0.100000\n",
      "  15204/175000: episode: 426, duration: 1.241s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 43.931 [9.000, 147.000], mean observation: 0.242 [0.000, 116.000], loss: 0.145862, mean_absolute_error: 0.272668, mean_q: 0.934793, mean_eps: 0.100000\n",
      "  15244/175000: episode: 427, duration: 0.838s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 152.550 [3.000, 216.000], mean observation: 0.416 [0.000, 80.000], loss: 0.137921, mean_absolute_error: 0.280218, mean_q: 0.987167, mean_eps: 0.100000\n",
      "  15281/175000: episode: 428, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 180.027 [45.000, 217.000], mean observation: 0.159 [0.000, 74.000], loss: 0.143158, mean_absolute_error: 0.265857, mean_q: 0.995800, mean_eps: 0.100000\n",
      "  15302/175000: episode: 429, duration: 0.422s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 193.524 [101.000, 203.000], mean observation: 0.074 [0.000, 42.000], loss: 0.122651, mean_absolute_error: 0.269372, mean_q: 1.041820, mean_eps: 0.100000\n",
      "  15334/175000: episode: 430, duration: 0.637s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 170.344 [15.000, 203.000], mean observation: 0.142 [0.000, 64.000], loss: 0.133824, mean_absolute_error: 0.254725, mean_q: 0.934054, mean_eps: 0.100000\n",
      "  15395/175000: episode: 431, duration: 1.077s, episode steps: 61, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 128.164 [45.000, 222.000], mean observation: 0.289 [0.000, 122.000], loss: 0.146915, mean_absolute_error: 0.254177, mean_q: 0.918821, mean_eps: 0.100000\n",
      "  15421/175000: episode: 432, duration: 0.536s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.731 [19.000, 220.000], mean observation: 0.127 [0.000, 52.000], loss: 0.136417, mean_absolute_error: 0.239133, mean_q: 1.097303, mean_eps: 0.100000\n",
      "  15446/175000: episode: 433, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 45.680 [6.000, 105.000], mean observation: 0.089 [0.000, 50.000], loss: 0.114318, mean_absolute_error: 0.253972, mean_q: 1.234239, mean_eps: 0.100000\n",
      "  15490/175000: episode: 434, duration: 0.834s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 82.409 [26.000, 217.000], mean observation: 0.277 [0.000, 88.000], loss: 0.145590, mean_absolute_error: 0.252755, mean_q: 1.107142, mean_eps: 0.100000\n",
      "  15526/175000: episode: 435, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 150.306 [12.000, 211.000], mean observation: 0.202 [0.000, 72.000], loss: 0.176160, mean_absolute_error: 0.266847, mean_q: 0.988659, mean_eps: 0.100000\n",
      "  15551/175000: episode: 436, duration: 0.463s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 169.320 [26.000, 211.000], mean observation: 0.114 [0.000, 50.000], loss: 0.219221, mean_absolute_error: 0.278660, mean_q: 1.013739, mean_eps: 0.100000\n",
      "  15582/175000: episode: 437, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.871 [75.000, 211.000], mean observation: 0.113 [0.000, 62.000], loss: 0.173067, mean_absolute_error: 0.288941, mean_q: 1.062579, mean_eps: 0.100000\n",
      "  15617/175000: episode: 438, duration: 0.683s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 138.343 [120.000, 211.000], mean observation: 0.114 [0.000, 70.000], loss: 0.185371, mean_absolute_error: 0.293367, mean_q: 1.092059, mean_eps: 0.100000\n",
      "  15644/175000: episode: 439, duration: 0.526s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 145.815 [38.000, 224.000], mean observation: 0.186 [0.000, 54.000], loss: 0.179838, mean_absolute_error: 0.317650, mean_q: 1.175358, mean_eps: 0.100000\n",
      "  15665/175000: episode: 440, duration: 0.452s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 128.476 [45.000, 217.000], mean observation: 0.087 [0.000, 42.000], loss: 0.239213, mean_absolute_error: 0.308892, mean_q: 1.035832, mean_eps: 0.100000\n",
      "  15696/175000: episode: 441, duration: 0.602s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 193.097 [94.000, 222.000], mean observation: 0.262 [0.000, 62.000], loss: 0.228478, mean_absolute_error: 0.303278, mean_q: 0.953749, mean_eps: 0.100000\n",
      "  15723/175000: episode: 442, duration: 0.580s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 110.407 [59.000, 209.000], mean observation: 0.157 [0.000, 54.000], loss: 0.179789, mean_absolute_error: 0.326793, mean_q: 1.067803, mean_eps: 0.100000\n",
      "  15749/175000: episode: 443, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 133.769 [45.000, 209.000], mean observation: 0.128 [0.000, 52.000], loss: 0.139730, mean_absolute_error: 0.321728, mean_q: 1.085518, mean_eps: 0.100000\n",
      "  15762/175000: episode: 444, duration: 0.235s, episode steps: 13, steps per second: 55, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 119.615 [5.000, 163.000], mean observation: 0.043 [0.000, 26.000], loss: 0.279703, mean_absolute_error: 0.318340, mean_q: 1.070270, mean_eps: 0.100000\n",
      "  15801/175000: episode: 445, duration: 0.783s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.103 [24.000, 209.000], mean observation: 0.284 [0.000, 78.000], loss: 0.147897, mean_absolute_error: 0.321831, mean_q: 1.078934, mean_eps: 0.100000\n",
      "  15854/175000: episode: 446, duration: 0.971s, episode steps: 53, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.453 [24.000, 209.000], mean observation: 0.404 [0.000, 106.000], loss: 0.155641, mean_absolute_error: 0.308782, mean_q: 1.074328, mean_eps: 0.100000\n",
      "  15899/175000: episode: 447, duration: 0.818s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.467 [80.000, 199.000], mean observation: 0.203 [0.000, 90.000], loss: 0.092879, mean_absolute_error: 0.271050, mean_q: 0.959959, mean_eps: 0.100000\n",
      "  15937/175000: episode: 448, duration: 0.762s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 132.763 [17.000, 199.000], mean observation: 0.256 [0.000, 76.000], loss: 0.217047, mean_absolute_error: 0.264108, mean_q: 0.919368, mean_eps: 0.100000\n",
      "  15977/175000: episode: 449, duration: 0.747s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.100 [30.000, 217.000], mean observation: 0.296 [0.000, 80.000], loss: 0.189972, mean_absolute_error: 0.284894, mean_q: 1.083203, mean_eps: 0.100000\n",
      "  16007/175000: episode: 450, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.467 [47.000, 217.000], mean observation: 0.135 [0.000, 60.000], loss: 0.202112, mean_absolute_error: 0.317823, mean_q: 1.444260, mean_eps: 0.100000\n",
      "  16048/175000: episode: 451, duration: 0.785s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 146.049 [21.000, 221.000], mean observation: 0.188 [0.000, 82.000], loss: 0.213001, mean_absolute_error: 0.326748, mean_q: 1.425570, mean_eps: 0.100000\n",
      "  16077/175000: episode: 452, duration: 0.613s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.517 [0.000, 215.000], mean observation: 0.180 [0.000, 58.000], loss: 0.157793, mean_absolute_error: 0.298427, mean_q: 1.254617, mean_eps: 0.100000\n",
      "  16119/175000: episode: 453, duration: 0.773s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.452 [39.000, 215.000], mean observation: 0.308 [0.000, 84.000], loss: 0.161327, mean_absolute_error: 0.287445, mean_q: 1.235862, mean_eps: 0.100000\n",
      "  16148/175000: episode: 454, duration: 0.588s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 186.414 [70.000, 221.000], mean observation: 0.133 [0.000, 58.000], loss: 0.183996, mean_absolute_error: 0.271922, mean_q: 1.215902, mean_eps: 0.100000\n",
      "  16191/175000: episode: 455, duration: 0.830s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 173.953 [111.000, 213.000], mean observation: 0.251 [0.000, 86.000], loss: 0.251460, mean_absolute_error: 0.266520, mean_q: 1.130898, mean_eps: 0.100000\n",
      "  16228/175000: episode: 456, duration: 0.728s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 103.676 [70.000, 224.000], mean observation: 0.099 [0.000, 74.000], loss: 0.212779, mean_absolute_error: 0.279337, mean_q: 1.183398, mean_eps: 0.100000\n",
      "  16264/175000: episode: 457, duration: 0.731s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 147.083 [42.000, 222.000], mean observation: 0.260 [0.000, 72.000], loss: 0.163251, mean_absolute_error: 0.282644, mean_q: 1.184162, mean_eps: 0.100000\n",
      "  16303/175000: episode: 458, duration: 0.719s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.590 [5.000, 222.000], mean observation: 0.294 [0.000, 78.000], loss: 0.288466, mean_absolute_error: 0.282795, mean_q: 1.208655, mean_eps: 0.100000\n",
      "  16357/175000: episode: 459, duration: 1.040s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 180.907 [41.000, 224.000], mean observation: 0.297 [0.000, 108.000], loss: 0.175037, mean_absolute_error: 0.317283, mean_q: 1.363765, mean_eps: 0.100000\n",
      "  16408/175000: episode: 460, duration: 0.982s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 183.098 [49.000, 224.000], mean observation: 0.255 [0.000, 102.000], loss: 0.265301, mean_absolute_error: 0.356049, mean_q: 1.245762, mean_eps: 0.100000\n",
      "  16440/175000: episode: 461, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 160.938 [55.000, 168.000], mean observation: 0.092 [0.000, 64.000], loss: 0.226944, mean_absolute_error: 0.331587, mean_q: 1.230312, mean_eps: 0.100000\n",
      "  16478/175000: episode: 462, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 168.500 [11.000, 224.000], mean observation: 0.155 [0.000, 76.000], loss: 0.182570, mean_absolute_error: 0.335493, mean_q: 1.435473, mean_eps: 0.100000\n",
      "  16509/175000: episode: 463, duration: 0.594s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 203.032 [168.000, 217.000], mean observation: 0.108 [0.000, 62.000], loss: 0.228300, mean_absolute_error: 0.327248, mean_q: 1.429965, mean_eps: 0.100000\n",
      "  16553/175000: episode: 464, duration: 0.828s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 185.091 [5.000, 217.000], mean observation: 0.165 [0.000, 88.000], loss: 0.220222, mean_absolute_error: 0.375791, mean_q: 1.410841, mean_eps: 0.100000\n",
      "  16578/175000: episode: 465, duration: 0.503s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 164.600 [90.000, 168.000], mean observation: 0.092 [0.000, 50.000], loss: 0.154371, mean_absolute_error: 0.385546, mean_q: 1.243673, mean_eps: 0.100000\n",
      "  16618/175000: episode: 466, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.500 [24.000, 214.000], mean observation: 0.370 [0.000, 80.000], loss: 0.205960, mean_absolute_error: 0.375118, mean_q: 1.190037, mean_eps: 0.100000\n",
      "  16668/175000: episode: 467, duration: 1.225s, episode steps: 50, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 175.860 [64.000, 224.000], mean observation: 0.294 [0.000, 100.000], loss: 0.174757, mean_absolute_error: 0.346732, mean_q: 1.209846, mean_eps: 0.100000\n",
      "  16681/175000: episode: 468, duration: 0.295s, episode steps: 13, steps per second: 44, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 190.000 [190.000, 190.000], mean observation: 0.033 [0.000, 26.000], loss: 0.144016, mean_absolute_error: 0.287701, mean_q: 1.205585, mean_eps: 0.100000\n",
      "  16716/175000: episode: 469, duration: 0.747s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 148.343 [25.000, 224.000], mean observation: 0.175 [0.000, 70.000], loss: 0.110684, mean_absolute_error: 0.313433, mean_q: 1.230349, mean_eps: 0.100000\n",
      "  16739/175000: episode: 470, duration: 0.498s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 138.826 [43.000, 166.000], mean observation: 0.107 [0.000, 46.000], loss: 0.094502, mean_absolute_error: 0.285637, mean_q: 1.105711, mean_eps: 0.100000\n",
      "  16783/175000: episode: 471, duration: 1.048s, episode steps: 44, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.886 [18.000, 188.000], mean observation: 0.300 [0.000, 88.000], loss: 0.087830, mean_absolute_error: 0.299964, mean_q: 1.188636, mean_eps: 0.100000\n",
      "  16815/175000: episode: 472, duration: 0.672s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 141.219 [36.000, 182.000], mean observation: 0.176 [0.000, 64.000], loss: 0.094186, mean_absolute_error: 0.283509, mean_q: 1.122860, mean_eps: 0.100000\n",
      "  16855/175000: episode: 473, duration: 0.784s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 165.725 [4.000, 187.000], mean observation: 0.211 [0.000, 80.000], loss: 0.160666, mean_absolute_error: 0.272017, mean_q: 1.081895, mean_eps: 0.100000\n",
      "  16901/175000: episode: 474, duration: 0.963s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 184.848 [50.000, 224.000], mean observation: 0.335 [0.000, 92.000], loss: 0.136206, mean_absolute_error: 0.283626, mean_q: 1.137024, mean_eps: 0.100000\n",
      "  16933/175000: episode: 475, duration: 0.624s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 169.438 [25.000, 220.000], mean observation: 0.153 [0.000, 64.000], loss: 0.096654, mean_absolute_error: 0.303613, mean_q: 1.231644, mean_eps: 0.100000\n",
      "  16981/175000: episode: 476, duration: 0.941s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 166.542 [34.000, 224.000], mean observation: 0.217 [0.000, 96.000], loss: 0.170363, mean_absolute_error: 0.347351, mean_q: 1.352304, mean_eps: 0.100000\n",
      "  17011/175000: episode: 477, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 161.067 [46.000, 224.000], mean observation: 0.156 [0.000, 60.000], loss: 0.077809, mean_absolute_error: 0.321863, mean_q: 1.237014, mean_eps: 0.100000\n",
      "  17048/175000: episode: 478, duration: 0.763s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.054 [25.000, 222.000], mean observation: 0.177 [0.000, 74.000], loss: 0.065389, mean_absolute_error: 0.317479, mean_q: 1.177558, mean_eps: 0.100000\n",
      "  17086/175000: episode: 479, duration: 0.785s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.263 [46.000, 198.000], mean observation: 0.246 [0.000, 76.000], loss: 0.110832, mean_absolute_error: 0.293675, mean_q: 1.152345, mean_eps: 0.100000\n",
      "  17120/175000: episode: 480, duration: 0.674s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.412 [44.000, 175.000], mean observation: 0.134 [0.000, 68.000], loss: 0.144149, mean_absolute_error: 0.291900, mean_q: 1.211507, mean_eps: 0.100000\n",
      "  17162/175000: episode: 481, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.381 [12.000, 216.000], mean observation: 0.360 [0.000, 84.000], loss: 0.128171, mean_absolute_error: 0.275816, mean_q: 1.129716, mean_eps: 0.100000\n",
      "  17186/175000: episode: 482, duration: 0.500s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 168.667 [168.000, 184.000], mean observation: 0.067 [0.000, 48.000], loss: 0.130374, mean_absolute_error: 0.278670, mean_q: 1.118325, mean_eps: 0.100000\n",
      "  17211/175000: episode: 483, duration: 0.536s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 175.320 [29.000, 203.000], mean observation: 0.117 [0.000, 50.000], loss: 0.174102, mean_absolute_error: 0.279828, mean_q: 1.154206, mean_eps: 0.100000\n",
      "  17245/175000: episode: 484, duration: 0.713s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 33.912 [1.000, 70.000], mean observation: 0.102 [0.000, 68.000], loss: 0.129625, mean_absolute_error: 0.263510, mean_q: 1.042938, mean_eps: 0.100000\n",
      "  17267/175000: episode: 485, duration: 0.411s, episode steps: 22, steps per second: 53, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 41.500 [1.000, 216.000], mean observation: 0.116 [0.000, 44.000], loss: 0.070884, mean_absolute_error: 0.285569, mean_q: 1.130210, mean_eps: 0.100000\n",
      "  17283/175000: episode: 486, duration: 0.384s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 7.250 [1.000, 101.000], mean observation: 0.060 [0.000, 32.000], loss: 0.110667, mean_absolute_error: 0.248219, mean_q: 0.959733, mean_eps: 0.100000\n",
      "  17308/175000: episode: 487, duration: 0.631s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 8.040 [1.000, 177.000], mean observation: 0.061 [0.000, 50.000], loss: 0.121055, mean_absolute_error: 0.261489, mean_q: 1.020399, mean_eps: 0.100000\n",
      "  17331/175000: episode: 488, duration: 0.451s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 82.348 [1.000, 168.000], mean observation: 0.119 [0.000, 46.000], loss: 0.125435, mean_absolute_error: 0.255939, mean_q: 1.081656, mean_eps: 0.100000\n",
      "  17352/175000: episode: 489, duration: 0.491s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 123.333 [1.000, 217.000], mean observation: 0.087 [0.000, 42.000], loss: 0.176207, mean_absolute_error: 0.269243, mean_q: 1.154294, mean_eps: 0.100000\n",
      "  17376/175000: episode: 490, duration: 0.668s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 163.958 [12.000, 217.000], mean observation: 0.137 [0.000, 48.000], loss: 0.128240, mean_absolute_error: 0.256473, mean_q: 1.106416, mean_eps: 0.100000\n",
      "  17416/175000: episode: 491, duration: 0.941s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.300 [5.000, 168.000], mean observation: 0.285 [0.000, 80.000], loss: 0.096204, mean_absolute_error: 0.246297, mean_q: 1.058063, mean_eps: 0.100000\n",
      "  17434/175000: episode: 492, duration: 0.473s, episode steps: 18, steps per second: 38, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 41.222 [8.000, 62.000], mean observation: 0.094 [0.000, 36.000], loss: 0.100162, mean_absolute_error: 0.238804, mean_q: 0.979280, mean_eps: 0.100000\n",
      "  17477/175000: episode: 493, duration: 0.912s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 42.907 [29.000, 158.000], mean observation: 0.157 [0.000, 86.000], loss: 0.172630, mean_absolute_error: 0.249188, mean_q: 1.027559, mean_eps: 0.100000\n",
      "  17527/175000: episode: 494, duration: 0.891s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.580 [38.000, 185.000], mean observation: 0.289 [0.000, 100.000], loss: 0.158822, mean_absolute_error: 0.273237, mean_q: 0.949943, mean_eps: 0.100000\n",
      "  17581/175000: episode: 495, duration: 1.125s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.926 [4.000, 175.000], mean observation: 0.272 [0.000, 108.000], loss: 0.212355, mean_absolute_error: 0.317800, mean_q: 1.013949, mean_eps: 0.100000\n",
      "  17642/175000: episode: 496, duration: 1.287s, episode steps: 61, steps per second: 47, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 38.361 [4.000, 161.000], mean observation: 0.348 [0.000, 122.000], loss: 0.126611, mean_absolute_error: 0.329265, mean_q: 1.034115, mean_eps: 0.100000\n",
      "  17673/175000: episode: 497, duration: 0.645s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 77.355 [4.000, 211.000], mean observation: 0.162 [0.000, 62.000], loss: 0.150378, mean_absolute_error: 0.321583, mean_q: 1.013974, mean_eps: 0.100000\n",
      "  17687/175000: episode: 498, duration: 0.261s, episode steps: 14, steps per second: 54, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 153.571 [93.000, 205.000], mean observation: 0.069 [0.000, 28.000], loss: 0.180209, mean_absolute_error: 0.279643, mean_q: 0.890032, mean_eps: 0.100000\n",
      "  17723/175000: episode: 499, duration: 0.721s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.583 [12.000, 203.000], mean observation: 0.221 [0.000, 72.000], loss: 0.096467, mean_absolute_error: 0.252881, mean_q: 0.879503, mean_eps: 0.100000\n",
      "  17759/175000: episode: 500, duration: 0.747s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 73.944 [13.000, 209.000], mean observation: 0.184 [0.000, 72.000], loss: 0.131287, mean_absolute_error: 0.258645, mean_q: 0.897354, mean_eps: 0.100000\n",
      "  17778/175000: episode: 501, duration: 0.456s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 66.789 [39.000, 223.000], mean observation: 0.080 [0.000, 38.000], loss: 0.141667, mean_absolute_error: 0.258274, mean_q: 0.912745, mean_eps: 0.100000\n",
      "  17825/175000: episode: 502, duration: 1.121s, episode steps: 47, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 51.851 [14.000, 185.000], mean observation: 0.289 [0.000, 94.000], loss: 0.100607, mean_absolute_error: 0.249853, mean_q: 0.902227, mean_eps: 0.100000\n",
      "  17870/175000: episode: 503, duration: 0.937s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 45.156 [4.000, 223.000], mean observation: 0.336 [0.000, 90.000], loss: 0.116010, mean_absolute_error: 0.247130, mean_q: 0.860526, mean_eps: 0.100000\n",
      "  17893/175000: episode: 504, duration: 0.496s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 146.261 [64.000, 150.000], mean observation: 0.066 [0.000, 46.000], loss: 0.106070, mean_absolute_error: 0.286888, mean_q: 1.020153, mean_eps: 0.100000\n",
      "  17913/175000: episode: 505, duration: 0.391s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 162.550 [28.000, 175.000], mean observation: 0.097 [0.000, 40.000], loss: 0.077582, mean_absolute_error: 0.239533, mean_q: 0.943504, mean_eps: 0.100000\n",
      "  17956/175000: episode: 506, duration: 0.884s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.930 [20.000, 175.000], mean observation: 0.222 [0.000, 86.000], loss: 0.083552, mean_absolute_error: 0.241852, mean_q: 1.040996, mean_eps: 0.100000\n",
      "  17987/175000: episode: 507, duration: 0.661s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 164.097 [5.000, 175.000], mean observation: 0.127 [0.000, 62.000], loss: 0.109159, mean_absolute_error: 0.245982, mean_q: 0.937267, mean_eps: 0.100000\n",
      "  18036/175000: episode: 508, duration: 1.031s, episode steps: 49, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.245 [33.000, 209.000], mean observation: 0.599 [0.000, 98.000], loss: 0.157380, mean_absolute_error: 0.259956, mean_q: 0.942627, mean_eps: 0.100000\n",
      "  18077/175000: episode: 509, duration: 0.836s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 151.293 [52.000, 223.000], mean observation: 0.224 [0.000, 82.000], loss: 0.106468, mean_absolute_error: 0.263586, mean_q: 0.998101, mean_eps: 0.100000\n",
      "  18119/175000: episode: 510, duration: 0.790s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.786 [52.000, 220.000], mean observation: 0.347 [0.000, 84.000], loss: 0.222130, mean_absolute_error: 0.274206, mean_q: 1.075836, mean_eps: 0.100000\n",
      "  18173/175000: episode: 511, duration: 1.122s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 140.648 [12.000, 223.000], mean observation: 0.442 [0.000, 108.000], loss: 0.171351, mean_absolute_error: 0.264035, mean_q: 1.049580, mean_eps: 0.100000\n",
      "  18218/175000: episode: 512, duration: 0.833s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.778 [37.000, 223.000], mean observation: 0.348 [0.000, 90.000], loss: 0.151612, mean_absolute_error: 0.272034, mean_q: 1.077665, mean_eps: 0.100000\n",
      "  18258/175000: episode: 513, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.375 [39.000, 203.000], mean observation: 0.193 [0.000, 80.000], loss: 0.242234, mean_absolute_error: 0.257966, mean_q: 0.999928, mean_eps: 0.100000\n",
      "  18289/175000: episode: 514, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 142.935 [118.000, 175.000], mean observation: 0.108 [0.000, 62.000], loss: 0.206159, mean_absolute_error: 0.255828, mean_q: 1.027550, mean_eps: 0.100000\n",
      "  18330/175000: episode: 515, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.049 [1.000, 208.000], mean observation: 0.185 [0.000, 82.000], loss: 0.241723, mean_absolute_error: 0.282424, mean_q: 1.253072, mean_eps: 0.100000\n",
      "  18375/175000: episode: 516, duration: 0.915s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.156 [2.000, 208.000], mean observation: 0.113 [0.000, 90.000], loss: 0.378948, mean_absolute_error: 0.280167, mean_q: 1.236795, mean_eps: 0.100000\n",
      "  18410/175000: episode: 517, duration: 0.692s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.600 [49.000, 208.000], mean observation: 0.199 [0.000, 70.000], loss: 0.342982, mean_absolute_error: 0.279382, mean_q: 1.096607, mean_eps: 0.100000\n",
      "  18451/175000: episode: 518, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.854 [24.000, 208.000], mean observation: 0.144 [0.000, 82.000], loss: 0.196798, mean_absolute_error: 0.279744, mean_q: 1.112750, mean_eps: 0.100000\n",
      "  18497/175000: episode: 519, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.543 [26.000, 213.000], mean observation: 0.367 [0.000, 92.000], loss: 0.262599, mean_absolute_error: 0.281171, mean_q: 1.115357, mean_eps: 0.100000\n",
      "  18528/175000: episode: 520, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 86.452 [42.000, 223.000], mean observation: 0.147 [0.000, 62.000], loss: 0.417579, mean_absolute_error: 0.279212, mean_q: 1.123532, mean_eps: 0.100000\n",
      "  18563/175000: episode: 521, duration: 0.791s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 184.657 [32.000, 223.000], mean observation: 0.188 [0.000, 70.000], loss: 0.274387, mean_absolute_error: 0.269234, mean_q: 1.065093, mean_eps: 0.100000\n",
      "  18590/175000: episode: 522, duration: 0.635s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 200.111 [9.000, 223.000], mean observation: 0.101 [0.000, 54.000], loss: 0.202154, mean_absolute_error: 0.270071, mean_q: 1.068365, mean_eps: 0.100000\n",
      "  18645/175000: episode: 523, duration: 1.295s, episode steps: 55, steps per second: 42, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 134.982 [19.000, 223.000], mean observation: 0.457 [0.000, 110.000], loss: 0.179638, mean_absolute_error: 0.273675, mean_q: 1.032870, mean_eps: 0.100000\n",
      "  18692/175000: episode: 524, duration: 1.042s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.191 [50.000, 209.000], mean observation: 0.258 [0.000, 94.000], loss: 0.242041, mean_absolute_error: 0.286365, mean_q: 1.049611, mean_eps: 0.100000\n",
      "  18737/175000: episode: 525, duration: 0.916s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 98.267 [18.000, 154.000], mean observation: 0.323 [0.000, 90.000], loss: 0.263881, mean_absolute_error: 0.306317, mean_q: 1.096301, mean_eps: 0.100000\n",
      "  18763/175000: episode: 526, duration: 0.490s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 90.038 [75.000, 148.000], mean observation: 0.080 [0.000, 52.000], loss: 0.450117, mean_absolute_error: 0.284020, mean_q: 1.088834, mean_eps: 0.100000\n",
      "  18817/175000: episode: 527, duration: 1.265s, episode steps: 54, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.444 [75.000, 183.000], mean observation: 0.353 [0.000, 108.000], loss: 0.271466, mean_absolute_error: 0.292785, mean_q: 1.159824, mean_eps: 0.100000\n",
      "  18865/175000: episode: 528, duration: 0.970s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.479 [66.000, 216.000], mean observation: 0.331 [0.000, 96.000], loss: 0.226457, mean_absolute_error: 0.278766, mean_q: 1.029312, mean_eps: 0.100000\n",
      "  18885/175000: episode: 529, duration: 0.434s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 128.050 [2.000, 150.000], mean observation: 0.076 [0.000, 40.000], loss: 0.065588, mean_absolute_error: 0.280392, mean_q: 1.081957, mean_eps: 0.100000\n",
      "  18917/175000: episode: 530, duration: 0.621s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 127.812 [17.000, 174.000], mean observation: 0.197 [0.000, 64.000], loss: 0.170034, mean_absolute_error: 0.266251, mean_q: 1.057301, mean_eps: 0.100000\n",
      "  18957/175000: episode: 531, duration: 0.829s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 134.925 [19.000, 220.000], mean observation: 0.369 [0.000, 80.000], loss: 0.182062, mean_absolute_error: 0.277409, mean_q: 1.126102, mean_eps: 0.100000\n",
      "  18971/175000: episode: 532, duration: 0.238s, episode steps: 14, steps per second: 59, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 117.214 [42.000, 148.000], mean observation: 0.064 [0.000, 28.000], loss: 0.188490, mean_absolute_error: 0.314646, mean_q: 1.234862, mean_eps: 0.100000\n",
      "  19010/175000: episode: 533, duration: 0.826s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.487 [7.000, 150.000], mean observation: 0.147 [0.000, 78.000], loss: 0.125724, mean_absolute_error: 0.301362, mean_q: 1.138351, mean_eps: 0.100000\n",
      "  19046/175000: episode: 534, duration: 0.725s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.833 [32.000, 219.000], mean observation: 0.193 [0.000, 72.000], loss: 0.112421, mean_absolute_error: 0.314064, mean_q: 1.144223, mean_eps: 0.100000\n",
      "  19091/175000: episode: 535, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.089 [52.000, 220.000], mean observation: 0.312 [0.000, 90.000], loss: 0.197041, mean_absolute_error: 0.304219, mean_q: 1.095794, mean_eps: 0.100000\n",
      "  19114/175000: episode: 536, duration: 0.468s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 183.174 [182.000, 209.000], mean observation: 0.069 [0.000, 46.000], loss: 0.094665, mean_absolute_error: 0.288858, mean_q: 1.032939, mean_eps: 0.100000\n",
      "  19166/175000: episode: 537, duration: 1.017s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 143.519 [30.000, 190.000], mean observation: 0.447 [0.000, 104.000], loss: 0.172790, mean_absolute_error: 0.312838, mean_q: 1.156146, mean_eps: 0.100000\n",
      "  19201/175000: episode: 538, duration: 0.741s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.514 [66.000, 219.000], mean observation: 0.365 [0.000, 70.000], loss: 0.113396, mean_absolute_error: 0.321009, mean_q: 1.175625, mean_eps: 0.100000\n",
      "  19243/175000: episode: 539, duration: 0.833s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.738 [9.000, 214.000], mean observation: 0.427 [0.000, 84.000], loss: 0.142302, mean_absolute_error: 0.316601, mean_q: 1.084190, mean_eps: 0.100000\n",
      "  19285/175000: episode: 540, duration: 0.883s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 159.357 [79.000, 179.000], mean observation: 0.288 [0.000, 84.000], loss: 0.134527, mean_absolute_error: 0.343442, mean_q: 1.113266, mean_eps: 0.100000\n",
      "  19301/175000: episode: 541, duration: 0.303s, episode steps: 16, steps per second: 53, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 160.562 [58.000, 180.000], mean observation: 0.047 [0.000, 32.000], loss: 0.180859, mean_absolute_error: 0.351425, mean_q: 1.084539, mean_eps: 0.100000\n",
      "  19329/175000: episode: 542, duration: 0.549s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 161.536 [29.000, 180.000], mean observation: 0.130 [0.000, 56.000], loss: 0.177803, mean_absolute_error: 0.370709, mean_q: 1.137235, mean_eps: 0.100000\n",
      "  19351/175000: episode: 543, duration: 0.371s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 168.000 [68.000, 190.000], mean observation: 0.090 [0.000, 44.000], loss: 0.211563, mean_absolute_error: 0.351812, mean_q: 1.075164, mean_eps: 0.100000\n",
      "  19376/175000: episode: 544, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.160 [28.000, 219.000], mean observation: 0.085 [0.000, 50.000], loss: 0.114552, mean_absolute_error: 0.320592, mean_q: 0.996271, mean_eps: 0.100000\n",
      "  19414/175000: episode: 545, duration: 0.757s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 153.237 [30.000, 186.000], mean observation: 0.251 [0.000, 76.000], loss: 0.155652, mean_absolute_error: 0.320320, mean_q: 1.041898, mean_eps: 0.100000\n",
      "  19454/175000: episode: 546, duration: 0.830s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.675 [39.000, 195.000], mean observation: 0.175 [0.000, 80.000], loss: 0.142738, mean_absolute_error: 0.306934, mean_q: 1.089576, mean_eps: 0.100000\n",
      "  19484/175000: episode: 547, duration: 0.710s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 139.800 [1.000, 152.000], mean observation: 0.105 [0.000, 60.000], loss: 0.121293, mean_absolute_error: 0.296893, mean_q: 1.074113, mean_eps: 0.100000\n",
      "  19527/175000: episode: 548, duration: 0.882s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 153.884 [32.000, 180.000], mean observation: 0.333 [0.000, 86.000], loss: 0.185359, mean_absolute_error: 0.309172, mean_q: 1.046322, mean_eps: 0.100000\n",
      "  19565/175000: episode: 549, duration: 0.782s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 157.289 [49.000, 180.000], mean observation: 0.199 [0.000, 76.000], loss: 0.124425, mean_absolute_error: 0.325015, mean_q: 1.135434, mean_eps: 0.100000\n",
      "  19603/175000: episode: 550, duration: 0.793s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 39.868 [21.000, 186.000], mean observation: 0.142 [0.000, 76.000], loss: 0.199546, mean_absolute_error: 0.341448, mean_q: 1.218604, mean_eps: 0.100000\n",
      "  19638/175000: episode: 551, duration: 0.737s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.486 [27.000, 187.000], mean observation: 0.182 [0.000, 70.000], loss: 0.147979, mean_absolute_error: 0.350858, mean_q: 1.156299, mean_eps: 0.100000\n",
      "  19677/175000: episode: 552, duration: 0.800s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.744 [10.000, 181.000], mean observation: 0.283 [0.000, 78.000], loss: 0.126835, mean_absolute_error: 0.346023, mean_q: 1.198237, mean_eps: 0.100000\n",
      "  19717/175000: episode: 553, duration: 0.761s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 14.625 [0.000, 15.000], mean observation: 0.094 [0.000, 80.000], loss: 0.169899, mean_absolute_error: 0.331073, mean_q: 1.323743, mean_eps: 0.100000\n",
      "  19753/175000: episode: 554, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.611 [15.000, 192.000], mean observation: 0.175 [0.000, 72.000], loss: 0.158458, mean_absolute_error: 0.305042, mean_q: 1.236903, mean_eps: 0.100000\n",
      "  19791/175000: episode: 555, duration: 0.722s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.605 [33.000, 220.000], mean observation: 0.180 [0.000, 76.000], loss: 0.119252, mean_absolute_error: 0.298868, mean_q: 1.147386, mean_eps: 0.100000\n",
      "  19827/175000: episode: 556, duration: 0.694s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 129.778 [10.000, 190.000], mean observation: 0.255 [0.000, 72.000], loss: 0.169090, mean_absolute_error: 0.318937, mean_q: 1.074313, mean_eps: 0.100000\n",
      "  19857/175000: episode: 557, duration: 0.597s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 132.533 [3.000, 174.000], mean observation: 0.254 [0.000, 60.000], loss: 0.158841, mean_absolute_error: 0.339767, mean_q: 1.072212, mean_eps: 0.100000\n",
      "  19869/175000: episode: 558, duration: 0.207s, episode steps: 12, steps per second: 58, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 150.500 [52.000, 174.000], mean observation: 0.032 [0.000, 24.000], loss: 0.101202, mean_absolute_error: 0.370487, mean_q: 1.158828, mean_eps: 0.100000\n",
      "  19912/175000: episode: 559, duration: 0.867s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.535 [13.000, 186.000], mean observation: 0.320 [0.000, 86.000], loss: 0.145264, mean_absolute_error: 0.363356, mean_q: 1.142929, mean_eps: 0.100000\n",
      "  19965/175000: episode: 560, duration: 1.064s, episode steps: 53, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 138.302 [13.000, 200.000], mean observation: 0.411 [0.000, 106.000], loss: 0.137063, mean_absolute_error: 0.363346, mean_q: 1.085772, mean_eps: 0.100000\n",
      "  19997/175000: episode: 561, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 156.844 [28.000, 205.000], mean observation: 0.235 [0.000, 64.000], loss: 0.141280, mean_absolute_error: 0.358132, mean_q: 1.099644, mean_eps: 0.100000\n",
      "  20022/175000: episode: 562, duration: 0.525s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.400 [59.000, 190.000], mean observation: 0.114 [0.000, 50.000], loss: 0.484973, mean_absolute_error: 0.389654, mean_q: 1.230363, mean_eps: 0.100000\n",
      "  20066/175000: episode: 563, duration: 0.830s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.773 [24.000, 217.000], mean observation: 0.350 [0.000, 88.000], loss: 0.796608, mean_absolute_error: 0.377445, mean_q: 1.205326, mean_eps: 0.100000\n",
      "  20094/175000: episode: 564, duration: 0.575s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.821 [59.000, 217.000], mean observation: 0.084 [0.000, 56.000], loss: 0.403252, mean_absolute_error: 0.409924, mean_q: 1.231298, mean_eps: 0.100000\n",
      "  20150/175000: episode: 565, duration: 1.053s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 110.893 [30.000, 202.000], mean observation: 0.483 [0.000, 112.000], loss: 1.256676, mean_absolute_error: 0.419707, mean_q: 1.283814, mean_eps: 0.100000\n",
      "  20196/175000: episode: 566, duration: 0.897s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.217 [20.000, 211.000], mean observation: 0.547 [0.000, 92.000], loss: 0.547799, mean_absolute_error: 0.442016, mean_q: 1.466383, mean_eps: 0.100000\n",
      "  20243/175000: episode: 567, duration: 0.897s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 105.553 [68.000, 204.000], mean observation: 0.125 [0.000, 94.000], loss: 0.682771, mean_absolute_error: 0.466522, mean_q: 1.320236, mean_eps: 0.100000\n",
      "  20261/175000: episode: 568, duration: 0.361s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 111.167 [6.000, 216.000], mean observation: 0.125 [0.000, 36.000], loss: 0.230293, mean_absolute_error: 0.482938, mean_q: 1.349676, mean_eps: 0.100000\n",
      "  20287/175000: episode: 569, duration: 0.481s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.577 [102.000, 204.000], mean observation: 0.137 [0.000, 52.000], loss: 0.535498, mean_absolute_error: 0.470923, mean_q: 1.341312, mean_eps: 0.100000\n",
      "  20327/175000: episode: 570, duration: 0.792s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 114.750 [33.000, 188.000], mean observation: 0.433 [0.000, 80.000], loss: 0.221186, mean_absolute_error: 0.451663, mean_q: 1.158251, mean_eps: 0.100000\n",
      "  20360/175000: episode: 571, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.030 [5.000, 152.000], mean observation: 0.148 [0.000, 66.000], loss: 0.171391, mean_absolute_error: 0.445640, mean_q: 1.051225, mean_eps: 0.100000\n",
      "  20389/175000: episode: 572, duration: 0.619s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.276 [42.000, 186.000], mean observation: 0.204 [0.000, 58.000], loss: 0.295047, mean_absolute_error: 0.448362, mean_q: 1.089435, mean_eps: 0.100000\n",
      "  20435/175000: episode: 573, duration: 0.861s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 204.261 [0.000, 224.000], mean observation: 0.231 [0.000, 92.000], loss: 0.413560, mean_absolute_error: 0.408762, mean_q: 1.032491, mean_eps: 0.100000\n",
      "  20470/175000: episode: 574, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 177.514 [29.000, 224.000], mean observation: 0.148 [0.000, 70.000], loss: 0.520423, mean_absolute_error: 0.385483, mean_q: 1.086628, mean_eps: 0.100000\n",
      "  20522/175000: episode: 575, duration: 0.955s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 139.904 [49.000, 224.000], mean observation: 0.360 [0.000, 104.000], loss: 0.569729, mean_absolute_error: 0.364905, mean_q: 1.039691, mean_eps: 0.100000\n",
      "  20556/175000: episode: 576, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 170.088 [15.000, 190.000], mean observation: 0.178 [0.000, 68.000], loss: 0.325906, mean_absolute_error: 0.397597, mean_q: 1.064407, mean_eps: 0.100000\n",
      "  20583/175000: episode: 577, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 97.852 [5.000, 105.000], mean observation: 0.120 [0.000, 54.000], loss: 0.275511, mean_absolute_error: 0.425369, mean_q: 1.110518, mean_eps: 0.100000\n",
      "  20635/175000: episode: 578, duration: 1.026s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 182.904 [51.000, 213.000], mean observation: 0.414 [0.000, 104.000], loss: 0.345527, mean_absolute_error: 0.455340, mean_q: 1.282807, mean_eps: 0.100000\n",
      "  20676/175000: episode: 579, duration: 0.918s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.463 [10.000, 190.000], mean observation: 0.510 [0.000, 82.000], loss: 0.485769, mean_absolute_error: 0.439251, mean_q: 1.162117, mean_eps: 0.100000\n",
      "  20716/175000: episode: 580, duration: 0.888s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 49.400 [14.000, 201.000], mean observation: 0.217 [0.000, 80.000], loss: 0.426229, mean_absolute_error: 0.415428, mean_q: 1.171813, mean_eps: 0.100000\n",
      "  20754/175000: episode: 581, duration: 0.753s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 14.289 [9.000, 30.000], mean observation: 0.114 [0.000, 76.000], loss: 0.377616, mean_absolute_error: 0.385302, mean_q: 1.224453, mean_eps: 0.100000\n",
      "  20787/175000: episode: 582, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.242 [14.000, 174.000], mean observation: 0.106 [0.000, 66.000], loss: 0.507505, mean_absolute_error: 0.377726, mean_q: 1.226620, mean_eps: 0.100000\n",
      "  20829/175000: episode: 583, duration: 0.846s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.095 [45.000, 201.000], mean observation: 0.322 [0.000, 84.000], loss: 0.606239, mean_absolute_error: 0.390032, mean_q: 1.321740, mean_eps: 0.100000\n",
      "  20860/175000: episode: 584, duration: 0.621s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.194 [14.000, 180.000], mean observation: 0.209 [0.000, 62.000], loss: 0.323497, mean_absolute_error: 0.403663, mean_q: 1.460509, mean_eps: 0.100000\n",
      "  20902/175000: episode: 585, duration: 0.877s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.810 [14.000, 223.000], mean observation: 0.235 [0.000, 84.000], loss: 0.459446, mean_absolute_error: 0.378726, mean_q: 1.450110, mean_eps: 0.100000\n",
      "  20946/175000: episode: 586, duration: 0.855s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.432 [23.000, 207.000], mean observation: 0.367 [0.000, 88.000], loss: 0.438205, mean_absolute_error: 0.354180, mean_q: 1.407470, mean_eps: 0.100000\n",
      "  20983/175000: episode: 587, duration: 0.710s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 64.568 [25.000, 208.000], mean observation: 0.281 [0.000, 74.000], loss: 0.240642, mean_absolute_error: 0.313651, mean_q: 1.240058, mean_eps: 0.100000\n",
      "  21009/175000: episode: 588, duration: 0.538s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 81.692 [25.000, 128.000], mean observation: 0.130 [0.000, 52.000], loss: 0.888059, mean_absolute_error: 0.339164, mean_q: 1.306192, mean_eps: 0.100000\n",
      "  21048/175000: episode: 589, duration: 0.810s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.513 [35.000, 210.000], mean observation: 0.360 [0.000, 78.000], loss: 0.230465, mean_absolute_error: 0.340090, mean_q: 1.179454, mean_eps: 0.100000\n",
      "  21080/175000: episode: 590, duration: 0.651s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 87.125 [60.000, 177.000], mean observation: 0.105 [0.000, 64.000], loss: 0.975718, mean_absolute_error: 0.387479, mean_q: 1.257296, mean_eps: 0.100000\n",
      "  21118/175000: episode: 591, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.447 [48.000, 212.000], mean observation: 0.332 [0.000, 76.000], loss: 0.443194, mean_absolute_error: 0.415692, mean_q: 1.314206, mean_eps: 0.100000\n",
      "  21142/175000: episode: 592, duration: 0.491s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 97.167 [48.000, 218.000], mean observation: 0.119 [0.000, 48.000], loss: 0.189138, mean_absolute_error: 0.400654, mean_q: 1.310429, mean_eps: 0.100000\n",
      "  21171/175000: episode: 593, duration: 0.540s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.862 [60.000, 153.000], mean observation: 0.103 [0.000, 58.000], loss: 0.262912, mean_absolute_error: 0.402956, mean_q: 1.388345, mean_eps: 0.100000\n",
      "  21228/175000: episode: 594, duration: 1.063s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 76.421 [17.000, 153.000], mean observation: 0.303 [0.000, 114.000], loss: 0.207333, mean_absolute_error: 0.399977, mean_q: 1.336604, mean_eps: 0.100000\n",
      "  21258/175000: episode: 595, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 102.933 [59.000, 211.000], mean observation: 0.209 [0.000, 60.000], loss: 0.459236, mean_absolute_error: 0.400387, mean_q: 1.172754, mean_eps: 0.100000\n",
      "  21299/175000: episode: 596, duration: 0.779s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.341 [52.000, 216.000], mean observation: 0.266 [0.000, 82.000], loss: 0.360706, mean_absolute_error: 0.395003, mean_q: 0.969602, mean_eps: 0.100000\n",
      "  21325/175000: episode: 597, duration: 0.548s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 135.731 [24.000, 208.000], mean observation: 0.117 [0.000, 52.000], loss: 0.447566, mean_absolute_error: 0.391335, mean_q: 1.039810, mean_eps: 0.100000\n",
      "  21364/175000: episode: 598, duration: 0.779s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.487 [25.000, 208.000], mean observation: 0.221 [0.000, 78.000], loss: 0.875223, mean_absolute_error: 0.383894, mean_q: 1.147284, mean_eps: 0.100000\n",
      "  21403/175000: episode: 599, duration: 0.802s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.923 [4.000, 207.000], mean observation: 0.286 [0.000, 78.000], loss: 0.242931, mean_absolute_error: 0.368375, mean_q: 1.220497, mean_eps: 0.100000\n",
      "  21441/175000: episode: 600, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 160.158 [6.000, 224.000], mean observation: 0.191 [0.000, 76.000], loss: 0.257857, mean_absolute_error: 0.384534, mean_q: 1.241030, mean_eps: 0.100000\n",
      "  21474/175000: episode: 601, duration: 0.601s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.667 [48.000, 208.000], mean observation: 0.162 [0.000, 66.000], loss: 0.409988, mean_absolute_error: 0.410040, mean_q: 1.350171, mean_eps: 0.100000\n",
      "  21515/175000: episode: 602, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.098 [48.000, 208.000], mean observation: 0.219 [0.000, 82.000], loss: 0.452975, mean_absolute_error: 0.396367, mean_q: 1.244869, mean_eps: 0.100000\n",
      "  21544/175000: episode: 603, duration: 0.641s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 162.345 [33.000, 208.000], mean observation: 0.134 [0.000, 58.000], loss: 0.417653, mean_absolute_error: 0.417836, mean_q: 1.192603, mean_eps: 0.100000\n",
      "  21600/175000: episode: 604, duration: 1.070s, episode steps: 56, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.911 [2.000, 219.000], mean observation: 0.687 [0.000, 112.000], loss: 0.535392, mean_absolute_error: 0.426820, mean_q: 1.303113, mean_eps: 0.100000\n",
      "  21633/175000: episode: 605, duration: 0.686s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.364 [15.000, 143.000], mean observation: 0.240 [0.000, 66.000], loss: 0.450952, mean_absolute_error: 0.408675, mean_q: 1.344492, mean_eps: 0.100000\n",
      "  21672/175000: episode: 606, duration: 0.751s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.564 [8.000, 193.000], mean observation: 0.212 [0.000, 78.000], loss: 0.607925, mean_absolute_error: 0.422574, mean_q: 1.378331, mean_eps: 0.100000\n",
      "  21709/175000: episode: 607, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 92.162 [39.000, 192.000], mean observation: 0.249 [0.000, 74.000], loss: 0.447521, mean_absolute_error: 0.441036, mean_q: 1.433671, mean_eps: 0.100000\n",
      "  21768/175000: episode: 608, duration: 1.126s, episode steps: 59, steps per second: 52, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 104.729 [33.000, 174.000], mean observation: 0.442 [0.000, 118.000], loss: 0.571916, mean_absolute_error: 0.470216, mean_q: 1.474226, mean_eps: 0.100000\n",
      "  21799/175000: episode: 609, duration: 0.611s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 36.161 [28.000, 197.000], mean observation: 0.090 [0.000, 62.000], loss: 0.433814, mean_absolute_error: 0.476171, mean_q: 1.599402, mean_eps: 0.100000\n",
      "  21822/175000: episode: 610, duration: 0.451s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 43.348 [23.000, 165.000], mean observation: 0.114 [0.000, 46.000], loss: 0.147005, mean_absolute_error: 0.452227, mean_q: 1.669852, mean_eps: 0.100000\n",
      "  21846/175000: episode: 611, duration: 0.495s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 90.083 [28.000, 130.000], mean observation: 0.104 [0.000, 48.000], loss: 0.429337, mean_absolute_error: 0.429041, mean_q: 1.511641, mean_eps: 0.100000\n",
      "  21894/175000: episode: 612, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 80.771 [40.000, 173.000], mean observation: 0.359 [0.000, 96.000], loss: 0.330023, mean_absolute_error: 0.446167, mean_q: 1.580136, mean_eps: 0.100000\n",
      "  21931/175000: episode: 613, duration: 0.713s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 10.270 [0.000, 136.000], mean observation: 0.194 [0.000, 74.000], loss: 0.918728, mean_absolute_error: 0.445647, mean_q: 1.482793, mean_eps: 0.100000\n",
      "  21957/175000: episode: 614, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 35.808 [0.000, 107.000], mean observation: 0.088 [0.000, 52.000], loss: 0.565618, mean_absolute_error: 0.432978, mean_q: 1.529728, mean_eps: 0.100000\n",
      "  21994/175000: episode: 615, duration: 0.689s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.973 [62.000, 224.000], mean observation: 0.179 [0.000, 74.000], loss: 0.380017, mean_absolute_error: 0.416820, mean_q: 1.791916, mean_eps: 0.100000\n",
      "  22023/175000: episode: 616, duration: 0.556s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 145.276 [59.000, 224.000], mean observation: 0.244 [0.000, 58.000], loss: 0.220446, mean_absolute_error: 0.406393, mean_q: 1.912289, mean_eps: 0.100000\n",
      "  22069/175000: episode: 617, duration: 0.884s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.783 [19.000, 204.000], mean observation: 0.265 [0.000, 92.000], loss: 0.480076, mean_absolute_error: 0.396231, mean_q: 1.905486, mean_eps: 0.100000\n",
      "  22109/175000: episode: 618, duration: 0.783s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.575 [27.000, 215.000], mean observation: 0.269 [0.000, 80.000], loss: 0.550771, mean_absolute_error: 0.437605, mean_q: 1.920129, mean_eps: 0.100000\n",
      "  22125/175000: episode: 619, duration: 0.304s, episode steps: 16, steps per second: 53, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 207.500 [204.000, 212.000], mean observation: 0.042 [0.000, 32.000], loss: 0.623437, mean_absolute_error: 0.421966, mean_q: 1.663626, mean_eps: 0.100000\n",
      "  22152/175000: episode: 620, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 166.222 [73.000, 208.000], mean observation: 0.144 [0.000, 54.000], loss: 0.269192, mean_absolute_error: 0.459503, mean_q: 1.878976, mean_eps: 0.100000\n",
      "  22184/175000: episode: 621, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.094 [7.000, 205.000], mean observation: 0.142 [0.000, 64.000], loss: 0.371734, mean_absolute_error: 0.467203, mean_q: 1.694888, mean_eps: 0.100000\n",
      "  22218/175000: episode: 622, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 166.059 [105.000, 211.000], mean observation: 0.124 [0.000, 68.000], loss: 0.473874, mean_absolute_error: 0.477343, mean_q: 1.763392, mean_eps: 0.100000\n",
      "  22256/175000: episode: 623, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.974 [20.000, 196.000], mean observation: 0.208 [0.000, 76.000], loss: 0.574921, mean_absolute_error: 0.434471, mean_q: 1.723557, mean_eps: 0.100000\n",
      "  22284/175000: episode: 624, duration: 0.583s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.679 [36.000, 196.000], mean observation: 0.156 [0.000, 56.000], loss: 0.862660, mean_absolute_error: 0.468583, mean_q: 1.809558, mean_eps: 0.100000\n",
      "  22331/175000: episode: 625, duration: 0.926s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.213 [37.000, 206.000], mean observation: 0.257 [0.000, 94.000], loss: 0.395562, mean_absolute_error: 0.449925, mean_q: 1.819170, mean_eps: 0.100000\n",
      "  22386/175000: episode: 626, duration: 1.088s, episode steps: 55, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 73.982 [1.000, 199.000], mean observation: 0.556 [0.000, 110.000], loss: 0.442510, mean_absolute_error: 0.475562, mean_q: 1.946681, mean_eps: 0.100000\n",
      "  22423/175000: episode: 627, duration: 0.701s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 46.324 [0.000, 209.000], mean observation: 0.278 [0.000, 74.000], loss: 0.371247, mean_absolute_error: 0.452525, mean_q: 1.792516, mean_eps: 0.100000\n",
      "  22467/175000: episode: 628, duration: 0.881s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 132.114 [0.000, 219.000], mean observation: 0.424 [0.000, 88.000], loss: 0.525119, mean_absolute_error: 0.446273, mean_q: 1.685403, mean_eps: 0.100000\n",
      "  22529/175000: episode: 629, duration: 1.251s, episode steps: 62, steps per second: 50, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 127.726 [20.000, 183.000], mean observation: 0.501 [0.000, 124.000], loss: 0.385614, mean_absolute_error: 0.424395, mean_q: 1.669120, mean_eps: 0.100000\n",
      "  22564/175000: episode: 630, duration: 0.716s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.086 [9.000, 182.000], mean observation: 0.285 [0.000, 70.000], loss: 0.615434, mean_absolute_error: 0.437311, mean_q: 1.665091, mean_eps: 0.100000\n",
      "  22619/175000: episode: 631, duration: 1.098s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 173.964 [16.000, 210.000], mean observation: 0.485 [0.000, 110.000], loss: 0.422697, mean_absolute_error: 0.437926, mean_q: 1.589907, mean_eps: 0.100000\n",
      "  22657/175000: episode: 632, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 169.184 [16.000, 192.000], mean observation: 0.276 [0.000, 76.000], loss: 0.741246, mean_absolute_error: 0.395120, mean_q: 1.487657, mean_eps: 0.100000\n",
      "  22677/175000: episode: 633, duration: 0.372s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 146.800 [16.000, 192.000], mean observation: 0.114 [0.000, 40.000], loss: 0.469556, mean_absolute_error: 0.402320, mean_q: 1.487534, mean_eps: 0.100000\n",
      "  22716/175000: episode: 634, duration: 0.795s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 135.821 [8.000, 192.000], mean observation: 0.171 [0.000, 78.000], loss: 0.362959, mean_absolute_error: 0.419681, mean_q: 1.507318, mean_eps: 0.100000\n",
      "  22756/175000: episode: 635, duration: 0.777s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 117.050 [40.000, 209.000], mean observation: 0.396 [0.000, 80.000], loss: 0.224972, mean_absolute_error: 0.361955, mean_q: 1.299322, mean_eps: 0.100000\n",
      "  22806/175000: episode: 636, duration: 1.111s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.320 [16.000, 205.000], mean observation: 0.416 [0.000, 100.000], loss: 0.266739, mean_absolute_error: 0.366804, mean_q: 1.390389, mean_eps: 0.100000\n",
      "  22853/175000: episode: 637, duration: 0.895s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 151.383 [37.000, 190.000], mean observation: 0.552 [0.000, 94.000], loss: 0.249625, mean_absolute_error: 0.384017, mean_q: 1.398373, mean_eps: 0.100000\n",
      "  22888/175000: episode: 638, duration: 0.718s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 144.171 [9.000, 212.000], mean observation: 0.243 [0.000, 70.000], loss: 0.170874, mean_absolute_error: 0.407837, mean_q: 1.486632, mean_eps: 0.100000\n",
      "  22914/175000: episode: 639, duration: 0.542s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.846 [52.000, 170.000], mean observation: 0.187 [0.000, 52.000], loss: 0.277861, mean_absolute_error: 0.432907, mean_q: 1.571377, mean_eps: 0.100000\n",
      "  22953/175000: episode: 640, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 122.179 [31.000, 223.000], mean observation: 0.446 [0.000, 78.000], loss: 0.320528, mean_absolute_error: 0.396552, mean_q: 1.469466, mean_eps: 0.100000\n",
      "  22982/175000: episode: 641, duration: 0.515s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.690 [2.000, 190.000], mean observation: 0.285 [0.000, 58.000], loss: 0.236988, mean_absolute_error: 0.373993, mean_q: 1.435320, mean_eps: 0.100000\n",
      "  23019/175000: episode: 642, duration: 0.711s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 169.432 [35.000, 216.000], mean observation: 0.373 [0.000, 74.000], loss: 0.311329, mean_absolute_error: 0.393071, mean_q: 1.435217, mean_eps: 0.100000\n",
      "  23057/175000: episode: 643, duration: 0.768s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.263 [21.000, 190.000], mean observation: 0.206 [0.000, 76.000], loss: 0.289863, mean_absolute_error: 0.417026, mean_q: 1.436624, mean_eps: 0.100000\n",
      "  23091/175000: episode: 644, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.676 [33.000, 210.000], mean observation: 0.236 [0.000, 68.000], loss: 0.281457, mean_absolute_error: 0.418881, mean_q: 1.508680, mean_eps: 0.100000\n",
      "  23115/175000: episode: 645, duration: 0.461s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 141.125 [20.000, 214.000], mean observation: 0.133 [0.000, 48.000], loss: 0.365080, mean_absolute_error: 0.399666, mean_q: 1.458460, mean_eps: 0.100000\n",
      "  23159/175000: episode: 646, duration: 0.871s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 65.773 [10.000, 176.000], mean observation: 0.210 [0.000, 88.000], loss: 0.335930, mean_absolute_error: 0.376795, mean_q: 1.387793, mean_eps: 0.100000\n",
      "  23177/175000: episode: 647, duration: 0.409s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 29.000 [13.000, 70.000], mean observation: 0.084 [0.000, 36.000], loss: 0.717964, mean_absolute_error: 0.397880, mean_q: 1.498716, mean_eps: 0.100000\n",
      "  23196/175000: episode: 648, duration: 0.375s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 13.000 [13.000, 13.000], mean observation: 0.046 [0.000, 38.000], loss: 0.921456, mean_absolute_error: 0.360526, mean_q: 1.454061, mean_eps: 0.100000\n",
      "  23226/175000: episode: 649, duration: 0.659s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 26.567 [13.000, 90.000], mean observation: 0.121 [0.000, 60.000], loss: 0.448051, mean_absolute_error: 0.361924, mean_q: 1.466739, mean_eps: 0.100000\n",
      "  23258/175000: episode: 650, duration: 0.594s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 76.781 [20.000, 218.000], mean observation: 0.156 [0.000, 64.000], loss: 0.347328, mean_absolute_error: 0.361064, mean_q: 1.438558, mean_eps: 0.100000\n",
      "  23306/175000: episode: 651, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 80.854 [14.000, 185.000], mean observation: 0.265 [0.000, 96.000], loss: 0.358237, mean_absolute_error: 0.365075, mean_q: 1.410686, mean_eps: 0.100000\n",
      "  23344/175000: episode: 652, duration: 0.814s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.526 [5.000, 169.000], mean observation: 0.206 [0.000, 76.000], loss: 0.348252, mean_absolute_error: 0.342851, mean_q: 1.336875, mean_eps: 0.100000\n",
      "  23396/175000: episode: 653, duration: 1.044s, episode steps: 52, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.038 [3.000, 185.000], mean observation: 0.490 [0.000, 104.000], loss: 0.265476, mean_absolute_error: 0.346458, mean_q: 1.412484, mean_eps: 0.100000\n",
      "  23416/175000: episode: 654, duration: 0.449s, episode steps: 20, steps per second: 44, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 62.050 [9.000, 212.000], mean observation: 0.114 [0.000, 40.000], loss: 0.149247, mean_absolute_error: 0.383355, mean_q: 1.516785, mean_eps: 0.100000\n",
      "  23450/175000: episode: 655, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.588 [2.000, 169.000], mean observation: 0.202 [0.000, 68.000], loss: 0.308875, mean_absolute_error: 0.348430, mean_q: 1.402023, mean_eps: 0.100000\n",
      "  23493/175000: episode: 656, duration: 0.799s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.116 [0.000, 197.000], mean observation: 0.178 [0.000, 86.000], loss: 0.309837, mean_absolute_error: 0.312389, mean_q: 1.198591, mean_eps: 0.100000\n",
      "  23512/175000: episode: 657, duration: 0.372s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 106.579 [19.000, 169.000], mean observation: 0.102 [0.000, 38.000], loss: 0.191203, mean_absolute_error: 0.331081, mean_q: 1.240020, mean_eps: 0.100000\n",
      "  23539/175000: episode: 658, duration: 0.546s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 143.037 [25.000, 210.000], mean observation: 0.199 [0.000, 54.000], loss: 0.235708, mean_absolute_error: 0.315273, mean_q: 1.250275, mean_eps: 0.100000\n",
      "  23575/175000: episode: 659, duration: 0.932s, episode steps: 36, steps per second: 39, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 78.417 [12.000, 169.000], mean observation: 0.269 [0.000, 72.000], loss: 0.383871, mean_absolute_error: 0.337827, mean_q: 1.364031, mean_eps: 0.100000\n",
      "  23609/175000: episode: 660, duration: 0.795s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.176 [12.000, 195.000], mean observation: 0.218 [0.000, 68.000], loss: 0.646250, mean_absolute_error: 0.329813, mean_q: 1.313760, mean_eps: 0.100000\n",
      "  23631/175000: episode: 661, duration: 0.448s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 21.182 [11.000, 154.000], mean observation: 0.068 [0.000, 44.000], loss: 0.594997, mean_absolute_error: 0.404482, mean_q: 1.537105, mean_eps: 0.100000\n",
      "  23679/175000: episode: 662, duration: 1.255s, episode steps: 48, steps per second: 38, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 75.583 [0.000, 203.000], mean observation: 0.474 [0.000, 96.000], loss: 0.471179, mean_absolute_error: 0.408422, mean_q: 1.435437, mean_eps: 0.100000\n",
      "  23712/175000: episode: 663, duration: 0.822s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 59.242 [11.000, 216.000], mean observation: 0.172 [0.000, 66.000], loss: 0.293691, mean_absolute_error: 0.380680, mean_q: 1.407300, mean_eps: 0.100000\n",
      "  23761/175000: episode: 664, duration: 1.197s, episode steps: 49, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.449 [11.000, 222.000], mean observation: 0.452 [0.000, 98.000], loss: 0.351583, mean_absolute_error: 0.352898, mean_q: 1.490450, mean_eps: 0.100000\n",
      "  23786/175000: episode: 665, duration: 0.542s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 178.240 [11.000, 206.000], mean observation: 0.138 [0.000, 50.000], loss: 0.580906, mean_absolute_error: 0.368834, mean_q: 1.681643, mean_eps: 0.100000\n",
      "  23812/175000: episode: 666, duration: 0.638s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 191.038 [12.000, 224.000], mean observation: 0.098 [0.000, 52.000], loss: 0.391349, mean_absolute_error: 0.375804, mean_q: 1.629955, mean_eps: 0.100000\n",
      "  23852/175000: episode: 667, duration: 0.960s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 167.750 [11.000, 206.000], mean observation: 0.258 [0.000, 80.000], loss: 0.380224, mean_absolute_error: 0.440202, mean_q: 1.801734, mean_eps: 0.100000\n",
      "  23897/175000: episode: 668, duration: 1.073s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.711 [4.000, 216.000], mean observation: 0.282 [0.000, 90.000], loss: 0.257958, mean_absolute_error: 0.468523, mean_q: 1.949751, mean_eps: 0.100000\n",
      "  23930/175000: episode: 669, duration: 0.712s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.485 [32.000, 223.000], mean observation: 0.177 [0.000, 66.000], loss: 0.710060, mean_absolute_error: 0.490726, mean_q: 2.016694, mean_eps: 0.100000\n",
      "  23968/175000: episode: 670, duration: 0.861s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.184 [85.000, 223.000], mean observation: 0.185 [0.000, 76.000], loss: 0.501321, mean_absolute_error: 0.487750, mean_q: 2.030464, mean_eps: 0.100000\n",
      "  23992/175000: episode: 671, duration: 0.665s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 85.083 [11.000, 223.000], mean observation: 0.168 [0.000, 48.000], loss: 0.597603, mean_absolute_error: 0.496760, mean_q: 1.964298, mean_eps: 0.100000\n",
      "  24012/175000: episode: 672, duration: 0.518s, episode steps: 20, steps per second: 39, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 25.950 [9.000, 114.000], mean observation: 0.059 [0.000, 40.000], loss: 0.238296, mean_absolute_error: 0.476814, mean_q: 1.792848, mean_eps: 0.100000\n",
      "  24061/175000: episode: 673, duration: 1.314s, episode steps: 49, steps per second: 37, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.980 [11.000, 212.000], mean observation: 0.591 [0.000, 98.000], loss: 0.418226, mean_absolute_error: 0.427675, mean_q: 1.607404, mean_eps: 0.100000\n",
      "  24086/175000: episode: 674, duration: 0.625s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 141.920 [12.000, 223.000], mean observation: 0.119 [0.000, 50.000], loss: 0.421663, mean_absolute_error: 0.408437, mean_q: 1.602616, mean_eps: 0.100000\n",
      "  24098/175000: episode: 675, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 160.500 [44.000, 223.000], mean observation: 0.056 [0.000, 24.000], loss: 0.541131, mean_absolute_error: 0.418387, mean_q: 1.591682, mean_eps: 0.100000\n",
      "  24136/175000: episode: 676, duration: 0.892s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.947 [11.000, 223.000], mean observation: 0.263 [0.000, 76.000], loss: 0.406184, mean_absolute_error: 0.465309, mean_q: 1.669629, mean_eps: 0.100000\n",
      "  24172/175000: episode: 677, duration: 0.849s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.139 [11.000, 224.000], mean observation: 0.222 [0.000, 72.000], loss: 0.252245, mean_absolute_error: 0.451554, mean_q: 1.437119, mean_eps: 0.100000\n",
      "  24197/175000: episode: 678, duration: 0.560s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 57.520 [19.000, 173.000], mean observation: 0.126 [0.000, 50.000], loss: 0.481245, mean_absolute_error: 0.440614, mean_q: 1.328013, mean_eps: 0.100000\n",
      "  24217/175000: episode: 679, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.650 [19.000, 151.000], mean observation: 0.087 [0.000, 40.000], loss: 0.899316, mean_absolute_error: 0.465290, mean_q: 1.397302, mean_eps: 0.100000\n",
      "  24266/175000: episode: 680, duration: 1.000s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 81.571 [11.000, 219.000], mean observation: 0.330 [0.000, 98.000], loss: 0.174724, mean_absolute_error: 0.453390, mean_q: 1.440912, mean_eps: 0.100000\n",
      "  24279/175000: episode: 681, duration: 0.295s, episode steps: 13, steps per second: 44, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 55.231 [20.000, 104.000], mean observation: 0.053 [0.000, 26.000], loss: 0.191059, mean_absolute_error: 0.482481, mean_q: 1.530111, mean_eps: 0.100000\n",
      "  24313/175000: episode: 682, duration: 0.731s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 57.882 [11.000, 215.000], mean observation: 0.173 [0.000, 68.000], loss: 0.211238, mean_absolute_error: 0.484921, mean_q: 1.480250, mean_eps: 0.100000\n",
      "  24333/175000: episode: 683, duration: 0.465s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 155.900 [47.000, 168.000], mean observation: 0.049 [0.000, 40.000], loss: 0.245451, mean_absolute_error: 0.488000, mean_q: 1.393254, mean_eps: 0.100000\n",
      "  24375/175000: episode: 684, duration: 0.877s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.952 [9.000, 223.000], mean observation: 0.241 [0.000, 84.000], loss: 0.223693, mean_absolute_error: 0.472540, mean_q: 1.419042, mean_eps: 0.100000\n",
      "  24412/175000: episode: 685, duration: 0.875s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 66.595 [19.000, 223.000], mean observation: 0.253 [0.000, 74.000], loss: 0.422503, mean_absolute_error: 0.467006, mean_q: 1.456913, mean_eps: 0.100000\n",
      "  24434/175000: episode: 686, duration: 0.596s, episode steps: 22, steps per second: 37, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 83.455 [19.000, 206.000], mean observation: 0.148 [0.000, 44.000], loss: 0.196796, mean_absolute_error: 0.470335, mean_q: 1.561531, mean_eps: 0.100000\n",
      "  24484/175000: episode: 687, duration: 1.160s, episode steps: 50, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.380 [19.000, 172.000], mean observation: 0.364 [0.000, 100.000], loss: 0.282228, mean_absolute_error: 0.429952, mean_q: 1.538232, mean_eps: 0.100000\n",
      "  24505/175000: episode: 688, duration: 0.460s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 66.190 [14.000, 220.000], mean observation: 0.149 [0.000, 42.000], loss: 0.218127, mean_absolute_error: 0.432370, mean_q: 1.497042, mean_eps: 0.100000\n",
      "  24518/175000: episode: 689, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.033 [0.000, 26.000], loss: 0.280896, mean_absolute_error: 0.404321, mean_q: 1.415525, mean_eps: 0.100000\n",
      "  24558/175000: episode: 690, duration: 0.837s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 35.925 [4.000, 148.000], mean observation: 0.284 [0.000, 80.000], loss: 0.220993, mean_absolute_error: 0.399693, mean_q: 1.438860, mean_eps: 0.100000\n",
      "  24586/175000: episode: 691, duration: 0.656s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.536 [14.000, 204.000], mean observation: 0.195 [0.000, 56.000], loss: 0.384621, mean_absolute_error: 0.405447, mean_q: 1.389456, mean_eps: 0.100000\n",
      "  24619/175000: episode: 692, duration: 0.695s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 92.758 [14.000, 191.000], mean observation: 0.188 [0.000, 66.000], loss: 0.186364, mean_absolute_error: 0.402187, mean_q: 1.367594, mean_eps: 0.100000\n",
      "  24654/175000: episode: 693, duration: 0.764s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.743 [14.000, 193.000], mean observation: 0.271 [0.000, 70.000], loss: 0.230594, mean_absolute_error: 0.384593, mean_q: 1.337357, mean_eps: 0.100000\n",
      "  24681/175000: episode: 694, duration: 0.594s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 140.667 [14.000, 199.000], mean observation: 0.205 [0.000, 54.000], loss: 0.478016, mean_absolute_error: 0.381451, mean_q: 1.380106, mean_eps: 0.100000\n",
      "  24725/175000: episode: 695, duration: 0.947s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 154.886 [22.000, 200.000], mean observation: 0.357 [0.000, 88.000], loss: 0.350057, mean_absolute_error: 0.360874, mean_q: 1.263461, mean_eps: 0.100000\n",
      "  24753/175000: episode: 696, duration: 0.627s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 155.321 [89.000, 212.000], mean observation: 0.100 [0.000, 56.000], loss: 0.174780, mean_absolute_error: 0.379292, mean_q: 1.332038, mean_eps: 0.100000\n",
      "  24778/175000: episode: 697, duration: 0.529s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 30.640 [23.000, 214.000], mean observation: 0.059 [0.000, 50.000], loss: 0.187197, mean_absolute_error: 0.397134, mean_q: 1.446177, mean_eps: 0.100000\n",
      "  24810/175000: episode: 698, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 82.125 [7.000, 211.000], mean observation: 0.150 [0.000, 64.000], loss: 0.305081, mean_absolute_error: 0.395136, mean_q: 1.381528, mean_eps: 0.100000\n",
      "  24846/175000: episode: 699, duration: 0.814s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 167.750 [18.000, 209.000], mean observation: 0.119 [0.000, 72.000], loss: 0.252311, mean_absolute_error: 0.408383, mean_q: 1.566531, mean_eps: 0.100000\n",
      "  24869/175000: episode: 700, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 185.087 [31.000, 200.000], mean observation: 0.056 [0.000, 46.000], loss: 0.597047, mean_absolute_error: 0.372359, mean_q: 1.697574, mean_eps: 0.100000\n",
      "  24889/175000: episode: 701, duration: 0.412s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 190.900 [105.000, 200.000], mean observation: 0.056 [0.000, 40.000], loss: 0.256740, mean_absolute_error: 0.342436, mean_q: 1.685908, mean_eps: 0.100000\n",
      "  24945/175000: episode: 702, duration: 1.214s, episode steps: 56, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 127.125 [19.000, 208.000], mean observation: 0.318 [0.000, 112.000], loss: 0.578487, mean_absolute_error: 0.334905, mean_q: 1.449527, mean_eps: 0.100000\n",
      "  25000/175000: episode: 703, duration: 1.220s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 138.291 [14.000, 194.000], mean observation: 0.364 [0.000, 110.000], loss: 0.251286, mean_absolute_error: 0.343068, mean_q: 1.296125, mean_eps: 0.100000\n",
      "  25017/175000: episode: 704, duration: 0.418s, episode steps: 17, steps per second: 41, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 72.000 [72.000, 72.000], mean observation: 0.041 [0.000, 34.000], loss: 0.203012, mean_absolute_error: 0.329148, mean_q: 1.210918, mean_eps: 0.100000\n",
      "  25063/175000: episode: 705, duration: 0.989s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.087 [39.000, 180.000], mean observation: 0.202 [0.000, 92.000], loss: 0.402508, mean_absolute_error: 0.367660, mean_q: 1.363147, mean_eps: 0.100000\n",
      "  25083/175000: episode: 706, duration: 0.464s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.700 [19.000, 203.000], mean observation: 0.064 [0.000, 40.000], loss: 0.234333, mean_absolute_error: 0.390657, mean_q: 1.398278, mean_eps: 0.100000\n",
      "  25107/175000: episode: 707, duration: 0.507s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 39.125 [19.000, 201.000], mean observation: 0.077 [0.000, 48.000], loss: 0.235138, mean_absolute_error: 0.412951, mean_q: 1.395722, mean_eps: 0.100000\n",
      "  25134/175000: episode: 708, duration: 0.611s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 53.889 [19.000, 207.000], mean observation: 0.098 [0.000, 54.000], loss: 0.186788, mean_absolute_error: 0.435621, mean_q: 1.384673, mean_eps: 0.100000\n",
      "  25170/175000: episode: 709, duration: 0.708s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.389 [17.000, 205.000], mean observation: 0.231 [0.000, 72.000], loss: 0.526575, mean_absolute_error: 0.423536, mean_q: 1.408473, mean_eps: 0.100000\n",
      "  25200/175000: episode: 710, duration: 0.612s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 101.067 [16.000, 169.000], mean observation: 0.137 [0.000, 60.000], loss: 0.427427, mean_absolute_error: 0.399760, mean_q: 1.480171, mean_eps: 0.100000\n",
      "  25230/175000: episode: 711, duration: 0.670s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 87.500 [18.000, 169.000], mean observation: 0.125 [0.000, 60.000], loss: 0.292181, mean_absolute_error: 0.394709, mean_q: 1.524953, mean_eps: 0.100000\n",
      "  25263/175000: episode: 712, duration: 0.654s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.077 [0.000, 66.000], loss: 0.171524, mean_absolute_error: 0.392568, mean_q: 1.496368, mean_eps: 0.100000\n",
      "  25305/175000: episode: 713, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.429 [12.000, 207.000], mean observation: 0.284 [0.000, 84.000], loss: 0.267579, mean_absolute_error: 0.394722, mean_q: 1.516645, mean_eps: 0.100000\n",
      "  25330/175000: episode: 714, duration: 0.492s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 181.720 [81.000, 207.000], mean observation: 0.106 [0.000, 50.000], loss: 0.197217, mean_absolute_error: 0.385332, mean_q: 1.404362, mean_eps: 0.100000\n",
      "  25368/175000: episode: 715, duration: 0.808s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.711 [30.000, 212.000], mean observation: 0.251 [0.000, 76.000], loss: 0.198019, mean_absolute_error: 0.386029, mean_q: 1.406273, mean_eps: 0.100000\n",
      "  25421/175000: episode: 716, duration: 1.028s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 190.547 [27.000, 221.000], mean observation: 0.274 [0.000, 106.000], loss: 0.541138, mean_absolute_error: 0.404848, mean_q: 1.309638, mean_eps: 0.100000\n",
      "  25480/175000: episode: 717, duration: 1.152s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 120.644 [10.000, 212.000], mean observation: 0.495 [0.000, 118.000], loss: 0.311784, mean_absolute_error: 0.454987, mean_q: 1.465623, mean_eps: 0.100000\n",
      "  25522/175000: episode: 718, duration: 0.890s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.476 [18.000, 194.000], mean observation: 0.181 [0.000, 84.000], loss: 0.191289, mean_absolute_error: 0.434200, mean_q: 1.456274, mean_eps: 0.100000\n",
      "  25548/175000: episode: 719, duration: 0.561s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 138.654 [54.000, 155.000], mean observation: 0.153 [0.000, 52.000], loss: 0.731210, mean_absolute_error: 0.403374, mean_q: 1.338929, mean_eps: 0.100000\n",
      "  25581/175000: episode: 720, duration: 0.809s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 134.576 [3.000, 219.000], mean observation: 0.206 [0.000, 66.000], loss: 0.405519, mean_absolute_error: 0.394711, mean_q: 1.368649, mean_eps: 0.100000\n",
      "  25596/175000: episode: 721, duration: 0.315s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 202.267 [62.000, 219.000], mean observation: 0.060 [0.000, 30.000], loss: 0.568499, mean_absolute_error: 0.423454, mean_q: 1.438507, mean_eps: 0.100000\n",
      "  25637/175000: episode: 722, duration: 0.846s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 30.951 [3.000, 219.000], mean observation: 0.228 [0.000, 82.000], loss: 0.402368, mean_absolute_error: 0.375396, mean_q: 1.359935, mean_eps: 0.100000\n",
      "  25676/175000: episode: 723, duration: 0.785s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 18.436 [5.000, 207.000], mean observation: 0.107 [0.000, 78.000], loss: 0.499785, mean_absolute_error: 0.383111, mean_q: 1.383855, mean_eps: 0.100000\n",
      "  25703/175000: episode: 724, duration: 0.608s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 39.296 [5.000, 205.000], mean observation: 0.188 [0.000, 54.000], loss: 0.493845, mean_absolute_error: 0.409125, mean_q: 1.498651, mean_eps: 0.100000\n",
      "  25731/175000: episode: 725, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.036 [5.000, 163.000], mean observation: 0.218 [0.000, 56.000], loss: 0.248367, mean_absolute_error: 0.390090, mean_q: 1.412042, mean_eps: 0.100000\n",
      "  25763/175000: episode: 726, duration: 0.690s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.156 [5.000, 219.000], mean observation: 0.199 [0.000, 64.000], loss: 0.224624, mean_absolute_error: 0.373341, mean_q: 1.537120, mean_eps: 0.100000\n",
      "  25798/175000: episode: 727, duration: 0.873s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.886 [5.000, 219.000], mean observation: 0.226 [0.000, 70.000], loss: 0.771946, mean_absolute_error: 0.376820, mean_q: 1.789783, mean_eps: 0.100000\n",
      "  25829/175000: episode: 728, duration: 0.672s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 199.484 [4.000, 219.000], mean observation: 0.155 [0.000, 62.000], loss: 0.271015, mean_absolute_error: 0.422088, mean_q: 1.922838, mean_eps: 0.100000\n",
      "  25861/175000: episode: 729, duration: 0.672s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 108.656 [5.000, 212.000], mean observation: 0.263 [0.000, 64.000], loss: 0.190757, mean_absolute_error: 0.418540, mean_q: 1.828934, mean_eps: 0.100000\n",
      "  25889/175000: episode: 730, duration: 0.568s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 138.036 [17.000, 212.000], mean observation: 0.166 [0.000, 56.000], loss: 0.486575, mean_absolute_error: 0.405487, mean_q: 1.710166, mean_eps: 0.100000\n",
      "  25944/175000: episode: 731, duration: 1.135s, episode steps: 55, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 114.673 [18.000, 212.000], mean observation: 0.609 [0.000, 110.000], loss: 0.381860, mean_absolute_error: 0.391464, mean_q: 1.564860, mean_eps: 0.100000\n",
      "  25955/175000: episode: 732, duration: 0.219s, episode steps: 11, steps per second: 50, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 34.091 [19.000, 137.000], mean observation: 0.058 [0.000, 22.000], loss: 1.580403, mean_absolute_error: 0.408931, mean_q: 1.617887, mean_eps: 0.100000\n",
      "  25984/175000: episode: 733, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.724 [19.000, 212.000], mean observation: 0.124 [0.000, 58.000], loss: 0.554869, mean_absolute_error: 0.405194, mean_q: 1.687614, mean_eps: 0.100000\n",
      "  26010/175000: episode: 734, duration: 0.530s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 104.962 [84.000, 146.000], mean observation: 0.073 [0.000, 52.000], loss: 0.520151, mean_absolute_error: 0.433734, mean_q: 1.697924, mean_eps: 0.100000\n",
      "  26034/175000: episode: 735, duration: 0.478s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.375 [19.000, 215.000], mean observation: 0.110 [0.000, 48.000], loss: 0.287558, mean_absolute_error: 0.459014, mean_q: 1.762546, mean_eps: 0.100000\n",
      "  26067/175000: episode: 736, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.515 [19.000, 214.000], mean observation: 0.322 [0.000, 66.000], loss: 0.553270, mean_absolute_error: 0.475222, mean_q: 1.800057, mean_eps: 0.100000\n",
      "  26099/175000: episode: 737, duration: 0.700s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 192.750 [19.000, 218.000], mean observation: 0.110 [0.000, 64.000], loss: 0.423570, mean_absolute_error: 0.461263, mean_q: 1.750527, mean_eps: 0.100000\n",
      "  26128/175000: episode: 738, duration: 0.656s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 40.759 [19.000, 222.000], mean observation: 0.128 [0.000, 58.000], loss: 0.318809, mean_absolute_error: 0.433914, mean_q: 1.749749, mean_eps: 0.100000\n",
      "  26171/175000: episode: 739, duration: 0.900s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 55.465 [19.000, 195.000], mean observation: 0.244 [0.000, 86.000], loss: 0.363213, mean_absolute_error: 0.422818, mean_q: 1.873171, mean_eps: 0.100000\n",
      "  26194/175000: episode: 740, duration: 0.492s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 27.957 [19.000, 150.000], mean observation: 0.091 [0.000, 46.000], loss: 0.453179, mean_absolute_error: 0.452480, mean_q: 1.966853, mean_eps: 0.100000\n",
      "  26228/175000: episode: 741, duration: 0.702s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.079 [0.000, 68.000], loss: 0.722679, mean_absolute_error: 0.439658, mean_q: 1.897936, mean_eps: 0.100000\n",
      "  26266/175000: episode: 742, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.947 [19.000, 194.000], mean observation: 0.283 [0.000, 76.000], loss: 0.248715, mean_absolute_error: 0.415683, mean_q: 1.825805, mean_eps: 0.100000\n",
      "  26300/175000: episode: 743, duration: 0.846s, episode steps: 34, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.618 [19.000, 189.000], mean observation: 0.205 [0.000, 68.000], loss: 0.672926, mean_absolute_error: 0.435726, mean_q: 2.015008, mean_eps: 0.100000\n",
      "  26317/175000: episode: 744, duration: 0.462s, episode steps: 17, steps per second: 37, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 102.235 [19.000, 187.000], mean observation: 0.085 [0.000, 34.000], loss: 0.337385, mean_absolute_error: 0.422674, mean_q: 2.033480, mean_eps: 0.100000\n",
      "  26359/175000: episode: 745, duration: 0.807s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.000 [19.000, 199.000], mean observation: 0.214 [0.000, 84.000], loss: 0.954233, mean_absolute_error: 0.434230, mean_q: 1.884501, mean_eps: 0.100000\n",
      "  26385/175000: episode: 746, duration: 0.552s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 90.692 [12.000, 187.000], mean observation: 0.097 [0.000, 52.000], loss: 0.174857, mean_absolute_error: 0.449357, mean_q: 1.870460, mean_eps: 0.100000\n",
      "  26423/175000: episode: 747, duration: 0.738s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.105 [19.000, 169.000], mean observation: 0.274 [0.000, 76.000], loss: 0.369142, mean_absolute_error: 0.474692, mean_q: 1.960413, mean_eps: 0.100000\n",
      "  26468/175000: episode: 748, duration: 0.964s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 74.556 [19.000, 212.000], mean observation: 0.446 [0.000, 90.000], loss: 0.516468, mean_absolute_error: 0.432290, mean_q: 1.828158, mean_eps: 0.100000\n",
      "  26492/175000: episode: 749, duration: 0.625s, episode steps: 24, steps per second: 38, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 103.500 [19.000, 188.000], mean observation: 0.067 [0.000, 48.000], loss: 0.365045, mean_absolute_error: 0.446639, mean_q: 1.866360, mean_eps: 0.100000\n",
      "  26518/175000: episode: 750, duration: 0.550s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 149.038 [6.000, 188.000], mean observation: 0.159 [0.000, 52.000], loss: 0.298891, mean_absolute_error: 0.413987, mean_q: 1.788586, mean_eps: 0.100000\n",
      "  26571/175000: episode: 751, duration: 0.995s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.434 [6.000, 212.000], mean observation: 0.422 [0.000, 106.000], loss: 0.607993, mean_absolute_error: 0.412257, mean_q: 1.787170, mean_eps: 0.100000\n",
      "  26606/175000: episode: 752, duration: 0.626s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 50.257 [19.000, 212.000], mean observation: 0.092 [0.000, 70.000], loss: 0.456203, mean_absolute_error: 0.455335, mean_q: 1.868152, mean_eps: 0.100000\n",
      "  26647/175000: episode: 753, duration: 0.711s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.561 [23.000, 215.000], mean observation: 0.344 [0.000, 82.000], loss: 0.367948, mean_absolute_error: 0.440221, mean_q: 1.861535, mean_eps: 0.100000\n",
      "  26680/175000: episode: 754, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.303 [21.000, 223.000], mean observation: 0.175 [0.000, 66.000], loss: 0.904333, mean_absolute_error: 0.413404, mean_q: 1.733468, mean_eps: 0.100000\n",
      "  26720/175000: episode: 755, duration: 0.799s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 94.475 [8.000, 219.000], mean observation: 0.288 [0.000, 80.000], loss: 0.410787, mean_absolute_error: 0.448848, mean_q: 1.871378, mean_eps: 0.100000\n",
      "  26759/175000: episode: 756, duration: 0.791s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.410 [21.000, 191.000], mean observation: 0.357 [0.000, 78.000], loss: 0.721748, mean_absolute_error: 0.453972, mean_q: 1.712246, mean_eps: 0.100000\n",
      "  26800/175000: episode: 757, duration: 0.895s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.585 [12.000, 200.000], mean observation: 0.309 [0.000, 82.000], loss: 0.811795, mean_absolute_error: 0.455711, mean_q: 1.862847, mean_eps: 0.100000\n",
      "  26833/175000: episode: 758, duration: 0.641s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.667 [6.000, 200.000], mean observation: 0.140 [0.000, 66.000], loss: 0.694646, mean_absolute_error: 0.461086, mean_q: 1.804151, mean_eps: 0.100000\n",
      "  26844/175000: episode: 759, duration: 0.221s, episode steps: 11, steps per second: 50, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 82.727 [28.000, 158.000], mean observation: 0.048 [0.000, 22.000], loss: 0.203105, mean_absolute_error: 0.456788, mean_q: 1.904118, mean_eps: 0.100000\n",
      "  26859/175000: episode: 760, duration: 0.275s, episode steps: 15, steps per second: 54, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 35.000 [28.000, 133.000], mean observation: 0.049 [0.000, 30.000], loss: 0.194441, mean_absolute_error: 0.436012, mean_q: 1.903095, mean_eps: 0.100000\n",
      "  26906/175000: episode: 761, duration: 0.903s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 113.745 [7.000, 188.000], mean observation: 0.356 [0.000, 94.000], loss: 0.357942, mean_absolute_error: 0.436784, mean_q: 1.931246, mean_eps: 0.100000\n",
      "  26967/175000: episode: 762, duration: 1.207s, episode steps: 61, steps per second: 51, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 92.066 [17.000, 219.000], mean observation: 0.399 [0.000, 122.000], loss: 0.804423, mean_absolute_error: 0.429034, mean_q: 1.834657, mean_eps: 0.100000\n",
      "  26998/175000: episode: 763, duration: 0.682s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 120.581 [1.000, 216.000], mean observation: 0.162 [0.000, 62.000], loss: 1.243092, mean_absolute_error: 0.452719, mean_q: 1.822053, mean_eps: 0.100000\n",
      "  27033/175000: episode: 764, duration: 0.622s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 86.543 [41.000, 216.000], mean observation: 0.137 [0.000, 70.000], loss: 0.587506, mean_absolute_error: 0.472287, mean_q: 1.903476, mean_eps: 0.100000\n",
      "  27091/175000: episode: 765, duration: 1.060s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 85.793 [30.000, 202.000], mean observation: 0.330 [0.000, 116.000], loss: 0.984262, mean_absolute_error: 0.480290, mean_q: 1.832969, mean_eps: 0.100000\n",
      "  27122/175000: episode: 766, duration: 0.678s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 175.548 [60.000, 204.000], mean observation: 0.144 [0.000, 62.000], loss: 0.214343, mean_absolute_error: 0.487537, mean_q: 1.921915, mean_eps: 0.100000\n",
      "  27149/175000: episode: 767, duration: 0.482s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.185 [21.000, 217.000], mean observation: 0.157 [0.000, 54.000], loss: 0.208474, mean_absolute_error: 0.461008, mean_q: 1.782471, mean_eps: 0.100000\n",
      "  27192/175000: episode: 768, duration: 0.923s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.744 [14.000, 195.000], mean observation: 0.318 [0.000, 86.000], loss: 0.300064, mean_absolute_error: 0.501673, mean_q: 1.844130, mean_eps: 0.100000\n",
      "  27216/175000: episode: 769, duration: 0.573s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 99.250 [16.000, 185.000], mean observation: 0.086 [0.000, 48.000], loss: 0.421507, mean_absolute_error: 0.472822, mean_q: 1.665245, mean_eps: 0.100000\n",
      "  27245/175000: episode: 770, duration: 0.618s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 72.483 [58.000, 73.000], mean observation: 0.071 [0.000, 58.000], loss: 0.232216, mean_absolute_error: 0.449423, mean_q: 1.672564, mean_eps: 0.100000\n",
      "  27281/175000: episode: 771, duration: 0.624s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 89.917 [16.000, 212.000], mean observation: 0.176 [0.000, 72.000], loss: 0.605964, mean_absolute_error: 0.403435, mean_q: 1.623740, mean_eps: 0.100000\n",
      "  27316/175000: episode: 772, duration: 0.680s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.829 [16.000, 188.000], mean observation: 0.161 [0.000, 70.000], loss: 0.463476, mean_absolute_error: 0.423058, mean_q: 1.823041, mean_eps: 0.100000\n",
      "  27345/175000: episode: 773, duration: 0.621s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 91.586 [16.000, 192.000], mean observation: 0.130 [0.000, 58.000], loss: 0.297276, mean_absolute_error: 0.392309, mean_q: 1.648769, mean_eps: 0.100000\n",
      "  27399/175000: episode: 774, duration: 0.971s, episode steps: 54, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 64.185 [16.000, 219.000], mean observation: 0.401 [0.000, 108.000], loss: 0.462512, mean_absolute_error: 0.401907, mean_q: 1.735920, mean_eps: 0.100000\n",
      "  27428/175000: episode: 775, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 91.207 [7.000, 184.000], mean observation: 0.150 [0.000, 58.000], loss: 0.353451, mean_absolute_error: 0.441213, mean_q: 1.749858, mean_eps: 0.100000\n",
      "  27458/175000: episode: 776, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 88.500 [15.000, 188.000], mean observation: 0.135 [0.000, 60.000], loss: 0.667952, mean_absolute_error: 0.490263, mean_q: 1.900130, mean_eps: 0.100000\n",
      "  27491/175000: episode: 777, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 100.091 [6.000, 187.000], mean observation: 0.242 [0.000, 66.000], loss: 0.182115, mean_absolute_error: 0.476325, mean_q: 1.805757, mean_eps: 0.100000\n",
      "  27514/175000: episode: 778, duration: 0.466s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 81.696 [16.000, 219.000], mean observation: 0.120 [0.000, 46.000], loss: 0.711968, mean_absolute_error: 0.480851, mean_q: 1.841685, mean_eps: 0.100000\n",
      "  27545/175000: episode: 779, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.742 [16.000, 197.000], mean observation: 0.170 [0.000, 62.000], loss: 0.458415, mean_absolute_error: 0.457626, mean_q: 1.804346, mean_eps: 0.100000\n",
      "  27589/175000: episode: 780, duration: 0.785s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 47.114 [13.000, 219.000], mean observation: 0.272 [0.000, 88.000], loss: 0.382906, mean_absolute_error: 0.422184, mean_q: 1.688167, mean_eps: 0.100000\n",
      "  27601/175000: episode: 781, duration: 0.212s, episode steps: 12, steps per second: 57, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 76.000 [76.000, 76.000], mean observation: 0.030 [0.000, 24.000], loss: 0.462540, mean_absolute_error: 0.400663, mean_q: 1.636298, mean_eps: 0.100000\n",
      "  27618/175000: episode: 782, duration: 0.326s, episode steps: 17, steps per second: 52, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 73.235 [16.000, 164.000], mean observation: 0.113 [0.000, 34.000], loss: 0.125811, mean_absolute_error: 0.399348, mean_q: 1.686995, mean_eps: 0.100000\n",
      "  27664/175000: episode: 783, duration: 0.838s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 55.826 [15.000, 220.000], mean observation: 0.207 [0.000, 92.000], loss: 0.426251, mean_absolute_error: 0.409097, mean_q: 1.899676, mean_eps: 0.100000\n",
      "  27688/175000: episode: 784, duration: 0.510s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 88.500 [21.000, 190.000], mean observation: 0.102 [0.000, 48.000], loss: 0.508708, mean_absolute_error: 0.421950, mean_q: 2.037261, mean_eps: 0.100000\n",
      "  27708/175000: episode: 785, duration: 0.421s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 103.300 [14.000, 140.000], mean observation: 0.088 [0.000, 40.000], loss: 0.966994, mean_absolute_error: 0.405778, mean_q: 1.897666, mean_eps: 0.100000\n",
      "  27727/175000: episode: 786, duration: 0.345s, episode steps: 19, steps per second: 55, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.211 [15.000, 221.000], mean observation: 0.113 [0.000, 38.000], loss: 0.369698, mean_absolute_error: 0.413921, mean_q: 1.924074, mean_eps: 0.100000\n",
      "  27762/175000: episode: 787, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.686 [19.000, 223.000], mean observation: 0.310 [0.000, 70.000], loss: 0.663959, mean_absolute_error: 0.403670, mean_q: 1.864919, mean_eps: 0.100000\n",
      "  27800/175000: episode: 788, duration: 0.762s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 122.974 [65.000, 222.000], mean observation: 0.319 [0.000, 76.000], loss: 0.444630, mean_absolute_error: 0.444301, mean_q: 1.932309, mean_eps: 0.100000\n",
      "  27829/175000: episode: 789, duration: 0.606s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 129.931 [51.000, 164.000], mean observation: 0.111 [0.000, 58.000], loss: 0.166682, mean_absolute_error: 0.470252, mean_q: 1.952647, mean_eps: 0.100000\n",
      "  27859/175000: episode: 790, duration: 0.603s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 117.400 [47.000, 135.000], mean observation: 0.073 [0.000, 60.000], loss: 0.101451, mean_absolute_error: 0.502647, mean_q: 1.874369, mean_eps: 0.100000\n",
      "  27910/175000: episode: 791, duration: 1.096s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 100.824 [21.000, 170.000], mean observation: 0.577 [0.000, 102.000], loss: 0.832561, mean_absolute_error: 0.551477, mean_q: 1.972203, mean_eps: 0.100000\n",
      "  27946/175000: episode: 792, duration: 0.644s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.556 [15.000, 193.000], mean observation: 0.212 [0.000, 72.000], loss: 0.581370, mean_absolute_error: 0.512133, mean_q: 1.864611, mean_eps: 0.100000\n",
      "  27982/175000: episode: 793, duration: 0.802s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 99.361 [45.000, 222.000], mean observation: 0.120 [0.000, 72.000], loss: 0.748203, mean_absolute_error: 0.470354, mean_q: 1.849466, mean_eps: 0.100000\n",
      "  28020/175000: episode: 794, duration: 0.917s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.368 [19.000, 197.000], mean observation: 0.218 [0.000, 76.000], loss: 0.318727, mean_absolute_error: 0.449134, mean_q: 1.822521, mean_eps: 0.100000\n",
      "  28068/175000: episode: 795, duration: 1.069s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 51.896 [15.000, 170.000], mean observation: 0.282 [0.000, 96.000], loss: 0.264682, mean_absolute_error: 0.412377, mean_q: 1.799206, mean_eps: 0.100000\n",
      "  28135/175000: episode: 796, duration: 1.491s, episode steps: 67, steps per second: 45, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 130.403 [6.000, 188.000], mean observation: 0.468 [0.000, 134.000], loss: 0.449101, mean_absolute_error: 0.368600, mean_q: 1.612078, mean_eps: 0.100000\n",
      "  28147/175000: episode: 797, duration: 0.255s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 5.917 [4.000, 27.000], mean observation: 0.040 [0.000, 24.000], loss: 0.662992, mean_absolute_error: 0.396907, mean_q: 1.762872, mean_eps: 0.100000\n",
      "  28183/175000: episode: 798, duration: 0.697s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 80.444 [4.000, 219.000], mean observation: 0.226 [0.000, 72.000], loss: 0.390566, mean_absolute_error: 0.403066, mean_q: 1.792714, mean_eps: 0.100000\n",
      "  28211/175000: episode: 799, duration: 0.592s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.536 [4.000, 189.000], mean observation: 0.227 [0.000, 56.000], loss: 0.386202, mean_absolute_error: 0.437509, mean_q: 1.893496, mean_eps: 0.100000\n",
      "  28251/175000: episode: 800, duration: 0.889s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 77.300 [7.000, 170.000], mean observation: 0.445 [0.000, 80.000], loss: 0.450072, mean_absolute_error: 0.498322, mean_q: 2.099913, mean_eps: 0.100000\n",
      "  28283/175000: episode: 801, duration: 0.712s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.562 [11.000, 188.000], mean observation: 0.187 [0.000, 64.000], loss: 0.501060, mean_absolute_error: 0.526468, mean_q: 2.149803, mean_eps: 0.100000\n",
      "  28306/175000: episode: 802, duration: 0.500s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 117.130 [8.000, 212.000], mean observation: 0.110 [0.000, 46.000], loss: 0.981752, mean_absolute_error: 0.572767, mean_q: 2.065874, mean_eps: 0.100000\n",
      "  28354/175000: episode: 803, duration: 0.900s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 114.812 [7.000, 220.000], mean observation: 0.495 [0.000, 96.000], loss: 0.469752, mean_absolute_error: 0.557216, mean_q: 2.155614, mean_eps: 0.100000\n",
      "  28408/175000: episode: 804, duration: 1.061s, episode steps: 54, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 165.148 [33.000, 222.000], mean observation: 0.363 [0.000, 108.000], loss: 0.650304, mean_absolute_error: 0.489852, mean_q: 2.405640, mean_eps: 0.100000\n",
      "  28438/175000: episode: 805, duration: 0.658s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 144.233 [47.000, 207.000], mean observation: 0.114 [0.000, 60.000], loss: 0.480774, mean_absolute_error: 0.426328, mean_q: 2.439352, mean_eps: 0.100000\n",
      "  28475/175000: episode: 806, duration: 0.833s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.216 [30.000, 212.000], mean observation: 0.255 [0.000, 74.000], loss: 1.187161, mean_absolute_error: 0.422675, mean_q: 2.418532, mean_eps: 0.100000\n",
      "  28521/175000: episode: 807, duration: 0.971s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.717 [19.000, 222.000], mean observation: 0.284 [0.000, 92.000], loss: 0.750094, mean_absolute_error: 0.444340, mean_q: 2.511404, mean_eps: 0.100000\n",
      "  28565/175000: episode: 808, duration: 0.776s, episode steps: 44, steps per second: 57, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 167.000 [6.000, 212.000], mean observation: 0.182 [0.000, 88.000], loss: 0.388706, mean_absolute_error: 0.410654, mean_q: 2.384034, mean_eps: 0.100000\n",
      "  28599/175000: episode: 809, duration: 0.742s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.588 [19.000, 212.000], mean observation: 0.223 [0.000, 68.000], loss: 0.761800, mean_absolute_error: 0.407101, mean_q: 2.192364, mean_eps: 0.100000\n",
      "  28626/175000: episode: 810, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 134.296 [80.000, 218.000], mean observation: 0.163 [0.000, 54.000], loss: 0.387629, mean_absolute_error: 0.398535, mean_q: 2.121914, mean_eps: 0.100000\n",
      "  28649/175000: episode: 811, duration: 0.448s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 117.217 [19.000, 218.000], mean observation: 0.130 [0.000, 46.000], loss: 0.291618, mean_absolute_error: 0.449618, mean_q: 2.170792, mean_eps: 0.100000\n",
      "  28697/175000: episode: 812, duration: 0.884s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 40.938 [8.000, 207.000], mean observation: 0.433 [0.000, 96.000], loss: 0.287224, mean_absolute_error: 0.465197, mean_q: 2.053435, mean_eps: 0.100000\n",
      "  28742/175000: episode: 813, duration: 0.822s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 90.733 [8.000, 204.000], mean observation: 0.412 [0.000, 90.000], loss: 0.488517, mean_absolute_error: 0.444116, mean_q: 1.977262, mean_eps: 0.100000\n",
      "  28773/175000: episode: 814, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.387 [8.000, 169.000], mean observation: 0.078 [0.000, 62.000], loss: 0.429254, mean_absolute_error: 0.449238, mean_q: 1.953818, mean_eps: 0.100000\n",
      "  28801/175000: episode: 815, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 138.929 [9.000, 199.000], mean observation: 0.136 [0.000, 56.000], loss: 0.167234, mean_absolute_error: 0.446600, mean_q: 1.898533, mean_eps: 0.100000\n",
      "  28828/175000: episode: 816, duration: 0.641s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.926 [39.000, 152.000], mean observation: 0.073 [0.000, 54.000], loss: 0.453642, mean_absolute_error: 0.484660, mean_q: 1.872168, mean_eps: 0.100000\n",
      "  28871/175000: episode: 817, duration: 0.855s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.465 [36.000, 224.000], mean observation: 0.227 [0.000, 86.000], loss: 0.254591, mean_absolute_error: 0.462178, mean_q: 1.832652, mean_eps: 0.100000\n",
      "  28905/175000: episode: 818, duration: 0.707s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.353 [29.000, 198.000], mean observation: 0.157 [0.000, 68.000], loss: 0.640593, mean_absolute_error: 0.453194, mean_q: 1.824734, mean_eps: 0.100000\n",
      "  28957/175000: episode: 819, duration: 0.980s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 155.962 [19.000, 198.000], mean observation: 0.506 [0.000, 104.000], loss: 0.417925, mean_absolute_error: 0.414682, mean_q: 1.699468, mean_eps: 0.100000\n",
      "  29003/175000: episode: 820, duration: 0.919s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 158.304 [3.000, 220.000], mean observation: 0.460 [0.000, 92.000], loss: 0.271526, mean_absolute_error: 0.354528, mean_q: 1.621311, mean_eps: 0.100000\n",
      "  29036/175000: episode: 821, duration: 0.721s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 102.818 [61.000, 179.000], mean observation: 0.187 [0.000, 66.000], loss: 0.886952, mean_absolute_error: 0.346502, mean_q: 1.754606, mean_eps: 0.100000\n",
      "  29078/175000: episode: 822, duration: 0.828s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.595 [8.000, 207.000], mean observation: 0.212 [0.000, 84.000], loss: 0.221694, mean_absolute_error: 0.373555, mean_q: 1.755288, mean_eps: 0.100000\n",
      "  29119/175000: episode: 823, duration: 0.782s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.244 [6.000, 169.000], mean observation: 0.353 [0.000, 82.000], loss: 0.434672, mean_absolute_error: 0.402876, mean_q: 1.700926, mean_eps: 0.100000\n",
      "  29161/175000: episode: 824, duration: 0.772s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 78.643 [24.000, 187.000], mean observation: 0.280 [0.000, 84.000], loss: 0.231062, mean_absolute_error: 0.461530, mean_q: 1.805125, mean_eps: 0.100000\n",
      "  29187/175000: episode: 825, duration: 0.434s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 64.231 [2.000, 184.000], mean observation: 0.116 [0.000, 52.000], loss: 0.372260, mean_absolute_error: 0.479597, mean_q: 1.976014, mean_eps: 0.100000\n",
      "  29237/175000: episode: 826, duration: 0.914s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.340 [2.000, 201.000], mean observation: 0.422 [0.000, 100.000], loss: 0.414872, mean_absolute_error: 0.441598, mean_q: 1.968086, mean_eps: 0.100000\n",
      "  29272/175000: episode: 827, duration: 0.890s, episode steps: 35, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.200 [25.000, 192.000], mean observation: 0.289 [0.000, 70.000], loss: 0.560549, mean_absolute_error: 0.396662, mean_q: 1.813253, mean_eps: 0.100000\n",
      "  29310/175000: episode: 828, duration: 0.907s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.000 [51.000, 189.000], mean observation: 0.244 [0.000, 76.000], loss: 0.628288, mean_absolute_error: 0.393945, mean_q: 1.796562, mean_eps: 0.100000\n",
      "  29346/175000: episode: 829, duration: 0.780s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.306 [53.000, 173.000], mean observation: 0.234 [0.000, 72.000], loss: 0.574286, mean_absolute_error: 0.416612, mean_q: 1.890813, mean_eps: 0.100000\n",
      "  29380/175000: episode: 830, duration: 0.824s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.088 [9.000, 199.000], mean observation: 0.283 [0.000, 68.000], loss: 0.416839, mean_absolute_error: 0.392095, mean_q: 1.810023, mean_eps: 0.100000\n",
      "  29425/175000: episode: 831, duration: 1.124s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.222 [19.000, 221.000], mean observation: 0.337 [0.000, 90.000], loss: 0.343639, mean_absolute_error: 0.401181, mean_q: 1.905235, mean_eps: 0.100000\n",
      "  29447/175000: episode: 832, duration: 0.540s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 21.182 [15.000, 151.000], mean observation: 0.062 [0.000, 44.000], loss: 0.252740, mean_absolute_error: 0.431287, mean_q: 1.953534, mean_eps: 0.100000\n",
      "  29485/175000: episode: 833, duration: 0.958s, episode steps: 38, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.000 [15.000, 199.000], mean observation: 0.459 [0.000, 76.000], loss: 0.188409, mean_absolute_error: 0.422420, mean_q: 1.931111, mean_eps: 0.100000\n",
      "  29528/175000: episode: 834, duration: 1.036s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.465 [1.000, 140.000], mean observation: 0.360 [0.000, 86.000], loss: 0.470490, mean_absolute_error: 0.390171, mean_q: 1.758818, mean_eps: 0.100000\n",
      "  29554/175000: episode: 835, duration: 0.627s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 98.115 [0.000, 143.000], mean observation: 0.116 [0.000, 52.000], loss: 0.158224, mean_absolute_error: 0.363458, mean_q: 1.711728, mean_eps: 0.100000\n",
      "  29598/175000: episode: 836, duration: 0.800s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.773 [22.000, 208.000], mean observation: 0.420 [0.000, 88.000], loss: 0.232312, mean_absolute_error: 0.375450, mean_q: 1.814772, mean_eps: 0.100000\n",
      "  29619/175000: episode: 837, duration: 0.367s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 11.286 [5.000, 120.000], mean observation: 0.064 [0.000, 42.000], loss: 0.335043, mean_absolute_error: 0.369383, mean_q: 1.869389, mean_eps: 0.100000\n",
      "  29655/175000: episode: 838, duration: 0.756s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 61.611 [5.000, 184.000], mean observation: 0.175 [0.000, 72.000], loss: 0.286229, mean_absolute_error: 0.382262, mean_q: 1.930583, mean_eps: 0.100000\n",
      "  29681/175000: episode: 839, duration: 0.701s, episode steps: 26, steps per second: 37, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 65.731 [22.000, 156.000], mean observation: 0.197 [0.000, 52.000], loss: 0.192820, mean_absolute_error: 0.393808, mean_q: 2.018563, mean_eps: 0.100000\n",
      "  29714/175000: episode: 840, duration: 0.754s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 105.394 [60.000, 208.000], mean observation: 0.222 [0.000, 66.000], loss: 0.108029, mean_absolute_error: 0.346164, mean_q: 1.809319, mean_eps: 0.100000\n",
      "  29747/175000: episode: 841, duration: 0.796s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 39.848 [31.000, 160.000], mean observation: 0.084 [0.000, 66.000], loss: 0.252011, mean_absolute_error: 0.355713, mean_q: 1.863812, mean_eps: 0.100000\n",
      "  29807/175000: episode: 842, duration: 1.504s, episode steps: 60, steps per second: 40, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 83.450 [8.000, 194.000], mean observation: 0.739 [0.000, 120.000], loss: 0.360688, mean_absolute_error: 0.338218, mean_q: 1.826368, mean_eps: 0.100000\n",
      "  29864/175000: episode: 843, duration: 1.556s, episode steps: 57, steps per second: 37, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 32.386 [8.000, 191.000], mean observation: 0.297 [0.000, 114.000], loss: 0.155168, mean_absolute_error: 0.334869, mean_q: 1.867931, mean_eps: 0.100000\n",
      "  29893/175000: episode: 844, duration: 0.637s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 47.345 [8.000, 209.000], mean observation: 0.195 [0.000, 58.000], loss: 0.225078, mean_absolute_error: 0.330326, mean_q: 1.792578, mean_eps: 0.100000\n",
      "  29925/175000: episode: 845, duration: 0.611s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 90.781 [89.000, 146.000], mean observation: 0.090 [0.000, 64.000], loss: 1.167621, mean_absolute_error: 0.388750, mean_q: 2.057199, mean_eps: 0.100000\n",
      "  29956/175000: episode: 846, duration: 0.660s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.000 [49.000, 202.000], mean observation: 0.074 [0.000, 62.000], loss: 0.479115, mean_absolute_error: 0.380376, mean_q: 2.021892, mean_eps: 0.100000\n",
      "  29983/175000: episode: 847, duration: 0.592s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 165.926 [22.000, 202.000], mean observation: 0.088 [0.000, 54.000], loss: 0.244322, mean_absolute_error: 0.378302, mean_q: 2.001509, mean_eps: 0.100000\n",
      "  30019/175000: episode: 848, duration: 0.809s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 164.028 [21.000, 202.000], mean observation: 0.131 [0.000, 72.000], loss: 1.576836, mean_absolute_error: 0.390263, mean_q: 2.018594, mean_eps: 0.100000\n",
      "  30035/175000: episode: 849, duration: 0.311s, episode steps: 16, steps per second: 51, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 191.062 [109.000, 202.000], mean observation: 0.049 [0.000, 32.000], loss: 0.600934, mean_absolute_error: 0.356765, mean_q: 1.866988, mean_eps: 0.100000\n",
      "  30071/175000: episode: 850, duration: 0.849s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [51.000, 202.000], mean observation: 0.375 [0.000, 72.000], loss: 2.182937, mean_absolute_error: 0.431253, mean_q: 2.050601, mean_eps: 0.100000\n",
      "  30108/175000: episode: 851, duration: 0.877s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 129.351 [2.000, 195.000], mean observation: 0.169 [0.000, 74.000], loss: 0.209897, mean_absolute_error: 0.440005, mean_q: 2.136141, mean_eps: 0.100000\n",
      "  30153/175000: episode: 852, duration: 1.087s, episode steps: 45, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.800 [41.000, 211.000], mean observation: 0.373 [0.000, 90.000], loss: 1.730468, mean_absolute_error: 0.453648, mean_q: 2.145526, mean_eps: 0.100000\n",
      "  30182/175000: episode: 853, duration: 0.611s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.069 [15.000, 135.000], mean observation: 0.112 [0.000, 58.000], loss: 2.157681, mean_absolute_error: 0.477894, mean_q: 2.083644, mean_eps: 0.100000\n",
      "  30213/175000: episode: 854, duration: 0.670s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 40.484 [12.000, 193.000], mean observation: 0.183 [0.000, 62.000], loss: 1.900839, mean_absolute_error: 0.482201, mean_q: 2.119053, mean_eps: 0.100000\n",
      "  30241/175000: episode: 855, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.321 [12.000, 218.000], mean observation: 0.125 [0.000, 56.000], loss: 0.809498, mean_absolute_error: 0.468022, mean_q: 1.907527, mean_eps: 0.100000\n",
      "  30274/175000: episode: 856, duration: 0.562s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.303 [12.000, 178.000], mean observation: 0.231 [0.000, 66.000], loss: 1.106085, mean_absolute_error: 0.477987, mean_q: 2.001715, mean_eps: 0.100000\n",
      "  30320/175000: episode: 857, duration: 0.870s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 81.370 [28.000, 212.000], mean observation: 0.245 [0.000, 92.000], loss: 0.802924, mean_absolute_error: 0.435283, mean_q: 1.812546, mean_eps: 0.100000\n",
      "  30359/175000: episode: 858, duration: 0.696s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.385 [29.000, 212.000], mean observation: 0.325 [0.000, 78.000], loss: 3.076598, mean_absolute_error: 0.413779, mean_q: 1.812920, mean_eps: 0.100000\n",
      "  30373/175000: episode: 859, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 95.143 [31.000, 116.000], mean observation: 0.052 [0.000, 28.000], loss: 5.948199, mean_absolute_error: 0.477817, mean_q: 1.956259, mean_eps: 0.100000\n",
      "  30426/175000: episode: 860, duration: 0.982s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 126.660 [74.000, 203.000], mean observation: 0.468 [0.000, 106.000], loss: 1.172773, mean_absolute_error: 0.444885, mean_q: 1.845571, mean_eps: 0.100000\n",
      "  30467/175000: episode: 861, duration: 0.756s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.220 [14.000, 224.000], mean observation: 0.199 [0.000, 82.000], loss: 3.811341, mean_absolute_error: 0.441018, mean_q: 1.949279, mean_eps: 0.100000\n",
      "  30503/175000: episode: 862, duration: 0.713s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 169.167 [74.000, 204.000], mean observation: 0.134 [0.000, 72.000], loss: 1.812635, mean_absolute_error: 0.418887, mean_q: 1.917670, mean_eps: 0.100000\n",
      "  30535/175000: episode: 863, duration: 0.642s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 130.562 [15.000, 210.000], mean observation: 0.291 [0.000, 64.000], loss: 2.515732, mean_absolute_error: 0.492585, mean_q: 2.148654, mean_eps: 0.100000\n",
      "  30577/175000: episode: 864, duration: 0.883s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 132.714 [9.000, 210.000], mean observation: 0.374 [0.000, 84.000], loss: 1.301363, mean_absolute_error: 0.431463, mean_q: 1.986507, mean_eps: 0.100000\n",
      "  30594/175000: episode: 865, duration: 0.332s, episode steps: 17, steps per second: 51, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 148.235 [140.000, 160.000], mean observation: 0.052 [0.000, 34.000], loss: 6.804219, mean_absolute_error: 0.467716, mean_q: 2.087343, mean_eps: 0.100000\n",
      "  30629/175000: episode: 866, duration: 0.703s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.800 [5.000, 189.000], mean observation: 0.307 [0.000, 70.000], loss: 0.240950, mean_absolute_error: 0.368359, mean_q: 1.801442, mean_eps: 0.100000\n",
      "  30670/175000: episode: 867, duration: 0.718s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.634 [19.000, 176.000], mean observation: 0.277 [0.000, 82.000], loss: 2.947955, mean_absolute_error: 0.420367, mean_q: 1.703449, mean_eps: 0.100000\n",
      "  30712/175000: episode: 868, duration: 0.797s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.690 [19.000, 171.000], mean observation: 0.250 [0.000, 84.000], loss: 1.448645, mean_absolute_error: 0.416226, mean_q: 1.724058, mean_eps: 0.100000\n",
      "  30758/175000: episode: 869, duration: 0.843s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 84.978 [19.000, 186.000], mean observation: 0.346 [0.000, 92.000], loss: 0.178697, mean_absolute_error: 0.390799, mean_q: 1.659181, mean_eps: 0.100000\n",
      "  30785/175000: episode: 870, duration: 0.479s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 70.704 [19.000, 116.000], mean observation: 0.177 [0.000, 54.000], loss: 1.389605, mean_absolute_error: 0.427694, mean_q: 1.764334, mean_eps: 0.100000\n",
      "  30811/175000: episode: 871, duration: 0.461s, episode steps: 26, steps per second: 56, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 67.923 [10.000, 205.000], mean observation: 0.175 [0.000, 52.000], loss: 1.717069, mean_absolute_error: 0.455903, mean_q: 1.770401, mean_eps: 0.100000\n",
      "  30848/175000: episode: 872, duration: 0.710s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 72.000 [2.000, 154.000], mean observation: 0.124 [0.000, 74.000], loss: 1.171065, mean_absolute_error: 0.473381, mean_q: 1.729650, mean_eps: 0.100000\n",
      "  30863/175000: episode: 873, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 71.000 [71.000, 71.000], mean observation: 0.037 [0.000, 30.000], loss: 2.325951, mean_absolute_error: 0.489395, mean_q: 1.719098, mean_eps: 0.100000\n",
      "  30890/175000: episode: 874, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 77.667 [2.000, 188.000], mean observation: 0.195 [0.000, 54.000], loss: 2.742257, mean_absolute_error: 0.529342, mean_q: 1.838260, mean_eps: 0.100000\n",
      "  30926/175000: episode: 875, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 88.389 [2.000, 207.000], mean observation: 0.271 [0.000, 72.000], loss: 0.260968, mean_absolute_error: 0.437989, mean_q: 1.593314, mean_eps: 0.100000\n",
      "  30975/175000: episode: 876, duration: 0.884s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.041 [14.000, 185.000], mean observation: 0.494 [0.000, 98.000], loss: 0.734190, mean_absolute_error: 0.426745, mean_q: 1.632642, mean_eps: 0.100000\n",
      "  31003/175000: episode: 877, duration: 0.517s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 85.357 [19.000, 189.000], mean observation: 0.159 [0.000, 56.000], loss: 0.528442, mean_absolute_error: 0.428592, mean_q: 1.610212, mean_eps: 0.100000\n",
      "  31036/175000: episode: 878, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 42.545 [19.000, 198.000], mean observation: 0.149 [0.000, 66.000], loss: 3.334421, mean_absolute_error: 0.465811, mean_q: 1.639884, mean_eps: 0.100000\n",
      "  31051/175000: episode: 879, duration: 0.287s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.037 [0.000, 30.000], loss: 0.274105, mean_absolute_error: 0.460896, mean_q: 1.500430, mean_eps: 0.100000\n",
      "  31091/175000: episode: 880, duration: 0.740s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 34.950 [19.000, 196.000], mean observation: 0.254 [0.000, 80.000], loss: 0.281435, mean_absolute_error: 0.477334, mean_q: 1.510640, mean_eps: 0.100000\n",
      "  31125/175000: episode: 881, duration: 0.624s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 52.824 [19.000, 224.000], mean observation: 0.222 [0.000, 68.000], loss: 0.863587, mean_absolute_error: 0.467201, mean_q: 1.469580, mean_eps: 0.100000\n",
      "  31138/175000: episode: 882, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.033 [0.000, 26.000], loss: 3.843583, mean_absolute_error: 0.582358, mean_q: 1.945393, mean_eps: 0.100000\n",
      "  31181/175000: episode: 883, duration: 0.774s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 29.930 [19.000, 156.000], mean observation: 0.225 [0.000, 86.000], loss: 1.546305, mean_absolute_error: 0.456436, mean_q: 1.710745, mean_eps: 0.100000\n",
      "  31229/175000: episode: 884, duration: 0.885s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 40.646 [16.000, 223.000], mean observation: 0.430 [0.000, 96.000], loss: 1.398746, mean_absolute_error: 0.433456, mean_q: 1.795279, mean_eps: 0.100000\n",
      "  31282/175000: episode: 885, duration: 1.167s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 66.189 [3.000, 216.000], mean observation: 0.652 [0.000, 106.000], loss: 1.788293, mean_absolute_error: 0.418707, mean_q: 1.862907, mean_eps: 0.100000\n",
      "  31308/175000: episode: 886, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 121.346 [18.000, 205.000], mean observation: 0.255 [0.000, 52.000], loss: 2.502336, mean_absolute_error: 0.430601, mean_q: 1.852657, mean_eps: 0.100000\n",
      "  31336/175000: episode: 887, duration: 0.541s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.571 [31.000, 203.000], mean observation: 0.229 [0.000, 56.000], loss: 0.687443, mean_absolute_error: 0.405458, mean_q: 1.760626, mean_eps: 0.100000\n",
      "  31359/175000: episode: 888, duration: 0.553s, episode steps: 23, steps per second: 42, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.913 [7.000, 204.000], mean observation: 0.094 [0.000, 46.000], loss: 1.630010, mean_absolute_error: 0.390277, mean_q: 1.687604, mean_eps: 0.100000\n",
      "  31390/175000: episode: 889, duration: 0.666s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.774 [7.000, 216.000], mean observation: 0.175 [0.000, 62.000], loss: 1.510445, mean_absolute_error: 0.410700, mean_q: 1.845908, mean_eps: 0.100000\n",
      "  31441/175000: episode: 890, duration: 0.929s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 156.039 [1.000, 212.000], mean observation: 0.565 [0.000, 102.000], loss: 0.561377, mean_absolute_error: 0.366300, mean_q: 1.713715, mean_eps: 0.100000\n",
      "  31485/175000: episode: 891, duration: 0.797s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.136 [11.000, 198.000], mean observation: 0.357 [0.000, 88.000], loss: 0.673640, mean_absolute_error: 0.410277, mean_q: 1.969061, mean_eps: 0.100000\n",
      "  31514/175000: episode: 892, duration: 0.513s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 88.483 [29.000, 158.000], mean observation: 0.201 [0.000, 58.000], loss: 0.720205, mean_absolute_error: 0.380574, mean_q: 1.870453, mean_eps: 0.100000\n",
      "  31541/175000: episode: 893, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 68.370 [3.000, 207.000], mean observation: 0.179 [0.000, 54.000], loss: 0.580814, mean_absolute_error: 0.410364, mean_q: 1.904515, mean_eps: 0.100000\n",
      "  31570/175000: episode: 894, duration: 0.506s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 51.241 [19.000, 202.000], mean observation: 0.110 [0.000, 58.000], loss: 0.247488, mean_absolute_error: 0.380855, mean_q: 1.577983, mean_eps: 0.100000\n",
      "  31602/175000: episode: 895, duration: 0.564s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.250 [19.000, 216.000], mean observation: 0.212 [0.000, 64.000], loss: 0.829138, mean_absolute_error: 0.464740, mean_q: 1.858201, mean_eps: 0.100000\n",
      "  31627/175000: episode: 896, duration: 0.437s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 63.080 [7.000, 187.000], mean observation: 0.097 [0.000, 50.000], loss: 3.296956, mean_absolute_error: 0.477102, mean_q: 1.893784, mean_eps: 0.100000\n",
      "  31666/175000: episode: 897, duration: 0.759s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.821 [19.000, 187.000], mean observation: 0.272 [0.000, 78.000], loss: 1.681027, mean_absolute_error: 0.451883, mean_q: 1.888267, mean_eps: 0.100000\n",
      "  31697/175000: episode: 898, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 74.161 [12.000, 217.000], mean observation: 0.224 [0.000, 62.000], loss: 2.184688, mean_absolute_error: 0.415611, mean_q: 1.845437, mean_eps: 0.100000\n",
      "  31742/175000: episode: 899, duration: 0.903s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 48.867 [12.000, 158.000], mean observation: 0.191 [0.000, 90.000], loss: 1.378907, mean_absolute_error: 0.404081, mean_q: 1.876229, mean_eps: 0.100000\n",
      "  31794/175000: episode: 900, duration: 0.919s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 111.231 [12.000, 223.000], mean observation: 0.453 [0.000, 104.000], loss: 3.040295, mean_absolute_error: 0.433794, mean_q: 1.967214, mean_eps: 0.100000\n",
      "  31807/175000: episode: 901, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 50.769 [39.000, 192.000], mean observation: 0.035 [0.000, 26.000], loss: 0.282629, mean_absolute_error: 0.419968, mean_q: 1.872801, mean_eps: 0.100000\n",
      "  31827/175000: episode: 902, duration: 0.362s, episode steps: 20, steps per second: 55, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 51.850 [39.000, 207.000], mean observation: 0.060 [0.000, 40.000], loss: 1.373142, mean_absolute_error: 0.466934, mean_q: 2.241988, mean_eps: 0.100000\n",
      "  31869/175000: episode: 903, duration: 0.825s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.286 [39.000, 189.000], mean observation: 0.299 [0.000, 84.000], loss: 0.922163, mean_absolute_error: 0.444588, mean_q: 2.040029, mean_eps: 0.100000\n",
      "  31897/175000: episode: 904, duration: 0.488s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.857 [19.000, 158.000], mean observation: 0.288 [0.000, 56.000], loss: 2.583693, mean_absolute_error: 0.592023, mean_q: 2.763897, mean_eps: 0.100000\n",
      "  31944/175000: episode: 905, duration: 0.820s, episode steps: 47, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 66.787 [28.000, 177.000], mean observation: 0.369 [0.000, 94.000], loss: 0.187373, mean_absolute_error: 0.457651, mean_q: 2.154426, mean_eps: 0.100000\n",
      "  31993/175000: episode: 906, duration: 0.959s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 120.837 [13.000, 218.000], mean observation: 0.372 [0.000, 98.000], loss: 2.568782, mean_absolute_error: 0.517128, mean_q: 2.446536, mean_eps: 0.100000\n",
      "  32007/175000: episode: 907, duration: 0.246s, episode steps: 14, steps per second: 57, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 108.143 [76.000, 170.000], mean observation: 0.051 [0.000, 28.000], loss: 1.630324, mean_absolute_error: 0.468116, mean_q: 2.275265, mean_eps: 0.100000\n",
      "  32020/175000: episode: 908, duration: 0.271s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 106.692 [76.000, 133.000], mean observation: 0.040 [0.000, 26.000], loss: 3.986799, mean_absolute_error: 0.605192, mean_q: 2.891181, mean_eps: 0.100000\n",
      "  32050/175000: episode: 909, duration: 0.581s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 129.533 [13.000, 205.000], mean observation: 0.267 [0.000, 60.000], loss: 0.890589, mean_absolute_error: 0.433316, mean_q: 2.180137, mean_eps: 0.100000\n",
      "  32081/175000: episode: 910, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 109.871 [13.000, 202.000], mean observation: 0.174 [0.000, 62.000], loss: 1.337349, mean_absolute_error: 0.462603, mean_q: 2.458629, mean_eps: 0.100000\n",
      "  32119/175000: episode: 911, duration: 0.697s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.000 [43.000, 202.000], mean observation: 0.289 [0.000, 76.000], loss: 0.673202, mean_absolute_error: 0.428848, mean_q: 2.165876, mean_eps: 0.100000\n",
      "  32154/175000: episode: 912, duration: 0.636s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.771 [6.000, 211.000], mean observation: 0.164 [0.000, 70.000], loss: 1.191694, mean_absolute_error: 0.429834, mean_q: 2.082751, mean_eps: 0.100000\n",
      "  32187/175000: episode: 913, duration: 0.560s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 186.879 [74.000, 222.000], mean observation: 0.222 [0.000, 66.000], loss: 0.525276, mean_absolute_error: 0.403728, mean_q: 2.024873, mean_eps: 0.100000\n",
      "  32224/175000: episode: 914, duration: 0.698s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.135 [13.000, 211.000], mean observation: 0.241 [0.000, 74.000], loss: 0.358763, mean_absolute_error: 0.391642, mean_q: 2.036756, mean_eps: 0.100000\n",
      "  32272/175000: episode: 915, duration: 0.936s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 182.812 [13.000, 224.000], mean observation: 0.344 [0.000, 96.000], loss: 1.095095, mean_absolute_error: 0.379629, mean_q: 2.008066, mean_eps: 0.100000\n",
      "  32308/175000: episode: 916, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 174.778 [13.000, 219.000], mean observation: 0.112 [0.000, 72.000], loss: 2.427423, mean_absolute_error: 0.395827, mean_q: 2.122543, mean_eps: 0.100000\n",
      "  32334/175000: episode: 917, duration: 0.494s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.962 [13.000, 216.000], mean observation: 0.113 [0.000, 52.000], loss: 0.847572, mean_absolute_error: 0.376633, mean_q: 2.001126, mean_eps: 0.100000\n",
      "  32380/175000: episode: 918, duration: 0.872s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 195.087 [86.000, 216.000], mean observation: 0.461 [0.000, 92.000], loss: 3.438962, mean_absolute_error: 0.401960, mean_q: 2.151839, mean_eps: 0.100000\n",
      "  32427/175000: episode: 919, duration: 0.866s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 167.915 [3.000, 216.000], mean observation: 0.455 [0.000, 94.000], loss: 0.324996, mean_absolute_error: 0.382537, mean_q: 2.133870, mean_eps: 0.100000\n",
      "  32478/175000: episode: 920, duration: 1.040s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 132.549 [7.000, 223.000], mean observation: 0.419 [0.000, 102.000], loss: 1.965153, mean_absolute_error: 0.404441, mean_q: 2.218567, mean_eps: 0.100000\n",
      "  32502/175000: episode: 921, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 19.042 [7.000, 153.000], mean observation: 0.101 [0.000, 48.000], loss: 3.820039, mean_absolute_error: 0.474495, mean_q: 2.388373, mean_eps: 0.100000\n",
      "  32538/175000: episode: 922, duration: 0.696s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 96.694 [13.000, 209.000], mean observation: 0.246 [0.000, 72.000], loss: 1.522788, mean_absolute_error: 0.483255, mean_q: 2.421044, mean_eps: 0.100000\n",
      "  32573/175000: episode: 923, duration: 0.758s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.457 [13.000, 193.000], mean observation: 0.258 [0.000, 70.000], loss: 2.554104, mean_absolute_error: 0.462144, mean_q: 2.258373, mean_eps: 0.100000\n",
      "  32615/175000: episode: 924, duration: 0.935s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.238 [13.000, 209.000], mean observation: 0.264 [0.000, 84.000], loss: 4.079457, mean_absolute_error: 0.499526, mean_q: 2.392747, mean_eps: 0.100000\n",
      "  32659/175000: episode: 925, duration: 1.000s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.659 [13.000, 189.000], mean observation: 0.381 [0.000, 88.000], loss: 2.870590, mean_absolute_error: 0.470398, mean_q: 2.408092, mean_eps: 0.100000\n",
      "  32714/175000: episode: 926, duration: 1.170s, episode steps: 55, steps per second: 47, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.218 [13.000, 189.000], mean observation: 0.474 [0.000, 110.000], loss: 2.023482, mean_absolute_error: 0.430027, mean_q: 2.377982, mean_eps: 0.100000\n",
      "  32745/175000: episode: 927, duration: 0.726s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.548 [39.000, 223.000], mean observation: 0.146 [0.000, 62.000], loss: 1.455780, mean_absolute_error: 0.405952, mean_q: 2.189995, mean_eps: 0.100000\n",
      "  32767/175000: episode: 928, duration: 0.412s, episode steps: 22, steps per second: 53, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 129.682 [15.000, 223.000], mean observation: 0.139 [0.000, 44.000], loss: 1.072769, mean_absolute_error: 0.467517, mean_q: 2.396438, mean_eps: 0.100000\n",
      "  32798/175000: episode: 929, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.194 [1.000, 162.000], mean observation: 0.150 [0.000, 62.000], loss: 1.202317, mean_absolute_error: 0.463758, mean_q: 2.312006, mean_eps: 0.100000\n",
      "  32839/175000: episode: 930, duration: 0.759s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.610 [22.000, 219.000], mean observation: 0.442 [0.000, 82.000], loss: 1.846961, mean_absolute_error: 0.477603, mean_q: 2.373224, mean_eps: 0.100000\n",
      "  32867/175000: episode: 931, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 99.607 [54.000, 204.000], mean observation: 0.170 [0.000, 56.000], loss: 0.420587, mean_absolute_error: 0.432785, mean_q: 2.090994, mean_eps: 0.100000\n",
      "  32904/175000: episode: 932, duration: 0.754s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.514 [56.000, 214.000], mean observation: 0.238 [0.000, 74.000], loss: 0.992074, mean_absolute_error: 0.431066, mean_q: 1.965223, mean_eps: 0.100000\n",
      "  32941/175000: episode: 933, duration: 0.764s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.676 [56.000, 213.000], mean observation: 0.215 [0.000, 74.000], loss: 1.698473, mean_absolute_error: 0.427392, mean_q: 1.745288, mean_eps: 0.100000\n",
      "  32974/175000: episode: 934, duration: 0.633s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 61.424 [22.000, 140.000], mean observation: 0.134 [0.000, 66.000], loss: 3.304379, mean_absolute_error: 0.415428, mean_q: 1.817175, mean_eps: 0.100000\n",
      "  33025/175000: episode: 935, duration: 1.122s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.804 [22.000, 219.000], mean observation: 0.308 [0.000, 102.000], loss: 0.598485, mean_absolute_error: 0.395026, mean_q: 1.916214, mean_eps: 0.100000\n",
      "  33061/175000: episode: 936, duration: 0.706s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 116.028 [2.000, 197.000], mean observation: 0.271 [0.000, 72.000], loss: 0.264061, mean_absolute_error: 0.346188, mean_q: 1.634527, mean_eps: 0.100000\n",
      "  33091/175000: episode: 937, duration: 0.570s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 164.300 [138.000, 195.000], mean observation: 0.084 [0.000, 60.000], loss: 0.699315, mean_absolute_error: 0.389704, mean_q: 1.822081, mean_eps: 0.100000\n",
      "  33110/175000: episode: 938, duration: 0.374s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 125.789 [74.000, 198.000], mean observation: 0.102 [0.000, 38.000], loss: 1.041271, mean_absolute_error: 0.396829, mean_q: 1.720405, mean_eps: 0.100000\n",
      "  33128/175000: episode: 939, duration: 0.381s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 134.444 [13.000, 190.000], mean observation: 0.112 [0.000, 36.000], loss: 3.214460, mean_absolute_error: 0.483182, mean_q: 2.135416, mean_eps: 0.100000\n",
      "  33176/175000: episode: 940, duration: 1.063s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 140.333 [13.000, 222.000], mean observation: 0.601 [0.000, 96.000], loss: 2.046392, mean_absolute_error: 0.429328, mean_q: 1.974891, mean_eps: 0.100000\n",
      "  33212/175000: episode: 941, duration: 0.776s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.472 [15.000, 118.000], mean observation: 0.148 [0.000, 72.000], loss: 1.819939, mean_absolute_error: 0.448636, mean_q: 2.325254, mean_eps: 0.100000\n",
      "  33257/175000: episode: 942, duration: 1.012s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.422 [23.000, 213.000], mean observation: 0.241 [0.000, 90.000], loss: 2.171712, mean_absolute_error: 0.423240, mean_q: 2.156290, mean_eps: 0.100000\n",
      "  33306/175000: episode: 943, duration: 0.923s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 131.531 [66.000, 198.000], mean observation: 0.326 [0.000, 98.000], loss: 0.898385, mean_absolute_error: 0.374689, mean_q: 1.891218, mean_eps: 0.100000\n",
      "  33347/175000: episode: 944, duration: 0.778s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.634 [27.000, 198.000], mean observation: 0.291 [0.000, 82.000], loss: 1.530292, mean_absolute_error: 0.397354, mean_q: 1.922460, mean_eps: 0.100000\n",
      "  33394/175000: episode: 945, duration: 0.984s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 14.809 [9.000, 210.000], mean observation: 0.144 [0.000, 94.000], loss: 1.992885, mean_absolute_error: 0.397914, mean_q: 2.021141, mean_eps: 0.100000\n",
      "  33414/175000: episode: 946, duration: 0.394s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 30.150 [9.000, 222.000], mean observation: 0.073 [0.000, 40.000], loss: 0.101832, mean_absolute_error: 0.366812, mean_q: 1.845405, mean_eps: 0.100000\n",
      "  33448/175000: episode: 947, duration: 0.722s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 89.676 [8.000, 164.000], mean observation: 0.241 [0.000, 68.000], loss: 0.967819, mean_absolute_error: 0.380155, mean_q: 1.852926, mean_eps: 0.100000\n",
      "  33473/175000: episode: 948, duration: 0.539s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 115.560 [56.000, 204.000], mean observation: 0.159 [0.000, 50.000], loss: 1.915869, mean_absolute_error: 0.387348, mean_q: 2.023424, mean_eps: 0.100000\n",
      "  33513/175000: episode: 949, duration: 0.765s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 85.225 [17.000, 191.000], mean observation: 0.306 [0.000, 80.000], loss: 3.559324, mean_absolute_error: 0.415256, mean_q: 2.154258, mean_eps: 0.100000\n",
      "  33555/175000: episode: 950, duration: 0.840s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.357 [18.000, 213.000], mean observation: 0.138 [0.000, 84.000], loss: 0.557298, mean_absolute_error: 0.316932, mean_q: 1.758578, mean_eps: 0.100000\n",
      "  33586/175000: episode: 951, duration: 0.757s, episode steps: 31, steps per second: 41, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 79.032 [0.000, 187.000], mean observation: 0.138 [0.000, 62.000], loss: 1.375399, mean_absolute_error: 0.386175, mean_q: 2.099490, mean_eps: 0.100000\n",
      "  33595/175000: episode: 952, duration: 0.215s, episode steps: 9, steps per second: 42, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 143.444 [75.000, 152.000], mean observation: 0.037 [0.000, 18.000], loss: 1.379974, mean_absolute_error: 0.391751, mean_q: 2.140618, mean_eps: 0.100000\n",
      "  33643/175000: episode: 953, duration: 1.067s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.146 [16.000, 177.000], mean observation: 0.225 [0.000, 96.000], loss: 2.949286, mean_absolute_error: 0.419061, mean_q: 2.164119, mean_eps: 0.100000\n",
      "  33670/175000: episode: 954, duration: 0.650s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 55.407 [24.000, 203.000], mean observation: 0.097 [0.000, 54.000], loss: 0.744269, mean_absolute_error: 0.382285, mean_q: 1.845433, mean_eps: 0.100000\n",
      "  33716/175000: episode: 955, duration: 1.110s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.565 [0.000, 189.000], mean observation: 0.336 [0.000, 92.000], loss: 0.890053, mean_absolute_error: 0.409628, mean_q: 2.025324, mean_eps: 0.100000\n",
      "  33761/175000: episode: 956, duration: 1.180s, episode steps: 45, steps per second: 38, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 95.333 [25.000, 208.000], mean observation: 0.271 [0.000, 90.000], loss: 0.105288, mean_absolute_error: 0.335992, mean_q: 1.775765, mean_eps: 0.100000\n",
      "  33797/175000: episode: 957, duration: 0.859s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 86.500 [10.000, 209.000], mean observation: 0.214 [0.000, 72.000], loss: 4.423489, mean_absolute_error: 0.390258, mean_q: 1.975487, mean_eps: 0.100000\n",
      "  33824/175000: episode: 958, duration: 0.753s, episode steps: 27, steps per second: 36, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.444 [3.000, 209.000], mean observation: 0.115 [0.000, 54.000], loss: 0.740320, mean_absolute_error: 0.363813, mean_q: 1.920976, mean_eps: 0.100000\n",
      "  33866/175000: episode: 959, duration: 1.078s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 78.286 [28.000, 208.000], mean observation: 0.224 [0.000, 84.000], loss: 1.301539, mean_absolute_error: 0.418026, mean_q: 2.035448, mean_eps: 0.100000\n",
      "  33917/175000: episode: 960, duration: 1.087s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.235 [8.000, 212.000], mean observation: 0.478 [0.000, 102.000], loss: 3.130201, mean_absolute_error: 0.521646, mean_q: 2.248283, mean_eps: 0.100000\n",
      "  33956/175000: episode: 961, duration: 0.787s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.231 [10.000, 145.000], mean observation: 0.240 [0.000, 78.000], loss: 2.632580, mean_absolute_error: 0.484820, mean_q: 2.377787, mean_eps: 0.100000\n",
      "  33989/175000: episode: 962, duration: 0.755s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.030 [50.000, 141.000], mean observation: 0.287 [0.000, 66.000], loss: 0.959975, mean_absolute_error: 0.427502, mean_q: 2.369527, mean_eps: 0.100000\n",
      "  34013/175000: episode: 963, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 107.125 [25.000, 125.000], mean observation: 0.146 [0.000, 48.000], loss: 0.488703, mean_absolute_error: 0.413659, mean_q: 2.335108, mean_eps: 0.100000\n",
      "  34046/175000: episode: 964, duration: 0.647s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.848 [43.000, 161.000], mean observation: 0.241 [0.000, 66.000], loss: 0.342007, mean_absolute_error: 0.436470, mean_q: 2.267998, mean_eps: 0.100000\n",
      "  34076/175000: episode: 965, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 106.233 [67.000, 222.000], mean observation: 0.243 [0.000, 60.000], loss: 0.865251, mean_absolute_error: 0.459048, mean_q: 2.295910, mean_eps: 0.100000\n",
      "  34124/175000: episode: 966, duration: 0.975s, episode steps: 48, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.979 [38.000, 209.000], mean observation: 0.265 [0.000, 96.000], loss: 0.492157, mean_absolute_error: 0.435148, mean_q: 2.133774, mean_eps: 0.100000\n",
      "  34160/175000: episode: 967, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 166.750 [55.000, 215.000], mean observation: 0.146 [0.000, 72.000], loss: 1.428366, mean_absolute_error: 0.443884, mean_q: 2.194524, mean_eps: 0.100000\n",
      "  34205/175000: episode: 968, duration: 0.884s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.956 [34.000, 204.000], mean observation: 0.412 [0.000, 90.000], loss: 0.524132, mean_absolute_error: 0.401585, mean_q: 1.973255, mean_eps: 0.100000\n",
      "  34267/175000: episode: 969, duration: 1.361s, episode steps: 62, steps per second: 46, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 138.790 [21.000, 221.000], mean observation: 0.786 [0.000, 124.000], loss: 0.625028, mean_absolute_error: 0.433933, mean_q: 2.220147, mean_eps: 0.100000\n",
      "  34305/175000: episode: 970, duration: 0.838s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 155.474 [15.000, 208.000], mean observation: 0.327 [0.000, 76.000], loss: 1.595600, mean_absolute_error: 0.451680, mean_q: 2.108759, mean_eps: 0.100000\n",
      "  34355/175000: episode: 971, duration: 0.967s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.700 [38.000, 214.000], mean observation: 0.392 [0.000, 100.000], loss: 0.358624, mean_absolute_error: 0.477170, mean_q: 2.270680, mean_eps: 0.100000\n",
      "  34396/175000: episode: 972, duration: 0.853s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 177.171 [48.000, 218.000], mean observation: 0.354 [0.000, 82.000], loss: 1.578184, mean_absolute_error: 0.436054, mean_q: 2.098200, mean_eps: 0.100000\n",
      "  34409/175000: episode: 973, duration: 0.283s, episode steps: 13, steps per second: 46, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 102.692 [63.000, 183.000], mean observation: 0.061 [0.000, 26.000], loss: 0.298775, mean_absolute_error: 0.354696, mean_q: 1.725728, mean_eps: 0.100000\n",
      "  34440/175000: episode: 974, duration: 0.679s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 143.806 [30.000, 186.000], mean observation: 0.216 [0.000, 62.000], loss: 0.612547, mean_absolute_error: 0.441715, mean_q: 2.234944, mean_eps: 0.100000\n",
      "  34465/175000: episode: 975, duration: 0.532s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 152.480 [19.000, 186.000], mean observation: 0.155 [0.000, 50.000], loss: 1.402055, mean_absolute_error: 0.374287, mean_q: 2.039159, mean_eps: 0.100000\n",
      "  34498/175000: episode: 976, duration: 0.685s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.091 [19.000, 209.000], mean observation: 0.275 [0.000, 66.000], loss: 0.585193, mean_absolute_error: 0.346413, mean_q: 2.005877, mean_eps: 0.100000\n",
      "  34537/175000: episode: 977, duration: 0.836s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.154 [19.000, 178.000], mean observation: 0.345 [0.000, 78.000], loss: 1.163560, mean_absolute_error: 0.351620, mean_q: 2.094034, mean_eps: 0.100000\n",
      "  34563/175000: episode: 978, duration: 0.623s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.692 [22.000, 179.000], mean observation: 0.192 [0.000, 52.000], loss: 0.625978, mean_absolute_error: 0.361461, mean_q: 2.079082, mean_eps: 0.100000\n",
      "  34599/175000: episode: 979, duration: 0.880s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 112.917 [26.000, 189.000], mean observation: 0.260 [0.000, 72.000], loss: 0.167984, mean_absolute_error: 0.339203, mean_q: 1.908597, mean_eps: 0.100000\n",
      "  34628/175000: episode: 980, duration: 0.628s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 95.103 [16.000, 175.000], mean observation: 0.260 [0.000, 58.000], loss: 1.016656, mean_absolute_error: 0.401911, mean_q: 2.197012, mean_eps: 0.100000\n",
      "  34658/175000: episode: 981, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 155.700 [19.000, 201.000], mean observation: 0.238 [0.000, 60.000], loss: 0.513021, mean_absolute_error: 0.372898, mean_q: 2.067816, mean_eps: 0.100000\n",
      "  34717/175000: episode: 982, duration: 1.194s, episode steps: 59, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.153 [19.000, 214.000], mean observation: 0.560 [0.000, 118.000], loss: 0.771898, mean_absolute_error: 0.351961, mean_q: 2.152541, mean_eps: 0.100000\n",
      "  34759/175000: episode: 983, duration: 0.913s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.857 [52.000, 179.000], mean observation: 0.346 [0.000, 84.000], loss: 1.134739, mean_absolute_error: 0.384117, mean_q: 2.351948, mean_eps: 0.100000\n",
      "  34800/175000: episode: 984, duration: 0.962s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.805 [42.000, 220.000], mean observation: 0.447 [0.000, 82.000], loss: 1.762310, mean_absolute_error: 0.388779, mean_q: 2.363724, mean_eps: 0.100000\n",
      "  34838/175000: episode: 985, duration: 0.863s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.395 [19.000, 184.000], mean observation: 0.360 [0.000, 76.000], loss: 0.396329, mean_absolute_error: 0.354448, mean_q: 1.875746, mean_eps: 0.100000\n",
      "  34862/175000: episode: 986, duration: 0.518s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 160.042 [16.000, 202.000], mean observation: 0.144 [0.000, 48.000], loss: 1.532184, mean_absolute_error: 0.357838, mean_q: 1.847799, mean_eps: 0.100000\n",
      "  34884/175000: episode: 987, duration: 0.479s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 120.773 [35.000, 200.000], mean observation: 0.093 [0.000, 44.000], loss: 0.860185, mean_absolute_error: 0.361303, mean_q: 1.914819, mean_eps: 0.100000\n",
      "  34930/175000: episode: 988, duration: 1.010s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.217 [30.000, 180.000], mean observation: 0.162 [0.000, 92.000], loss: 2.234085, mean_absolute_error: 0.397807, mean_q: 2.239561, mean_eps: 0.100000\n",
      "  34958/175000: episode: 989, duration: 0.675s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.786 [62.000, 150.000], mean observation: 0.135 [0.000, 56.000], loss: 0.976727, mean_absolute_error: 0.395214, mean_q: 2.065568, mean_eps: 0.100000\n",
      "  34999/175000: episode: 990, duration: 0.828s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.829 [8.000, 197.000], mean observation: 0.483 [0.000, 82.000], loss: 0.413731, mean_absolute_error: 0.402991, mean_q: 1.921232, mean_eps: 0.100000\n",
      "  35035/175000: episode: 991, duration: 0.743s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.028 [23.000, 218.000], mean observation: 0.263 [0.000, 72.000], loss: 0.177740, mean_absolute_error: 0.402736, mean_q: 1.737930, mean_eps: 0.100000\n",
      "  35065/175000: episode: 992, duration: 0.721s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 100.533 [12.000, 218.000], mean observation: 0.237 [0.000, 60.000], loss: 0.195019, mean_absolute_error: 0.371996, mean_q: 1.633163, mean_eps: 0.100000\n",
      "  35094/175000: episode: 993, duration: 0.599s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 89.172 [7.000, 119.000], mean observation: 0.238 [0.000, 58.000], loss: 0.081360, mean_absolute_error: 0.351968, mean_q: 1.582228, mean_eps: 0.100000\n",
      "  35121/175000: episode: 994, duration: 0.573s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.037 [36.000, 197.000], mean observation: 0.227 [0.000, 54.000], loss: 0.621498, mean_absolute_error: 0.378186, mean_q: 1.696476, mean_eps: 0.100000\n",
      "  35179/175000: episode: 995, duration: 1.270s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 108.810 [28.000, 197.000], mean observation: 0.673 [0.000, 116.000], loss: 0.534732, mean_absolute_error: 0.351964, mean_q: 1.601677, mean_eps: 0.100000\n",
      "  35205/175000: episode: 996, duration: 0.563s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.269 [49.000, 140.000], mean observation: 0.121 [0.000, 52.000], loss: 3.210087, mean_absolute_error: 0.435782, mean_q: 1.936990, mean_eps: 0.100000\n",
      "  35238/175000: episode: 997, duration: 0.681s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.394 [19.000, 221.000], mean observation: 0.179 [0.000, 66.000], loss: 0.415744, mean_absolute_error: 0.417165, mean_q: 1.683759, mean_eps: 0.100000\n",
      "  35267/175000: episode: 998, duration: 0.653s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 143.448 [19.000, 184.000], mean observation: 0.142 [0.000, 58.000], loss: 1.150902, mean_absolute_error: 0.439039, mean_q: 1.707491, mean_eps: 0.100000\n",
      "  35300/175000: episode: 999, duration: 0.792s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.667 [49.000, 158.000], mean observation: 0.128 [0.000, 66.000], loss: 0.758791, mean_absolute_error: 0.431799, mean_q: 1.500425, mean_eps: 0.100000\n",
      "  35345/175000: episode: 1000, duration: 1.139s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 136.267 [1.000, 220.000], mean observation: 0.537 [0.000, 90.000], loss: 1.528182, mean_absolute_error: 0.469986, mean_q: 1.651931, mean_eps: 0.100000\n",
      "  35361/175000: episode: 1001, duration: 0.294s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 136.500 [58.000, 161.000], mean observation: 0.096 [0.000, 32.000], loss: 0.684301, mean_absolute_error: 0.391601, mean_q: 1.444103, mean_eps: 0.100000\n",
      "  35386/175000: episode: 1002, duration: 0.503s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 217.560 [155.000, 224.000], mean observation: 0.084 [0.000, 50.000], loss: 0.470023, mean_absolute_error: 0.394704, mean_q: 1.607464, mean_eps: 0.100000\n",
      "  35414/175000: episode: 1003, duration: 0.597s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 169.607 [12.000, 220.000], mean observation: 0.093 [0.000, 56.000], loss: 3.867312, mean_absolute_error: 0.465507, mean_q: 1.891151, mean_eps: 0.100000\n",
      "  35459/175000: episode: 1004, duration: 1.097s, episode steps: 45, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.911 [10.000, 212.000], mean observation: 0.395 [0.000, 90.000], loss: 3.975795, mean_absolute_error: 0.460414, mean_q: 1.850776, mean_eps: 0.100000\n",
      "  35510/175000: episode: 1005, duration: 1.310s, episode steps: 51, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 138.098 [16.000, 212.000], mean observation: 0.430 [0.000, 102.000], loss: 1.842816, mean_absolute_error: 0.420222, mean_q: 1.942713, mean_eps: 0.100000\n",
      "  35538/175000: episode: 1006, duration: 0.557s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 146.786 [16.000, 204.000], mean observation: 0.158 [0.000, 56.000], loss: 3.293489, mean_absolute_error: 0.537351, mean_q: 2.479485, mean_eps: 0.100000\n",
      "  35575/175000: episode: 1007, duration: 0.704s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 202.541 [98.000, 208.000], mean observation: 0.099 [0.000, 74.000], loss: 1.231824, mean_absolute_error: 0.437317, mean_q: 1.995783, mean_eps: 0.100000\n",
      "  35618/175000: episode: 1008, duration: 0.848s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 196.674 [16.000, 219.000], mean observation: 0.366 [0.000, 86.000], loss: 0.587183, mean_absolute_error: 0.378611, mean_q: 1.999849, mean_eps: 0.100000\n",
      "  35652/175000: episode: 1009, duration: 0.692s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.529 [16.000, 208.000], mean observation: 0.360 [0.000, 68.000], loss: 0.998633, mean_absolute_error: 0.401900, mean_q: 2.033626, mean_eps: 0.100000\n",
      "  35695/175000: episode: 1010, duration: 0.963s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 79.698 [0.000, 188.000], mean observation: 0.385 [0.000, 86.000], loss: 0.919840, mean_absolute_error: 0.402739, mean_q: 2.117695, mean_eps: 0.100000\n",
      "  35727/175000: episode: 1011, duration: 0.673s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 83.344 [10.000, 220.000], mean observation: 0.161 [0.000, 64.000], loss: 0.664951, mean_absolute_error: 0.397525, mean_q: 2.050117, mean_eps: 0.100000\n",
      "  35772/175000: episode: 1012, duration: 0.886s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 155.022 [12.000, 208.000], mean observation: 0.354 [0.000, 90.000], loss: 4.439710, mean_absolute_error: 0.501513, mean_q: 2.343568, mean_eps: 0.100000\n",
      "  35803/175000: episode: 1013, duration: 0.636s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 82.226 [42.000, 216.000], mean observation: 0.139 [0.000, 62.000], loss: 0.795488, mean_absolute_error: 0.408855, mean_q: 2.002181, mean_eps: 0.100000\n",
      "  35827/175000: episode: 1014, duration: 0.524s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 69.958 [32.000, 213.000], mean observation: 0.071 [0.000, 48.000], loss: 0.539637, mean_absolute_error: 0.431491, mean_q: 2.145693, mean_eps: 0.100000\n",
      "  35857/175000: episode: 1015, duration: 0.668s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.633 [43.000, 219.000], mean observation: 0.200 [0.000, 60.000], loss: 0.430246, mean_absolute_error: 0.408433, mean_q: 2.094347, mean_eps: 0.100000\n",
      "  35898/175000: episode: 1016, duration: 0.799s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.780 [14.000, 191.000], mean observation: 0.398 [0.000, 82.000], loss: 0.499904, mean_absolute_error: 0.393329, mean_q: 1.995600, mean_eps: 0.100000\n",
      "  35922/175000: episode: 1017, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 114.542 [42.000, 163.000], mean observation: 0.133 [0.000, 48.000], loss: 0.632761, mean_absolute_error: 0.412490, mean_q: 2.072085, mean_eps: 0.100000\n",
      "  35967/175000: episode: 1018, duration: 0.879s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 58.556 [3.000, 206.000], mean observation: 0.376 [0.000, 90.000], loss: 0.803012, mean_absolute_error: 0.450754, mean_q: 2.090617, mean_eps: 0.100000\n",
      "  36005/175000: episode: 1019, duration: 0.800s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.947 [15.000, 209.000], mean observation: 0.341 [0.000, 76.000], loss: 0.253741, mean_absolute_error: 0.396244, mean_q: 1.695365, mean_eps: 0.100000\n",
      "  36039/175000: episode: 1020, duration: 0.685s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.794 [19.000, 142.000], mean observation: 0.233 [0.000, 68.000], loss: 0.805728, mean_absolute_error: 0.371179, mean_q: 1.701428, mean_eps: 0.100000\n",
      "  36075/175000: episode: 1021, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 63.639 [19.000, 218.000], mean observation: 0.162 [0.000, 72.000], loss: 1.254946, mean_absolute_error: 0.383451, mean_q: 1.838298, mean_eps: 0.100000\n",
      "  36123/175000: episode: 1022, duration: 0.907s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 46.375 [7.000, 197.000], mean observation: 0.365 [0.000, 96.000], loss: 1.569841, mean_absolute_error: 0.373314, mean_q: 1.877793, mean_eps: 0.100000\n",
      "  36156/175000: episode: 1023, duration: 0.665s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.061 [19.000, 183.000], mean observation: 0.137 [0.000, 66.000], loss: 1.277883, mean_absolute_error: 0.415079, mean_q: 1.989740, mean_eps: 0.100000\n",
      "  36181/175000: episode: 1024, duration: 0.547s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 95.920 [6.000, 153.000], mean observation: 0.178 [0.000, 50.000], loss: 1.653713, mean_absolute_error: 0.443082, mean_q: 1.961375, mean_eps: 0.100000\n",
      "  36221/175000: episode: 1025, duration: 0.858s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 102.650 [6.000, 151.000], mean observation: 0.230 [0.000, 80.000], loss: 1.989866, mean_absolute_error: 0.405707, mean_q: 1.943424, mean_eps: 0.100000\n",
      "  36260/175000: episode: 1026, duration: 0.793s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.205 [21.000, 175.000], mean observation: 0.243 [0.000, 78.000], loss: 1.142394, mean_absolute_error: 0.395862, mean_q: 1.940066, mean_eps: 0.100000\n",
      "  36298/175000: episode: 1027, duration: 0.821s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.526 [19.000, 222.000], mean observation: 0.277 [0.000, 76.000], loss: 1.146935, mean_absolute_error: 0.465947, mean_q: 2.203907, mean_eps: 0.100000\n",
      "  36332/175000: episode: 1028, duration: 0.737s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.676 [98.000, 222.000], mean observation: 0.165 [0.000, 68.000], loss: 0.844429, mean_absolute_error: 0.518575, mean_q: 2.531006, mean_eps: 0.100000\n",
      "  36357/175000: episode: 1029, duration: 0.529s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 157.040 [108.000, 203.000], mean observation: 0.093 [0.000, 50.000], loss: 2.712969, mean_absolute_error: 0.490183, mean_q: 2.224808, mean_eps: 0.100000\n",
      "  36378/175000: episode: 1030, duration: 0.471s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 134.190 [66.000, 213.000], mean observation: 0.135 [0.000, 42.000], loss: 3.855714, mean_absolute_error: 0.488624, mean_q: 2.198656, mean_eps: 0.100000\n",
      "  36424/175000: episode: 1031, duration: 1.016s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.891 [66.000, 191.000], mean observation: 0.306 [0.000, 92.000], loss: 0.246908, mean_absolute_error: 0.394355, mean_q: 1.614681, mean_eps: 0.100000\n",
      "  36451/175000: episode: 1032, duration: 0.574s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.222 [9.000, 180.000], mean observation: 0.144 [0.000, 54.000], loss: 2.308978, mean_absolute_error: 0.456342, mean_q: 1.954949, mean_eps: 0.100000\n",
      "  36494/175000: episode: 1033, duration: 0.955s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 146.605 [3.000, 223.000], mean observation: 0.405 [0.000, 86.000], loss: 1.167455, mean_absolute_error: 0.423130, mean_q: 1.730864, mean_eps: 0.100000\n",
      "  36540/175000: episode: 1034, duration: 0.956s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 144.413 [20.000, 213.000], mean observation: 0.395 [0.000, 92.000], loss: 2.653200, mean_absolute_error: 0.474460, mean_q: 1.970847, mean_eps: 0.100000\n",
      "  36582/175000: episode: 1035, duration: 0.884s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.190 [115.000, 191.000], mean observation: 0.280 [0.000, 84.000], loss: 0.184413, mean_absolute_error: 0.405855, mean_q: 1.849759, mean_eps: 0.100000\n",
      "  36621/175000: episode: 1036, duration: 0.794s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 129.179 [49.000, 217.000], mean observation: 0.301 [0.000, 78.000], loss: 0.355859, mean_absolute_error: 0.398853, mean_q: 1.832207, mean_eps: 0.100000\n",
      "  36666/175000: episode: 1037, duration: 0.888s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 120.178 [19.000, 199.000], mean observation: 0.273 [0.000, 90.000], loss: 0.333741, mean_absolute_error: 0.375047, mean_q: 1.660595, mean_eps: 0.100000\n",
      "  36698/175000: episode: 1038, duration: 0.655s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 114.406 [19.000, 193.000], mean observation: 0.203 [0.000, 64.000], loss: 0.694983, mean_absolute_error: 0.386946, mean_q: 1.702139, mean_eps: 0.100000\n",
      "  36728/175000: episode: 1039, duration: 0.695s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.267 [2.000, 173.000], mean observation: 0.163 [0.000, 60.000], loss: 2.117112, mean_absolute_error: 0.503039, mean_q: 2.153629, mean_eps: 0.100000\n",
      "  36758/175000: episode: 1040, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 143.233 [1.000, 214.000], mean observation: 0.330 [0.000, 60.000], loss: 0.200798, mean_absolute_error: 0.418815, mean_q: 1.859711, mean_eps: 0.100000\n",
      "  36787/175000: episode: 1041, duration: 0.552s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 36.931 [16.000, 174.000], mean observation: 0.114 [0.000, 58.000], loss: 1.392774, mean_absolute_error: 0.408944, mean_q: 1.859578, mean_eps: 0.100000\n",
      "  36830/175000: episode: 1042, duration: 0.879s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.233 [0.000, 179.000], mean observation: 0.267 [0.000, 86.000], loss: 2.496085, mean_absolute_error: 0.421245, mean_q: 2.069841, mean_eps: 0.100000\n",
      "  36855/175000: episode: 1043, duration: 0.482s, episode steps: 25, steps per second: 52, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 93.560 [0.000, 215.000], mean observation: 0.177 [0.000, 50.000], loss: 2.770767, mean_absolute_error: 0.477780, mean_q: 2.264618, mean_eps: 0.100000\n",
      "  36889/175000: episode: 1044, duration: 0.772s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.471 [15.000, 190.000], mean observation: 0.196 [0.000, 68.000], loss: 0.261165, mean_absolute_error: 0.381675, mean_q: 1.848793, mean_eps: 0.100000\n",
      "  36911/175000: episode: 1045, duration: 0.433s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 71.182 [13.000, 140.000], mean observation: 0.105 [0.000, 44.000], loss: 0.807369, mean_absolute_error: 0.410905, mean_q: 1.895268, mean_eps: 0.100000\n",
      "  36944/175000: episode: 1046, duration: 0.744s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 86.636 [13.000, 213.000], mean observation: 0.154 [0.000, 66.000], loss: 1.371806, mean_absolute_error: 0.443163, mean_q: 2.005339, mean_eps: 0.100000\n",
      "  37010/175000: episode: 1047, duration: 1.324s, episode steps: 66, steps per second: 50, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 126.894 [13.000, 218.000], mean observation: 0.849 [0.000, 132.000], loss: 1.186167, mean_absolute_error: 0.436103, mean_q: 2.022280, mean_eps: 0.100000\n",
      "  37067/175000: episode: 1048, duration: 1.105s, episode steps: 57, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 146.316 [16.000, 215.000], mean observation: 0.606 [0.000, 114.000], loss: 0.403155, mean_absolute_error: 0.406909, mean_q: 1.986429, mean_eps: 0.100000\n",
      "  37109/175000: episode: 1049, duration: 0.854s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 130.000 [1.000, 209.000], mean observation: 0.283 [0.000, 84.000], loss: 2.798690, mean_absolute_error: 0.464554, mean_q: 2.284588, mean_eps: 0.100000\n",
      "  37159/175000: episode: 1050, duration: 0.992s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 53.240 [1.000, 203.000], mean observation: 0.364 [0.000, 100.000], loss: 1.037445, mean_absolute_error: 0.421814, mean_q: 2.192749, mean_eps: 0.100000\n",
      "  37191/175000: episode: 1051, duration: 0.607s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 81.312 [5.000, 221.000], mean observation: 0.326 [0.000, 64.000], loss: 3.748017, mean_absolute_error: 0.477117, mean_q: 2.446203, mean_eps: 0.100000\n",
      "  37227/175000: episode: 1052, duration: 0.782s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 30.417 [6.000, 217.000], mean observation: 0.181 [0.000, 72.000], loss: 1.227682, mean_absolute_error: 0.456896, mean_q: 2.364789, mean_eps: 0.100000\n",
      "  37263/175000: episode: 1053, duration: 0.683s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.750 [6.000, 177.000], mean observation: 0.206 [0.000, 72.000], loss: 1.098259, mean_absolute_error: 0.392620, mean_q: 1.982067, mean_eps: 0.100000\n",
      "  37296/175000: episode: 1054, duration: 0.727s, episode steps: 33, steps per second: 45, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.303 [6.000, 177.000], mean observation: 0.189 [0.000, 66.000], loss: 1.824322, mean_absolute_error: 0.426174, mean_q: 2.263823, mean_eps: 0.100000\n",
      "  37326/175000: episode: 1055, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 82.067 [6.000, 177.000], mean observation: 0.215 [0.000, 60.000], loss: 1.801567, mean_absolute_error: 0.436510, mean_q: 2.254470, mean_eps: 0.100000\n",
      "  37355/175000: episode: 1056, duration: 0.592s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 99.690 [6.000, 217.000], mean observation: 0.156 [0.000, 58.000], loss: 0.175095, mean_absolute_error: 0.322187, mean_q: 1.774232, mean_eps: 0.100000\n",
      "  37381/175000: episode: 1057, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 34.885 [6.000, 177.000], mean observation: 0.072 [0.000, 52.000], loss: 0.828539, mean_absolute_error: 0.390436, mean_q: 2.117138, mean_eps: 0.100000\n",
      "  37418/175000: episode: 1058, duration: 0.774s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 114.622 [1.000, 202.000], mean observation: 0.272 [0.000, 74.000], loss: 0.459594, mean_absolute_error: 0.375202, mean_q: 1.972408, mean_eps: 0.100000\n",
      "  37456/175000: episode: 1059, duration: 0.759s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.079 [6.000, 208.000], mean observation: 0.243 [0.000, 76.000], loss: 1.218352, mean_absolute_error: 0.402302, mean_q: 2.274423, mean_eps: 0.100000\n",
      "  37484/175000: episode: 1060, duration: 0.709s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 157.571 [53.000, 202.000], mean observation: 0.194 [0.000, 56.000], loss: 2.413677, mean_absolute_error: 0.420996, mean_q: 2.149356, mean_eps: 0.100000\n",
      "  37529/175000: episode: 1061, duration: 0.982s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 138.222 [29.000, 202.000], mean observation: 0.360 [0.000, 90.000], loss: 2.065917, mean_absolute_error: 0.401783, mean_q: 1.788256, mean_eps: 0.100000\n",
      "  37562/175000: episode: 1062, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 161.727 [6.000, 224.000], mean observation: 0.372 [0.000, 66.000], loss: 0.678488, mean_absolute_error: 0.426710, mean_q: 1.936707, mean_eps: 0.100000\n",
      "  37608/175000: episode: 1063, duration: 0.959s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 172.957 [14.000, 224.000], mean observation: 0.477 [0.000, 92.000], loss: 1.335684, mean_absolute_error: 0.439704, mean_q: 1.825878, mean_eps: 0.100000\n",
      "  37638/175000: episode: 1064, duration: 0.665s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 61.300 [0.000, 220.000], mean observation: 0.167 [0.000, 60.000], loss: 0.960993, mean_absolute_error: 0.474020, mean_q: 1.942782, mean_eps: 0.100000\n",
      "  37665/175000: episode: 1065, duration: 0.647s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 116.037 [21.000, 200.000], mean observation: 0.125 [0.000, 54.000], loss: 0.225028, mean_absolute_error: 0.421059, mean_q: 1.759087, mean_eps: 0.100000\n",
      "  37698/175000: episode: 1066, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 152.364 [2.000, 200.000], mean observation: 0.241 [0.000, 66.000], loss: 0.266691, mean_absolute_error: 0.421033, mean_q: 1.870338, mean_eps: 0.100000\n",
      "  37732/175000: episode: 1067, duration: 0.703s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.794 [60.000, 161.000], mean observation: 0.160 [0.000, 68.000], loss: 0.157975, mean_absolute_error: 0.377501, mean_q: 1.856318, mean_eps: 0.100000\n",
      "  37768/175000: episode: 1068, duration: 0.775s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.778 [7.000, 161.000], mean observation: 0.336 [0.000, 72.000], loss: 0.363976, mean_absolute_error: 0.380213, mean_q: 1.853026, mean_eps: 0.100000\n",
      "  37790/175000: episode: 1069, duration: 0.535s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 26.636 [21.000, 145.000], mean observation: 0.064 [0.000, 44.000], loss: 1.740038, mean_absolute_error: 0.452368, mean_q: 2.131953, mean_eps: 0.100000\n",
      "  37831/175000: episode: 1070, duration: 0.771s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 76.024 [21.000, 198.000], mean observation: 0.221 [0.000, 82.000], loss: 0.994209, mean_absolute_error: 0.511397, mean_q: 2.363125, mean_eps: 0.100000\n",
      "  37866/175000: episode: 1071, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.886 [1.000, 198.000], mean observation: 0.346 [0.000, 70.000], loss: 2.496412, mean_absolute_error: 0.563658, mean_q: 2.458278, mean_eps: 0.100000\n",
      "  37897/175000: episode: 1072, duration: 0.582s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 75.065 [1.000, 191.000], mean observation: 0.253 [0.000, 62.000], loss: 3.216961, mean_absolute_error: 0.562061, mean_q: 2.507462, mean_eps: 0.100000\n",
      "  37927/175000: episode: 1073, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 14.867 [14.000, 15.000], mean observation: 0.106 [0.000, 60.000], loss: 0.636348, mean_absolute_error: 0.468841, mean_q: 2.178030, mean_eps: 0.100000\n",
      "  37963/175000: episode: 1074, duration: 0.693s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 51.222 [2.000, 215.000], mean observation: 0.233 [0.000, 72.000], loss: 0.462289, mean_absolute_error: 0.483507, mean_q: 2.241560, mean_eps: 0.100000\n",
      "  37987/175000: episode: 1075, duration: 0.488s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 40.000 [40.000, 40.000], mean observation: 0.057 [0.000, 48.000], loss: 3.294738, mean_absolute_error: 0.477053, mean_q: 2.221840, mean_eps: 0.100000\n",
      "  38026/175000: episode: 1076, duration: 0.804s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 70.026 [22.000, 160.000], mean observation: 0.262 [0.000, 78.000], loss: 2.187293, mean_absolute_error: 0.456201, mean_q: 2.183293, mean_eps: 0.100000\n",
      "  38076/175000: episode: 1077, duration: 1.011s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 96.920 [1.000, 187.000], mean observation: 0.576 [0.000, 100.000], loss: 0.363156, mean_absolute_error: 0.403927, mean_q: 1.886736, mean_eps: 0.100000\n",
      "  38098/175000: episode: 1078, duration: 0.502s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 70.273 [19.000, 153.000], mean observation: 0.095 [0.000, 44.000], loss: 0.204136, mean_absolute_error: 0.371202, mean_q: 1.693286, mean_eps: 0.100000\n",
      "  38130/175000: episode: 1079, duration: 0.704s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 143.656 [15.000, 219.000], mean observation: 0.225 [0.000, 64.000], loss: 0.486624, mean_absolute_error: 0.426765, mean_q: 2.056154, mean_eps: 0.100000\n",
      "  38158/175000: episode: 1080, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 149.357 [9.000, 159.000], mean observation: 0.144 [0.000, 56.000], loss: 0.555721, mean_absolute_error: 0.412368, mean_q: 1.887002, mean_eps: 0.100000\n",
      "  38182/175000: episode: 1081, duration: 0.606s, episode steps: 24, steps per second: 40, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 109.917 [19.000, 209.000], mean observation: 0.130 [0.000, 48.000], loss: 0.108574, mean_absolute_error: 0.375107, mean_q: 1.653414, mean_eps: 0.100000\n",
      "  38216/175000: episode: 1082, duration: 0.870s, episode steps: 34, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 124.618 [51.000, 196.000], mean observation: 0.257 [0.000, 68.000], loss: 2.000323, mean_absolute_error: 0.436163, mean_q: 1.743665, mean_eps: 0.100000\n",
      "  38243/175000: episode: 1083, duration: 0.577s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 192.370 [0.000, 208.000], mean observation: 0.080 [0.000, 54.000], loss: 0.373491, mean_absolute_error: 0.434837, mean_q: 2.084011, mean_eps: 0.100000\n",
      "  38273/175000: episode: 1084, duration: 0.711s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 160.300 [28.000, 213.000], mean observation: 0.248 [0.000, 60.000], loss: 0.440009, mean_absolute_error: 0.389508, mean_q: 1.844108, mean_eps: 0.100000\n",
      "  38315/175000: episode: 1085, duration: 0.903s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 148.262 [5.000, 215.000], mean observation: 0.275 [0.000, 84.000], loss: 1.021350, mean_absolute_error: 0.388253, mean_q: 1.804690, mean_eps: 0.100000\n",
      "  38326/175000: episode: 1086, duration: 0.295s, episode steps: 11, steps per second: 37, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 150.636 [67.000, 159.000], mean observation: 0.029 [0.000, 22.000], loss: 1.499034, mean_absolute_error: 0.449347, mean_q: 2.013661, mean_eps: 0.100000\n",
      "  38346/175000: episode: 1087, duration: 0.579s, episode steps: 20, steps per second: 35, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 169.300 [67.000, 215.000], mean observation: 0.082 [0.000, 40.000], loss: 0.593802, mean_absolute_error: 0.476388, mean_q: 2.182410, mean_eps: 0.100000\n",
      "  38370/175000: episode: 1088, duration: 0.523s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 179.125 [7.000, 215.000], mean observation: 0.122 [0.000, 48.000], loss: 2.737619, mean_absolute_error: 0.398420, mean_q: 1.792970, mean_eps: 0.100000\n",
      "  38408/175000: episode: 1089, duration: 0.947s, episode steps: 38, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 155.711 [30.000, 219.000], mean observation: 0.289 [0.000, 76.000], loss: 0.281801, mean_absolute_error: 0.365925, mean_q: 1.738689, mean_eps: 0.100000\n",
      "  38445/175000: episode: 1090, duration: 0.835s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 147.378 [36.000, 219.000], mean observation: 0.311 [0.000, 74.000], loss: 4.085842, mean_absolute_error: 0.498658, mean_q: 2.263127, mean_eps: 0.100000\n",
      "  38464/175000: episode: 1091, duration: 0.356s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 102.842 [16.000, 219.000], mean observation: 0.130 [0.000, 38.000], loss: 0.739665, mean_absolute_error: 0.386067, mean_q: 1.859219, mean_eps: 0.100000\n",
      "  38495/175000: episode: 1092, duration: 0.629s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 111.548 [14.000, 197.000], mean observation: 0.199 [0.000, 62.000], loss: 1.523062, mean_absolute_error: 0.424906, mean_q: 2.073175, mean_eps: 0.100000\n",
      "  38545/175000: episode: 1093, duration: 0.952s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.140 [66.000, 219.000], mean observation: 0.419 [0.000, 100.000], loss: 0.963998, mean_absolute_error: 0.391870, mean_q: 1.877410, mean_eps: 0.100000\n",
      "  38584/175000: episode: 1094, duration: 0.811s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.769 [7.000, 185.000], mean observation: 0.307 [0.000, 78.000], loss: 0.305312, mean_absolute_error: 0.408575, mean_q: 1.717593, mean_eps: 0.100000\n",
      "  38621/175000: episode: 1095, duration: 0.944s, episode steps: 37, steps per second: 39, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.541 [14.000, 196.000], mean observation: 0.243 [0.000, 74.000], loss: 0.794022, mean_absolute_error: 0.524538, mean_q: 1.994553, mean_eps: 0.100000\n",
      "  38643/175000: episode: 1096, duration: 0.387s, episode steps: 22, steps per second: 57, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.091 [6.000, 198.000], mean observation: 0.161 [0.000, 44.000], loss: 5.863587, mean_absolute_error: 0.619860, mean_q: 2.745254, mean_eps: 0.100000\n",
      "  38667/175000: episode: 1097, duration: 0.520s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 101.667 [14.000, 137.000], mean observation: 0.143 [0.000, 48.000], loss: 1.980538, mean_absolute_error: 0.531577, mean_q: 2.206663, mean_eps: 0.100000\n",
      "  38707/175000: episode: 1098, duration: 1.041s, episode steps: 40, steps per second: 38, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.200 [14.000, 212.000], mean observation: 0.457 [0.000, 80.000], loss: 0.706321, mean_absolute_error: 0.449034, mean_q: 2.081301, mean_eps: 0.100000\n",
      "  38758/175000: episode: 1099, duration: 1.083s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 47.020 [2.000, 116.000], mean observation: 0.407 [0.000, 102.000], loss: 0.513703, mean_absolute_error: 0.383984, mean_q: 1.880576, mean_eps: 0.100000\n",
      "  38781/175000: episode: 1100, duration: 0.455s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 66.957 [25.000, 191.000], mean observation: 0.124 [0.000, 46.000], loss: 0.181921, mean_absolute_error: 0.340953, mean_q: 1.672332, mean_eps: 0.100000\n",
      "  38818/175000: episode: 1101, duration: 0.728s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 94.892 [42.000, 200.000], mean observation: 0.340 [0.000, 74.000], loss: 2.625719, mean_absolute_error: 0.427894, mean_q: 1.957959, mean_eps: 0.100000\n",
      "  38841/175000: episode: 1102, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 60.130 [25.000, 116.000], mean observation: 0.165 [0.000, 46.000], loss: 3.293710, mean_absolute_error: 0.534380, mean_q: 2.316172, mean_eps: 0.100000\n",
      "  38875/175000: episode: 1103, duration: 0.648s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 86.971 [23.000, 169.000], mean observation: 0.127 [0.000, 68.000], loss: 0.305549, mean_absolute_error: 0.438465, mean_q: 1.723641, mean_eps: 0.100000\n",
      "  38914/175000: episode: 1104, duration: 0.934s, episode steps: 39, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 67.564 [27.000, 189.000], mean observation: 0.195 [0.000, 78.000], loss: 0.398827, mean_absolute_error: 0.491564, mean_q: 1.980822, mean_eps: 0.100000\n",
      "  38949/175000: episode: 1105, duration: 0.695s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 54.857 [25.000, 207.000], mean observation: 0.299 [0.000, 70.000], loss: 0.489693, mean_absolute_error: 0.431876, mean_q: 1.749560, mean_eps: 0.100000\n",
      "  38982/175000: episode: 1106, duration: 0.645s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.515 [27.000, 217.000], mean observation: 0.195 [0.000, 66.000], loss: 2.601106, mean_absolute_error: 0.416521, mean_q: 1.801690, mean_eps: 0.100000\n",
      "  39006/175000: episode: 1107, duration: 0.491s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 166.792 [27.000, 217.000], mean observation: 0.073 [0.000, 48.000], loss: 0.664172, mean_absolute_error: 0.400841, mean_q: 1.973041, mean_eps: 0.100000\n",
      "  39035/175000: episode: 1108, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.483 [26.000, 217.000], mean observation: 0.150 [0.000, 58.000], loss: 2.689514, mean_absolute_error: 0.435286, mean_q: 2.188006, mean_eps: 0.100000\n",
      "  39063/175000: episode: 1109, duration: 0.691s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 144.714 [17.000, 217.000], mean observation: 0.216 [0.000, 56.000], loss: 5.254273, mean_absolute_error: 0.494587, mean_q: 2.482879, mean_eps: 0.100000\n",
      "  39097/175000: episode: 1110, duration: 0.733s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 194.912 [76.000, 217.000], mean observation: 0.301 [0.000, 68.000], loss: 0.625689, mean_absolute_error: 0.413659, mean_q: 2.211203, mean_eps: 0.100000\n",
      "  39117/175000: episode: 1111, duration: 0.438s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 195.300 [23.000, 217.000], mean observation: 0.106 [0.000, 40.000], loss: 4.077189, mean_absolute_error: 0.453287, mean_q: 2.196670, mean_eps: 0.100000\n",
      "  39162/175000: episode: 1112, duration: 0.885s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 177.089 [128.000, 217.000], mean observation: 0.312 [0.000, 90.000], loss: 0.896372, mean_absolute_error: 0.444347, mean_q: 2.110367, mean_eps: 0.100000\n",
      "  39209/175000: episode: 1113, duration: 0.989s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 161.638 [90.000, 224.000], mean observation: 0.443 [0.000, 94.000], loss: 2.402519, mean_absolute_error: 0.474683, mean_q: 2.204709, mean_eps: 0.100000\n",
      "  39246/175000: episode: 1114, duration: 0.774s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 77.541 [8.000, 201.000], mean observation: 0.313 [0.000, 74.000], loss: 3.662541, mean_absolute_error: 0.419675, mean_q: 1.834208, mean_eps: 0.100000\n",
      "  39284/175000: episode: 1115, duration: 0.833s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 86.158 [55.000, 224.000], mean observation: 0.343 [0.000, 76.000], loss: 0.340679, mean_absolute_error: 0.391323, mean_q: 1.849779, mean_eps: 0.100000\n",
      "  39335/175000: episode: 1116, duration: 0.985s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.137 [4.000, 188.000], mean observation: 0.591 [0.000, 102.000], loss: 0.553275, mean_absolute_error: 0.387326, mean_q: 1.889567, mean_eps: 0.100000\n",
      "  39390/175000: episode: 1117, duration: 1.100s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 161.927 [24.000, 188.000], mean observation: 0.286 [0.000, 110.000], loss: 0.152610, mean_absolute_error: 0.311895, mean_q: 1.544845, mean_eps: 0.100000\n",
      "  39436/175000: episode: 1118, duration: 0.982s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.587 [2.000, 181.000], mean observation: 0.188 [0.000, 92.000], loss: 0.444408, mean_absolute_error: 0.453040, mean_q: 2.275676, mean_eps: 0.100000\n",
      "  39472/175000: episode: 1119, duration: 0.760s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 132.750 [14.000, 211.000], mean observation: 0.321 [0.000, 72.000], loss: 1.903203, mean_absolute_error: 0.533468, mean_q: 2.634401, mean_eps: 0.100000\n",
      "  39514/175000: episode: 1120, duration: 0.938s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.690 [46.000, 188.000], mean observation: 0.209 [0.000, 84.000], loss: 1.421685, mean_absolute_error: 0.457858, mean_q: 2.331563, mean_eps: 0.100000\n",
      "  39542/175000: episode: 1121, duration: 0.636s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.857 [19.000, 188.000], mean observation: 0.245 [0.000, 56.000], loss: 0.711529, mean_absolute_error: 0.486538, mean_q: 2.481628, mean_eps: 0.100000\n",
      "  39575/175000: episode: 1122, duration: 0.675s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.727 [46.000, 188.000], mean observation: 0.261 [0.000, 66.000], loss: 2.874992, mean_absolute_error: 0.514994, mean_q: 2.509152, mean_eps: 0.100000\n",
      "  39616/175000: episode: 1123, duration: 0.901s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 132.024 [5.000, 202.000], mean observation: 0.396 [0.000, 82.000], loss: 1.573981, mean_absolute_error: 0.492395, mean_q: 2.353119, mean_eps: 0.100000\n",
      "  39663/175000: episode: 1124, duration: 0.944s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 179.064 [8.000, 202.000], mean observation: 0.372 [0.000, 94.000], loss: 0.602813, mean_absolute_error: 0.480385, mean_q: 2.457567, mean_eps: 0.100000\n",
      "  39697/175000: episode: 1125, duration: 0.785s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 188.500 [101.000, 202.000], mean observation: 0.172 [0.000, 68.000], loss: 0.335186, mean_absolute_error: 0.465078, mean_q: 2.408453, mean_eps: 0.100000\n",
      "  39753/175000: episode: 1126, duration: 1.061s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 93.196 [22.000, 196.000], mean observation: 0.531 [0.000, 112.000], loss: 2.148138, mean_absolute_error: 0.512449, mean_q: 2.429006, mean_eps: 0.100000\n",
      "  39786/175000: episode: 1127, duration: 0.684s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.212 [40.000, 222.000], mean observation: 0.250 [0.000, 66.000], loss: 0.475237, mean_absolute_error: 0.425192, mean_q: 1.915751, mean_eps: 0.100000\n",
      "  39812/175000: episode: 1128, duration: 0.544s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 159.885 [74.000, 208.000], mean observation: 0.152 [0.000, 52.000], loss: 0.281384, mean_absolute_error: 0.497431, mean_q: 2.489579, mean_eps: 0.100000\n",
      "  39847/175000: episode: 1129, duration: 0.818s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.000 [22.000, 222.000], mean observation: 0.213 [0.000, 70.000], loss: 0.398712, mean_absolute_error: 0.482997, mean_q: 2.459255, mean_eps: 0.100000\n",
      "  39882/175000: episode: 1130, duration: 0.744s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 136.314 [1.000, 224.000], mean observation: 0.326 [0.000, 70.000], loss: 0.447694, mean_absolute_error: 0.452479, mean_q: 2.326213, mean_eps: 0.100000\n",
      "  39919/175000: episode: 1131, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 156.649 [92.000, 184.000], mean observation: 0.226 [0.000, 74.000], loss: 1.041056, mean_absolute_error: 0.462224, mean_q: 2.297958, mean_eps: 0.100000\n",
      "  39968/175000: episode: 1132, duration: 1.126s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 156.857 [30.000, 222.000], mean observation: 0.577 [0.000, 98.000], loss: 1.421821, mean_absolute_error: 0.424614, mean_q: 1.959880, mean_eps: 0.100000\n",
      "  40007/175000: episode: 1133, duration: 0.839s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 62.718 [30.000, 222.000], mean observation: 0.256 [0.000, 78.000], loss: 0.543910, mean_absolute_error: 0.433730, mean_q: 2.137681, mean_eps: 0.100000\n",
      "  40052/175000: episode: 1134, duration: 1.162s, episode steps: 45, steps per second: 39, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.622 [17.000, 222.000], mean observation: 0.465 [0.000, 90.000], loss: 1.283066, mean_absolute_error: 0.383228, mean_q: 1.751626, mean_eps: 0.100000\n",
      "  40111/175000: episode: 1135, duration: 1.287s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.288 [1.000, 216.000], mean observation: 0.579 [0.000, 118.000], loss: 2.913383, mean_absolute_error: 0.474174, mean_q: 2.098608, mean_eps: 0.100000\n",
      "  40144/175000: episode: 1136, duration: 0.879s, episode steps: 33, steps per second: 38, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 122.333 [104.000, 140.000], mean observation: 0.117 [0.000, 66.000], loss: 0.320468, mean_absolute_error: 0.389704, mean_q: 1.747658, mean_eps: 0.100000\n",
      "  40174/175000: episode: 1137, duration: 0.802s, episode steps: 30, steps per second: 37, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 109.833 [30.000, 207.000], mean observation: 0.163 [0.000, 60.000], loss: 2.146541, mean_absolute_error: 0.384389, mean_q: 1.876046, mean_eps: 0.100000\n",
      "  40228/175000: episode: 1138, duration: 1.187s, episode steps: 54, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.130 [7.000, 214.000], mean observation: 0.746 [0.000, 108.000], loss: 5.144331, mean_absolute_error: 0.423846, mean_q: 2.059471, mean_eps: 0.100000\n",
      "  40246/175000: episode: 1139, duration: 0.432s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 207.556 [98.000, 214.000], mean observation: 0.044 [0.000, 36.000], loss: 4.057935, mean_absolute_error: 0.409110, mean_q: 2.026552, mean_eps: 0.100000\n",
      "  40275/175000: episode: 1140, duration: 0.785s, episode steps: 29, steps per second: 37, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 180.172 [2.000, 218.000], mean observation: 0.084 [0.000, 58.000], loss: 4.874667, mean_absolute_error: 0.424251, mean_q: 2.018797, mean_eps: 0.100000\n",
      "  40305/175000: episode: 1141, duration: 0.771s, episode steps: 30, steps per second: 39, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 131.333 [2.000, 214.000], mean observation: 0.170 [0.000, 60.000], loss: 1.221432, mean_absolute_error: 0.417955, mean_q: 2.109011, mean_eps: 0.100000\n",
      "  40336/175000: episode: 1142, duration: 0.860s, episode steps: 31, steps per second: 36, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 162.710 [2.000, 214.000], mean observation: 0.132 [0.000, 62.000], loss: 0.466011, mean_absolute_error: 0.368579, mean_q: 1.954690, mean_eps: 0.100000\n",
      "  40373/175000: episode: 1143, duration: 0.982s, episode steps: 37, steps per second: 38, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 109.595 [2.000, 128.000], mean observation: 0.163 [0.000, 74.000], loss: 1.053861, mean_absolute_error: 0.365291, mean_q: 2.043710, mean_eps: 0.100000\n",
      "  40416/175000: episode: 1144, duration: 1.025s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.256 [2.000, 202.000], mean observation: 0.582 [0.000, 86.000], loss: 5.564116, mean_absolute_error: 0.419378, mean_q: 2.147796, mean_eps: 0.100000\n",
      "  40438/175000: episode: 1145, duration: 0.577s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.818 [12.000, 152.000], mean observation: 0.103 [0.000, 44.000], loss: 4.797705, mean_absolute_error: 0.413852, mean_q: 2.074689, mean_eps: 0.100000\n",
      "  40472/175000: episode: 1146, duration: 0.918s, episode steps: 34, steps per second: 37, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 60.059 [12.000, 210.000], mean observation: 0.171 [0.000, 68.000], loss: 2.408540, mean_absolute_error: 0.376178, mean_q: 1.812653, mean_eps: 0.100000\n",
      "  40525/175000: episode: 1147, duration: 1.163s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 99.868 [12.000, 221.000], mean observation: 0.406 [0.000, 106.000], loss: 4.018116, mean_absolute_error: 0.375763, mean_q: 1.799172, mean_eps: 0.100000\n",
      "  40567/175000: episode: 1148, duration: 0.873s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 72.024 [12.000, 167.000], mean observation: 0.176 [0.000, 84.000], loss: 6.125892, mean_absolute_error: 0.463485, mean_q: 2.188241, mean_eps: 0.100000\n",
      "  40608/175000: episode: 1149, duration: 0.991s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 64.854 [0.000, 217.000], mean observation: 0.230 [0.000, 82.000], loss: 8.082304, mean_absolute_error: 0.469823, mean_q: 2.068748, mean_eps: 0.100000\n",
      "  40647/175000: episode: 1150, duration: 0.938s, episode steps: 39, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 175.128 [21.000, 221.000], mean observation: 0.366 [0.000, 78.000], loss: 3.605019, mean_absolute_error: 0.406569, mean_q: 1.925922, mean_eps: 0.100000\n",
      "  40683/175000: episode: 1151, duration: 0.788s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 171.444 [25.000, 221.000], mean observation: 0.288 [0.000, 72.000], loss: 0.221041, mean_absolute_error: 0.343694, mean_q: 1.652480, mean_eps: 0.100000\n",
      "  40715/175000: episode: 1152, duration: 0.627s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 125.375 [25.000, 221.000], mean observation: 0.200 [0.000, 64.000], loss: 7.561351, mean_absolute_error: 0.508872, mean_q: 2.233221, mean_eps: 0.100000\n",
      "  40742/175000: episode: 1153, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 79.667 [25.000, 211.000], mean observation: 0.179 [0.000, 54.000], loss: 3.045541, mean_absolute_error: 0.545051, mean_q: 2.510636, mean_eps: 0.100000\n",
      "  40785/175000: episode: 1154, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.395 [25.000, 216.000], mean observation: 0.369 [0.000, 86.000], loss: 2.473823, mean_absolute_error: 0.467942, mean_q: 2.086476, mean_eps: 0.100000\n",
      "  40819/175000: episode: 1155, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.118 [13.000, 222.000], mean observation: 0.287 [0.000, 68.000], loss: 3.230809, mean_absolute_error: 0.519287, mean_q: 2.266144, mean_eps: 0.100000\n",
      "  40841/175000: episode: 1156, duration: 0.467s, episode steps: 22, steps per second: 47, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 58.727 [2.000, 140.000], mean observation: 0.158 [0.000, 44.000], loss: 2.563908, mean_absolute_error: 0.486754, mean_q: 2.210629, mean_eps: 0.100000\n",
      "  40882/175000: episode: 1157, duration: 0.929s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 28.951 [2.000, 164.000], mean observation: 0.316 [0.000, 82.000], loss: 5.270693, mean_absolute_error: 0.505480, mean_q: 2.287762, mean_eps: 0.100000\n",
      "  40925/175000: episode: 1158, duration: 1.029s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 67.209 [2.000, 220.000], mean observation: 0.429 [0.000, 86.000], loss: 4.141234, mean_absolute_error: 0.490537, mean_q: 2.268334, mean_eps: 0.100000\n",
      "  40965/175000: episode: 1159, duration: 0.808s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 186.850 [25.000, 222.000], mean observation: 0.164 [0.000, 80.000], loss: 3.082929, mean_absolute_error: 0.458091, mean_q: 2.126931, mean_eps: 0.100000\n",
      "  41023/175000: episode: 1160, duration: 1.107s, episode steps: 58, steps per second: 52, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 46.448 [1.000, 222.000], mean observation: 0.608 [0.000, 116.000], loss: 3.359857, mean_absolute_error: 0.433456, mean_q: 2.030130, mean_eps: 0.100000\n",
      "  41051/175000: episode: 1161, duration: 0.550s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.071 [74.000, 222.000], mean observation: 0.264 [0.000, 56.000], loss: 0.177408, mean_absolute_error: 0.313465, mean_q: 1.610644, mean_eps: 0.100000\n",
      "  41088/175000: episode: 1162, duration: 0.841s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 102.432 [90.000, 103.000], mean observation: 0.120 [0.000, 74.000], loss: 0.167641, mean_absolute_error: 0.340289, mean_q: 1.579546, mean_eps: 0.100000\n",
      "  41130/175000: episode: 1163, duration: 0.879s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.500 [34.000, 140.000], mean observation: 0.240 [0.000, 84.000], loss: 1.632545, mean_absolute_error: 0.402801, mean_q: 1.780929, mean_eps: 0.100000\n",
      "  41177/175000: episode: 1164, duration: 0.942s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 118.319 [16.000, 190.000], mean observation: 0.507 [0.000, 94.000], loss: 1.422751, mean_absolute_error: 0.413743, mean_q: 1.889969, mean_eps: 0.100000\n",
      "  41221/175000: episode: 1165, duration: 0.881s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.477 [21.000, 222.000], mean observation: 0.357 [0.000, 88.000], loss: 1.165675, mean_absolute_error: 0.384890, mean_q: 1.821150, mean_eps: 0.100000\n",
      "  41265/175000: episode: 1166, duration: 0.927s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.114 [46.000, 222.000], mean observation: 0.553 [0.000, 88.000], loss: 13.545392, mean_absolute_error: 0.597798, mean_q: 2.623497, mean_eps: 0.100000\n",
      "  41298/175000: episode: 1167, duration: 0.673s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 90.758 [32.000, 196.000], mean observation: 0.174 [0.000, 66.000], loss: 0.420701, mean_absolute_error: 0.427529, mean_q: 2.074564, mean_eps: 0.100000\n",
      "  41325/175000: episode: 1168, duration: 0.544s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 84.778 [14.000, 224.000], mean observation: 0.160 [0.000, 54.000], loss: 0.217933, mean_absolute_error: 0.377032, mean_q: 1.820366, mean_eps: 0.100000\n",
      "  41354/175000: episode: 1169, duration: 0.583s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.172 [4.000, 190.000], mean observation: 0.219 [0.000, 58.000], loss: 2.201852, mean_absolute_error: 0.453440, mean_q: 2.184471, mean_eps: 0.100000\n",
      "  41397/175000: episode: 1170, duration: 0.884s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.721 [25.000, 219.000], mean observation: 0.424 [0.000, 86.000], loss: 2.513341, mean_absolute_error: 0.374910, mean_q: 1.775444, mean_eps: 0.100000\n",
      "  41436/175000: episode: 1171, duration: 0.831s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 135.667 [30.000, 190.000], mean observation: 0.243 [0.000, 78.000], loss: 0.102134, mean_absolute_error: 0.316083, mean_q: 1.608620, mean_eps: 0.100000\n",
      "  41487/175000: episode: 1172, duration: 1.023s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.157 [14.000, 218.000], mean observation: 0.280 [0.000, 102.000], loss: 3.503754, mean_absolute_error: 0.338787, mean_q: 1.638244, mean_eps: 0.100000\n",
      "  41512/175000: episode: 1173, duration: 0.562s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 132.000 [54.000, 210.000], mean observation: 0.186 [0.000, 50.000], loss: 3.096036, mean_absolute_error: 0.443432, mean_q: 2.035592, mean_eps: 0.100000\n",
      "  41555/175000: episode: 1174, duration: 0.965s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 75.395 [17.000, 172.000], mean observation: 0.431 [0.000, 86.000], loss: 5.093032, mean_absolute_error: 0.490149, mean_q: 2.139858, mean_eps: 0.100000\n",
      "  41592/175000: episode: 1175, duration: 0.795s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 41.027 [20.000, 202.000], mean observation: 0.311 [0.000, 74.000], loss: 4.153787, mean_absolute_error: 0.409944, mean_q: 1.885994, mean_eps: 0.100000\n",
      "  41633/175000: episode: 1176, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 59.171 [20.000, 193.000], mean observation: 0.429 [0.000, 82.000], loss: 3.601349, mean_absolute_error: 0.443416, mean_q: 2.086829, mean_eps: 0.100000\n",
      "  41684/175000: episode: 1177, duration: 1.045s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 51.980 [20.000, 223.000], mean observation: 0.369 [0.000, 102.000], loss: 1.033950, mean_absolute_error: 0.380825, mean_q: 1.884913, mean_eps: 0.100000\n",
      "  41708/175000: episode: 1178, duration: 0.669s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 83.250 [54.000, 207.000], mean observation: 0.178 [0.000, 48.000], loss: 1.762019, mean_absolute_error: 0.422497, mean_q: 2.099072, mean_eps: 0.100000\n",
      "  41756/175000: episode: 1179, duration: 1.155s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 182.938 [54.000, 223.000], mean observation: 0.431 [0.000, 96.000], loss: 4.359509, mean_absolute_error: 0.440931, mean_q: 1.929239, mean_eps: 0.100000\n",
      "  41803/175000: episode: 1180, duration: 0.962s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 155.957 [5.000, 202.000], mean observation: 0.499 [0.000, 94.000], loss: 0.141775, mean_absolute_error: 0.328379, mean_q: 1.504038, mean_eps: 0.100000\n",
      "  41836/175000: episode: 1181, duration: 0.716s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.182 [33.000, 214.000], mean observation: 0.268 [0.000, 66.000], loss: 0.246419, mean_absolute_error: 0.351999, mean_q: 1.677075, mean_eps: 0.100000\n",
      "  41872/175000: episode: 1182, duration: 0.813s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 147.750 [40.000, 185.000], mean observation: 0.310 [0.000, 72.000], loss: 9.646737, mean_absolute_error: 0.498464, mean_q: 2.183691, mean_eps: 0.100000\n",
      "  41909/175000: episode: 1183, duration: 0.755s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.378 [10.000, 194.000], mean observation: 0.332 [0.000, 74.000], loss: 2.676664, mean_absolute_error: 0.428879, mean_q: 1.984334, mean_eps: 0.100000\n",
      "  41943/175000: episode: 1184, duration: 0.801s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.088 [50.000, 208.000], mean observation: 0.346 [0.000, 68.000], loss: 8.066359, mean_absolute_error: 0.529394, mean_q: 2.360398, mean_eps: 0.100000\n",
      "  41971/175000: episode: 1185, duration: 0.568s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 132.607 [2.000, 159.000], mean observation: 0.176 [0.000, 56.000], loss: 2.205248, mean_absolute_error: 0.405455, mean_q: 1.990331, mean_eps: 0.100000\n",
      "  42006/175000: episode: 1186, duration: 0.805s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 177.257 [61.000, 216.000], mean observation: 0.251 [0.000, 70.000], loss: 4.419576, mean_absolute_error: 0.459257, mean_q: 2.126104, mean_eps: 0.100000\n",
      "  42037/175000: episode: 1187, duration: 0.627s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 152.032 [20.000, 211.000], mean observation: 0.276 [0.000, 62.000], loss: 3.000350, mean_absolute_error: 0.446769, mean_q: 2.042874, mean_eps: 0.100000\n",
      "  42061/175000: episode: 1188, duration: 0.525s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.542 [86.000, 140.000], mean observation: 0.186 [0.000, 48.000], loss: 4.042609, mean_absolute_error: 0.450241, mean_q: 2.173163, mean_eps: 0.100000\n",
      "  42105/175000: episode: 1189, duration: 0.916s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 119.091 [61.000, 213.000], mean observation: 0.302 [0.000, 88.000], loss: 2.883256, mean_absolute_error: 0.438203, mean_q: 2.108170, mean_eps: 0.100000\n",
      "  42134/175000: episode: 1190, duration: 0.600s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 51.552 [27.000, 162.000], mean observation: 0.180 [0.000, 58.000], loss: 3.217082, mean_absolute_error: 0.478066, mean_q: 2.194699, mean_eps: 0.100000\n",
      "  42178/175000: episode: 1191, duration: 0.930s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 36.659 [27.000, 109.000], mean observation: 0.312 [0.000, 88.000], loss: 3.052137, mean_absolute_error: 0.527853, mean_q: 2.380878, mean_eps: 0.100000\n",
      "  42218/175000: episode: 1192, duration: 0.864s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 56.575 [18.000, 210.000], mean observation: 0.286 [0.000, 80.000], loss: 6.305719, mean_absolute_error: 0.529504, mean_q: 2.399448, mean_eps: 0.100000\n",
      "  42259/175000: episode: 1193, duration: 0.783s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 57.878 [24.000, 213.000], mean observation: 0.390 [0.000, 82.000], loss: 1.516139, mean_absolute_error: 0.408639, mean_q: 2.022669, mean_eps: 0.100000\n",
      "  42275/175000: episode: 1194, duration: 0.328s, episode steps: 16, steps per second: 49, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 135.125 [24.000, 189.000], mean observation: 0.062 [0.000, 32.000], loss: 0.230983, mean_absolute_error: 0.321879, mean_q: 1.727226, mean_eps: 0.100000\n",
      "  42305/175000: episode: 1195, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 153.100 [24.000, 219.000], mean observation: 0.170 [0.000, 60.000], loss: 7.484634, mean_absolute_error: 0.411218, mean_q: 1.995621, mean_eps: 0.100000\n",
      "  42330/175000: episode: 1196, duration: 0.461s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 161.920 [93.000, 209.000], mean observation: 0.100 [0.000, 50.000], loss: 5.791261, mean_absolute_error: 0.606525, mean_q: 2.871103, mean_eps: 0.100000\n",
      "  42376/175000: episode: 1197, duration: 1.081s, episode steps: 46, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 166.565 [32.000, 196.000], mean observation: 0.318 [0.000, 92.000], loss: 0.142679, mean_absolute_error: 0.333878, mean_q: 1.803605, mean_eps: 0.100000\n",
      "  42422/175000: episode: 1198, duration: 0.992s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 68.543 [24.000, 157.000], mean observation: 0.267 [0.000, 92.000], loss: 3.453154, mean_absolute_error: 0.543568, mean_q: 2.551800, mean_eps: 0.100000\n",
      "  42460/175000: episode: 1199, duration: 0.828s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.526 [28.000, 217.000], mean observation: 0.272 [0.000, 76.000], loss: 0.125101, mean_absolute_error: 0.321970, mean_q: 1.566316, mean_eps: 0.100000\n",
      "  42491/175000: episode: 1200, duration: 0.644s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 124.774 [6.000, 187.000], mean observation: 0.290 [0.000, 62.000], loss: 4.908935, mean_absolute_error: 0.449570, mean_q: 2.069656, mean_eps: 0.100000\n",
      "  42545/175000: episode: 1201, duration: 1.180s, episode steps: 54, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 88.963 [35.000, 187.000], mean observation: 0.325 [0.000, 108.000], loss: 4.014646, mean_absolute_error: 0.451869, mean_q: 2.159744, mean_eps: 0.100000\n",
      "  42584/175000: episode: 1202, duration: 0.839s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 80.641 [4.000, 200.000], mean observation: 0.387 [0.000, 78.000], loss: 0.577158, mean_absolute_error: 0.395831, mean_q: 2.047513, mean_eps: 0.100000\n",
      "  42623/175000: episode: 1203, duration: 0.788s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.795 [5.000, 222.000], mean observation: 0.271 [0.000, 78.000], loss: 3.471261, mean_absolute_error: 0.566201, mean_q: 2.666537, mean_eps: 0.100000\n",
      "  42654/175000: episode: 1204, duration: 0.673s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 44.355 [5.000, 200.000], mean observation: 0.165 [0.000, 62.000], loss: 4.026572, mean_absolute_error: 0.536797, mean_q: 2.584600, mean_eps: 0.100000\n",
      "  42692/175000: episode: 1205, duration: 0.804s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 48.842 [11.000, 60.000], mean observation: 0.141 [0.000, 76.000], loss: 4.052021, mean_absolute_error: 0.527013, mean_q: 2.437695, mean_eps: 0.100000\n",
      "  42725/175000: episode: 1206, duration: 0.773s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 157.091 [60.000, 221.000], mean observation: 0.249 [0.000, 66.000], loss: 2.395375, mean_absolute_error: 0.561429, mean_q: 2.636087, mean_eps: 0.100000\n",
      "  42771/175000: episode: 1207, duration: 0.983s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 76.326 [19.000, 200.000], mean observation: 0.218 [0.000, 92.000], loss: 1.394422, mean_absolute_error: 0.453290, mean_q: 2.211818, mean_eps: 0.100000\n",
      "  42801/175000: episode: 1208, duration: 0.618s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.067 [7.000, 140.000], mean observation: 0.329 [0.000, 60.000], loss: 2.619155, mean_absolute_error: 0.546570, mean_q: 2.695083, mean_eps: 0.100000\n",
      "  42819/175000: episode: 1209, duration: 0.409s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 39.167 [22.000, 140.000], mean observation: 0.113 [0.000, 36.000], loss: 2.871495, mean_absolute_error: 0.495109, mean_q: 2.447755, mean_eps: 0.100000\n",
      "  42849/175000: episode: 1210, duration: 0.728s, episode steps: 30, steps per second: 41, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 28.400 [3.000, 159.000], mean observation: 0.268 [0.000, 60.000], loss: 9.779677, mean_absolute_error: 0.548226, mean_q: 2.477645, mean_eps: 0.100000\n",
      "  42856/175000: episode: 1211, duration: 0.202s, episode steps: 7, steps per second: 35, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 135.143 [58.000, 148.000], mean observation: 0.027 [0.000, 14.000], loss: 0.113461, mean_absolute_error: 0.342706, mean_q: 1.710265, mean_eps: 0.100000\n",
      "  42896/175000: episode: 1212, duration: 0.939s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 120.200 [28.000, 222.000], mean observation: 0.337 [0.000, 80.000], loss: 0.890440, mean_absolute_error: 0.420308, mean_q: 2.084811, mean_eps: 0.100000\n",
      "  42917/175000: episode: 1213, duration: 0.524s, episode steps: 21, steps per second: 40, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 80.667 [6.000, 140.000], mean observation: 0.136 [0.000, 42.000], loss: 5.672188, mean_absolute_error: 0.458731, mean_q: 2.238208, mean_eps: 0.100000\n",
      "  42960/175000: episode: 1214, duration: 0.916s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 53.209 [28.000, 172.000], mean observation: 0.400 [0.000, 86.000], loss: 4.767280, mean_absolute_error: 0.424113, mean_q: 2.121513, mean_eps: 0.100000\n",
      "  42996/175000: episode: 1215, duration: 0.787s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 93.472 [16.000, 176.000], mean observation: 0.182 [0.000, 72.000], loss: 4.822450, mean_absolute_error: 0.445573, mean_q: 2.254131, mean_eps: 0.100000\n",
      "  43029/175000: episode: 1216, duration: 0.752s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 130.152 [16.000, 176.000], mean observation: 0.208 [0.000, 66.000], loss: 2.083253, mean_absolute_error: 0.432492, mean_q: 2.399431, mean_eps: 0.100000\n",
      "  43088/175000: episode: 1217, duration: 1.262s, episode steps: 59, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 129.797 [2.000, 189.000], mean observation: 0.409 [0.000, 118.000], loss: 5.482405, mean_absolute_error: 0.475011, mean_q: 2.439195, mean_eps: 0.100000\n",
      "  43124/175000: episode: 1218, duration: 0.819s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.778 [2.000, 30.000], mean observation: 0.084 [0.000, 72.000], loss: 8.687422, mean_absolute_error: 0.538059, mean_q: 2.766378, mean_eps: 0.100000\n",
      "  43164/175000: episode: 1219, duration: 0.977s, episode steps: 40, steps per second: 41, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 76.750 [2.000, 183.000], mean observation: 0.206 [0.000, 80.000], loss: 1.423102, mean_absolute_error: 0.354884, mean_q: 2.075368, mean_eps: 0.100000\n",
      "  43209/175000: episode: 1220, duration: 0.989s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.733 [5.000, 152.000], mean observation: 0.324 [0.000, 90.000], loss: 3.283796, mean_absolute_error: 0.480153, mean_q: 2.549921, mean_eps: 0.100000\n",
      "  43256/175000: episode: 1221, duration: 0.923s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.745 [2.000, 219.000], mean observation: 0.324 [0.000, 94.000], loss: 5.476370, mean_absolute_error: 0.469419, mean_q: 2.496622, mean_eps: 0.100000\n",
      "  43306/175000: episode: 1222, duration: 1.161s, episode steps: 50, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 126.360 [21.000, 162.000], mean observation: 0.481 [0.000, 100.000], loss: 0.472405, mean_absolute_error: 0.464268, mean_q: 2.459166, mean_eps: 0.100000\n",
      "  43338/175000: episode: 1223, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 135.812 [46.000, 182.000], mean observation: 0.278 [0.000, 64.000], loss: 1.991958, mean_absolute_error: 0.461151, mean_q: 2.455903, mean_eps: 0.100000\n",
      "  43370/175000: episode: 1224, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.938 [3.000, 186.000], mean observation: 0.208 [0.000, 64.000], loss: 4.494615, mean_absolute_error: 0.594464, mean_q: 3.013190, mean_eps: 0.100000\n",
      "  43398/175000: episode: 1225, duration: 0.572s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 54.857 [3.000, 105.000], mean observation: 0.160 [0.000, 56.000], loss: 8.290328, mean_absolute_error: 0.734080, mean_q: 3.636357, mean_eps: 0.100000\n",
      "  43414/175000: episode: 1226, duration: 0.343s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 109.750 [3.000, 224.000], mean observation: 0.114 [0.000, 32.000], loss: 0.368867, mean_absolute_error: 0.418799, mean_q: 2.422562, mean_eps: 0.100000\n",
      "  43450/175000: episode: 1227, duration: 0.703s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.056 [2.000, 224.000], mean observation: 0.210 [0.000, 72.000], loss: 3.088450, mean_absolute_error: 0.570328, mean_q: 3.079220, mean_eps: 0.100000\n",
      "  43477/175000: episode: 1228, duration: 0.570s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 157.741 [2.000, 224.000], mean observation: 0.176 [0.000, 54.000], loss: 4.004288, mean_absolute_error: 0.600717, mean_q: 3.221952, mean_eps: 0.100000\n",
      "  43509/175000: episode: 1229, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.469 [47.000, 222.000], mean observation: 0.191 [0.000, 64.000], loss: 2.058065, mean_absolute_error: 0.518184, mean_q: 2.909089, mean_eps: 0.100000\n",
      "  43546/175000: episode: 1230, duration: 0.738s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 95.243 [53.000, 222.000], mean observation: 0.166 [0.000, 74.000], loss: 1.950913, mean_absolute_error: 0.521967, mean_q: 2.896326, mean_eps: 0.100000\n",
      "  43578/175000: episode: 1231, duration: 0.611s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 156.188 [53.000, 222.000], mean observation: 0.185 [0.000, 64.000], loss: 3.149549, mean_absolute_error: 0.500601, mean_q: 2.774648, mean_eps: 0.100000\n",
      "  43621/175000: episode: 1232, duration: 0.903s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 163.791 [39.000, 222.000], mean observation: 0.208 [0.000, 86.000], loss: 0.760605, mean_absolute_error: 0.387615, mean_q: 2.374838, mean_eps: 0.100000\n",
      "  43642/175000: episode: 1233, duration: 0.475s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 63.190 [53.000, 204.000], mean observation: 0.069 [0.000, 42.000], loss: 0.660919, mean_absolute_error: 0.400335, mean_q: 2.496142, mean_eps: 0.100000\n",
      "  43681/175000: episode: 1234, duration: 0.883s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.538 [53.000, 219.000], mean observation: 0.247 [0.000, 78.000], loss: 4.657092, mean_absolute_error: 0.655178, mean_q: 3.600782, mean_eps: 0.100000\n",
      "  43720/175000: episode: 1235, duration: 0.905s, episode steps: 39, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.615 [5.000, 200.000], mean observation: 0.366 [0.000, 78.000], loss: 3.873622, mean_absolute_error: 0.423090, mean_q: 2.555205, mean_eps: 0.100000\n",
      "  43767/175000: episode: 1236, duration: 0.964s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 114.894 [53.000, 206.000], mean observation: 0.353 [0.000, 94.000], loss: 2.540675, mean_absolute_error: 0.529901, mean_q: 3.156990, mean_eps: 0.100000\n",
      "  43825/175000: episode: 1237, duration: 1.254s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 147.397 [28.000, 218.000], mean observation: 0.419 [0.000, 116.000], loss: 8.265475, mean_absolute_error: 0.616753, mean_q: 3.562024, mean_eps: 0.100000\n",
      "  43854/175000: episode: 1238, duration: 0.598s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.310 [53.000, 183.000], mean observation: 0.151 [0.000, 58.000], loss: 0.199475, mean_absolute_error: 0.396416, mean_q: 2.823287, mean_eps: 0.100000\n",
      "  43876/175000: episode: 1239, duration: 0.456s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 86.455 [83.000, 154.000], mean observation: 0.086 [0.000, 44.000], loss: 6.791997, mean_absolute_error: 0.535395, mean_q: 3.344936, mean_eps: 0.100000\n",
      "  43922/175000: episode: 1240, duration: 1.017s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 65.826 [28.000, 224.000], mean observation: 0.244 [0.000, 92.000], loss: 2.322396, mean_absolute_error: 0.573456, mean_q: 3.406792, mean_eps: 0.100000\n",
      "  43944/175000: episode: 1241, duration: 0.520s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 57.182 [21.000, 140.000], mean observation: 0.161 [0.000, 44.000], loss: 0.270003, mean_absolute_error: 0.497685, mean_q: 3.008490, mean_eps: 0.100000\n",
      "  43973/175000: episode: 1242, duration: 0.679s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 130.034 [14.000, 199.000], mean observation: 0.088 [0.000, 58.000], loss: 1.823331, mean_absolute_error: 0.424153, mean_q: 2.706211, mean_eps: 0.100000\n",
      "  44007/175000: episode: 1243, duration: 0.621s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 181.500 [3.000, 215.000], mean observation: 0.248 [0.000, 68.000], loss: 0.546248, mean_absolute_error: 0.403148, mean_q: 2.635035, mean_eps: 0.100000\n",
      "  44045/175000: episode: 1244, duration: 0.795s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.132 [24.000, 217.000], mean observation: 0.241 [0.000, 76.000], loss: 3.948570, mean_absolute_error: 0.422447, mean_q: 2.578354, mean_eps: 0.100000\n",
      "  44076/175000: episode: 1245, duration: 0.647s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 145.097 [6.000, 211.000], mean observation: 0.203 [0.000, 62.000], loss: 2.320244, mean_absolute_error: 0.463895, mean_q: 2.860930, mean_eps: 0.100000\n",
      "  44117/175000: episode: 1246, duration: 0.896s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.829 [69.000, 199.000], mean observation: 0.273 [0.000, 82.000], loss: 1.569967, mean_absolute_error: 0.457306, mean_q: 2.714450, mean_eps: 0.100000\n",
      "  44158/175000: episode: 1247, duration: 0.801s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.146 [2.000, 215.000], mean observation: 0.316 [0.000, 82.000], loss: 1.634096, mean_absolute_error: 0.405119, mean_q: 2.476941, mean_eps: 0.100000\n",
      "  44198/175000: episode: 1248, duration: 0.842s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 182.000 [15.000, 215.000], mean observation: 0.387 [0.000, 80.000], loss: 1.778603, mean_absolute_error: 0.530112, mean_q: 3.138502, mean_eps: 0.100000\n",
      "  44228/175000: episode: 1249, duration: 0.681s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.267 [1.000, 218.000], mean observation: 0.270 [0.000, 60.000], loss: 2.282068, mean_absolute_error: 0.528808, mean_q: 2.834562, mean_eps: 0.100000\n",
      "  44272/175000: episode: 1250, duration: 0.944s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.295 [9.000, 219.000], mean observation: 0.469 [0.000, 88.000], loss: 0.502266, mean_absolute_error: 0.470561, mean_q: 2.607858, mean_eps: 0.100000\n",
      "  44316/175000: episode: 1251, duration: 0.992s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 119.795 [17.000, 199.000], mean observation: 0.346 [0.000, 88.000], loss: 3.114926, mean_absolute_error: 0.487217, mean_q: 2.755363, mean_eps: 0.100000\n",
      "  44356/175000: episode: 1252, duration: 0.855s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 142.250 [0.000, 223.000], mean observation: 0.249 [0.000, 80.000], loss: 3.989068, mean_absolute_error: 0.486181, mean_q: 2.727052, mean_eps: 0.100000\n",
      "  44399/175000: episode: 1253, duration: 0.820s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 123.907 [32.000, 183.000], mean observation: 0.170 [0.000, 86.000], loss: 1.756224, mean_absolute_error: 0.459958, mean_q: 2.570846, mean_eps: 0.100000\n",
      "  44433/175000: episode: 1254, duration: 0.720s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 72.000 [49.000, 186.000], mean observation: 0.178 [0.000, 68.000], loss: 3.333979, mean_absolute_error: 0.533105, mean_q: 3.029151, mean_eps: 0.100000\n",
      "  44467/175000: episode: 1255, duration: 0.700s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 53.235 [49.000, 193.000], mean observation: 0.083 [0.000, 68.000], loss: 1.834539, mean_absolute_error: 0.420290, mean_q: 2.448847, mean_eps: 0.100000\n",
      "  44493/175000: episode: 1256, duration: 0.534s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 68.577 [27.000, 211.000], mean observation: 0.116 [0.000, 52.000], loss: 1.968554, mean_absolute_error: 0.459706, mean_q: 2.710154, mean_eps: 0.100000\n",
      "  44528/175000: episode: 1257, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 51.600 [3.000, 136.000], mean observation: 0.099 [0.000, 70.000], loss: 1.946780, mean_absolute_error: 0.495660, mean_q: 2.932773, mean_eps: 0.100000\n",
      "  44565/175000: episode: 1258, duration: 0.796s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.622 [11.000, 183.000], mean observation: 0.190 [0.000, 74.000], loss: 0.176445, mean_absolute_error: 0.351328, mean_q: 2.080100, mean_eps: 0.100000\n",
      "  44603/175000: episode: 1259, duration: 0.720s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.974 [11.000, 222.000], mean observation: 0.107 [0.000, 76.000], loss: 0.192029, mean_absolute_error: 0.351711, mean_q: 2.126264, mean_eps: 0.100000\n",
      "  44639/175000: episode: 1260, duration: 0.703s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 19.611 [11.000, 141.000], mean observation: 0.200 [0.000, 72.000], loss: 0.794498, mean_absolute_error: 0.404592, mean_q: 2.257928, mean_eps: 0.100000\n",
      "  44675/175000: episode: 1261, duration: 0.702s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 113.528 [11.000, 204.000], mean observation: 0.305 [0.000, 72.000], loss: 3.916768, mean_absolute_error: 0.580850, mean_q: 3.194670, mean_eps: 0.100000\n",
      "  44702/175000: episode: 1262, duration: 0.547s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 51.037 [11.000, 219.000], mean observation: 0.178 [0.000, 54.000], loss: 6.077535, mean_absolute_error: 0.575780, mean_q: 3.155228, mean_eps: 0.100000\n",
      "  44733/175000: episode: 1263, duration: 0.623s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 37.290 [11.000, 210.000], mean observation: 0.230 [0.000, 62.000], loss: 0.212999, mean_absolute_error: 0.463012, mean_q: 2.796545, mean_eps: 0.100000\n",
      "  44756/175000: episode: 1264, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 35.522 [11.000, 188.000], mean observation: 0.132 [0.000, 46.000], loss: 2.278330, mean_absolute_error: 0.472425, mean_q: 2.943165, mean_eps: 0.100000\n",
      "  44781/175000: episode: 1265, duration: 0.525s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 13.840 [11.000, 37.000], mean observation: 0.097 [0.000, 50.000], loss: 4.056911, mean_absolute_error: 0.539592, mean_q: 3.139714, mean_eps: 0.100000\n",
      "  44821/175000: episode: 1266, duration: 0.837s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 26.375 [11.000, 219.000], mean observation: 0.183 [0.000, 80.000], loss: 8.710400, mean_absolute_error: 0.566399, mean_q: 3.193938, mean_eps: 0.100000\n",
      "  44850/175000: episode: 1267, duration: 0.581s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 48.483 [11.000, 221.000], mean observation: 0.130 [0.000, 58.000], loss: 2.398418, mean_absolute_error: 0.545498, mean_q: 3.156063, mean_eps: 0.100000\n",
      "  44878/175000: episode: 1268, duration: 0.582s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.321 [11.000, 161.000], mean observation: 0.124 [0.000, 56.000], loss: 5.745887, mean_absolute_error: 0.582717, mean_q: 3.291665, mean_eps: 0.100000\n",
      "  44913/175000: episode: 1269, duration: 0.775s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.800 [11.000, 81.000], mean observation: 0.141 [0.000, 70.000], loss: 6.112890, mean_absolute_error: 0.467690, mean_q: 2.699285, mean_eps: 0.100000\n",
      "  44940/175000: episode: 1270, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.741 [53.000, 211.000], mean observation: 0.165 [0.000, 54.000], loss: 3.581352, mean_absolute_error: 0.464552, mean_q: 2.692848, mean_eps: 0.100000\n",
      "  44982/175000: episode: 1271, duration: 0.831s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.571 [49.000, 211.000], mean observation: 0.292 [0.000, 84.000], loss: 8.425577, mean_absolute_error: 0.537372, mean_q: 3.048779, mean_eps: 0.100000\n",
      "  45025/175000: episode: 1272, duration: 0.818s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 146.581 [49.000, 216.000], mean observation: 0.149 [0.000, 86.000], loss: 1.945188, mean_absolute_error: 0.409282, mean_q: 2.419596, mean_eps: 0.100000\n",
      "  45056/175000: episode: 1273, duration: 0.665s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 194.935 [43.000, 216.000], mean observation: 0.142 [0.000, 62.000], loss: 4.839852, mean_absolute_error: 0.540482, mean_q: 3.095657, mean_eps: 0.100000\n",
      "  45103/175000: episode: 1274, duration: 0.982s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 95.979 [43.000, 216.000], mean observation: 0.426 [0.000, 94.000], loss: 0.922804, mean_absolute_error: 0.474387, mean_q: 2.788541, mean_eps: 0.100000\n",
      "  45146/175000: episode: 1275, duration: 0.917s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.419 [16.000, 216.000], mean observation: 0.242 [0.000, 86.000], loss: 1.395662, mean_absolute_error: 0.490199, mean_q: 2.896722, mean_eps: 0.100000\n",
      "  45177/175000: episode: 1276, duration: 0.675s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 132.452 [0.000, 216.000], mean observation: 0.128 [0.000, 62.000], loss: 6.617824, mean_absolute_error: 0.577823, mean_q: 3.220451, mean_eps: 0.100000\n",
      "  45203/175000: episode: 1277, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 179.115 [107.000, 182.000], mean observation: 0.063 [0.000, 52.000], loss: 0.263658, mean_absolute_error: 0.413763, mean_q: 2.425485, mean_eps: 0.100000\n",
      "  45234/175000: episode: 1278, duration: 0.663s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 169.935 [35.000, 182.000], mean observation: 0.152 [0.000, 62.000], loss: 4.415204, mean_absolute_error: 0.517676, mean_q: 2.929167, mean_eps: 0.100000\n",
      "  45266/175000: episode: 1279, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 98.688 [2.000, 218.000], mean observation: 0.207 [0.000, 64.000], loss: 2.935054, mean_absolute_error: 0.527206, mean_q: 2.903256, mean_eps: 0.100000\n",
      "  45295/175000: episode: 1280, duration: 0.603s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 173.897 [41.000, 182.000], mean observation: 0.138 [0.000, 58.000], loss: 5.078295, mean_absolute_error: 0.645647, mean_q: 3.456616, mean_eps: 0.100000\n",
      "  45348/175000: episode: 1281, duration: 1.200s, episode steps: 53, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 144.075 [49.000, 189.000], mean observation: 0.241 [0.000, 106.000], loss: 6.978615, mean_absolute_error: 0.599133, mean_q: 3.061131, mean_eps: 0.100000\n",
      "  45376/175000: episode: 1282, duration: 0.658s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 169.964 [13.000, 182.000], mean observation: 0.082 [0.000, 56.000], loss: 1.095484, mean_absolute_error: 0.521090, mean_q: 2.661598, mean_eps: 0.100000\n",
      "  45417/175000: episode: 1283, duration: 0.976s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.439 [37.000, 182.000], mean observation: 0.110 [0.000, 82.000], loss: 1.946255, mean_absolute_error: 0.547197, mean_q: 2.778563, mean_eps: 0.100000\n",
      "  45455/175000: episode: 1284, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.737 [3.000, 191.000], mean observation: 0.173 [0.000, 76.000], loss: 0.309175, mean_absolute_error: 0.457116, mean_q: 2.261442, mean_eps: 0.100000\n",
      "  45491/175000: episode: 1285, duration: 0.790s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.083 [11.000, 212.000], mean observation: 0.256 [0.000, 72.000], loss: 0.953197, mean_absolute_error: 0.534486, mean_q: 2.750731, mean_eps: 0.100000\n",
      "  45511/175000: episode: 1286, duration: 0.412s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.250 [11.000, 191.000], mean observation: 0.113 [0.000, 40.000], loss: 3.151748, mean_absolute_error: 0.624947, mean_q: 3.210395, mean_eps: 0.100000\n",
      "  45535/175000: episode: 1287, duration: 0.498s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 172.167 [87.000, 182.000], mean observation: 0.072 [0.000, 48.000], loss: 0.343609, mean_absolute_error: 0.532791, mean_q: 2.744599, mean_eps: 0.100000\n",
      "  45568/175000: episode: 1288, duration: 0.784s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 127.364 [5.000, 182.000], mean observation: 0.202 [0.000, 66.000], loss: 0.610852, mean_absolute_error: 0.531226, mean_q: 2.783368, mean_eps: 0.100000\n",
      "  45602/175000: episode: 1289, duration: 0.779s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 165.853 [41.000, 182.000], mean observation: 0.108 [0.000, 68.000], loss: 2.397456, mean_absolute_error: 0.521017, mean_q: 2.697017, mean_eps: 0.100000\n",
      "  45628/175000: episode: 1290, duration: 0.587s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.269 [80.000, 135.000], mean observation: 0.093 [0.000, 52.000], loss: 0.424348, mean_absolute_error: 0.516902, mean_q: 2.785217, mean_eps: 0.100000\n",
      "  45652/175000: episode: 1291, duration: 0.601s, episode steps: 24, steps per second: 40, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 82.708 [80.000, 138.000], mean observation: 0.102 [0.000, 48.000], loss: 0.170596, mean_absolute_error: 0.469628, mean_q: 2.622819, mean_eps: 0.100000\n",
      "  45699/175000: episode: 1292, duration: 1.170s, episode steps: 47, steps per second: 40, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 69.191 [19.000, 170.000], mean observation: 0.332 [0.000, 94.000], loss: 4.646150, mean_absolute_error: 0.527588, mean_q: 2.856086, mean_eps: 0.100000\n",
      "  45743/175000: episode: 1293, duration: 1.022s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.136 [16.000, 192.000], mean observation: 0.269 [0.000, 88.000], loss: 7.619244, mean_absolute_error: 0.510177, mean_q: 2.715968, mean_eps: 0.100000\n",
      "  45784/175000: episode: 1294, duration: 1.037s, episode steps: 41, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 61.122 [28.000, 215.000], mean observation: 0.242 [0.000, 82.000], loss: 0.527177, mean_absolute_error: 0.434533, mean_q: 2.521634, mean_eps: 0.100000\n",
      "  45811/175000: episode: 1295, duration: 0.594s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 129.185 [11.000, 156.000], mean observation: 0.160 [0.000, 54.000], loss: 2.092845, mean_absolute_error: 0.427458, mean_q: 2.461393, mean_eps: 0.100000\n",
      "  45849/175000: episode: 1296, duration: 0.997s, episode steps: 38, steps per second: 38, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.605 [11.000, 156.000], mean observation: 0.392 [0.000, 76.000], loss: 4.921760, mean_absolute_error: 0.511120, mean_q: 2.812434, mean_eps: 0.100000\n",
      "  45891/175000: episode: 1297, duration: 0.936s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 96.238 [11.000, 193.000], mean observation: 0.306 [0.000, 84.000], loss: 5.406665, mean_absolute_error: 0.564420, mean_q: 3.110890, mean_eps: 0.100000\n",
      "  45933/175000: episode: 1298, duration: 0.985s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.595 [11.000, 223.000], mean observation: 0.634 [0.000, 84.000], loss: 4.216367, mean_absolute_error: 0.510073, mean_q: 2.897108, mean_eps: 0.100000\n",
      "  45969/175000: episode: 1299, duration: 0.957s, episode steps: 36, steps per second: 38, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 93.250 [7.000, 208.000], mean observation: 0.421 [0.000, 72.000], loss: 8.940468, mean_absolute_error: 0.516944, mean_q: 2.835510, mean_eps: 0.100000\n",
      "  46005/175000: episode: 1300, duration: 0.800s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 121.889 [11.000, 222.000], mean observation: 0.330 [0.000, 72.000], loss: 0.329564, mean_absolute_error: 0.443677, mean_q: 2.625647, mean_eps: 0.100000\n",
      "  46040/175000: episode: 1301, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.800 [16.000, 163.000], mean observation: 0.211 [0.000, 70.000], loss: 0.424804, mean_absolute_error: 0.457763, mean_q: 2.689158, mean_eps: 0.100000\n",
      "  46084/175000: episode: 1302, duration: 0.983s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.000 [0.000, 212.000], mean observation: 0.262 [0.000, 88.000], loss: 1.024603, mean_absolute_error: 0.482219, mean_q: 2.868059, mean_eps: 0.100000\n",
      "  46130/175000: episode: 1303, duration: 0.948s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.109 [20.000, 158.000], mean observation: 0.276 [0.000, 92.000], loss: 1.420863, mean_absolute_error: 0.442311, mean_q: 2.700012, mean_eps: 0.100000\n",
      "  46171/175000: episode: 1304, duration: 0.895s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.415 [18.000, 151.000], mean observation: 0.228 [0.000, 82.000], loss: 0.463002, mean_absolute_error: 0.388197, mean_q: 2.492075, mean_eps: 0.100000\n",
      "  46212/175000: episode: 1305, duration: 0.971s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.098 [18.000, 221.000], mean observation: 0.253 [0.000, 82.000], loss: 0.815895, mean_absolute_error: 0.449280, mean_q: 2.668922, mean_eps: 0.100000\n",
      "  46234/175000: episode: 1306, duration: 0.516s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 122.182 [18.000, 221.000], mean observation: 0.129 [0.000, 44.000], loss: 2.573150, mean_absolute_error: 0.625566, mean_q: 3.502190, mean_eps: 0.100000\n",
      "  46257/175000: episode: 1307, duration: 0.550s, episode steps: 23, steps per second: 42, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 95.391 [34.000, 224.000], mean observation: 0.111 [0.000, 46.000], loss: 3.460170, mean_absolute_error: 0.576612, mean_q: 3.256637, mean_eps: 0.100000\n",
      "  46283/175000: episode: 1308, duration: 0.544s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 85.885 [0.000, 214.000], mean observation: 0.157 [0.000, 52.000], loss: 1.521285, mean_absolute_error: 0.435787, mean_q: 2.579296, mean_eps: 0.100000\n",
      "  46331/175000: episode: 1309, duration: 1.115s, episode steps: 48, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.812 [0.000, 212.000], mean observation: 0.412 [0.000, 96.000], loss: 1.269192, mean_absolute_error: 0.504660, mean_q: 2.930367, mean_eps: 0.100000\n",
      "  46379/175000: episode: 1310, duration: 0.964s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 87.042 [0.000, 188.000], mean observation: 0.414 [0.000, 96.000], loss: 4.856408, mean_absolute_error: 0.684927, mean_q: 3.739371, mean_eps: 0.100000\n",
      "  46417/175000: episode: 1311, duration: 0.863s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 64.816 [0.000, 223.000], mean observation: 0.210 [0.000, 76.000], loss: 2.925447, mean_absolute_error: 0.565538, mean_q: 3.294419, mean_eps: 0.100000\n",
      "  46459/175000: episode: 1312, duration: 0.852s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.786 [0.000, 205.000], mean observation: 0.418 [0.000, 84.000], loss: 0.377337, mean_absolute_error: 0.598062, mean_q: 3.511036, mean_eps: 0.100000\n",
      "  46496/175000: episode: 1313, duration: 0.839s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 97.676 [47.000, 221.000], mean observation: 0.226 [0.000, 74.000], loss: 11.676919, mean_absolute_error: 0.833735, mean_q: 4.410857, mean_eps: 0.100000\n",
      "  46537/175000: episode: 1314, duration: 0.937s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.780 [11.000, 218.000], mean observation: 0.327 [0.000, 82.000], loss: 3.188343, mean_absolute_error: 0.550278, mean_q: 3.406333, mean_eps: 0.100000\n",
      "  46574/175000: episode: 1315, duration: 0.760s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.459 [1.000, 97.000], mean observation: 0.211 [0.000, 74.000], loss: 4.857095, mean_absolute_error: 0.706508, mean_q: 4.101905, mean_eps: 0.100000\n",
      "  46608/175000: episode: 1316, duration: 0.781s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.471 [1.000, 199.000], mean observation: 0.268 [0.000, 68.000], loss: 1.753264, mean_absolute_error: 0.490931, mean_q: 3.222777, mean_eps: 0.100000\n",
      "  46653/175000: episode: 1317, duration: 1.064s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.844 [1.000, 213.000], mean observation: 0.341 [0.000, 90.000], loss: 2.232551, mean_absolute_error: 0.482467, mean_q: 3.085295, mean_eps: 0.100000\n",
      "  46675/175000: episode: 1318, duration: 0.422s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 87.273 [1.000, 214.000], mean observation: 0.155 [0.000, 44.000], loss: 0.174658, mean_absolute_error: 0.398859, mean_q: 2.727041, mean_eps: 0.100000\n",
      "  46687/175000: episode: 1319, duration: 0.235s, episode steps: 12, steps per second: 51, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 210.583 [206.000, 213.000], mean observation: 0.046 [0.000, 24.000], loss: 5.307523, mean_absolute_error: 0.661880, mean_q: 3.685313, mean_eps: 0.100000\n",
      "  46738/175000: episode: 1320, duration: 1.164s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 181.529 [29.000, 213.000], mean observation: 0.368 [0.000, 102.000], loss: 0.679656, mean_absolute_error: 0.368000, mean_q: 2.540163, mean_eps: 0.100000\n",
      "  46780/175000: episode: 1321, duration: 1.045s, episode steps: 42, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.429 [44.000, 214.000], mean observation: 0.335 [0.000, 84.000], loss: 1.046857, mean_absolute_error: 0.429306, mean_q: 2.799351, mean_eps: 0.100000\n",
      "  46813/175000: episode: 1322, duration: 0.849s, episode steps: 33, steps per second: 39, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.061 [1.000, 162.000], mean observation: 0.189 [0.000, 66.000], loss: 0.690635, mean_absolute_error: 0.444727, mean_q: 2.905831, mean_eps: 0.100000\n",
      "  46857/175000: episode: 1323, duration: 0.983s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.273 [40.000, 135.000], mean observation: 0.247 [0.000, 88.000], loss: 3.361691, mean_absolute_error: 0.484555, mean_q: 3.106783, mean_eps: 0.100000\n",
      "  46895/175000: episode: 1324, duration: 0.834s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.816 [25.000, 218.000], mean observation: 0.305 [0.000, 76.000], loss: 0.238055, mean_absolute_error: 0.375230, mean_q: 2.485620, mean_eps: 0.100000\n",
      "  46923/175000: episode: 1325, duration: 0.632s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.714 [39.000, 204.000], mean observation: 0.142 [0.000, 56.000], loss: 0.194212, mean_absolute_error: 0.394188, mean_q: 2.614317, mean_eps: 0.100000\n",
      "  46956/175000: episode: 1326, duration: 0.803s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.394 [26.000, 151.000], mean observation: 0.294 [0.000, 66.000], loss: 11.195768, mean_absolute_error: 0.681972, mean_q: 3.735626, mean_eps: 0.100000\n",
      "  46998/175000: episode: 1327, duration: 1.002s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 72.976 [4.000, 139.000], mean observation: 0.167 [0.000, 84.000], loss: 3.398360, mean_absolute_error: 0.539636, mean_q: 3.212929, mean_eps: 0.100000\n",
      "  47039/175000: episode: 1328, duration: 0.953s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.537 [3.000, 221.000], mean observation: 0.307 [0.000, 82.000], loss: 1.418336, mean_absolute_error: 0.452562, mean_q: 2.841273, mean_eps: 0.100000\n",
      "  47057/175000: episode: 1329, duration: 0.403s, episode steps: 18, steps per second: 45, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 106.500 [32.000, 211.000], mean observation: 0.144 [0.000, 36.000], loss: 6.976594, mean_absolute_error: 0.801844, mean_q: 4.460484, mean_eps: 0.100000\n",
      "  47089/175000: episode: 1330, duration: 0.742s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 75.656 [9.000, 203.000], mean observation: 0.282 [0.000, 64.000], loss: 5.132276, mean_absolute_error: 0.638696, mean_q: 3.716978, mean_eps: 0.100000\n",
      "  47128/175000: episode: 1331, duration: 0.846s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 69.462 [9.000, 197.000], mean observation: 0.219 [0.000, 78.000], loss: 1.279539, mean_absolute_error: 0.577551, mean_q: 3.538867, mean_eps: 0.100000\n",
      "  47186/175000: episode: 1332, duration: 1.369s, episode steps: 58, steps per second: 42, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 57.569 [11.000, 196.000], mean observation: 0.343 [0.000, 116.000], loss: 2.603953, mean_absolute_error: 0.513073, mean_q: 3.197390, mean_eps: 0.100000\n",
      "  47238/175000: episode: 1333, duration: 1.151s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.538 [50.000, 214.000], mean observation: 0.233 [0.000, 104.000], loss: 6.235244, mean_absolute_error: 0.593729, mean_q: 3.522736, mean_eps: 0.100000\n",
      "  47267/175000: episode: 1334, duration: 0.637s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.379 [25.000, 209.000], mean observation: 0.123 [0.000, 58.000], loss: 4.362929, mean_absolute_error: 0.463936, mean_q: 2.944398, mean_eps: 0.100000\n",
      "  47288/175000: episode: 1335, duration: 0.514s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 179.190 [67.000, 209.000], mean observation: 0.077 [0.000, 42.000], loss: 0.199241, mean_absolute_error: 0.488334, mean_q: 3.175151, mean_eps: 0.100000\n",
      "  47329/175000: episode: 1336, duration: 0.973s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.195 [38.000, 209.000], mean observation: 0.202 [0.000, 82.000], loss: 4.342822, mean_absolute_error: 0.611689, mean_q: 3.755630, mean_eps: 0.100000\n",
      "  47358/175000: episode: 1337, duration: 0.630s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.414 [3.000, 209.000], mean observation: 0.104 [0.000, 58.000], loss: 1.013974, mean_absolute_error: 0.507070, mean_q: 3.163554, mean_eps: 0.100000\n",
      "  47397/175000: episode: 1338, duration: 0.882s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.538 [15.000, 209.000], mean observation: 0.265 [0.000, 78.000], loss: 0.338862, mean_absolute_error: 0.445960, mean_q: 2.924083, mean_eps: 0.100000\n",
      "  47445/175000: episode: 1339, duration: 1.131s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.729 [9.000, 217.000], mean observation: 0.515 [0.000, 96.000], loss: 1.765265, mean_absolute_error: 0.589128, mean_q: 3.607549, mean_eps: 0.100000\n",
      "  47488/175000: episode: 1340, duration: 0.961s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.628 [15.000, 201.000], mean observation: 0.252 [0.000, 86.000], loss: 11.191080, mean_absolute_error: 0.591195, mean_q: 3.296627, mean_eps: 0.100000\n",
      "  47547/175000: episode: 1341, duration: 1.288s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 87.475 [15.000, 152.000], mean observation: 0.516 [0.000, 118.000], loss: 2.749710, mean_absolute_error: 0.554546, mean_q: 3.305110, mean_eps: 0.100000\n",
      "  47595/175000: episode: 1342, duration: 1.127s, episode steps: 48, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 136.312 [27.000, 163.000], mean observation: 0.243 [0.000, 96.000], loss: 3.465869, mean_absolute_error: 0.466724, mean_q: 2.896026, mean_eps: 0.100000\n",
      "  47635/175000: episode: 1343, duration: 0.883s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 82.600 [15.000, 208.000], mean observation: 0.345 [0.000, 80.000], loss: 1.012712, mean_absolute_error: 0.496001, mean_q: 3.043369, mean_eps: 0.100000\n",
      "  47663/175000: episode: 1344, duration: 0.620s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 145.786 [56.000, 211.000], mean observation: 0.129 [0.000, 56.000], loss: 3.330934, mean_absolute_error: 0.604684, mean_q: 3.447396, mean_eps: 0.100000\n",
      "  47702/175000: episode: 1345, duration: 0.844s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 150.513 [69.000, 167.000], mean observation: 0.164 [0.000, 78.000], loss: 2.379299, mean_absolute_error: 0.451481, mean_q: 2.675409, mean_eps: 0.100000\n",
      "  47740/175000: episode: 1346, duration: 0.907s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.526 [69.000, 177.000], mean observation: 0.158 [0.000, 76.000], loss: 1.131413, mean_absolute_error: 0.492580, mean_q: 2.936021, mean_eps: 0.100000\n",
      "  47782/175000: episode: 1347, duration: 0.996s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.905 [52.000, 211.000], mean observation: 0.273 [0.000, 84.000], loss: 3.372980, mean_absolute_error: 0.632922, mean_q: 3.454270, mean_eps: 0.100000\n",
      "  47807/175000: episode: 1348, duration: 0.518s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 109.400 [65.000, 152.000], mean observation: 0.126 [0.000, 50.000], loss: 0.936654, mean_absolute_error: 0.441098, mean_q: 2.640427, mean_eps: 0.100000\n",
      "  47847/175000: episode: 1349, duration: 0.885s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 116.175 [7.000, 163.000], mean observation: 0.378 [0.000, 80.000], loss: 6.565986, mean_absolute_error: 0.627090, mean_q: 3.387035, mean_eps: 0.100000\n",
      "  47886/175000: episode: 1350, duration: 0.812s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 108.667 [2.000, 170.000], mean observation: 0.208 [0.000, 78.000], loss: 1.891782, mean_absolute_error: 0.618006, mean_q: 3.482416, mean_eps: 0.100000\n",
      "  47911/175000: episode: 1351, duration: 0.499s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 114.720 [45.000, 190.000], mean observation: 0.158 [0.000, 50.000], loss: 0.985854, mean_absolute_error: 0.596674, mean_q: 3.489968, mean_eps: 0.100000\n",
      "  47951/175000: episode: 1352, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.125 [14.000, 202.000], mean observation: 0.209 [0.000, 80.000], loss: 3.311729, mean_absolute_error: 0.583524, mean_q: 3.489506, mean_eps: 0.100000\n",
      "  48002/175000: episode: 1353, duration: 1.115s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.627 [14.000, 192.000], mean observation: 0.306 [0.000, 102.000], loss: 2.803753, mean_absolute_error: 0.578449, mean_q: 3.507605, mean_eps: 0.100000\n",
      "  48038/175000: episode: 1354, duration: 0.884s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.972 [12.000, 158.000], mean observation: 0.108 [0.000, 72.000], loss: 0.566469, mean_absolute_error: 0.486370, mean_q: 3.125243, mean_eps: 0.100000\n",
      "  48075/175000: episode: 1355, duration: 0.808s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.703 [11.000, 107.000], mean observation: 0.162 [0.000, 74.000], loss: 0.453430, mean_absolute_error: 0.557462, mean_q: 3.402035, mean_eps: 0.100000\n",
      "  48122/175000: episode: 1356, duration: 1.035s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 74.532 [11.000, 204.000], mean observation: 0.290 [0.000, 94.000], loss: 0.737003, mean_absolute_error: 0.493852, mean_q: 3.173508, mean_eps: 0.100000\n",
      "  48160/175000: episode: 1357, duration: 0.841s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.289 [3.000, 174.000], mean observation: 0.266 [0.000, 76.000], loss: 8.462336, mean_absolute_error: 0.647488, mean_q: 3.692753, mean_eps: 0.100000\n",
      "  48191/175000: episode: 1358, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 56.032 [11.000, 111.000], mean observation: 0.153 [0.000, 62.000], loss: 2.409754, mean_absolute_error: 0.579192, mean_q: 3.476103, mean_eps: 0.100000\n",
      "  48226/175000: episode: 1359, duration: 0.773s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 57.914 [11.000, 173.000], mean observation: 0.227 [0.000, 70.000], loss: 3.262756, mean_absolute_error: 0.584861, mean_q: 3.499800, mean_eps: 0.100000\n",
      "  48274/175000: episode: 1360, duration: 1.050s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 104.771 [7.000, 216.000], mean observation: 0.505 [0.000, 96.000], loss: 2.920008, mean_absolute_error: 0.537299, mean_q: 3.297148, mean_eps: 0.100000\n",
      "  48327/175000: episode: 1361, duration: 1.234s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 127.736 [15.000, 177.000], mean observation: 0.185 [0.000, 106.000], loss: 0.367258, mean_absolute_error: 0.422552, mean_q: 2.830739, mean_eps: 0.100000\n",
      "  48351/175000: episode: 1362, duration: 0.573s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 136.375 [7.000, 142.000], mean observation: 0.071 [0.000, 48.000], loss: 0.761032, mean_absolute_error: 0.492132, mean_q: 3.195595, mean_eps: 0.100000\n",
      "  48398/175000: episode: 1363, duration: 1.026s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.021 [67.000, 224.000], mean observation: 0.175 [0.000, 94.000], loss: 0.783751, mean_absolute_error: 0.557010, mean_q: 3.561195, mean_eps: 0.100000\n",
      "  48445/175000: episode: 1364, duration: 0.979s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 142.936 [56.000, 218.000], mean observation: 0.229 [0.000, 94.000], loss: 12.697129, mean_absolute_error: 0.714843, mean_q: 4.072659, mean_eps: 0.100000\n",
      "  48479/175000: episode: 1365, duration: 0.730s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.353 [1.000, 222.000], mean observation: 0.124 [0.000, 68.000], loss: 1.359800, mean_absolute_error: 0.596463, mean_q: 3.603343, mean_eps: 0.100000\n",
      "  48517/175000: episode: 1366, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 150.921 [38.000, 222.000], mean observation: 0.202 [0.000, 76.000], loss: 1.797494, mean_absolute_error: 0.589552, mean_q: 3.591598, mean_eps: 0.100000\n",
      "  48558/175000: episode: 1367, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.659 [11.000, 222.000], mean observation: 0.417 [0.000, 82.000], loss: 0.277906, mean_absolute_error: 0.414327, mean_q: 2.816772, mean_eps: 0.100000\n",
      "  48590/175000: episode: 1368, duration: 0.711s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 168.375 [48.000, 222.000], mean observation: 0.190 [0.000, 64.000], loss: 4.663086, mean_absolute_error: 0.688053, mean_q: 4.057435, mean_eps: 0.100000\n",
      "  48624/175000: episode: 1369, duration: 0.798s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 143.265 [9.000, 222.000], mean observation: 0.208 [0.000, 68.000], loss: 0.336955, mean_absolute_error: 0.471004, mean_q: 3.078533, mean_eps: 0.100000\n",
      "  48666/175000: episode: 1370, duration: 0.970s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 191.690 [53.000, 222.000], mean observation: 0.152 [0.000, 84.000], loss: 2.696778, mean_absolute_error: 0.551961, mean_q: 3.367146, mean_eps: 0.100000\n",
      "  48698/175000: episode: 1371, duration: 0.682s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 169.312 [11.000, 222.000], mean observation: 0.124 [0.000, 64.000], loss: 1.220486, mean_absolute_error: 0.485206, mean_q: 2.998784, mean_eps: 0.100000\n",
      "  48740/175000: episode: 1372, duration: 0.990s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 117.452 [16.000, 222.000], mean observation: 0.403 [0.000, 84.000], loss: 4.734957, mean_absolute_error: 0.621077, mean_q: 3.535571, mean_eps: 0.100000\n",
      "  48770/175000: episode: 1373, duration: 0.669s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 137.633 [47.000, 204.000], mean observation: 0.110 [0.000, 60.000], loss: 4.805708, mean_absolute_error: 0.666726, mean_q: 3.758099, mean_eps: 0.100000\n",
      "  48805/175000: episode: 1374, duration: 0.804s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.971 [74.000, 224.000], mean observation: 0.136 [0.000, 70.000], loss: 2.393486, mean_absolute_error: 0.575007, mean_q: 3.402181, mean_eps: 0.100000\n",
      "  48861/175000: episode: 1375, duration: 1.270s, episode steps: 56, steps per second: 44, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 81.107 [3.000, 201.000], mean observation: 0.303 [0.000, 112.000], loss: 3.779722, mean_absolute_error: 0.531438, mean_q: 3.094923, mean_eps: 0.100000\n",
      "  48886/175000: episode: 1376, duration: 0.619s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 110.560 [21.000, 192.000], mean observation: 0.072 [0.000, 50.000], loss: 1.851297, mean_absolute_error: 0.663544, mean_q: 3.821379, mean_eps: 0.100000\n",
      "  48921/175000: episode: 1377, duration: 0.865s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.486 [21.000, 181.000], mean observation: 0.132 [0.000, 70.000], loss: 3.295253, mean_absolute_error: 0.698919, mean_q: 3.863283, mean_eps: 0.100000\n",
      "  48965/175000: episode: 1378, duration: 0.970s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.818 [11.000, 218.000], mean observation: 0.359 [0.000, 88.000], loss: 1.944403, mean_absolute_error: 0.587513, mean_q: 3.489508, mean_eps: 0.100000\n",
      "  48971/175000: episode: 1379, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 107.833 [83.000, 141.000], mean observation: 0.027 [0.000, 12.000], loss: 0.153134, mean_absolute_error: 0.373544, mean_q: 2.529405, mean_eps: 0.100000\n",
      "  49010/175000: episode: 1380, duration: 0.874s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.179 [11.000, 181.000], mean observation: 0.339 [0.000, 78.000], loss: 4.128558, mean_absolute_error: 0.565253, mean_q: 3.463062, mean_eps: 0.100000\n",
      "  49044/175000: episode: 1381, duration: 0.822s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.059 [9.000, 175.000], mean observation: 0.302 [0.000, 68.000], loss: 0.453251, mean_absolute_error: 0.454715, mean_q: 3.168888, mean_eps: 0.100000\n",
      "  49073/175000: episode: 1382, duration: 0.605s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 127.966 [11.000, 211.000], mean observation: 0.207 [0.000, 58.000], loss: 0.806276, mean_absolute_error: 0.479887, mean_q: 3.278107, mean_eps: 0.100000\n",
      "  49089/175000: episode: 1383, duration: 0.311s, episode steps: 16, steps per second: 51, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 94.812 [8.000, 141.000], mean observation: 0.104 [0.000, 32.000], loss: 0.232820, mean_absolute_error: 0.399527, mean_q: 3.011750, mean_eps: 0.100000\n",
      "  49127/175000: episode: 1384, duration: 0.790s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.421 [11.000, 211.000], mean observation: 0.293 [0.000, 76.000], loss: 0.180105, mean_absolute_error: 0.383174, mean_q: 2.802491, mean_eps: 0.100000\n",
      "  49142/175000: episode: 1385, duration: 0.374s, episode steps: 15, steps per second: 40, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 118.533 [11.000, 181.000], mean observation: 0.076 [0.000, 30.000], loss: 0.271376, mean_absolute_error: 0.423998, mean_q: 2.895520, mean_eps: 0.100000\n",
      "  49174/175000: episode: 1386, duration: 0.715s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 103.906 [12.000, 222.000], mean observation: 0.236 [0.000, 64.000], loss: 1.119892, mean_absolute_error: 0.519998, mean_q: 3.318883, mean_eps: 0.100000\n",
      "  49197/175000: episode: 1387, duration: 0.566s, episode steps: 23, steps per second: 41, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 107.522 [11.000, 181.000], mean observation: 0.180 [0.000, 46.000], loss: 3.128134, mean_absolute_error: 0.580399, mean_q: 3.543434, mean_eps: 0.100000\n",
      "  49241/175000: episode: 1388, duration: 0.987s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.523 [26.000, 223.000], mean observation: 0.261 [0.000, 88.000], loss: 1.958570, mean_absolute_error: 0.616530, mean_q: 3.726809, mean_eps: 0.100000\n",
      "  49275/175000: episode: 1389, duration: 0.707s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.882 [9.000, 181.000], mean observation: 0.292 [0.000, 68.000], loss: 0.261725, mean_absolute_error: 0.502897, mean_q: 3.340910, mean_eps: 0.100000\n",
      "  49295/175000: episode: 1390, duration: 0.443s, episode steps: 20, steps per second: 45, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 153.100 [141.000, 181.000], mean observation: 0.101 [0.000, 40.000], loss: 0.172388, mean_absolute_error: 0.445235, mean_q: 3.040928, mean_eps: 0.100000\n",
      "  49323/175000: episode: 1391, duration: 0.636s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 60.679 [10.000, 141.000], mean observation: 0.202 [0.000, 56.000], loss: 2.328072, mean_absolute_error: 0.587189, mean_q: 3.833028, mean_eps: 0.100000\n",
      "  49360/175000: episode: 1392, duration: 0.951s, episode steps: 37, steps per second: 39, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 92.270 [24.000, 223.000], mean observation: 0.230 [0.000, 74.000], loss: 1.635115, mean_absolute_error: 0.537501, mean_q: 3.650488, mean_eps: 0.100000\n",
      "  49396/175000: episode: 1393, duration: 0.868s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.083 [24.000, 193.000], mean observation: 0.232 [0.000, 72.000], loss: 2.268496, mean_absolute_error: 0.478471, mean_q: 3.420859, mean_eps: 0.100000\n",
      "  49445/175000: episode: 1394, duration: 1.124s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.061 [20.000, 223.000], mean observation: 0.346 [0.000, 98.000], loss: 0.767728, mean_absolute_error: 0.486190, mean_q: 3.427422, mean_eps: 0.100000\n",
      "  49467/175000: episode: 1395, duration: 0.440s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 35.955 [24.000, 128.000], mean observation: 0.066 [0.000, 44.000], loss: 3.761217, mean_absolute_error: 0.691532, mean_q: 4.213485, mean_eps: 0.100000\n",
      "  49513/175000: episode: 1396, duration: 1.016s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.000 [24.000, 192.000], mean observation: 0.294 [0.000, 92.000], loss: 2.329607, mean_absolute_error: 0.517189, mean_q: 3.453793, mean_eps: 0.100000\n",
      "  49558/175000: episode: 1397, duration: 1.023s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.733 [24.000, 193.000], mean observation: 0.298 [0.000, 90.000], loss: 0.641137, mean_absolute_error: 0.515277, mean_q: 3.542118, mean_eps: 0.100000\n",
      "  49608/175000: episode: 1398, duration: 1.087s, episode steps: 50, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 39.060 [2.000, 148.000], mean observation: 0.170 [0.000, 100.000], loss: 0.306081, mean_absolute_error: 0.470099, mean_q: 3.280974, mean_eps: 0.100000\n",
      "  49653/175000: episode: 1399, duration: 0.926s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 27.178 [24.000, 167.000], mean observation: 0.118 [0.000, 90.000], loss: 0.827941, mean_absolute_error: 0.431409, mean_q: 3.137305, mean_eps: 0.100000\n",
      "  49684/175000: episode: 1400, duration: 0.788s, episode steps: 31, steps per second: 39, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.581 [24.000, 141.000], mean observation: 0.100 [0.000, 62.000], loss: 8.102303, mean_absolute_error: 0.719983, mean_q: 4.293749, mean_eps: 0.100000\n",
      "  49719/175000: episode: 1401, duration: 0.804s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.543 [24.000, 181.000], mean observation: 0.347 [0.000, 70.000], loss: 2.350683, mean_absolute_error: 0.534307, mean_q: 3.618840, mean_eps: 0.100000\n",
      "  49755/175000: episode: 1402, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 113.472 [6.000, 184.000], mean observation: 0.379 [0.000, 72.000], loss: 0.363370, mean_absolute_error: 0.468338, mean_q: 3.379471, mean_eps: 0.100000\n",
      "  49793/175000: episode: 1403, duration: 0.880s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.289 [6.000, 221.000], mean observation: 0.432 [0.000, 76.000], loss: 1.810488, mean_absolute_error: 0.602886, mean_q: 3.891772, mean_eps: 0.100000\n",
      "  49820/175000: episode: 1404, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 136.074 [61.000, 221.000], mean observation: 0.269 [0.000, 54.000], loss: 0.352256, mean_absolute_error: 0.485641, mean_q: 3.396765, mean_eps: 0.100000\n",
      "  49871/175000: episode: 1405, duration: 1.017s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.020 [13.000, 185.000], mean observation: 0.666 [0.000, 102.000], loss: 0.893881, mean_absolute_error: 0.530263, mean_q: 3.630663, mean_eps: 0.100000\n",
      "  49902/175000: episode: 1406, duration: 0.713s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 61.935 [2.000, 169.000], mean observation: 0.161 [0.000, 62.000], loss: 1.485621, mean_absolute_error: 0.548570, mean_q: 3.730303, mean_eps: 0.100000\n",
      "  49941/175000: episode: 1407, duration: 0.795s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.410 [35.000, 221.000], mean observation: 0.243 [0.000, 78.000], loss: 0.677156, mean_absolute_error: 0.455901, mean_q: 3.202958, mean_eps: 0.100000\n",
      "  49992/175000: episode: 1408, duration: 1.101s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.804 [25.000, 221.000], mean observation: 0.496 [0.000, 102.000], loss: 2.841810, mean_absolute_error: 0.507995, mean_q: 3.424939, mean_eps: 0.100000\n",
      "  50019/175000: episode: 1409, duration: 0.620s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 60.148 [53.000, 209.000], mean observation: 0.104 [0.000, 54.000], loss: 0.697853, mean_absolute_error: 0.635581, mean_q: 4.022233, mean_eps: 0.100000\n",
      "  50080/175000: episode: 1410, duration: 1.322s, episode steps: 61, steps per second: 46, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 91.328 [11.000, 193.000], mean observation: 0.226 [0.000, 122.000], loss: 25.369234, mean_absolute_error: 0.827342, mean_q: 4.508752, mean_eps: 0.100000\n",
      "  50129/175000: episode: 1411, duration: 1.099s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.571 [4.000, 221.000], mean observation: 0.241 [0.000, 98.000], loss: 12.724407, mean_absolute_error: 0.541195, mean_q: 3.411422, mean_eps: 0.100000\n",
      "  50169/175000: episode: 1412, duration: 0.870s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.275 [17.000, 216.000], mean observation: 0.134 [0.000, 80.000], loss: 12.006957, mean_absolute_error: 0.586382, mean_q: 3.679672, mean_eps: 0.100000\n",
      "  50195/175000: episode: 1413, duration: 0.534s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.231 [28.000, 100.000], mean observation: 0.073 [0.000, 52.000], loss: 4.190702, mean_absolute_error: 0.515692, mean_q: 3.524902, mean_eps: 0.100000\n",
      "  50232/175000: episode: 1414, duration: 0.861s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.541 [2.000, 218.000], mean observation: 0.206 [0.000, 74.000], loss: 7.265421, mean_absolute_error: 0.545209, mean_q: 3.558122, mean_eps: 0.100000\n",
      "  50277/175000: episode: 1415, duration: 0.942s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.000 [96.000, 216.000], mean observation: 0.182 [0.000, 90.000], loss: 7.312521, mean_absolute_error: 0.512896, mean_q: 3.348810, mean_eps: 0.100000\n",
      "  50323/175000: episode: 1416, duration: 0.974s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.174 [16.000, 204.000], mean observation: 0.281 [0.000, 92.000], loss: 5.286148, mean_absolute_error: 0.540826, mean_q: 3.555848, mean_eps: 0.100000\n",
      "  50351/175000: episode: 1417, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 112.500 [60.000, 180.000], mean observation: 0.166 [0.000, 56.000], loss: 5.830413, mean_absolute_error: 0.548537, mean_q: 3.680075, mean_eps: 0.100000\n",
      "  50388/175000: episode: 1418, duration: 0.770s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 115.243 [42.000, 212.000], mean observation: 0.221 [0.000, 74.000], loss: 0.311096, mean_absolute_error: 0.428863, mean_q: 3.211696, mean_eps: 0.100000\n",
      "  50451/175000: episode: 1419, duration: 1.457s, episode steps: 63, steps per second: 43, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 115.206 [28.000, 199.000], mean observation: 0.441 [0.000, 126.000], loss: 13.759096, mean_absolute_error: 0.701574, mean_q: 4.100510, mean_eps: 0.100000\n",
      "  50489/175000: episode: 1420, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.763 [28.000, 167.000], mean observation: 0.144 [0.000, 76.000], loss: 8.945423, mean_absolute_error: 0.596173, mean_q: 3.712201, mean_eps: 0.100000\n",
      "  50533/175000: episode: 1421, duration: 0.984s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.318 [28.000, 173.000], mean observation: 0.298 [0.000, 88.000], loss: 7.665238, mean_absolute_error: 0.528818, mean_q: 3.451375, mean_eps: 0.100000\n",
      "  50561/175000: episode: 1422, duration: 0.649s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 110.750 [28.000, 210.000], mean observation: 0.128 [0.000, 56.000], loss: 9.253344, mean_absolute_error: 0.592394, mean_q: 3.686323, mean_eps: 0.100000\n",
      "  50583/175000: episode: 1423, duration: 0.408s, episode steps: 22, steps per second: 54, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 210.000 [210.000, 210.000], mean observation: 0.053 [0.000, 44.000], loss: 9.042503, mean_absolute_error: 0.768389, mean_q: 4.615783, mean_eps: 0.100000\n",
      "  50618/175000: episode: 1424, duration: 0.773s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.514 [16.000, 223.000], mean observation: 0.231 [0.000, 70.000], loss: 11.690769, mean_absolute_error: 0.675684, mean_q: 4.157070, mean_eps: 0.100000\n",
      "  50668/175000: episode: 1425, duration: 1.186s, episode steps: 50, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 150.560 [17.000, 210.000], mean observation: 0.325 [0.000, 100.000], loss: 1.307423, mean_absolute_error: 0.432688, mean_q: 3.378243, mean_eps: 0.100000\n",
      "  50711/175000: episode: 1426, duration: 0.916s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 172.047 [1.000, 210.000], mean observation: 0.212 [0.000, 86.000], loss: 16.026788, mean_absolute_error: 0.671467, mean_q: 4.125902, mean_eps: 0.100000\n",
      "  50730/175000: episode: 1427, duration: 0.454s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 210.000 [210.000, 210.000], mean observation: 0.046 [0.000, 38.000], loss: 0.130312, mean_absolute_error: 0.358748, mean_q: 2.960749, mean_eps: 0.100000\n",
      "  50756/175000: episode: 1428, duration: 0.548s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 182.269 [0.000, 210.000], mean observation: 0.073 [0.000, 52.000], loss: 3.654425, mean_absolute_error: 0.654168, mean_q: 4.162488, mean_eps: 0.100000\n",
      "  50812/175000: episode: 1429, duration: 1.358s, episode steps: 56, steps per second: 41, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 174.429 [0.000, 210.000], mean observation: 0.194 [0.000, 112.000], loss: 5.192043, mean_absolute_error: 0.569910, mean_q: 3.799836, mean_eps: 0.100000\n",
      "  50839/175000: episode: 1430, duration: 0.614s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 142.704 [0.000, 210.000], mean observation: 0.165 [0.000, 54.000], loss: 4.237853, mean_absolute_error: 0.505531, mean_q: 3.528954, mean_eps: 0.100000\n",
      "  50860/175000: episode: 1431, duration: 0.473s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 192.048 [17.000, 210.000], mean observation: 0.098 [0.000, 42.000], loss: 8.378397, mean_absolute_error: 0.589038, mean_q: 3.818135, mean_eps: 0.100000\n",
      "  50889/175000: episode: 1432, duration: 0.640s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 75.138 [22.000, 210.000], mean observation: 0.127 [0.000, 58.000], loss: 1.143141, mean_absolute_error: 0.561908, mean_q: 3.920753, mean_eps: 0.100000\n",
      "  50923/175000: episode: 1433, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.971 [18.000, 223.000], mean observation: 0.333 [0.000, 68.000], loss: 27.717953, mean_absolute_error: 0.911276, mean_q: 4.932851, mean_eps: 0.100000\n",
      "  50958/175000: episode: 1434, duration: 0.853s, episode steps: 35, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.086 [0.000, 210.000], mean observation: 0.246 [0.000, 70.000], loss: 29.865350, mean_absolute_error: 0.990387, mean_q: 5.258156, mean_eps: 0.100000\n",
      "  50979/175000: episode: 1435, duration: 0.433s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 154.714 [150.000, 160.000], mean observation: 0.070 [0.000, 42.000], loss: 18.995407, mean_absolute_error: 0.846212, mean_q: 4.805124, mean_eps: 0.100000\n",
      "  51014/175000: episode: 1436, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 159.629 [54.000, 210.000], mean observation: 0.215 [0.000, 70.000], loss: 20.971460, mean_absolute_error: 0.907813, mean_q: 5.115514, mean_eps: 0.100000\n",
      "  51039/175000: episode: 1437, duration: 0.497s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 153.440 [150.000, 215.000], mean observation: 0.061 [0.000, 50.000], loss: 4.721346, mean_absolute_error: 0.678587, mean_q: 4.578498, mean_eps: 0.100000\n",
      "  51068/175000: episode: 1438, duration: 0.629s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 152.517 [54.000, 194.000], mean observation: 0.132 [0.000, 58.000], loss: 2.963250, mean_absolute_error: 0.725140, mean_q: 4.889092, mean_eps: 0.100000\n",
      "  51111/175000: episode: 1439, duration: 1.006s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.256 [6.000, 205.000], mean observation: 0.434 [0.000, 86.000], loss: 8.868018, mean_absolute_error: 0.667040, mean_q: 4.534442, mean_eps: 0.100000\n",
      "  51155/175000: episode: 1440, duration: 1.063s, episode steps: 44, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 155.159 [22.000, 214.000], mean observation: 0.398 [0.000, 88.000], loss: 2.876038, mean_absolute_error: 0.538236, mean_q: 4.024938, mean_eps: 0.100000\n",
      "  51168/175000: episode: 1441, duration: 0.372s, episode steps: 13, steps per second: 35, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 128.308 [22.000, 205.000], mean observation: 0.077 [0.000, 26.000], loss: 4.261552, mean_absolute_error: 0.668213, mean_q: 4.529806, mean_eps: 0.100000\n",
      "  51217/175000: episode: 1442, duration: 1.186s, episode steps: 49, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 152.122 [31.000, 218.000], mean observation: 0.564 [0.000, 98.000], loss: 15.656995, mean_absolute_error: 0.831381, mean_q: 5.252382, mean_eps: 0.100000\n",
      "  51266/175000: episode: 1443, duration: 1.004s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 150.796 [11.000, 218.000], mean observation: 0.377 [0.000, 98.000], loss: 9.683254, mean_absolute_error: 0.697684, mean_q: 4.867325, mean_eps: 0.100000\n",
      "  51308/175000: episode: 1444, duration: 0.925s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 169.810 [10.000, 219.000], mean observation: 0.357 [0.000, 84.000], loss: 5.990058, mean_absolute_error: 0.594547, mean_q: 4.501570, mean_eps: 0.100000\n",
      "  51346/175000: episode: 1445, duration: 0.879s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 161.158 [11.000, 218.000], mean observation: 0.260 [0.000, 76.000], loss: 22.706436, mean_absolute_error: 0.728095, mean_q: 4.766508, mean_eps: 0.100000\n",
      "  51371/175000: episode: 1446, duration: 0.533s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 143.560 [11.000, 218.000], mean observation: 0.128 [0.000, 50.000], loss: 2.277086, mean_absolute_error: 0.845254, mean_q: 5.773095, mean_eps: 0.100000\n",
      "  51408/175000: episode: 1447, duration: 0.872s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 125.189 [11.000, 210.000], mean observation: 0.372 [0.000, 74.000], loss: 15.161803, mean_absolute_error: 0.652365, mean_q: 4.613285, mean_eps: 0.100000\n",
      "  51432/175000: episode: 1448, duration: 0.543s, episode steps: 24, steps per second: 44, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 134.625 [8.000, 205.000], mean observation: 0.183 [0.000, 48.000], loss: 7.560375, mean_absolute_error: 0.689963, mean_q: 4.960673, mean_eps: 0.100000\n",
      "  51469/175000: episode: 1449, duration: 0.799s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.730 [58.000, 204.000], mean observation: 0.297 [0.000, 74.000], loss: 4.759404, mean_absolute_error: 0.622589, mean_q: 4.621041, mean_eps: 0.100000\n",
      "  51516/175000: episode: 1450, duration: 0.969s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 139.277 [43.000, 182.000], mean observation: 0.400 [0.000, 94.000], loss: 5.595720, mean_absolute_error: 0.658125, mean_q: 4.763998, mean_eps: 0.100000\n",
      "  51564/175000: episode: 1451, duration: 1.046s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.125 [60.000, 203.000], mean observation: 0.371 [0.000, 96.000], loss: 0.261949, mean_absolute_error: 0.440015, mean_q: 3.831345, mean_eps: 0.100000\n",
      "  51585/175000: episode: 1452, duration: 0.515s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 166.333 [101.000, 182.000], mean observation: 0.117 [0.000, 42.000], loss: 10.803931, mean_absolute_error: 0.546136, mean_q: 3.998606, mean_eps: 0.100000\n",
      "  51633/175000: episode: 1453, duration: 1.051s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 150.562 [2.000, 200.000], mean observation: 0.422 [0.000, 96.000], loss: 10.056943, mean_absolute_error: 0.676382, mean_q: 4.710404, mean_eps: 0.100000\n",
      "  51657/175000: episode: 1454, duration: 0.576s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 156.542 [110.000, 207.000], mean observation: 0.122 [0.000, 48.000], loss: 0.619366, mean_absolute_error: 0.536283, mean_q: 4.339050, mean_eps: 0.100000\n",
      "  51703/175000: episode: 1455, duration: 0.912s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.304 [22.000, 182.000], mean observation: 0.416 [0.000, 92.000], loss: 9.808875, mean_absolute_error: 0.626430, mean_q: 4.509296, mean_eps: 0.100000\n",
      "  51744/175000: episode: 1456, duration: 0.911s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.854 [8.000, 159.000], mean observation: 0.499 [0.000, 82.000], loss: 12.617119, mean_absolute_error: 0.597118, mean_q: 4.349414, mean_eps: 0.100000\n",
      "  51785/175000: episode: 1457, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 157.976 [1.000, 221.000], mean observation: 0.280 [0.000, 82.000], loss: 1.407247, mean_absolute_error: 0.548218, mean_q: 4.296673, mean_eps: 0.100000\n",
      "  51809/175000: episode: 1458, duration: 0.533s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 197.542 [33.000, 213.000], mean observation: 0.140 [0.000, 48.000], loss: 0.572801, mean_absolute_error: 0.532052, mean_q: 4.252767, mean_eps: 0.100000\n",
      "  51853/175000: episode: 1459, duration: 0.933s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 203.136 [61.000, 213.000], mean observation: 0.272 [0.000, 88.000], loss: 7.965817, mean_absolute_error: 0.618186, mean_q: 4.476963, mean_eps: 0.100000\n",
      "  51886/175000: episode: 1460, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 196.727 [67.000, 213.000], mean observation: 0.175 [0.000, 66.000], loss: 9.629847, mean_absolute_error: 0.567635, mean_q: 4.160142, mean_eps: 0.100000\n",
      "  51914/175000: episode: 1461, duration: 0.564s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 196.036 [48.000, 213.000], mean observation: 0.125 [0.000, 56.000], loss: 3.897262, mean_absolute_error: 0.518223, mean_q: 3.987534, mean_eps: 0.100000\n",
      "  51946/175000: episode: 1462, duration: 0.608s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 202.219 [39.000, 213.000], mean observation: 0.088 [0.000, 64.000], loss: 14.189194, mean_absolute_error: 0.714589, mean_q: 4.881717, mean_eps: 0.100000\n",
      "  51977/175000: episode: 1463, duration: 0.639s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.774 [29.000, 213.000], mean observation: 0.213 [0.000, 62.000], loss: 9.108847, mean_absolute_error: 0.526471, mean_q: 3.966371, mean_eps: 0.100000\n",
      "  52015/175000: episode: 1464, duration: 0.761s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 202.421 [43.000, 213.000], mean observation: 0.127 [0.000, 76.000], loss: 5.569040, mean_absolute_error: 0.520163, mean_q: 4.091415, mean_eps: 0.100000\n",
      "  52052/175000: episode: 1465, duration: 0.788s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 191.865 [45.000, 213.000], mean observation: 0.194 [0.000, 74.000], loss: 9.736114, mean_absolute_error: 0.617639, mean_q: 4.465214, mean_eps: 0.100000\n",
      "  52079/175000: episode: 1466, duration: 0.563s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 213.000 [213.000, 213.000], mean observation: 0.064 [0.000, 54.000], loss: 18.963775, mean_absolute_error: 0.700482, mean_q: 4.596093, mean_eps: 0.100000\n",
      "  52098/175000: episode: 1467, duration: 0.448s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 203.211 [148.000, 213.000], mean observation: 0.050 [0.000, 38.000], loss: 0.280108, mean_absolute_error: 0.449302, mean_q: 3.766561, mean_eps: 0.100000\n",
      "  52143/175000: episode: 1468, duration: 0.931s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 205.689 [24.000, 222.000], mean observation: 0.184 [0.000, 90.000], loss: 14.604291, mean_absolute_error: 0.639011, mean_q: 4.473388, mean_eps: 0.100000\n",
      "  52182/175000: episode: 1469, duration: 0.788s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.462 [16.000, 213.000], mean observation: 0.328 [0.000, 78.000], loss: 0.858170, mean_absolute_error: 0.490495, mean_q: 3.985049, mean_eps: 0.100000\n",
      "  52224/175000: episode: 1470, duration: 0.920s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.857 [89.000, 213.000], mean observation: 0.188 [0.000, 84.000], loss: 8.047802, mean_absolute_error: 0.626748, mean_q: 4.601269, mean_eps: 0.100000\n",
      "  52269/175000: episode: 1471, duration: 0.942s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 149.733 [7.000, 213.000], mean observation: 0.498 [0.000, 90.000], loss: 0.242107, mean_absolute_error: 0.530295, mean_q: 4.380727, mean_eps: 0.100000\n",
      "  52303/175000: episode: 1472, duration: 0.632s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 170.147 [66.000, 213.000], mean observation: 0.148 [0.000, 68.000], loss: 2.137868, mean_absolute_error: 0.633468, mean_q: 4.774744, mean_eps: 0.100000\n",
      "  52332/175000: episode: 1473, duration: 0.670s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.586 [28.000, 205.000], mean observation: 0.212 [0.000, 58.000], loss: 0.576667, mean_absolute_error: 0.625434, mean_q: 4.716885, mean_eps: 0.100000\n",
      "  52354/175000: episode: 1474, duration: 0.516s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.364 [23.000, 213.000], mean observation: 0.091 [0.000, 44.000], loss: 18.089309, mean_absolute_error: 0.717892, mean_q: 4.813962, mean_eps: 0.100000\n",
      "  52396/175000: episode: 1475, duration: 0.981s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 165.524 [75.000, 213.000], mean observation: 0.259 [0.000, 84.000], loss: 1.348326, mean_absolute_error: 0.515340, mean_q: 4.235298, mean_eps: 0.100000\n",
      "  52455/175000: episode: 1476, duration: 1.233s, episode steps: 59, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 133.373 [91.000, 214.000], mean observation: 0.671 [0.000, 118.000], loss: 0.695012, mean_absolute_error: 0.544350, mean_q: 4.399865, mean_eps: 0.100000\n",
      "  52486/175000: episode: 1477, duration: 0.698s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 87.419 [5.000, 214.000], mean observation: 0.245 [0.000, 62.000], loss: 4.374273, mean_absolute_error: 0.554934, mean_q: 4.330021, mean_eps: 0.100000\n",
      "  52522/175000: episode: 1478, duration: 0.784s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 205.111 [14.000, 215.000], mean observation: 0.121 [0.000, 72.000], loss: 16.442439, mean_absolute_error: 0.645986, mean_q: 4.516903, mean_eps: 0.100000\n",
      "  52578/175000: episode: 1479, duration: 1.231s, episode steps: 56, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 174.143 [5.000, 220.000], mean observation: 0.246 [0.000, 112.000], loss: 9.713419, mean_absolute_error: 0.685356, mean_q: 4.903910, mean_eps: 0.100000\n",
      "  52610/175000: episode: 1480, duration: 0.666s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 187.156 [12.000, 218.000], mean observation: 0.222 [0.000, 64.000], loss: 6.191670, mean_absolute_error: 0.594588, mean_q: 4.456434, mean_eps: 0.100000\n",
      "  52646/175000: episode: 1481, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 153.222 [105.000, 215.000], mean observation: 0.245 [0.000, 72.000], loss: 10.056928, mean_absolute_error: 0.693710, mean_q: 4.871799, mean_eps: 0.100000\n",
      "  52706/175000: episode: 1482, duration: 1.247s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 149.533 [36.000, 215.000], mean observation: 0.524 [0.000, 120.000], loss: 6.559228, mean_absolute_error: 0.553730, mean_q: 4.265946, mean_eps: 0.100000\n",
      "  52744/175000: episode: 1483, duration: 0.883s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 146.868 [37.000, 220.000], mean observation: 0.234 [0.000, 76.000], loss: 12.351929, mean_absolute_error: 0.609284, mean_q: 4.346174, mean_eps: 0.100000\n",
      "  52785/175000: episode: 1484, duration: 0.915s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.390 [14.000, 213.000], mean observation: 0.438 [0.000, 82.000], loss: 5.744723, mean_absolute_error: 0.505051, mean_q: 4.010641, mean_eps: 0.100000\n",
      "  52825/175000: episode: 1485, duration: 0.828s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 73.525 [4.000, 201.000], mean observation: 0.255 [0.000, 80.000], loss: 13.990340, mean_absolute_error: 0.752208, mean_q: 5.086269, mean_eps: 0.100000\n",
      "  52864/175000: episode: 1486, duration: 0.876s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 29.846 [7.000, 166.000], mean observation: 0.214 [0.000, 78.000], loss: 4.289713, mean_absolute_error: 0.555854, mean_q: 4.362439, mean_eps: 0.100000\n",
      "  52913/175000: episode: 1487, duration: 1.100s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 56.816 [20.000, 189.000], mean observation: 0.371 [0.000, 98.000], loss: 2.032108, mean_absolute_error: 0.484137, mean_q: 4.070392, mean_eps: 0.100000\n",
      "  52945/175000: episode: 1488, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.562 [20.000, 165.000], mean observation: 0.196 [0.000, 64.000], loss: 0.256578, mean_absolute_error: 0.447139, mean_q: 3.929540, mean_eps: 0.100000\n",
      "  52988/175000: episode: 1489, duration: 0.907s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 128.698 [81.000, 190.000], mean observation: 0.138 [0.000, 86.000], loss: 6.517298, mean_absolute_error: 0.617175, mean_q: 4.676004, mean_eps: 0.100000\n",
      "  53007/175000: episode: 1490, duration: 0.369s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 103.526 [31.000, 149.000], mean observation: 0.119 [0.000, 38.000], loss: 0.267028, mean_absolute_error: 0.447421, mean_q: 3.955770, mean_eps: 0.100000\n",
      "  53053/175000: episode: 1491, duration: 0.978s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 98.413 [31.000, 197.000], mean observation: 0.391 [0.000, 92.000], loss: 14.333003, mean_absolute_error: 0.627978, mean_q: 4.487075, mean_eps: 0.100000\n",
      "  53091/175000: episode: 1492, duration: 0.761s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.289 [24.000, 167.000], mean observation: 0.177 [0.000, 76.000], loss: 5.258950, mean_absolute_error: 0.539339, mean_q: 4.246764, mean_eps: 0.100000\n",
      "  53127/175000: episode: 1493, duration: 0.819s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.639 [7.000, 220.000], mean observation: 0.225 [0.000, 72.000], loss: 8.150340, mean_absolute_error: 0.625154, mean_q: 4.581124, mean_eps: 0.100000\n",
      "  53167/175000: episode: 1494, duration: 0.805s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 160.550 [75.000, 167.000], mean observation: 0.152 [0.000, 80.000], loss: 3.453491, mean_absolute_error: 0.559042, mean_q: 4.353645, mean_eps: 0.100000\n",
      "  53181/175000: episode: 1495, duration: 0.331s, episode steps: 14, steps per second: 42, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 140.286 [8.000, 174.000], mean observation: 0.069 [0.000, 28.000], loss: 11.379117, mean_absolute_error: 0.637193, mean_q: 4.456205, mean_eps: 0.100000\n",
      "  53234/175000: episode: 1496, duration: 1.100s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 131.660 [74.000, 167.000], mean observation: 0.315 [0.000, 106.000], loss: 5.391470, mean_absolute_error: 0.759303, mean_q: 5.279871, mean_eps: 0.100000\n",
      "  53272/175000: episode: 1497, duration: 0.797s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 129.237 [49.000, 167.000], mean observation: 0.191 [0.000, 76.000], loss: 5.231686, mean_absolute_error: 0.626787, mean_q: 4.452097, mean_eps: 0.100000\n",
      "  53310/175000: episode: 1498, duration: 0.815s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 148.526 [42.000, 217.000], mean observation: 0.208 [0.000, 76.000], loss: 9.161141, mean_absolute_error: 0.704933, mean_q: 4.870490, mean_eps: 0.100000\n",
      "  53339/175000: episode: 1499, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 57.000 [57.000, 57.000], mean observation: 0.068 [0.000, 58.000], loss: 20.583160, mean_absolute_error: 0.868975, mean_q: 5.659205, mean_eps: 0.100000\n",
      "  53374/175000: episode: 1500, duration: 0.786s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.771 [8.000, 224.000], mean observation: 0.212 [0.000, 70.000], loss: 1.434391, mean_absolute_error: 0.571107, mean_q: 4.406654, mean_eps: 0.100000\n",
      "  53422/175000: episode: 1501, duration: 1.013s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 58.417 [19.000, 107.000], mean observation: 0.138 [0.000, 96.000], loss: 25.885951, mean_absolute_error: 0.981597, mean_q: 6.326784, mean_eps: 0.100000\n",
      "  53454/175000: episode: 1502, duration: 0.705s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 63.969 [33.000, 213.000], mean observation: 0.187 [0.000, 64.000], loss: 9.780356, mean_absolute_error: 0.598904, mean_q: 4.496477, mean_eps: 0.100000\n",
      "  53496/175000: episode: 1503, duration: 0.890s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.667 [39.000, 211.000], mean observation: 0.238 [0.000, 84.000], loss: 8.157729, mean_absolute_error: 0.839536, mean_q: 6.020617, mean_eps: 0.100000\n",
      "  53525/175000: episode: 1504, duration: 0.631s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 55.517 [14.000, 57.000], mean observation: 0.085 [0.000, 58.000], loss: 1.377109, mean_absolute_error: 0.742109, mean_q: 5.582978, mean_eps: 0.100000\n",
      "  53579/175000: episode: 1505, duration: 1.148s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 62.926 [16.000, 180.000], mean observation: 0.442 [0.000, 108.000], loss: 9.646923, mean_absolute_error: 0.667673, mean_q: 4.839232, mean_eps: 0.100000\n",
      "  53624/175000: episode: 1506, duration: 1.066s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 68.022 [21.000, 209.000], mean observation: 0.195 [0.000, 90.000], loss: 0.603385, mean_absolute_error: 0.516239, mean_q: 4.064598, mean_eps: 0.100000\n",
      "  53657/175000: episode: 1507, duration: 0.762s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 64.485 [55.000, 203.000], mean observation: 0.134 [0.000, 66.000], loss: 3.889578, mean_absolute_error: 0.536414, mean_q: 4.002591, mean_eps: 0.100000\n",
      "  53715/175000: episode: 1508, duration: 1.273s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 64.517 [23.000, 185.000], mean observation: 0.380 [0.000, 116.000], loss: 7.907887, mean_absolute_error: 0.719269, mean_q: 5.045881, mean_eps: 0.100000\n",
      "  53746/175000: episode: 1509, duration: 0.700s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 82.065 [49.000, 205.000], mean observation: 0.153 [0.000, 62.000], loss: 4.888928, mean_absolute_error: 0.591765, mean_q: 4.329775, mean_eps: 0.100000\n",
      "  53782/175000: episode: 1510, duration: 0.809s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 73.778 [7.000, 210.000], mean observation: 0.233 [0.000, 72.000], loss: 2.054070, mean_absolute_error: 0.647358, mean_q: 4.737191, mean_eps: 0.100000\n",
      "  53803/175000: episode: 1511, duration: 0.453s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 68.762 [7.000, 118.000], mean observation: 0.128 [0.000, 42.000], loss: 0.616906, mean_absolute_error: 0.520970, mean_q: 3.964434, mean_eps: 0.100000\n",
      "  53836/175000: episode: 1512, duration: 0.768s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.152 [51.000, 211.000], mean observation: 0.106 [0.000, 66.000], loss: 8.678915, mean_absolute_error: 0.594174, mean_q: 4.152794, mean_eps: 0.100000\n",
      "  53878/175000: episode: 1513, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.095 [68.000, 193.000], mean observation: 0.144 [0.000, 84.000], loss: 4.251747, mean_absolute_error: 0.603425, mean_q: 4.335544, mean_eps: 0.100000\n",
      "  53906/175000: episode: 1514, duration: 0.664s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.893 [39.000, 88.000], mean observation: 0.100 [0.000, 56.000], loss: 13.620082, mean_absolute_error: 0.641824, mean_q: 4.238893, mean_eps: 0.100000\n",
      "  53932/175000: episode: 1515, duration: 0.556s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 75.462 [55.000, 79.000], mean observation: 0.081 [0.000, 52.000], loss: 35.950741, mean_absolute_error: 0.989408, mean_q: 5.816061, mean_eps: 0.100000\n",
      "  53958/175000: episode: 1516, duration: 0.604s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 49.000 [28.000, 79.000], mean observation: 0.113 [0.000, 52.000], loss: 2.813124, mean_absolute_error: 0.477968, mean_q: 3.412152, mean_eps: 0.100000\n",
      "  54007/175000: episode: 1517, duration: 1.032s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 80.265 [28.000, 168.000], mean observation: 0.321 [0.000, 98.000], loss: 0.335909, mean_absolute_error: 0.520964, mean_q: 3.688237, mean_eps: 0.100000\n",
      "  54032/175000: episode: 1518, duration: 0.618s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.059 [0.000, 50.000], loss: 0.236849, mean_absolute_error: 0.451984, mean_q: 3.224291, mean_eps: 0.100000\n",
      "  54068/175000: episode: 1519, duration: 0.799s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 118.778 [18.000, 170.000], mean observation: 0.339 [0.000, 72.000], loss: 6.473144, mean_absolute_error: 0.515873, mean_q: 3.428044, mean_eps: 0.100000\n",
      "  54089/175000: episode: 1520, duration: 0.499s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 65.810 [39.000, 222.000], mean observation: 0.091 [0.000, 42.000], loss: 0.363249, mean_absolute_error: 0.487019, mean_q: 3.418077, mean_eps: 0.100000\n",
      "  54131/175000: episode: 1521, duration: 0.885s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.333 [0.000, 208.000], mean observation: 0.276 [0.000, 84.000], loss: 15.823330, mean_absolute_error: 0.678927, mean_q: 4.209115, mean_eps: 0.100000\n",
      "  54172/175000: episode: 1522, duration: 0.905s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 37.902 [6.000, 97.000], mean observation: 0.159 [0.000, 82.000], loss: 4.952637, mean_absolute_error: 0.571963, mean_q: 3.955883, mean_eps: 0.100000\n",
      "  54218/175000: episode: 1523, duration: 0.997s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.413 [8.000, 214.000], mean observation: 0.404 [0.000, 92.000], loss: 1.494882, mean_absolute_error: 0.534575, mean_q: 3.841535, mean_eps: 0.100000\n",
      "  54248/175000: episode: 1524, duration: 0.749s, episode steps: 30, steps per second: 40, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 92.567 [8.000, 205.000], mean observation: 0.263 [0.000, 60.000], loss: 7.326492, mean_absolute_error: 0.578410, mean_q: 4.043291, mean_eps: 0.100000\n",
      "  54276/175000: episode: 1525, duration: 0.681s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 55.643 [20.000, 212.000], mean observation: 0.141 [0.000, 56.000], loss: 1.187516, mean_absolute_error: 0.670838, mean_q: 4.816987, mean_eps: 0.100000\n",
      "  54299/175000: episode: 1526, duration: 0.496s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 143.565 [43.000, 222.000], mean observation: 0.135 [0.000, 46.000], loss: 2.878043, mean_absolute_error: 0.547091, mean_q: 3.967980, mean_eps: 0.100000\n",
      "  54344/175000: episode: 1527, duration: 1.008s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 113.044 [2.000, 222.000], mean observation: 0.358 [0.000, 90.000], loss: 11.801882, mean_absolute_error: 0.727964, mean_q: 4.916427, mean_eps: 0.100000\n",
      "  54393/175000: episode: 1528, duration: 1.176s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 179.245 [13.000, 222.000], mean observation: 0.395 [0.000, 98.000], loss: 13.110520, mean_absolute_error: 0.663743, mean_q: 4.409699, mean_eps: 0.100000\n",
      "  54439/175000: episode: 1529, duration: 1.007s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 184.739 [42.000, 222.000], mean observation: 0.312 [0.000, 92.000], loss: 9.058060, mean_absolute_error: 0.527570, mean_q: 3.812599, mean_eps: 0.100000\n",
      "  54483/175000: episode: 1530, duration: 0.955s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.955 [28.000, 208.000], mean observation: 0.385 [0.000, 88.000], loss: 2.984327, mean_absolute_error: 0.537064, mean_q: 4.104581, mean_eps: 0.100000\n",
      "  54517/175000: episode: 1531, duration: 0.721s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.529 [138.000, 192.000], mean observation: 0.141 [0.000, 68.000], loss: 8.105937, mean_absolute_error: 0.602828, mean_q: 4.228654, mean_eps: 0.100000\n",
      "  54575/175000: episode: 1532, duration: 1.301s, episode steps: 58, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 125.345 [23.000, 220.000], mean observation: 0.288 [0.000, 116.000], loss: 11.447982, mean_absolute_error: 0.665597, mean_q: 4.406071, mean_eps: 0.100000\n",
      "  54600/175000: episode: 1533, duration: 0.626s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 64.040 [26.000, 191.000], mean observation: 0.179 [0.000, 50.000], loss: 6.892288, mean_absolute_error: 0.568570, mean_q: 3.842962, mean_eps: 0.100000\n",
      "  54626/175000: episode: 1534, duration: 0.634s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.154 [19.000, 199.000], mean observation: 0.226 [0.000, 52.000], loss: 8.344347, mean_absolute_error: 0.560527, mean_q: 3.832258, mean_eps: 0.100000\n",
      "  54651/175000: episode: 1535, duration: 0.600s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 67.200 [26.000, 182.000], mean observation: 0.159 [0.000, 50.000], loss: 1.195186, mean_absolute_error: 0.565497, mean_q: 4.168435, mean_eps: 0.100000\n",
      "  54680/175000: episode: 1536, duration: 0.724s, episode steps: 29, steps per second: 40, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 139.414 [26.000, 209.000], mean observation: 0.248 [0.000, 58.000], loss: 2.717662, mean_absolute_error: 0.616748, mean_q: 4.390754, mean_eps: 0.100000\n",
      "  54706/175000: episode: 1537, duration: 0.588s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 101.192 [22.000, 204.000], mean observation: 0.255 [0.000, 52.000], loss: 3.792678, mean_absolute_error: 0.537384, mean_q: 3.958266, mean_eps: 0.100000\n",
      "  54745/175000: episode: 1538, duration: 0.836s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 53.333 [9.000, 214.000], mean observation: 0.291 [0.000, 78.000], loss: 2.461739, mean_absolute_error: 0.709700, mean_q: 4.998109, mean_eps: 0.100000\n",
      "  54773/175000: episode: 1539, duration: 0.656s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 49.643 [26.000, 108.000], mean observation: 0.142 [0.000, 56.000], loss: 0.326468, mean_absolute_error: 0.422992, mean_q: 3.383011, mean_eps: 0.100000\n",
      "  54826/175000: episode: 1540, duration: 1.239s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 73.509 [5.000, 183.000], mean observation: 0.417 [0.000, 106.000], loss: 9.424042, mean_absolute_error: 0.639019, mean_q: 4.492000, mean_eps: 0.100000\n",
      "  54867/175000: episode: 1541, duration: 0.925s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.829 [23.000, 204.000], mean observation: 0.322 [0.000, 82.000], loss: 6.139265, mean_absolute_error: 0.598323, mean_q: 4.182908, mean_eps: 0.100000\n",
      "  54905/175000: episode: 1542, duration: 0.885s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 96.105 [24.000, 222.000], mean observation: 0.332 [0.000, 76.000], loss: 0.303819, mean_absolute_error: 0.458843, mean_q: 3.495080, mean_eps: 0.100000\n",
      "  54928/175000: episode: 1543, duration: 0.495s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 38.609 [24.000, 215.000], mean observation: 0.133 [0.000, 46.000], loss: 0.667924, mean_absolute_error: 0.444758, mean_q: 3.506197, mean_eps: 0.100000\n",
      "  54960/175000: episode: 1544, duration: 0.775s, episode steps: 32, steps per second: 41, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 32.375 [0.000, 215.000], mean observation: 0.162 [0.000, 64.000], loss: 0.219720, mean_absolute_error: 0.408715, mean_q: 3.328277, mean_eps: 0.100000\n",
      "  55022/175000: episode: 1545, duration: 1.385s, episode steps: 62, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 68.758 [24.000, 222.000], mean observation: 0.459 [0.000, 124.000], loss: 8.775417, mean_absolute_error: 0.570290, mean_q: 4.136487, mean_eps: 0.100000\n",
      "  55070/175000: episode: 1546, duration: 1.007s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 111.292 [19.000, 220.000], mean observation: 0.409 [0.000, 96.000], loss: 6.113498, mean_absolute_error: 0.573819, mean_q: 4.256839, mean_eps: 0.100000\n",
      "  55121/175000: episode: 1547, duration: 1.133s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.569 [4.000, 162.000], mean observation: 0.172 [0.000, 102.000], loss: 6.344545, mean_absolute_error: 0.557850, mean_q: 4.167610, mean_eps: 0.100000\n",
      "  55163/175000: episode: 1548, duration: 0.938s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 36.595 [24.000, 168.000], mean observation: 0.330 [0.000, 84.000], loss: 12.674917, mean_absolute_error: 0.671531, mean_q: 4.700743, mean_eps: 0.100000\n",
      "  55195/175000: episode: 1549, duration: 0.733s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 30.406 [24.000, 145.000], mean observation: 0.092 [0.000, 64.000], loss: 11.063596, mean_absolute_error: 0.729585, mean_q: 5.203867, mean_eps: 0.100000\n",
      "  55234/175000: episode: 1550, duration: 0.859s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.487 [12.000, 196.000], mean observation: 0.214 [0.000, 78.000], loss: 2.512905, mean_absolute_error: 0.486292, mean_q: 3.934921, mean_eps: 0.100000\n",
      "  55275/175000: episode: 1551, duration: 0.927s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.146 [55.000, 210.000], mean observation: 0.270 [0.000, 82.000], loss: 11.062264, mean_absolute_error: 0.816237, mean_q: 5.809162, mean_eps: 0.100000\n",
      "  55316/175000: episode: 1552, duration: 0.938s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.439 [9.000, 222.000], mean observation: 0.402 [0.000, 82.000], loss: 0.535547, mean_absolute_error: 0.396319, mean_q: 3.441228, mean_eps: 0.100000\n",
      "  55341/175000: episode: 1553, duration: 0.585s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 128.440 [32.000, 162.000], mean observation: 0.094 [0.000, 50.000], loss: 7.480511, mean_absolute_error: 0.646929, mean_q: 4.865459, mean_eps: 0.100000\n",
      "  55388/175000: episode: 1554, duration: 1.049s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 156.660 [42.000, 221.000], mean observation: 0.512 [0.000, 94.000], loss: 0.665279, mean_absolute_error: 0.497496, mean_q: 4.055415, mean_eps: 0.100000\n",
      "  55412/175000: episode: 1555, duration: 0.623s, episode steps: 24, steps per second: 39, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 77.958 [33.000, 214.000], mean observation: 0.138 [0.000, 48.000], loss: 0.207849, mean_absolute_error: 0.390134, mean_q: 3.286407, mean_eps: 0.100000\n",
      "  55456/175000: episode: 1556, duration: 1.030s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 55.205 [28.000, 142.000], mean observation: 0.146 [0.000, 88.000], loss: 13.879715, mean_absolute_error: 0.693319, mean_q: 4.877512, mean_eps: 0.100000\n",
      "  55487/175000: episode: 1557, duration: 0.658s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 29.935 [28.000, 88.000], mean observation: 0.073 [0.000, 62.000], loss: 0.874900, mean_absolute_error: 0.467364, mean_q: 3.882614, mean_eps: 0.100000\n",
      "  55505/175000: episode: 1558, duration: 0.452s, episode steps: 18, steps per second: 40, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 38.667 [11.000, 132.000], mean observation: 0.073 [0.000, 36.000], loss: 4.072525, mean_absolute_error: 0.412269, mean_q: 3.375680, mean_eps: 0.100000\n",
      "  55545/175000: episode: 1559, duration: 0.895s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.900 [28.000, 217.000], mean observation: 0.376 [0.000, 80.000], loss: 2.209227, mean_absolute_error: 0.432059, mean_q: 3.703155, mean_eps: 0.100000\n",
      "  55569/175000: episode: 1560, duration: 0.576s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 138.708 [28.000, 207.000], mean observation: 0.131 [0.000, 48.000], loss: 6.378321, mean_absolute_error: 0.695558, mean_q: 5.156571, mean_eps: 0.100000\n",
      "  55607/175000: episode: 1561, duration: 0.812s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 30.289 [28.000, 104.000], mean observation: 0.117 [0.000, 76.000], loss: 12.130128, mean_absolute_error: 0.758073, mean_q: 5.433328, mean_eps: 0.100000\n",
      "  55638/175000: episode: 1562, duration: 0.673s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 39.000 [7.000, 212.000], mean observation: 0.151 [0.000, 62.000], loss: 6.444292, mean_absolute_error: 0.653553, mean_q: 4.965588, mean_eps: 0.100000\n",
      "  55665/175000: episode: 1563, duration: 0.598s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.667 [2.000, 64.000], mean observation: 0.124 [0.000, 54.000], loss: 1.005852, mean_absolute_error: 0.607452, mean_q: 4.786840, mean_eps: 0.100000\n",
      "  55697/175000: episode: 1564, duration: 0.654s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 59.938 [28.000, 104.000], mean observation: 0.147 [0.000, 64.000], loss: 11.964987, mean_absolute_error: 0.652459, mean_q: 4.803770, mean_eps: 0.100000\n",
      "  55725/175000: episode: 1565, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 82.000 [64.000, 163.000], mean observation: 0.084 [0.000, 56.000], loss: 5.636026, mean_absolute_error: 0.744393, mean_q: 5.616975, mean_eps: 0.100000\n",
      "  55760/175000: episode: 1566, duration: 0.748s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 80.229 [57.000, 207.000], mean observation: 0.185 [0.000, 70.000], loss: 9.702895, mean_absolute_error: 0.672390, mean_q: 5.131438, mean_eps: 0.100000\n",
      "  55799/175000: episode: 1567, duration: 0.794s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.513 [8.000, 217.000], mean observation: 0.406 [0.000, 78.000], loss: 10.673881, mean_absolute_error: 0.618890, mean_q: 4.739794, mean_eps: 0.100000\n",
      "  55824/175000: episode: 1568, duration: 0.537s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 118.000 [26.000, 207.000], mean observation: 0.127 [0.000, 50.000], loss: 9.589331, mean_absolute_error: 0.691326, mean_q: 5.157554, mean_eps: 0.100000\n",
      "  55841/175000: episode: 1569, duration: 0.431s, episode steps: 17, steps per second: 39, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 86.824 [28.000, 110.000], mean observation: 0.064 [0.000, 34.000], loss: 0.382052, mean_absolute_error: 0.409155, mean_q: 3.624578, mean_eps: 0.100000\n",
      "  55887/175000: episode: 1570, duration: 1.025s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.304 [21.000, 169.000], mean observation: 0.386 [0.000, 92.000], loss: 10.507962, mean_absolute_error: 0.790014, mean_q: 5.771977, mean_eps: 0.100000\n",
      "  55927/175000: episode: 1571, duration: 0.848s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 94.625 [7.000, 210.000], mean observation: 0.149 [0.000, 80.000], loss: 5.146607, mean_absolute_error: 0.503092, mean_q: 4.092715, mean_eps: 0.100000\n",
      "  55953/175000: episode: 1572, duration: 0.593s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 69.462 [7.000, 176.000], mean observation: 0.080 [0.000, 52.000], loss: 8.279879, mean_absolute_error: 0.563740, mean_q: 4.286078, mean_eps: 0.100000\n",
      "  55981/175000: episode: 1573, duration: 0.567s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 93.536 [7.000, 217.000], mean observation: 0.144 [0.000, 56.000], loss: 1.054633, mean_absolute_error: 0.579867, mean_q: 4.697052, mean_eps: 0.100000\n",
      "  56005/175000: episode: 1574, duration: 0.497s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 65.833 [7.000, 176.000], mean observation: 0.124 [0.000, 48.000], loss: 13.724016, mean_absolute_error: 0.710499, mean_q: 5.125188, mean_eps: 0.100000\n",
      "  56035/175000: episode: 1575, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 58.633 [7.000, 176.000], mean observation: 0.137 [0.000, 60.000], loss: 5.196870, mean_absolute_error: 0.667902, mean_q: 5.106653, mean_eps: 0.100000\n",
      "  56069/175000: episode: 1576, duration: 0.757s, episode steps: 34, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.794 [3.000, 221.000], mean observation: 0.261 [0.000, 68.000], loss: 3.249656, mean_absolute_error: 0.609855, mean_q: 4.783056, mean_eps: 0.100000\n",
      "  56109/175000: episode: 1577, duration: 0.806s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 114.350 [23.000, 198.000], mean observation: 0.302 [0.000, 80.000], loss: 5.363272, mean_absolute_error: 0.577549, mean_q: 4.632449, mean_eps: 0.100000\n",
      "  56159/175000: episode: 1578, duration: 1.052s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.900 [8.000, 194.000], mean observation: 0.358 [0.000, 100.000], loss: 4.941119, mean_absolute_error: 0.475610, mean_q: 3.974075, mean_eps: 0.100000\n",
      "  56185/175000: episode: 1579, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 49.846 [28.000, 162.000], mean observation: 0.112 [0.000, 52.000], loss: 15.032805, mean_absolute_error: 0.869012, mean_q: 6.113208, mean_eps: 0.100000\n",
      "  56213/175000: episode: 1580, duration: 0.524s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 41.536 [28.000, 195.000], mean observation: 0.092 [0.000, 56.000], loss: 1.417425, mean_absolute_error: 0.435622, mean_q: 3.913975, mean_eps: 0.100000\n",
      "  56262/175000: episode: 1581, duration: 1.011s, episode steps: 49, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.816 [21.000, 212.000], mean observation: 0.179 [0.000, 98.000], loss: 19.309645, mean_absolute_error: 0.728159, mean_q: 5.127856, mean_eps: 0.100000\n",
      "  56287/175000: episode: 1582, duration: 0.526s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 42.560 [28.000, 209.000], mean observation: 0.071 [0.000, 50.000], loss: 0.373427, mean_absolute_error: 0.411185, mean_q: 3.683105, mean_eps: 0.100000\n",
      "  56317/175000: episode: 1583, duration: 0.622s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 36.433 [28.000, 170.000], mean observation: 0.155 [0.000, 60.000], loss: 4.618830, mean_absolute_error: 0.649028, mean_q: 5.106291, mean_eps: 0.100000\n",
      "  56327/175000: episode: 1584, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 61.300 [3.000, 212.000], mean observation: 0.053 [0.000, 20.000], loss: 3.283117, mean_absolute_error: 0.747959, mean_q: 5.913825, mean_eps: 0.100000\n",
      "  56368/175000: episode: 1585, duration: 0.892s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.756 [28.000, 221.000], mean observation: 0.219 [0.000, 82.000], loss: 4.147097, mean_absolute_error: 0.586357, mean_q: 4.817129, mean_eps: 0.100000\n",
      "  56405/175000: episode: 1586, duration: 0.806s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 48.730 [28.000, 167.000], mean observation: 0.221 [0.000, 74.000], loss: 16.593582, mean_absolute_error: 0.663096, mean_q: 5.084386, mean_eps: 0.100000\n",
      "  56443/175000: episode: 1587, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 35.974 [28.000, 201.000], mean observation: 0.159 [0.000, 76.000], loss: 5.021916, mean_absolute_error: 0.549995, mean_q: 4.790676, mean_eps: 0.100000\n",
      "  56477/175000: episode: 1588, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.794 [22.000, 215.000], mean observation: 0.240 [0.000, 68.000], loss: 2.040301, mean_absolute_error: 0.675046, mean_q: 5.583856, mean_eps: 0.100000\n",
      "  56515/175000: episode: 1589, duration: 0.747s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 31.921 [27.000, 115.000], mean observation: 0.127 [0.000, 76.000], loss: 5.300569, mean_absolute_error: 0.618379, mean_q: 5.199807, mean_eps: 0.100000\n",
      "  56550/175000: episode: 1590, duration: 0.800s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 27.400 [7.000, 28.000], mean observation: 0.088 [0.000, 70.000], loss: 9.187183, mean_absolute_error: 0.531800, mean_q: 4.545769, mean_eps: 0.100000\n",
      "  56586/175000: episode: 1591, duration: 0.751s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 32.306 [0.000, 211.000], mean observation: 0.101 [0.000, 72.000], loss: 0.235004, mean_absolute_error: 0.544912, mean_q: 4.796157, mean_eps: 0.100000\n",
      "  56627/175000: episode: 1592, duration: 0.960s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 47.756 [8.000, 220.000], mean observation: 0.214 [0.000, 82.000], loss: 0.856427, mean_absolute_error: 0.503390, mean_q: 4.514312, mean_eps: 0.100000\n",
      "  56675/175000: episode: 1593, duration: 1.013s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 74.188 [28.000, 219.000], mean observation: 0.236 [0.000, 96.000], loss: 5.140057, mean_absolute_error: 0.556735, mean_q: 4.732043, mean_eps: 0.100000\n",
      "  56721/175000: episode: 1594, duration: 1.012s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 85.630 [28.000, 207.000], mean observation: 0.337 [0.000, 92.000], loss: 12.661700, mean_absolute_error: 0.626874, mean_q: 4.966370, mean_eps: 0.100000\n",
      "  56745/175000: episode: 1595, duration: 0.534s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 175.125 [7.000, 221.000], mean observation: 0.092 [0.000, 48.000], loss: 0.092949, mean_absolute_error: 0.393062, mean_q: 3.815544, mean_eps: 0.100000\n",
      "  56767/175000: episode: 1596, duration: 0.501s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 167.682 [7.000, 221.000], mean observation: 0.100 [0.000, 44.000], loss: 9.839968, mean_absolute_error: 0.726164, mean_q: 5.623345, mean_eps: 0.100000\n",
      "  56795/175000: episode: 1597, duration: 0.625s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.714 [6.000, 217.000], mean observation: 0.174 [0.000, 56.000], loss: 6.375897, mean_absolute_error: 0.613440, mean_q: 5.000583, mean_eps: 0.100000\n",
      "  56839/175000: episode: 1598, duration: 0.962s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 155.545 [29.000, 215.000], mean observation: 0.236 [0.000, 88.000], loss: 4.969499, mean_absolute_error: 0.564798, mean_q: 4.759268, mean_eps: 0.100000\n",
      "  56888/175000: episode: 1599, duration: 1.079s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 62.796 [2.000, 215.000], mean observation: 0.275 [0.000, 98.000], loss: 0.882367, mean_absolute_error: 0.457589, mean_q: 4.301828, mean_eps: 0.100000\n",
      "  56933/175000: episode: 1600, duration: 0.928s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 82.511 [25.000, 182.000], mean observation: 0.289 [0.000, 90.000], loss: 7.000874, mean_absolute_error: 0.566131, mean_q: 4.722329, mean_eps: 0.100000\n",
      "  56962/175000: episode: 1601, duration: 0.689s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 121.724 [5.000, 177.000], mean observation: 0.188 [0.000, 58.000], loss: 19.791453, mean_absolute_error: 0.797536, mean_q: 5.667100, mean_eps: 0.100000\n",
      "  56979/175000: episode: 1602, duration: 0.340s, episode steps: 17, steps per second: 50, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 116.647 [58.000, 177.000], mean observation: 0.060 [0.000, 34.000], loss: 17.652748, mean_absolute_error: 0.934397, mean_q: 6.662557, mean_eps: 0.100000\n",
      "  57006/175000: episode: 1603, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.185 [58.000, 221.000], mean observation: 0.120 [0.000, 54.000], loss: 4.510584, mean_absolute_error: 0.596008, mean_q: 4.857464, mean_eps: 0.100000\n",
      "  57038/175000: episode: 1604, duration: 0.669s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.469 [28.000, 192.000], mean observation: 0.226 [0.000, 64.000], loss: 4.579614, mean_absolute_error: 0.619541, mean_q: 5.076294, mean_eps: 0.100000\n",
      "  57065/175000: episode: 1605, duration: 0.617s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 114.148 [58.000, 177.000], mean observation: 0.103 [0.000, 54.000], loss: 1.956947, mean_absolute_error: 0.601705, mean_q: 5.006450, mean_eps: 0.100000\n",
      "  57091/175000: episode: 1606, duration: 0.515s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 152.500 [33.000, 177.000], mean observation: 0.134 [0.000, 52.000], loss: 10.353190, mean_absolute_error: 0.558041, mean_q: 4.501030, mean_eps: 0.100000\n",
      "  57118/175000: episode: 1607, duration: 0.619s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 151.148 [4.000, 215.000], mean observation: 0.209 [0.000, 54.000], loss: 1.552299, mean_absolute_error: 0.641275, mean_q: 5.368451, mean_eps: 0.100000\n",
      "  57144/175000: episode: 1608, duration: 0.601s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 125.538 [58.000, 177.000], mean observation: 0.185 [0.000, 52.000], loss: 8.542819, mean_absolute_error: 0.686895, mean_q: 5.316065, mean_eps: 0.100000\n",
      "  57181/175000: episode: 1609, duration: 0.817s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 105.568 [7.000, 215.000], mean observation: 0.276 [0.000, 74.000], loss: 12.264800, mean_absolute_error: 0.738879, mean_q: 5.578620, mean_eps: 0.100000\n",
      "  57224/175000: episode: 1610, duration: 0.922s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.326 [18.000, 178.000], mean observation: 0.359 [0.000, 86.000], loss: 17.266279, mean_absolute_error: 0.804132, mean_q: 5.929602, mean_eps: 0.100000\n",
      "  57244/175000: episode: 1611, duration: 0.532s, episode steps: 20, steps per second: 38, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 103.150 [26.000, 193.000], mean observation: 0.113 [0.000, 40.000], loss: 8.190167, mean_absolute_error: 0.639594, mean_q: 5.288075, mean_eps: 0.100000\n",
      "  57269/175000: episode: 1612, duration: 0.554s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 93.520 [26.000, 223.000], mean observation: 0.117 [0.000, 50.000], loss: 0.294013, mean_absolute_error: 0.382242, mean_q: 3.876955, mean_eps: 0.100000\n",
      "  57286/175000: episode: 1613, duration: 0.364s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 91.941 [26.000, 177.000], mean observation: 0.061 [0.000, 34.000], loss: 8.531670, mean_absolute_error: 0.604376, mean_q: 5.017163, mean_eps: 0.100000\n",
      "  57314/175000: episode: 1614, duration: 0.576s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.036 [1.000, 182.000], mean observation: 0.256 [0.000, 56.000], loss: 8.357963, mean_absolute_error: 0.559643, mean_q: 4.710922, mean_eps: 0.100000\n",
      "  57345/175000: episode: 1615, duration: 0.693s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 77.226 [24.000, 177.000], mean observation: 0.168 [0.000, 62.000], loss: 0.368908, mean_absolute_error: 0.432136, mean_q: 4.221892, mean_eps: 0.100000\n",
      "  57383/175000: episode: 1616, duration: 0.840s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 87.316 [7.000, 223.000], mean observation: 0.335 [0.000, 76.000], loss: 10.912548, mean_absolute_error: 0.724448, mean_q: 5.685862, mean_eps: 0.100000\n",
      "  57421/175000: episode: 1617, duration: 0.846s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.105 [12.000, 219.000], mean observation: 0.357 [0.000, 76.000], loss: 0.239307, mean_absolute_error: 0.523676, mean_q: 4.748284, mean_eps: 0.100000\n",
      "  57453/175000: episode: 1618, duration: 0.656s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.438 [28.000, 192.000], mean observation: 0.229 [0.000, 64.000], loss: 13.760105, mean_absolute_error: 0.870284, mean_q: 6.511605, mean_eps: 0.100000\n",
      "  57489/175000: episode: 1619, duration: 0.739s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 44.083 [28.000, 220.000], mean observation: 0.099 [0.000, 72.000], loss: 2.190101, mean_absolute_error: 0.596687, mean_q: 5.124616, mean_eps: 0.100000\n",
      "  57528/175000: episode: 1620, duration: 0.882s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 56.538 [2.000, 206.000], mean observation: 0.238 [0.000, 78.000], loss: 5.620002, mean_absolute_error: 0.666030, mean_q: 5.505485, mean_eps: 0.100000\n",
      "  57544/175000: episode: 1621, duration: 0.402s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 34.188 [28.000, 127.000], mean observation: 0.050 [0.000, 32.000], loss: 11.666658, mean_absolute_error: 0.709251, mean_q: 5.536005, mean_eps: 0.100000\n",
      "  57566/175000: episode: 1622, duration: 0.479s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 30.818 [28.000, 90.000], mean observation: 0.079 [0.000, 44.000], loss: 2.406646, mean_absolute_error: 0.619291, mean_q: 5.269806, mean_eps: 0.100000\n",
      "  57595/175000: episode: 1623, duration: 0.604s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 46.690 [28.000, 223.000], mean observation: 0.083 [0.000, 58.000], loss: 0.229980, mean_absolute_error: 0.434230, mean_q: 4.283807, mean_eps: 0.100000\n",
      "  57636/175000: episode: 1624, duration: 0.875s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.244 [0.000, 215.000], mean observation: 0.323 [0.000, 82.000], loss: 0.350864, mean_absolute_error: 0.465566, mean_q: 4.310979, mean_eps: 0.100000\n",
      "  57674/175000: episode: 1625, duration: 0.793s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.500 [17.000, 172.000], mean observation: 0.284 [0.000, 76.000], loss: 18.007057, mean_absolute_error: 0.772127, mean_q: 5.720407, mean_eps: 0.100000\n",
      "  57700/175000: episode: 1626, duration: 0.616s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 143.962 [34.000, 179.000], mean observation: 0.153 [0.000, 52.000], loss: 0.168800, mean_absolute_error: 0.416424, mean_q: 4.058294, mean_eps: 0.100000\n",
      "  57762/175000: episode: 1627, duration: 1.364s, episode steps: 62, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.613 [51.000, 202.000], mean observation: 0.468 [0.000, 124.000], loss: 3.849147, mean_absolute_error: 0.490027, mean_q: 4.489534, mean_eps: 0.100000\n",
      "  57798/175000: episode: 1628, duration: 0.772s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 143.861 [44.000, 154.000], mean observation: 0.110 [0.000, 72.000], loss: 6.678929, mean_absolute_error: 0.604463, mean_q: 5.015589, mean_eps: 0.100000\n",
      "  57836/175000: episode: 1629, duration: 0.833s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 138.553 [36.000, 180.000], mean observation: 0.161 [0.000, 76.000], loss: 10.190105, mean_absolute_error: 0.692936, mean_q: 5.477002, mean_eps: 0.100000\n",
      "  57873/175000: episode: 1630, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 140.351 [76.000, 180.000], mean observation: 0.127 [0.000, 74.000], loss: 11.444116, mean_absolute_error: 0.640070, mean_q: 5.043344, mean_eps: 0.100000\n",
      "  57890/175000: episode: 1631, duration: 0.301s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 125.000 [125.000, 125.000], mean observation: 0.041 [0.000, 34.000], loss: 0.166441, mean_absolute_error: 0.380258, mean_q: 3.877445, mean_eps: 0.100000\n",
      "  57917/175000: episode: 1632, duration: 0.518s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 122.370 [0.000, 189.000], mean observation: 0.085 [0.000, 54.000], loss: 3.504898, mean_absolute_error: 0.628133, mean_q: 5.302195, mean_eps: 0.100000\n",
      "  57945/175000: episode: 1633, duration: 0.613s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 121.536 [5.000, 177.000], mean observation: 0.126 [0.000, 56.000], loss: 2.118299, mean_absolute_error: 0.616763, mean_q: 5.282612, mean_eps: 0.100000\n",
      "  57987/175000: episode: 1634, duration: 0.871s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.524 [68.000, 177.000], mean observation: 0.198 [0.000, 84.000], loss: 6.992834, mean_absolute_error: 0.521549, mean_q: 4.547003, mean_eps: 0.100000\n",
      "  58031/175000: episode: 1635, duration: 0.896s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.250 [4.000, 154.000], mean observation: 0.181 [0.000, 88.000], loss: 8.301491, mean_absolute_error: 0.664245, mean_q: 5.465325, mean_eps: 0.100000\n",
      "  58074/175000: episode: 1636, duration: 0.910s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 46.907 [28.000, 194.000], mean observation: 0.124 [0.000, 86.000], loss: 3.960728, mean_absolute_error: 0.473192, mean_q: 4.408805, mean_eps: 0.100000\n",
      "  58124/175000: episode: 1637, duration: 1.067s, episode steps: 50, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 84.140 [28.000, 215.000], mean observation: 0.277 [0.000, 100.000], loss: 11.569916, mean_absolute_error: 0.635121, mean_q: 5.198634, mean_eps: 0.100000\n",
      "  58150/175000: episode: 1638, duration: 0.644s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 48.885 [48.000, 71.000], mean observation: 0.094 [0.000, 52.000], loss: 0.218375, mean_absolute_error: 0.426555, mean_q: 4.258832, mean_eps: 0.100000\n",
      "  58195/175000: episode: 1639, duration: 1.018s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.000 [48.000, 222.000], mean observation: 0.237 [0.000, 90.000], loss: 0.151001, mean_absolute_error: 0.457187, mean_q: 4.533607, mean_eps: 0.100000\n",
      "  58245/175000: episode: 1640, duration: 1.109s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.920 [1.000, 162.000], mean observation: 0.345 [0.000, 100.000], loss: 2.690514, mean_absolute_error: 0.513767, mean_q: 4.720468, mean_eps: 0.100000\n",
      "  58286/175000: episode: 1641, duration: 0.811s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.976 [1.000, 217.000], mean observation: 0.267 [0.000, 82.000], loss: 7.501027, mean_absolute_error: 0.696066, mean_q: 5.648218, mean_eps: 0.100000\n",
      "  58318/175000: episode: 1642, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 116.188 [32.000, 154.000], mean observation: 0.217 [0.000, 64.000], loss: 14.276983, mean_absolute_error: 0.676690, mean_q: 5.407278, mean_eps: 0.100000\n",
      "  58346/175000: episode: 1643, duration: 0.573s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.714 [3.000, 204.000], mean observation: 0.131 [0.000, 56.000], loss: 3.834811, mean_absolute_error: 0.616698, mean_q: 5.322791, mean_eps: 0.100000\n",
      "  58392/175000: episode: 1644, duration: 1.069s, episode steps: 46, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 86.304 [1.000, 193.000], mean observation: 0.422 [0.000, 92.000], loss: 12.128870, mean_absolute_error: 0.681688, mean_q: 5.548238, mean_eps: 0.100000\n",
      "  58434/175000: episode: 1645, duration: 0.905s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 67.310 [20.000, 221.000], mean observation: 0.140 [0.000, 84.000], loss: 0.257677, mean_absolute_error: 0.420062, mean_q: 4.201980, mean_eps: 0.100000\n",
      "  58469/175000: episode: 1646, duration: 0.759s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.143 [28.000, 189.000], mean observation: 0.140 [0.000, 70.000], loss: 2.326031, mean_absolute_error: 0.499853, mean_q: 4.651866, mean_eps: 0.100000\n",
      "  58527/175000: episode: 1647, duration: 1.204s, episode steps: 58, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 127.948 [11.000, 204.000], mean observation: 0.490 [0.000, 116.000], loss: 0.297361, mean_absolute_error: 0.476317, mean_q: 4.631590, mean_eps: 0.100000\n",
      "  58567/175000: episode: 1648, duration: 0.784s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 67.325 [28.000, 172.000], mean observation: 0.223 [0.000, 80.000], loss: 9.476713, mean_absolute_error: 0.619391, mean_q: 5.171517, mean_eps: 0.100000\n",
      "  58609/175000: episode: 1649, duration: 0.989s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.643 [28.000, 184.000], mean observation: 0.311 [0.000, 84.000], loss: 8.873924, mean_absolute_error: 0.690948, mean_q: 5.650604, mean_eps: 0.100000\n",
      "  58650/175000: episode: 1650, duration: 0.841s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.220 [10.000, 224.000], mean observation: 0.259 [0.000, 82.000], loss: 6.864399, mean_absolute_error: 0.573007, mean_q: 5.007933, mean_eps: 0.100000\n",
      "  58709/175000: episode: 1651, duration: 1.292s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 67.525 [32.000, 201.000], mean observation: 0.453 [0.000, 118.000], loss: 16.480970, mean_absolute_error: 0.627335, mean_q: 5.053859, mean_eps: 0.100000\n",
      "  58752/175000: episode: 1652, duration: 0.946s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.070 [47.000, 176.000], mean observation: 0.410 [0.000, 86.000], loss: 0.545972, mean_absolute_error: 0.544291, mean_q: 5.030826, mean_eps: 0.100000\n",
      "  58799/175000: episode: 1653, duration: 0.987s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.447 [48.000, 176.000], mean observation: 0.390 [0.000, 94.000], loss: 3.268312, mean_absolute_error: 0.589408, mean_q: 5.226615, mean_eps: 0.100000\n",
      "  58835/175000: episode: 1654, duration: 0.765s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 64.111 [37.000, 222.000], mean observation: 0.216 [0.000, 72.000], loss: 13.315586, mean_absolute_error: 0.824189, mean_q: 6.274832, mean_eps: 0.100000\n",
      "  58857/175000: episode: 1655, duration: 0.558s, episode steps: 22, steps per second: 39, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 78.273 [38.000, 149.000], mean observation: 0.123 [0.000, 44.000], loss: 10.381297, mean_absolute_error: 0.719902, mean_q: 5.769134, mean_eps: 0.100000\n",
      "  58884/175000: episode: 1656, duration: 0.601s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 73.259 [37.000, 125.000], mean observation: 0.094 [0.000, 54.000], loss: 0.492751, mean_absolute_error: 0.535414, mean_q: 4.979130, mean_eps: 0.100000\n",
      "  58937/175000: episode: 1657, duration: 1.125s, episode steps: 53, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 94.472 [26.000, 217.000], mean observation: 0.374 [0.000, 106.000], loss: 5.110913, mean_absolute_error: 0.610356, mean_q: 5.264334, mean_eps: 0.100000\n",
      "  58966/175000: episode: 1658, duration: 0.594s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 111.690 [28.000, 144.000], mean observation: 0.150 [0.000, 58.000], loss: 0.180560, mean_absolute_error: 0.447482, mean_q: 4.376171, mean_eps: 0.100000\n",
      "  59001/175000: episode: 1659, duration: 0.747s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.571 [48.000, 218.000], mean observation: 0.206 [0.000, 70.000], loss: 2.177029, mean_absolute_error: 0.597639, mean_q: 5.296827, mean_eps: 0.100000\n",
      "  59022/175000: episode: 1660, duration: 0.512s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 101.286 [47.000, 172.000], mean observation: 0.166 [0.000, 42.000], loss: 13.708733, mean_absolute_error: 0.794661, mean_q: 6.084163, mean_eps: 0.100000\n",
      "  59046/175000: episode: 1661, duration: 0.541s, episode steps: 24, steps per second: 44, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 163.208 [26.000, 203.000], mean observation: 0.117 [0.000, 48.000], loss: 15.572655, mean_absolute_error: 0.648730, mean_q: 5.291874, mean_eps: 0.100000\n",
      "  59072/175000: episode: 1662, duration: 0.607s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 132.692 [32.000, 177.000], mean observation: 0.125 [0.000, 52.000], loss: 8.599587, mean_absolute_error: 0.806192, mean_q: 6.428931, mean_eps: 0.100000\n",
      "  59131/175000: episode: 1663, duration: 1.315s, episode steps: 59, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 99.949 [32.000, 205.000], mean observation: 0.401 [0.000, 118.000], loss: 2.126438, mean_absolute_error: 0.488404, mean_q: 4.702550, mean_eps: 0.100000\n",
      "  59162/175000: episode: 1664, duration: 0.697s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.129 [32.000, 204.000], mean observation: 0.169 [0.000, 62.000], loss: 4.004638, mean_absolute_error: 0.610449, mean_q: 5.351303, mean_eps: 0.100000\n",
      "  59194/175000: episode: 1665, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 41.844 [32.000, 177.000], mean observation: 0.145 [0.000, 64.000], loss: 6.524590, mean_absolute_error: 0.680653, mean_q: 5.724111, mean_eps: 0.100000\n",
      "  59227/175000: episode: 1666, duration: 0.702s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 72.939 [16.000, 224.000], mean observation: 0.221 [0.000, 66.000], loss: 2.528947, mean_absolute_error: 0.584009, mean_q: 5.354770, mean_eps: 0.100000\n",
      "  59267/175000: episode: 1667, duration: 0.854s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 110.075 [8.000, 223.000], mean observation: 0.322 [0.000, 80.000], loss: 0.431663, mean_absolute_error: 0.453684, mean_q: 4.673921, mean_eps: 0.100000\n",
      "  59291/175000: episode: 1668, duration: 0.525s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 149.792 [32.000, 223.000], mean observation: 0.130 [0.000, 48.000], loss: 12.975854, mean_absolute_error: 0.744274, mean_q: 5.989864, mean_eps: 0.100000\n",
      "  59337/175000: episode: 1669, duration: 0.996s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.283 [26.000, 223.000], mean observation: 0.364 [0.000, 92.000], loss: 10.267079, mean_absolute_error: 0.676877, mean_q: 5.825233, mean_eps: 0.100000\n",
      "  59378/175000: episode: 1670, duration: 0.871s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 151.171 [14.000, 223.000], mean observation: 0.323 [0.000, 82.000], loss: 9.285623, mean_absolute_error: 0.675009, mean_q: 5.737013, mean_eps: 0.100000\n",
      "  59432/175000: episode: 1671, duration: 1.145s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 140.926 [16.000, 223.000], mean observation: 0.488 [0.000, 108.000], loss: 18.205689, mean_absolute_error: 0.774839, mean_q: 6.172124, mean_eps: 0.100000\n",
      "  59470/175000: episode: 1672, duration: 0.810s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.974 [39.000, 167.000], mean observation: 0.217 [0.000, 76.000], loss: 1.623706, mean_absolute_error: 0.588658, mean_q: 5.511117, mean_eps: 0.100000\n",
      "  59514/175000: episode: 1673, duration: 0.946s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 66.614 [32.000, 198.000], mean observation: 0.306 [0.000, 88.000], loss: 7.028875, mean_absolute_error: 0.612400, mean_q: 5.426429, mean_eps: 0.100000\n",
      "  59535/175000: episode: 1674, duration: 0.449s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 80.571 [39.000, 181.000], mean observation: 0.135 [0.000, 42.000], loss: 13.246789, mean_absolute_error: 0.811987, mean_q: 6.520494, mean_eps: 0.100000\n",
      "  59553/175000: episode: 1675, duration: 0.408s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 97.722 [36.000, 164.000], mean observation: 0.088 [0.000, 36.000], loss: 0.239921, mean_absolute_error: 0.590922, mean_q: 5.622192, mean_eps: 0.100000\n",
      "  59597/175000: episode: 1676, duration: 0.934s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.386 [39.000, 210.000], mean observation: 0.389 [0.000, 88.000], loss: 1.815249, mean_absolute_error: 0.486961, mean_q: 4.832787, mean_eps: 0.100000\n",
      "  59630/175000: episode: 1677, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 134.152 [67.000, 222.000], mean observation: 0.267 [0.000, 66.000], loss: 0.360858, mean_absolute_error: 0.521004, mean_q: 5.038550, mean_eps: 0.100000\n",
      "  59681/175000: episode: 1678, duration: 1.121s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.196 [27.000, 209.000], mean observation: 0.429 [0.000, 102.000], loss: 12.906561, mean_absolute_error: 0.818569, mean_q: 6.502695, mean_eps: 0.100000\n",
      "  59726/175000: episode: 1679, duration: 1.020s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 48.756 [15.000, 153.000], mean observation: 0.235 [0.000, 90.000], loss: 5.897121, mean_absolute_error: 0.504108, mean_q: 4.785029, mean_eps: 0.100000\n",
      "  59764/175000: episode: 1680, duration: 0.872s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 49.105 [4.000, 193.000], mean observation: 0.159 [0.000, 76.000], loss: 0.180607, mean_absolute_error: 0.422883, mean_q: 4.606428, mean_eps: 0.100000\n",
      "  59800/175000: episode: 1681, duration: 0.881s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 92.056 [29.000, 204.000], mean observation: 0.227 [0.000, 72.000], loss: 0.170827, mean_absolute_error: 0.402119, mean_q: 4.268855, mean_eps: 0.100000\n",
      "  59822/175000: episode: 1682, duration: 0.520s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 52.182 [39.000, 97.000], mean observation: 0.055 [0.000, 44.000], loss: 0.295703, mean_absolute_error: 0.478814, mean_q: 4.623590, mean_eps: 0.100000\n",
      "  59866/175000: episode: 1683, duration: 0.921s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.659 [39.000, 204.000], mean observation: 0.490 [0.000, 88.000], loss: 5.617218, mean_absolute_error: 0.518135, mean_q: 4.773291, mean_eps: 0.100000\n",
      "  59908/175000: episode: 1684, duration: 0.907s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.262 [27.000, 216.000], mean observation: 0.299 [0.000, 84.000], loss: 6.408302, mean_absolute_error: 0.513388, mean_q: 4.714973, mean_eps: 0.100000\n",
      "  59955/175000: episode: 1685, duration: 1.008s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.702 [15.000, 209.000], mean observation: 0.466 [0.000, 94.000], loss: 0.131246, mean_absolute_error: 0.431856, mean_q: 4.557708, mean_eps: 0.100000\n",
      "  59983/175000: episode: 1686, duration: 0.601s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 40.607 [5.000, 224.000], mean observation: 0.097 [0.000, 56.000], loss: 8.369215, mean_absolute_error: 0.554745, mean_q: 5.023637, mean_eps: 0.100000\n",
      "  60032/175000: episode: 1687, duration: 1.073s, episode steps: 49, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.429 [1.000, 167.000], mean observation: 0.203 [0.000, 98.000], loss: 24.702770, mean_absolute_error: 0.723602, mean_q: 5.660991, mean_eps: 0.100000\n",
      "  60072/175000: episode: 1688, duration: 0.978s, episode steps: 40, steps per second: 41, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 31.800 [28.000, 180.000], mean observation: 0.102 [0.000, 80.000], loss: 19.246681, mean_absolute_error: 0.696676, mean_q: 5.751792, mean_eps: 0.100000\n",
      "  60117/175000: episode: 1689, duration: 0.971s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.622 [10.000, 176.000], mean observation: 0.255 [0.000, 90.000], loss: 21.558830, mean_absolute_error: 0.766214, mean_q: 6.233960, mean_eps: 0.100000\n",
      "  60168/175000: episode: 1690, duration: 1.128s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.294 [14.000, 138.000], mean observation: 0.305 [0.000, 102.000], loss: 11.244192, mean_absolute_error: 0.753446, mean_q: 6.455717, mean_eps: 0.100000\n",
      "  60195/175000: episode: 1691, duration: 0.593s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.667 [27.000, 148.000], mean observation: 0.151 [0.000, 54.000], loss: 16.955656, mean_absolute_error: 0.717211, mean_q: 6.082788, mean_eps: 0.100000\n",
      "  60205/175000: episode: 1692, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 79.300 [58.000, 196.000], mean observation: 0.050 [0.000, 20.000], loss: 0.133522, mean_absolute_error: 0.434735, mean_q: 4.894471, mean_eps: 0.100000\n",
      "  60252/175000: episode: 1693, duration: 1.076s, episode steps: 47, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 68.340 [26.000, 188.000], mean observation: 0.262 [0.000, 94.000], loss: 0.524582, mean_absolute_error: 0.425977, mean_q: 4.728412, mean_eps: 0.100000\n",
      "  60298/175000: episode: 1694, duration: 1.027s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 61.522 [58.000, 220.000], mean observation: 0.129 [0.000, 92.000], loss: 0.213825, mean_absolute_error: 0.498025, mean_q: 5.029126, mean_eps: 0.100000\n",
      "  60330/175000: episode: 1695, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 95.406 [6.000, 179.000], mean observation: 0.243 [0.000, 64.000], loss: 0.146887, mean_absolute_error: 0.435275, mean_q: 4.661815, mean_eps: 0.100000\n",
      "  60391/175000: episode: 1696, duration: 1.310s, episode steps: 61, steps per second: 47, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 83.705 [28.000, 201.000], mean observation: 0.488 [0.000, 122.000], loss: 31.466210, mean_absolute_error: 0.725658, mean_q: 5.668744, mean_eps: 0.100000\n",
      "  60428/175000: episode: 1697, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 60.162 [32.000, 67.000], mean observation: 0.146 [0.000, 74.000], loss: 12.800471, mean_absolute_error: 0.651876, mean_q: 5.715330, mean_eps: 0.100000\n",
      "  60494/175000: episode: 1698, duration: 1.379s, episode steps: 66, steps per second: 48, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 87.894 [5.000, 221.000], mean observation: 0.536 [0.000, 132.000], loss: 23.821400, mean_absolute_error: 0.761200, mean_q: 6.204952, mean_eps: 0.100000\n",
      "  60516/175000: episode: 1699, duration: 0.504s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 164.682 [39.000, 216.000], mean observation: 0.132 [0.000, 44.000], loss: 12.322367, mean_absolute_error: 0.641532, mean_q: 5.797558, mean_eps: 0.100000\n",
      "  60569/175000: episode: 1700, duration: 1.161s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 96.887 [33.000, 190.000], mean observation: 0.312 [0.000, 106.000], loss: 18.897382, mean_absolute_error: 0.707968, mean_q: 6.073641, mean_eps: 0.100000\n",
      "  60591/175000: episode: 1701, duration: 0.442s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 124.636 [62.000, 190.000], mean observation: 0.091 [0.000, 44.000], loss: 0.136660, mean_absolute_error: 0.502232, mean_q: 5.300583, mean_eps: 0.100000\n",
      "  60628/175000: episode: 1702, duration: 0.836s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 137.027 [48.000, 190.000], mean observation: 0.237 [0.000, 74.000], loss: 0.117478, mean_absolute_error: 0.430598, mean_q: 4.735228, mean_eps: 0.100000\n",
      "  60671/175000: episode: 1703, duration: 1.004s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.279 [13.000, 216.000], mean observation: 0.197 [0.000, 86.000], loss: 32.238664, mean_absolute_error: 0.910507, mean_q: 6.935763, mean_eps: 0.100000\n",
      "  60712/175000: episode: 1704, duration: 0.923s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 129.683 [19.000, 135.000], mean observation: 0.147 [0.000, 82.000], loss: 0.297739, mean_absolute_error: 0.501785, mean_q: 5.123521, mean_eps: 0.100000\n",
      "  60747/175000: episode: 1705, duration: 0.758s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 133.000 [58.000, 142.000], mean observation: 0.099 [0.000, 70.000], loss: 0.135228, mean_absolute_error: 0.452913, mean_q: 4.787676, mean_eps: 0.100000\n",
      "  60774/175000: episode: 1706, duration: 0.602s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 130.333 [97.000, 214.000], mean observation: 0.178 [0.000, 54.000], loss: 0.418899, mean_absolute_error: 0.453571, mean_q: 4.739082, mean_eps: 0.100000\n",
      "  60815/175000: episode: 1707, duration: 0.825s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.732 [44.000, 219.000], mean observation: 0.171 [0.000, 82.000], loss: 30.110801, mean_absolute_error: 0.909413, mean_q: 6.864361, mean_eps: 0.100000\n",
      "  60835/175000: episode: 1708, duration: 0.397s, episode steps: 20, steps per second: 50, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 135.000 [135.000, 135.000], mean observation: 0.048 [0.000, 40.000], loss: 0.389230, mean_absolute_error: 0.517185, mean_q: 5.173217, mean_eps: 0.100000\n",
      "  60888/175000: episode: 1709, duration: 1.247s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 126.792 [37.000, 216.000], mean observation: 0.526 [0.000, 106.000], loss: 4.414619, mean_absolute_error: 0.619890, mean_q: 5.802641, mean_eps: 0.100000\n",
      "  60926/175000: episode: 1710, duration: 0.834s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.711 [58.000, 219.000], mean observation: 0.242 [0.000, 76.000], loss: 27.503890, mean_absolute_error: 0.745985, mean_q: 5.899248, mean_eps: 0.100000\n",
      "  60982/175000: episode: 1711, duration: 1.216s, episode steps: 56, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 142.339 [37.000, 188.000], mean observation: 0.301 [0.000, 112.000], loss: 12.636260, mean_absolute_error: 0.639187, mean_q: 5.618490, mean_eps: 0.100000\n",
      "  61018/175000: episode: 1712, duration: 0.762s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 152.639 [15.000, 183.000], mean observation: 0.196 [0.000, 72.000], loss: 11.772203, mean_absolute_error: 0.688546, mean_q: 5.945489, mean_eps: 0.100000\n",
      "  61043/175000: episode: 1713, duration: 0.524s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 150.760 [58.000, 176.000], mean observation: 0.097 [0.000, 50.000], loss: 0.308449, mean_absolute_error: 0.445803, mean_q: 4.791879, mean_eps: 0.100000\n",
      "  61091/175000: episode: 1714, duration: 1.037s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 137.375 [21.000, 176.000], mean observation: 0.261 [0.000, 96.000], loss: 5.837027, mean_absolute_error: 0.613812, mean_q: 5.727990, mean_eps: 0.100000\n",
      "  61126/175000: episode: 1715, duration: 0.769s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.514 [4.000, 223.000], mean observation: 0.375 [0.000, 70.000], loss: 3.058018, mean_absolute_error: 0.554817, mean_q: 5.560183, mean_eps: 0.100000\n",
      "  61174/175000: episode: 1716, duration: 1.066s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.729 [22.000, 212.000], mean observation: 0.404 [0.000, 96.000], loss: 27.013983, mean_absolute_error: 0.703434, mean_q: 5.823912, mean_eps: 0.100000\n",
      "  61197/175000: episode: 1717, duration: 0.527s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 115.870 [76.000, 176.000], mean observation: 0.104 [0.000, 46.000], loss: 18.909102, mean_absolute_error: 0.715227, mean_q: 6.155952, mean_eps: 0.100000\n",
      "  61209/175000: episode: 1718, duration: 0.265s, episode steps: 12, steps per second: 45, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 78.083 [1.000, 176.000], mean observation: 0.039 [0.000, 24.000], loss: 52.298720, mean_absolute_error: 0.935109, mean_q: 6.359159, mean_eps: 0.100000\n",
      "  61221/175000: episode: 1719, duration: 0.260s, episode steps: 12, steps per second: 46, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 84.333 [76.000, 176.000], mean observation: 0.038 [0.000, 24.000], loss: 0.123059, mean_absolute_error: 0.400986, mean_q: 4.492319, mean_eps: 0.100000\n",
      "  61246/175000: episode: 1720, duration: 0.494s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 104.640 [29.000, 176.000], mean observation: 0.106 [0.000, 50.000], loss: 0.246805, mean_absolute_error: 0.519297, mean_q: 5.252873, mean_eps: 0.100000\n",
      "  61291/175000: episode: 1721, duration: 0.941s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.267 [5.000, 182.000], mean observation: 0.297 [0.000, 90.000], loss: 10.786651, mean_absolute_error: 0.633956, mean_q: 5.689885, mean_eps: 0.100000\n",
      "  61322/175000: episode: 1722, duration: 0.716s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 120.774 [21.000, 211.000], mean observation: 0.228 [0.000, 62.000], loss: 5.119672, mean_absolute_error: 0.657606, mean_q: 5.995652, mean_eps: 0.100000\n",
      "  61364/175000: episode: 1723, duration: 0.916s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.452 [24.000, 191.000], mean observation: 0.311 [0.000, 84.000], loss: 11.601989, mean_absolute_error: 0.692856, mean_q: 6.022731, mean_eps: 0.100000\n",
      "  61407/175000: episode: 1724, duration: 0.964s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.907 [28.000, 176.000], mean observation: 0.267 [0.000, 86.000], loss: 3.411199, mean_absolute_error: 0.641752, mean_q: 5.899288, mean_eps: 0.100000\n",
      "  61466/175000: episode: 1725, duration: 1.295s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 147.136 [8.000, 218.000], mean observation: 0.445 [0.000, 118.000], loss: 0.287989, mean_absolute_error: 0.520759, mean_q: 5.194358, mean_eps: 0.100000\n",
      "  61503/175000: episode: 1726, duration: 0.764s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.108 [28.000, 216.000], mean observation: 0.189 [0.000, 74.000], loss: 17.903895, mean_absolute_error: 0.654819, mean_q: 5.536022, mean_eps: 0.100000\n",
      "  61539/175000: episode: 1727, duration: 0.695s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.111 [28.000, 223.000], mean observation: 0.442 [0.000, 72.000], loss: 0.205395, mean_absolute_error: 0.433026, mean_q: 4.490056, mean_eps: 0.100000\n",
      "  61595/175000: episode: 1728, duration: 1.165s, episode steps: 56, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 104.696 [13.000, 212.000], mean observation: 0.325 [0.000, 112.000], loss: 0.550871, mean_absolute_error: 0.521968, mean_q: 5.012722, mean_eps: 0.100000\n",
      "  61623/175000: episode: 1729, duration: 0.602s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.500 [8.000, 203.000], mean observation: 0.129 [0.000, 56.000], loss: 32.023632, mean_absolute_error: 0.967616, mean_q: 6.917364, mean_eps: 0.100000\n",
      "  61674/175000: episode: 1730, duration: 1.117s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.196 [25.000, 176.000], mean observation: 0.234 [0.000, 102.000], loss: 11.029651, mean_absolute_error: 0.716311, mean_q: 5.951287, mean_eps: 0.100000\n",
      "  61718/175000: episode: 1731, duration: 0.964s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 31.000 [28.000, 153.000], mean observation: 0.165 [0.000, 88.000], loss: 10.489231, mean_absolute_error: 0.793882, mean_q: 6.478064, mean_eps: 0.100000\n",
      "  61755/175000: episode: 1732, duration: 0.806s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 30.081 [26.000, 107.000], mean observation: 0.122 [0.000, 74.000], loss: 4.874324, mean_absolute_error: 0.799423, mean_q: 6.702920, mean_eps: 0.100000\n",
      "  61781/175000: episode: 1733, duration: 0.616s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.269 [28.000, 224.000], mean observation: 0.181 [0.000, 52.000], loss: 33.936865, mean_absolute_error: 0.716328, mean_q: 5.456864, mean_eps: 0.100000\n",
      "  61821/175000: episode: 1734, duration: 0.868s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.275 [28.000, 208.000], mean observation: 0.283 [0.000, 80.000], loss: 27.435671, mean_absolute_error: 0.855961, mean_q: 6.597758, mean_eps: 0.100000\n",
      "  61838/175000: episode: 1735, duration: 0.350s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 151.353 [83.000, 176.000], mean observation: 0.069 [0.000, 34.000], loss: 0.132712, mean_absolute_error: 0.433143, mean_q: 4.700998, mean_eps: 0.100000\n",
      "  61893/175000: episode: 1736, duration: 1.278s, episode steps: 55, steps per second: 43, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 166.618 [32.000, 212.000], mean observation: 0.495 [0.000, 110.000], loss: 9.815995, mean_absolute_error: 0.612409, mean_q: 5.630738, mean_eps: 0.100000\n",
      "  61947/175000: episode: 1737, duration: 1.104s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 158.444 [15.000, 221.000], mean observation: 0.641 [0.000, 108.000], loss: 24.413026, mean_absolute_error: 0.710657, mean_q: 6.009098, mean_eps: 0.100000\n",
      "  61981/175000: episode: 1738, duration: 0.797s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.324 [118.000, 135.000], mean observation: 0.118 [0.000, 68.000], loss: 0.196325, mean_absolute_error: 0.451179, mean_q: 5.050821, mean_eps: 0.100000\n",
      "  62000/175000: episode: 1739, duration: 0.464s, episode steps: 19, steps per second: 41, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 120.421 [50.000, 161.000], mean observation: 0.116 [0.000, 38.000], loss: 13.419942, mean_absolute_error: 0.729197, mean_q: 6.476435, mean_eps: 0.100000\n",
      "  62025/175000: episode: 1740, duration: 0.561s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 117.960 [7.000, 176.000], mean observation: 0.094 [0.000, 50.000], loss: 1.290111, mean_absolute_error: 0.414101, mean_q: 4.677970, mean_eps: 0.100000\n",
      "  62053/175000: episode: 1741, duration: 0.626s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.143 [11.000, 122.000], mean observation: 0.150 [0.000, 56.000], loss: 9.281042, mean_absolute_error: 0.630053, mean_q: 5.857211, mean_eps: 0.100000\n",
      "  62096/175000: episode: 1742, duration: 0.924s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.419 [20.000, 204.000], mean observation: 0.263 [0.000, 86.000], loss: 8.083178, mean_absolute_error: 0.756992, mean_q: 6.749409, mean_eps: 0.100000\n",
      "  62143/175000: episode: 1743, duration: 0.975s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.681 [36.000, 224.000], mean observation: 0.374 [0.000, 94.000], loss: 6.032317, mean_absolute_error: 0.621658, mean_q: 5.920766, mean_eps: 0.100000\n",
      "  62199/175000: episode: 1744, duration: 1.189s, episode steps: 56, steps per second: 47, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 102.143 [14.000, 170.000], mean observation: 0.539 [0.000, 112.000], loss: 0.745892, mean_absolute_error: 0.512484, mean_q: 5.316025, mean_eps: 0.100000\n",
      "  62228/175000: episode: 1745, duration: 0.655s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 116.586 [28.000, 174.000], mean observation: 0.182 [0.000, 58.000], loss: 5.566424, mean_absolute_error: 0.631826, mean_q: 5.931098, mean_eps: 0.100000\n",
      "  62260/175000: episode: 1746, duration: 0.722s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 113.719 [29.000, 192.000], mean observation: 0.237 [0.000, 64.000], loss: 18.799284, mean_absolute_error: 0.852866, mean_q: 6.907928, mean_eps: 0.100000\n",
      "  62298/175000: episode: 1747, duration: 0.819s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.605 [9.000, 171.000], mean observation: 0.273 [0.000, 76.000], loss: 35.364278, mean_absolute_error: 1.109412, mean_q: 8.169170, mean_eps: 0.100000\n",
      "  62330/175000: episode: 1748, duration: 0.706s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.031 [43.000, 214.000], mean observation: 0.241 [0.000, 64.000], loss: 60.078225, mean_absolute_error: 0.999375, mean_q: 6.659664, mean_eps: 0.100000\n",
      "  62363/175000: episode: 1749, duration: 0.701s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 102.242 [36.000, 176.000], mean observation: 0.280 [0.000, 66.000], loss: 13.558572, mean_absolute_error: 0.674751, mean_q: 5.839313, mean_eps: 0.100000\n",
      "  62407/175000: episode: 1750, duration: 0.997s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.682 [10.000, 212.000], mean observation: 0.430 [0.000, 88.000], loss: 4.533536, mean_absolute_error: 0.551094, mean_q: 5.325033, mean_eps: 0.100000\n",
      "  62438/175000: episode: 1751, duration: 0.696s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 116.452 [35.000, 221.000], mean observation: 0.173 [0.000, 62.000], loss: 9.353217, mean_absolute_error: 0.623481, mean_q: 5.640208, mean_eps: 0.100000\n",
      "  62470/175000: episode: 1752, duration: 0.699s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 89.375 [23.000, 122.000], mean observation: 0.138 [0.000, 64.000], loss: 0.273228, mean_absolute_error: 0.455710, mean_q: 4.785941, mean_eps: 0.100000\n",
      "  62509/175000: episode: 1753, duration: 0.845s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 90.615 [0.000, 194.000], mean observation: 0.213 [0.000, 78.000], loss: 0.217131, mean_absolute_error: 0.457700, mean_q: 4.761196, mean_eps: 0.100000\n",
      "  62547/175000: episode: 1754, duration: 0.797s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.500 [57.000, 122.000], mean observation: 0.192 [0.000, 76.000], loss: 0.283006, mean_absolute_error: 0.474397, mean_q: 4.850465, mean_eps: 0.100000\n",
      "  62580/175000: episode: 1755, duration: 0.758s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 107.000 [37.000, 190.000], mean observation: 0.171 [0.000, 66.000], loss: 18.194761, mean_absolute_error: 0.598322, mean_q: 4.973907, mean_eps: 0.100000\n",
      "  62618/175000: episode: 1756, duration: 0.892s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.026 [37.000, 202.000], mean observation: 0.303 [0.000, 76.000], loss: 3.074280, mean_absolute_error: 0.440080, mean_q: 4.356324, mean_eps: 0.100000\n",
      "  62667/175000: episode: 1757, duration: 1.121s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.776 [7.000, 195.000], mean observation: 0.337 [0.000, 98.000], loss: 7.304338, mean_absolute_error: 0.486231, mean_q: 4.610131, mean_eps: 0.100000\n",
      "  62694/175000: episode: 1758, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 123.556 [29.000, 203.000], mean observation: 0.161 [0.000, 54.000], loss: 2.698652, mean_absolute_error: 0.431758, mean_q: 4.264270, mean_eps: 0.100000\n",
      "  62730/175000: episode: 1759, duration: 0.778s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 140.778 [40.000, 205.000], mean observation: 0.197 [0.000, 72.000], loss: 29.743113, mean_absolute_error: 0.849093, mean_q: 6.151817, mean_eps: 0.100000\n",
      "  62772/175000: episode: 1760, duration: 0.961s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.190 [10.000, 190.000], mean observation: 0.257 [0.000, 84.000], loss: 14.251815, mean_absolute_error: 0.694046, mean_q: 5.605181, mean_eps: 0.100000\n",
      "  62808/175000: episode: 1761, duration: 0.825s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 123.500 [7.000, 190.000], mean observation: 0.269 [0.000, 72.000], loss: 16.771960, mean_absolute_error: 0.589940, mean_q: 4.843120, mean_eps: 0.100000\n",
      "  62853/175000: episode: 1762, duration: 1.013s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.911 [48.000, 190.000], mean observation: 0.236 [0.000, 90.000], loss: 12.304883, mean_absolute_error: 0.557624, mean_q: 4.824182, mean_eps: 0.100000\n",
      "  62897/175000: episode: 1763, duration: 0.987s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 121.205 [38.000, 218.000], mean observation: 0.331 [0.000, 88.000], loss: 4.226644, mean_absolute_error: 0.615331, mean_q: 5.431546, mean_eps: 0.100000\n",
      "  62943/175000: episode: 1764, duration: 0.946s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.304 [10.000, 200.000], mean observation: 0.402 [0.000, 92.000], loss: 2.330231, mean_absolute_error: 0.453984, mean_q: 4.378090, mean_eps: 0.100000\n",
      "  62987/175000: episode: 1765, duration: 0.930s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.545 [10.000, 200.000], mean observation: 0.432 [0.000, 88.000], loss: 17.247672, mean_absolute_error: 0.690169, mean_q: 5.468667, mean_eps: 0.100000\n",
      "  63023/175000: episode: 1766, duration: 0.824s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.889 [10.000, 200.000], mean observation: 0.351 [0.000, 72.000], loss: 0.357015, mean_absolute_error: 0.436614, mean_q: 4.199301, mean_eps: 0.100000\n",
      "  63084/175000: episode: 1767, duration: 1.478s, episode steps: 61, steps per second: 41, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 104.262 [7.000, 218.000], mean observation: 0.622 [0.000, 122.000], loss: 19.258680, mean_absolute_error: 0.645154, mean_q: 5.178587, mean_eps: 0.100000\n",
      "  63099/175000: episode: 1768, duration: 0.350s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 72.400 [28.000, 118.000], mean observation: 0.065 [0.000, 30.000], loss: 0.798480, mean_absolute_error: 0.416874, mean_q: 4.051844, mean_eps: 0.100000\n",
      "  63129/175000: episode: 1769, duration: 0.796s, episode steps: 30, steps per second: 38, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 89.933 [13.000, 215.000], mean observation: 0.192 [0.000, 60.000], loss: 0.344334, mean_absolute_error: 0.453434, mean_q: 4.581277, mean_eps: 0.100000\n",
      "  63166/175000: episode: 1770, duration: 0.856s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.865 [74.000, 208.000], mean observation: 0.115 [0.000, 74.000], loss: 26.926913, mean_absolute_error: 0.675172, mean_q: 5.187296, mean_eps: 0.100000\n",
      "  63218/175000: episode: 1771, duration: 1.164s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 82.404 [29.000, 209.000], mean observation: 0.289 [0.000, 104.000], loss: 19.104153, mean_absolute_error: 0.585908, mean_q: 4.806631, mean_eps: 0.100000\n",
      "  63243/175000: episode: 1772, duration: 0.538s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 88.360 [33.000, 144.000], mean observation: 0.179 [0.000, 50.000], loss: 4.937593, mean_absolute_error: 0.587860, mean_q: 5.241570, mean_eps: 0.100000\n",
      "  63271/175000: episode: 1773, duration: 0.648s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.500 [65.000, 165.000], mean observation: 0.169 [0.000, 56.000], loss: 39.103653, mean_absolute_error: 0.736663, mean_q: 5.206864, mean_eps: 0.100000\n",
      "  63311/175000: episode: 1774, duration: 0.905s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 108.100 [35.000, 220.000], mean observation: 0.280 [0.000, 80.000], loss: 27.927302, mean_absolute_error: 0.822677, mean_q: 6.148434, mean_eps: 0.100000\n",
      "  63356/175000: episode: 1775, duration: 1.122s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 73.156 [15.000, 208.000], mean observation: 0.156 [0.000, 90.000], loss: 40.345204, mean_absolute_error: 0.759599, mean_q: 5.400432, mean_eps: 0.100000\n",
      "  63384/175000: episode: 1776, duration: 0.709s, episode steps: 28, steps per second: 40, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 78.714 [12.000, 209.000], mean observation: 0.153 [0.000, 56.000], loss: 34.679404, mean_absolute_error: 0.869005, mean_q: 6.454244, mean_eps: 0.100000\n",
      "  63406/175000: episode: 1777, duration: 0.493s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 105.545 [15.000, 211.000], mean observation: 0.109 [0.000, 44.000], loss: 1.314702, mean_absolute_error: 0.566403, mean_q: 5.392848, mean_eps: 0.100000\n",
      "  63440/175000: episode: 1778, duration: 0.787s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.500 [15.000, 190.000], mean observation: 0.256 [0.000, 68.000], loss: 0.183029, mean_absolute_error: 0.442544, mean_q: 4.585345, mean_eps: 0.100000\n",
      "  63481/175000: episode: 1779, duration: 0.954s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.829 [15.000, 190.000], mean observation: 0.268 [0.000, 82.000], loss: 13.900172, mean_absolute_error: 0.630817, mean_q: 5.386855, mean_eps: 0.100000\n",
      "  63530/175000: episode: 1780, duration: 1.130s, episode steps: 49, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.510 [5.000, 186.000], mean observation: 0.235 [0.000, 98.000], loss: 11.987877, mean_absolute_error: 0.735340, mean_q: 6.123842, mean_eps: 0.100000\n",
      "  63572/175000: episode: 1781, duration: 0.970s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.381 [28.000, 203.000], mean observation: 0.210 [0.000, 84.000], loss: 12.907634, mean_absolute_error: 0.689595, mean_q: 5.849271, mean_eps: 0.100000\n",
      "  63600/175000: episode: 1782, duration: 0.620s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 108.643 [86.000, 191.000], mean observation: 0.125 [0.000, 56.000], loss: 9.706859, mean_absolute_error: 0.649981, mean_q: 5.663269, mean_eps: 0.100000\n",
      "  63645/175000: episode: 1783, duration: 1.004s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.711 [77.000, 219.000], mean observation: 0.253 [0.000, 90.000], loss: 1.192874, mean_absolute_error: 0.555698, mean_q: 5.272187, mean_eps: 0.100000\n",
      "  63688/175000: episode: 1784, duration: 0.899s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 105.953 [16.000, 218.000], mean observation: 0.261 [0.000, 86.000], loss: 0.184159, mean_absolute_error: 0.388973, mean_q: 4.198218, mean_eps: 0.100000\n",
      "  63723/175000: episode: 1785, duration: 0.761s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.486 [15.000, 213.000], mean observation: 0.212 [0.000, 70.000], loss: 22.362254, mean_absolute_error: 0.768730, mean_q: 6.044090, mean_eps: 0.100000\n",
      "  63754/175000: episode: 1786, duration: 0.742s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.323 [15.000, 210.000], mean observation: 0.221 [0.000, 62.000], loss: 0.175917, mean_absolute_error: 0.551053, mean_q: 5.266751, mean_eps: 0.100000\n",
      "  63788/175000: episode: 1787, duration: 0.690s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.912 [15.000, 182.000], mean observation: 0.266 [0.000, 68.000], loss: 2.075606, mean_absolute_error: 0.762781, mean_q: 6.544966, mean_eps: 0.100000\n",
      "  63810/175000: episode: 1788, duration: 0.581s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 175.136 [43.000, 182.000], mean observation: 0.086 [0.000, 44.000], loss: 16.594903, mean_absolute_error: 0.675966, mean_q: 5.586617, mean_eps: 0.100000\n",
      "  63857/175000: episode: 1789, duration: 1.053s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 183.426 [28.000, 218.000], mean observation: 0.205 [0.000, 94.000], loss: 11.430303, mean_absolute_error: 0.661297, mean_q: 5.646856, mean_eps: 0.100000\n",
      "  63897/175000: episode: 1790, duration: 0.939s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 170.150 [70.000, 218.000], mean observation: 0.153 [0.000, 80.000], loss: 0.273677, mean_absolute_error: 0.607301, mean_q: 5.564585, mean_eps: 0.100000\n",
      "  63942/175000: episode: 1791, duration: 0.953s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.022 [17.000, 218.000], mean observation: 0.314 [0.000, 90.000], loss: 4.409014, mean_absolute_error: 0.613074, mean_q: 5.602030, mean_eps: 0.100000\n",
      "  63979/175000: episode: 1792, duration: 0.736s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 120.568 [70.000, 218.000], mean observation: 0.138 [0.000, 74.000], loss: 15.471942, mean_absolute_error: 0.728351, mean_q: 6.014742, mean_eps: 0.100000\n",
      "  64011/175000: episode: 1793, duration: 0.718s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.688 [76.000, 221.000], mean observation: 0.083 [0.000, 64.000], loss: 11.231212, mean_absolute_error: 0.831945, mean_q: 6.739021, mean_eps: 0.100000\n",
      "  64057/175000: episode: 1794, duration: 1.027s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 183.000 [39.000, 221.000], mean observation: 0.241 [0.000, 92.000], loss: 43.687640, mean_absolute_error: 0.872710, mean_q: 6.090187, mean_eps: 0.100000\n",
      "  64087/175000: episode: 1795, duration: 0.646s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 193.000 [1.000, 221.000], mean observation: 0.132 [0.000, 60.000], loss: 31.387544, mean_absolute_error: 1.260505, mean_q: 9.033992, mean_eps: 0.100000\n",
      "  64131/175000: episode: 1796, duration: 1.012s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.659 [15.000, 221.000], mean observation: 0.211 [0.000, 88.000], loss: 2.946675, mean_absolute_error: 0.739620, mean_q: 6.506133, mean_eps: 0.100000\n",
      "  64160/175000: episode: 1797, duration: 0.706s, episode steps: 29, steps per second: 41, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 83.897 [3.000, 221.000], mean observation: 0.220 [0.000, 58.000], loss: 43.802495, mean_absolute_error: 0.893402, mean_q: 6.347217, mean_eps: 0.100000\n",
      "  64181/175000: episode: 1798, duration: 0.493s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 136.905 [12.000, 212.000], mean observation: 0.072 [0.000, 42.000], loss: 13.105470, mean_absolute_error: 0.782382, mean_q: 6.669906, mean_eps: 0.100000\n",
      "  64207/175000: episode: 1799, duration: 0.435s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 158.038 [87.000, 204.000], mean observation: 0.096 [0.000, 52.000], loss: 24.178375, mean_absolute_error: 0.895324, mean_q: 7.274692, mean_eps: 0.100000\n",
      "  64256/175000: episode: 1800, duration: 0.955s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.673 [3.000, 186.000], mean observation: 0.389 [0.000, 98.000], loss: 11.707150, mean_absolute_error: 0.658797, mean_q: 6.102135, mean_eps: 0.100000\n",
      "  64298/175000: episode: 1801, duration: 0.809s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.762 [3.000, 186.000], mean observation: 0.165 [0.000, 84.000], loss: 1.923204, mean_absolute_error: 0.522727, mean_q: 5.627380, mean_eps: 0.100000\n",
      "  64330/175000: episode: 1802, duration: 0.635s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.750 [1.000, 221.000], mean observation: 0.377 [0.000, 64.000], loss: 9.044634, mean_absolute_error: 0.598423, mean_q: 5.776255, mean_eps: 0.100000\n",
      "  64364/175000: episode: 1803, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.941 [27.000, 221.000], mean observation: 0.117 [0.000, 68.000], loss: 0.205932, mean_absolute_error: 0.548456, mean_q: 5.672620, mean_eps: 0.100000\n",
      "  64414/175000: episode: 1804, duration: 1.026s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 50.900 [27.000, 137.000], mean observation: 0.372 [0.000, 100.000], loss: 9.361634, mean_absolute_error: 0.721820, mean_q: 6.747935, mean_eps: 0.100000\n",
      "  64436/175000: episode: 1805, duration: 0.425s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 33.864 [27.000, 95.000], mean observation: 0.090 [0.000, 44.000], loss: 0.114913, mean_absolute_error: 0.403427, mean_q: 4.861033, mean_eps: 0.100000\n",
      "  64452/175000: episode: 1806, duration: 0.377s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 47.438 [27.000, 116.000], mean observation: 0.063 [0.000, 32.000], loss: 0.226946, mean_absolute_error: 0.390941, mean_q: 4.698382, mean_eps: 0.100000\n",
      "  64508/175000: episode: 1807, duration: 1.142s, episode steps: 56, steps per second: 49, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 57.893 [27.000, 218.000], mean observation: 0.188 [0.000, 112.000], loss: 4.131946, mean_absolute_error: 0.591153, mean_q: 5.943386, mean_eps: 0.100000\n",
      "  64544/175000: episode: 1808, duration: 0.761s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 49.250 [40.000, 180.000], mean observation: 0.120 [0.000, 72.000], loss: 0.236020, mean_absolute_error: 0.418410, mean_q: 5.015518, mean_eps: 0.100000\n",
      "  64590/175000: episode: 1809, duration: 0.911s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 59.326 [0.000, 162.000], mean observation: 0.332 [0.000, 92.000], loss: 3.039708, mean_absolute_error: 0.507219, mean_q: 5.392679, mean_eps: 0.100000\n",
      "  64643/175000: episode: 1810, duration: 1.164s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 109.189 [40.000, 176.000], mean observation: 0.379 [0.000, 106.000], loss: 0.499131, mean_absolute_error: 0.485500, mean_q: 5.266628, mean_eps: 0.100000\n",
      "  64674/175000: episode: 1811, duration: 0.749s, episode steps: 31, steps per second: 41, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 81.000 [10.000, 176.000], mean observation: 0.223 [0.000, 62.000], loss: 0.155399, mean_absolute_error: 0.402608, mean_q: 4.742933, mean_eps: 0.100000\n",
      "  64707/175000: episode: 1812, duration: 0.667s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 68.212 [27.000, 181.000], mean observation: 0.139 [0.000, 66.000], loss: 6.047505, mean_absolute_error: 0.671356, mean_q: 6.210323, mean_eps: 0.100000\n",
      "  64756/175000: episode: 1813, duration: 1.255s, episode steps: 49, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 144.980 [40.000, 207.000], mean observation: 0.274 [0.000, 98.000], loss: 11.626582, mean_absolute_error: 0.735953, mean_q: 6.498959, mean_eps: 0.100000\n",
      "  64794/175000: episode: 1814, duration: 0.919s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.474 [27.000, 172.000], mean observation: 0.320 [0.000, 76.000], loss: 11.944664, mean_absolute_error: 0.672939, mean_q: 6.224966, mean_eps: 0.100000\n",
      "  64828/175000: episode: 1815, duration: 0.807s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 143.853 [40.000, 207.000], mean observation: 0.215 [0.000, 68.000], loss: 13.628434, mean_absolute_error: 0.626585, mean_q: 5.809583, mean_eps: 0.100000\n",
      "  64847/175000: episode: 1816, duration: 0.443s, episode steps: 19, steps per second: 43, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 156.895 [99.000, 172.000], mean observation: 0.069 [0.000, 38.000], loss: 27.359208, mean_absolute_error: 0.707546, mean_q: 5.807482, mean_eps: 0.100000\n",
      "  64863/175000: episode: 1817, duration: 0.421s, episode steps: 16, steps per second: 38, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 129.688 [27.000, 162.000], mean observation: 0.061 [0.000, 32.000], loss: 0.068187, mean_absolute_error: 0.414672, mean_q: 4.794432, mean_eps: 0.100000\n",
      "  64890/175000: episode: 1818, duration: 0.682s, episode steps: 27, steps per second: 40, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.185 [27.000, 194.000], mean observation: 0.167 [0.000, 54.000], loss: 54.748811, mean_absolute_error: 1.069510, mean_q: 7.531328, mean_eps: 0.100000\n",
      "  64907/175000: episode: 1819, duration: 0.375s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 134.000 [27.000, 162.000], mean observation: 0.072 [0.000, 34.000], loss: 58.173192, mean_absolute_error: 0.934740, mean_q: 6.686133, mean_eps: 0.100000\n",
      "  64940/175000: episode: 1820, duration: 0.787s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.333 [13.000, 172.000], mean observation: 0.236 [0.000, 66.000], loss: 0.300013, mean_absolute_error: 0.406583, mean_q: 4.829399, mean_eps: 0.100000\n",
      "  64968/175000: episode: 1821, duration: 0.713s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 115.893 [27.000, 218.000], mean observation: 0.246 [0.000, 56.000], loss: 0.089917, mean_absolute_error: 0.400520, mean_q: 4.906330, mean_eps: 0.100000\n",
      "  64992/175000: episode: 1822, duration: 0.569s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 126.958 [22.000, 216.000], mean observation: 0.146 [0.000, 48.000], loss: 9.225678, mean_absolute_error: 0.645482, mean_q: 6.242297, mean_eps: 0.100000\n",
      "  65022/175000: episode: 1823, duration: 0.648s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 133.133 [9.000, 162.000], mean observation: 0.186 [0.000, 60.000], loss: 11.903264, mean_absolute_error: 0.592004, mean_q: 5.604559, mean_eps: 0.100000\n",
      "  65071/175000: episode: 1824, duration: 0.916s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 106.469 [39.000, 200.000], mean observation: 0.423 [0.000, 98.000], loss: 0.179484, mean_absolute_error: 0.407802, mean_q: 4.755869, mean_eps: 0.100000\n",
      "  65115/175000: episode: 1825, duration: 0.900s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.386 [41.000, 222.000], mean observation: 0.363 [0.000, 88.000], loss: 42.084188, mean_absolute_error: 0.881463, mean_q: 6.707853, mean_eps: 0.100000\n",
      "  65155/175000: episode: 1826, duration: 0.877s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 106.825 [3.000, 209.000], mean observation: 0.303 [0.000, 80.000], loss: 0.305657, mean_absolute_error: 0.530122, mean_q: 5.631816, mean_eps: 0.100000\n",
      "  65171/175000: episode: 1827, duration: 0.340s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 115.312 [28.000, 162.000], mean observation: 0.070 [0.000, 32.000], loss: 0.171139, mean_absolute_error: 0.441197, mean_q: 5.006794, mean_eps: 0.100000\n",
      "  65210/175000: episode: 1828, duration: 0.798s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.949 [8.000, 144.000], mean observation: 0.240 [0.000, 78.000], loss: 25.745268, mean_absolute_error: 0.828178, mean_q: 6.881493, mean_eps: 0.100000\n",
      "  65253/175000: episode: 1829, duration: 0.836s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 140.651 [0.000, 216.000], mean observation: 0.240 [0.000, 86.000], loss: 16.875629, mean_absolute_error: 0.755807, mean_q: 6.732322, mean_eps: 0.100000\n",
      "  65296/175000: episode: 1830, duration: 0.815s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 157.279 [28.000, 207.000], mean observation: 0.285 [0.000, 86.000], loss: 0.136982, mean_absolute_error: 0.426032, mean_q: 4.965025, mean_eps: 0.100000\n",
      "  65336/175000: episode: 1831, duration: 0.809s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 138.250 [1.000, 207.000], mean observation: 0.325 [0.000, 80.000], loss: 18.587289, mean_absolute_error: 0.720498, mean_q: 6.457002, mean_eps: 0.100000\n",
      "  65371/175000: episode: 1832, duration: 0.702s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.514 [94.000, 207.000], mean observation: 0.148 [0.000, 70.000], loss: 24.821724, mean_absolute_error: 0.688831, mean_q: 6.068582, mean_eps: 0.100000\n",
      "  65395/175000: episode: 1833, duration: 0.460s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 87.833 [68.000, 173.000], mean observation: 0.132 [0.000, 48.000], loss: 28.022229, mean_absolute_error: 0.861470, mean_q: 7.171325, mean_eps: 0.100000\n",
      "  65410/175000: episode: 1834, duration: 0.330s, episode steps: 15, steps per second: 45, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 112.133 [107.000, 118.000], mean observation: 0.043 [0.000, 30.000], loss: 65.090411, mean_absolute_error: 1.196428, mean_q: 8.177466, mean_eps: 0.100000\n",
      "  65435/175000: episode: 1835, duration: 0.490s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 121.400 [6.000, 167.000], mean observation: 0.092 [0.000, 50.000], loss: 40.123982, mean_absolute_error: 0.767553, mean_q: 6.243529, mean_eps: 0.100000\n",
      "  65469/175000: episode: 1836, duration: 0.662s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.324 [70.000, 172.000], mean observation: 0.180 [0.000, 68.000], loss: 8.471733, mean_absolute_error: 0.623867, mean_q: 6.244227, mean_eps: 0.100000\n",
      "  65526/175000: episode: 1837, duration: 1.082s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 141.158 [14.000, 202.000], mean observation: 0.386 [0.000, 114.000], loss: 12.187156, mean_absolute_error: 0.596447, mean_q: 5.875235, mean_eps: 0.100000\n",
      "  65564/175000: episode: 1838, duration: 0.730s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 147.526 [52.000, 183.000], mean observation: 0.340 [0.000, 76.000], loss: 48.674703, mean_absolute_error: 1.030795, mean_q: 7.815511, mean_eps: 0.100000\n",
      "  65622/175000: episode: 1839, duration: 1.216s, episode steps: 58, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 61.638 [9.000, 172.000], mean observation: 0.272 [0.000, 116.000], loss: 12.269754, mean_absolute_error: 0.606628, mean_q: 6.092140, mean_eps: 0.100000\n",
      "  65665/175000: episode: 1840, duration: 1.041s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.116 [9.000, 218.000], mean observation: 0.291 [0.000, 86.000], loss: 41.697173, mean_absolute_error: 1.058605, mean_q: 8.032584, mean_eps: 0.100000\n",
      "  65703/175000: episode: 1841, duration: 0.806s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.632 [52.000, 174.000], mean observation: 0.228 [0.000, 76.000], loss: 23.327541, mean_absolute_error: 0.719604, mean_q: 6.399108, mean_eps: 0.100000\n",
      "  65737/175000: episode: 1842, duration: 0.894s, episode steps: 34, steps per second: 38, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.559 [52.000, 198.000], mean observation: 0.221 [0.000, 68.000], loss: 1.511504, mean_absolute_error: 0.463966, mean_q: 5.358101, mean_eps: 0.100000\n",
      "  65768/175000: episode: 1843, duration: 0.804s, episode steps: 31, steps per second: 39, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.387 [51.000, 174.000], mean observation: 0.195 [0.000, 62.000], loss: 14.514331, mean_absolute_error: 0.627918, mean_q: 6.315702, mean_eps: 0.100000\n",
      "  65808/175000: episode: 1844, duration: 1.003s, episode steps: 40, steps per second: 40, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 136.075 [25.000, 207.000], mean observation: 0.369 [0.000, 80.000], loss: 19.483722, mean_absolute_error: 0.639184, mean_q: 6.147862, mean_eps: 0.100000\n",
      "  65869/175000: episode: 1845, duration: 1.582s, episode steps: 61, steps per second: 39, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 139.180 [13.000, 220.000], mean observation: 0.561 [0.000, 122.000], loss: 0.554991, mean_absolute_error: 0.538107, mean_q: 6.069471, mean_eps: 0.100000\n",
      "  65889/175000: episode: 1846, duration: 0.527s, episode steps: 20, steps per second: 38, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.750 [28.000, 162.000], mean observation: 0.053 [0.000, 40.000], loss: 56.237822, mean_absolute_error: 0.865908, mean_q: 6.634864, mean_eps: 0.100000\n",
      "  65907/175000: episode: 1847, duration: 0.416s, episode steps: 18, steps per second: 43, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.044 [0.000, 36.000], loss: 0.188293, mean_absolute_error: 0.465053, mean_q: 5.580339, mean_eps: 0.100000\n",
      "  65957/175000: episode: 1848, duration: 1.211s, episode steps: 50, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 80.800 [28.000, 177.000], mean observation: 0.268 [0.000, 100.000], loss: 9.387882, mean_absolute_error: 0.525484, mean_q: 5.719881, mean_eps: 0.100000\n",
      "  66005/175000: episode: 1849, duration: 1.101s, episode steps: 48, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.083 [10.000, 162.000], mean observation: 0.367 [0.000, 96.000], loss: 10.025339, mean_absolute_error: 0.560488, mean_q: 5.883733, mean_eps: 0.100000\n",
      "  66038/175000: episode: 1850, duration: 0.652s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 48.667 [28.000, 196.000], mean observation: 0.136 [0.000, 66.000], loss: 26.659511, mean_absolute_error: 0.681452, mean_q: 6.337864, mean_eps: 0.100000\n",
      "  66097/175000: episode: 1851, duration: 1.245s, episode steps: 59, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.424 [9.000, 218.000], mean observation: 0.426 [0.000, 118.000], loss: 2.498662, mean_absolute_error: 0.492606, mean_q: 5.701628, mean_eps: 0.100000\n",
      "  66125/175000: episode: 1852, duration: 0.685s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.066 [0.000, 56.000], loss: 51.867539, mean_absolute_error: 0.977149, mean_q: 7.672759, mean_eps: 0.100000\n",
      "  66150/175000: episode: 1853, duration: 0.561s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 81.680 [28.000, 202.000], mean observation: 0.121 [0.000, 50.000], loss: 33.729602, mean_absolute_error: 0.726530, mean_q: 6.463658, mean_eps: 0.100000\n",
      "  66190/175000: episode: 1854, duration: 0.854s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 30.275 [28.000, 119.000], mean observation: 0.116 [0.000, 80.000], loss: 41.672383, mean_absolute_error: 0.839719, mean_q: 7.089033, mean_eps: 0.100000\n",
      "  66217/175000: episode: 1855, duration: 0.634s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.889 [28.000, 128.000], mean observation: 0.082 [0.000, 54.000], loss: 4.924550, mean_absolute_error: 0.590088, mean_q: 6.395655, mean_eps: 0.100000\n",
      "  66252/175000: episode: 1856, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.286 [28.000, 203.000], mean observation: 0.113 [0.000, 70.000], loss: 19.985986, mean_absolute_error: 0.701838, mean_q: 6.966030, mean_eps: 0.100000\n",
      "  66282/175000: episode: 1857, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 85.200 [28.000, 159.000], mean observation: 0.138 [0.000, 60.000], loss: 0.214326, mean_absolute_error: 0.430781, mean_q: 5.460288, mean_eps: 0.100000\n",
      "  66324/175000: episode: 1858, duration: 0.878s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.595 [0.000, 207.000], mean observation: 0.375 [0.000, 84.000], loss: 14.556595, mean_absolute_error: 0.796270, mean_q: 7.546836, mean_eps: 0.100000\n",
      "  66352/175000: episode: 1859, duration: 0.554s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 90.500 [18.000, 159.000], mean observation: 0.127 [0.000, 56.000], loss: 10.439262, mean_absolute_error: 0.813022, mean_q: 7.759048, mean_eps: 0.100000\n",
      "  66402/175000: episode: 1860, duration: 1.062s, episode steps: 50, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 59.780 [26.000, 159.000], mean observation: 0.255 [0.000, 100.000], loss: 34.308664, mean_absolute_error: 0.913422, mean_q: 7.790582, mean_eps: 0.100000\n",
      "  66424/175000: episode: 1861, duration: 0.491s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 109.045 [28.000, 215.000], mean observation: 0.099 [0.000, 44.000], loss: 44.099279, mean_absolute_error: 0.868418, mean_q: 7.168876, mean_eps: 0.100000\n",
      "  66468/175000: episode: 1862, duration: 0.925s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.273 [15.000, 191.000], mean observation: 0.413 [0.000, 88.000], loss: 9.302595, mean_absolute_error: 0.650678, mean_q: 6.847604, mean_eps: 0.100000\n",
      "  66502/175000: episode: 1863, duration: 0.668s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 77.588 [28.000, 208.000], mean observation: 0.124 [0.000, 68.000], loss: 0.238790, mean_absolute_error: 0.488507, mean_q: 6.014602, mean_eps: 0.100000\n",
      "  66538/175000: episode: 1864, duration: 0.711s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 40.083 [8.000, 118.000], mean observation: 0.134 [0.000, 72.000], loss: 27.896081, mean_absolute_error: 0.937297, mean_q: 8.315169, mean_eps: 0.100000\n",
      "  66586/175000: episode: 1865, duration: 0.904s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.104 [28.000, 205.000], mean observation: 0.585 [0.000, 96.000], loss: 19.573184, mean_absolute_error: 0.871768, mean_q: 8.011470, mean_eps: 0.100000\n",
      "  66622/175000: episode: 1866, duration: 0.679s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.111 [28.000, 200.000], mean observation: 0.223 [0.000, 72.000], loss: 3.893359, mean_absolute_error: 0.616415, mean_q: 6.811553, mean_eps: 0.100000\n",
      "  66659/175000: episode: 1867, duration: 0.682s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 88.108 [4.000, 158.000], mean observation: 0.239 [0.000, 74.000], loss: 3.364942, mean_absolute_error: 0.576851, mean_q: 6.538236, mean_eps: 0.100000\n",
      "  66688/175000: episode: 1868, duration: 0.566s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 97.621 [4.000, 178.000], mean observation: 0.254 [0.000, 58.000], loss: 17.739083, mean_absolute_error: 0.690476, mean_q: 7.015155, mean_eps: 0.100000\n",
      "  66729/175000: episode: 1869, duration: 0.889s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 128.829 [1.000, 221.000], mean observation: 0.493 [0.000, 82.000], loss: 4.036756, mean_absolute_error: 0.576850, mean_q: 6.597752, mean_eps: 0.100000\n",
      "  66759/175000: episode: 1870, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 215.367 [52.000, 221.000], mean observation: 0.083 [0.000, 60.000], loss: 2.676035, mean_absolute_error: 0.733594, mean_q: 7.521869, mean_eps: 0.100000\n",
      "  66811/175000: episode: 1871, duration: 1.090s, episode steps: 52, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 166.923 [49.000, 221.000], mean observation: 0.240 [0.000, 104.000], loss: 18.288676, mean_absolute_error: 0.624051, mean_q: 6.334397, mean_eps: 0.100000\n",
      "  66854/175000: episode: 1872, duration: 0.826s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 152.605 [28.000, 221.000], mean observation: 0.339 [0.000, 86.000], loss: 31.949816, mean_absolute_error: 0.793620, mean_q: 7.333094, mean_eps: 0.100000\n",
      "  66897/175000: episode: 1873, duration: 0.766s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 78.628 [28.000, 221.000], mean observation: 0.205 [0.000, 86.000], loss: 7.533470, mean_absolute_error: 0.580334, mean_q: 6.434816, mean_eps: 0.100000\n",
      "  66916/175000: episode: 1874, duration: 0.352s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 39.316 [13.000, 223.000], mean observation: 0.100 [0.000, 38.000], loss: 0.130172, mean_absolute_error: 0.419223, mean_q: 5.400960, mean_eps: 0.100000\n",
      "  66935/175000: episode: 1875, duration: 0.358s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 31.105 [28.000, 87.000], mean observation: 0.059 [0.000, 38.000], loss: 5.027721, mean_absolute_error: 0.666898, mean_q: 7.156794, mean_eps: 0.100000\n",
      "  66973/175000: episode: 1876, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.763 [28.000, 197.000], mean observation: 0.224 [0.000, 76.000], loss: 1.790554, mean_absolute_error: 0.549523, mean_q: 6.391402, mean_eps: 0.100000\n",
      "  66988/175000: episode: 1877, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 45.667 [28.000, 177.000], mean observation: 0.049 [0.000, 30.000], loss: 86.826890, mean_absolute_error: 1.552031, mean_q: 10.916541, mean_eps: 0.100000\n",
      "  67015/175000: episode: 1878, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.064 [0.000, 54.000], loss: 18.049036, mean_absolute_error: 0.694017, mean_q: 7.059081, mean_eps: 0.100000\n",
      "  67050/175000: episode: 1879, duration: 0.729s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 35.343 [6.000, 199.000], mean observation: 0.193 [0.000, 70.000], loss: 4.456017, mean_absolute_error: 0.568580, mean_q: 6.454556, mean_eps: 0.100000\n",
      "  67078/175000: episode: 1880, duration: 0.646s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 31.286 [28.000, 101.000], mean observation: 0.111 [0.000, 56.000], loss: 17.648944, mean_absolute_error: 0.760986, mean_q: 7.521955, mean_eps: 0.100000\n",
      "  67107/175000: episode: 1881, duration: 0.511s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 29.552 [28.000, 57.000], mean observation: 0.114 [0.000, 58.000], loss: 74.134250, mean_absolute_error: 1.302634, mean_q: 9.425617, mean_eps: 0.100000\n",
      "  67134/175000: episode: 1882, duration: 0.512s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 30.741 [28.000, 102.000], mean observation: 0.065 [0.000, 54.000], loss: 0.270906, mean_absolute_error: 0.480243, mean_q: 5.958020, mean_eps: 0.100000\n",
      "  67171/175000: episode: 1883, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 36.135 [28.000, 184.000], mean observation: 0.125 [0.000, 74.000], loss: 1.298039, mean_absolute_error: 0.604359, mean_q: 6.928610, mean_eps: 0.100000\n",
      "  67219/175000: episode: 1884, duration: 0.856s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.146 [3.000, 181.000], mean observation: 0.272 [0.000, 96.000], loss: 0.356468, mean_absolute_error: 0.490615, mean_q: 6.234986, mean_eps: 0.100000\n",
      "  67245/175000: episode: 1885, duration: 0.505s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 52.538 [28.000, 187.000], mean observation: 0.110 [0.000, 52.000], loss: 13.518095, mean_absolute_error: 0.689468, mean_q: 6.948525, mean_eps: 0.100000\n",
      "  67282/175000: episode: 1886, duration: 0.670s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 50.351 [39.000, 168.000], mean observation: 0.126 [0.000, 74.000], loss: 0.129865, mean_absolute_error: 0.501697, mean_q: 6.014815, mean_eps: 0.100000\n",
      "  67317/175000: episode: 1887, duration: 0.685s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.771 [28.000, 221.000], mean observation: 0.199 [0.000, 70.000], loss: 15.083063, mean_absolute_error: 0.641696, mean_q: 6.722633, mean_eps: 0.100000\n",
      "  67369/175000: episode: 1888, duration: 0.980s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.538 [28.000, 221.000], mean observation: 0.718 [0.000, 104.000], loss: 37.567189, mean_absolute_error: 0.939722, mean_q: 8.037430, mean_eps: 0.100000\n",
      "  67408/175000: episode: 1889, duration: 0.776s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 165.256 [59.000, 221.000], mean observation: 0.241 [0.000, 78.000], loss: 19.053333, mean_absolute_error: 0.750433, mean_q: 7.269653, mean_eps: 0.100000\n",
      "  67434/175000: episode: 1890, duration: 0.507s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 173.269 [59.000, 221.000], mean observation: 0.123 [0.000, 52.000], loss: 0.149313, mean_absolute_error: 0.445798, mean_q: 5.886907, mean_eps: 0.100000\n",
      "  67469/175000: episode: 1891, duration: 0.697s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 165.771 [59.000, 221.000], mean observation: 0.345 [0.000, 70.000], loss: 13.510916, mean_absolute_error: 0.714852, mean_q: 7.178523, mean_eps: 0.100000\n",
      "  67510/175000: episode: 1892, duration: 0.740s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.829 [28.000, 221.000], mean observation: 0.232 [0.000, 82.000], loss: 4.679031, mean_absolute_error: 0.566078, mean_q: 6.608760, mean_eps: 0.100000\n",
      "  67539/175000: episode: 1893, duration: 0.532s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.897 [48.000, 221.000], mean observation: 0.136 [0.000, 58.000], loss: 0.251169, mean_absolute_error: 0.632142, mean_q: 7.326321, mean_eps: 0.100000\n",
      "  67576/175000: episode: 1894, duration: 0.711s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 138.189 [59.000, 191.000], mean observation: 0.188 [0.000, 74.000], loss: 0.555716, mean_absolute_error: 0.558641, mean_q: 6.713364, mean_eps: 0.100000\n",
      "  67609/175000: episode: 1895, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 34.758 [3.000, 184.000], mean observation: 0.143 [0.000, 66.000], loss: 24.138039, mean_absolute_error: 0.664705, mean_q: 6.813918, mean_eps: 0.100000\n",
      "  67636/175000: episode: 1896, duration: 0.502s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 49.370 [19.000, 203.000], mean observation: 0.232 [0.000, 54.000], loss: 8.662758, mean_absolute_error: 0.992083, mean_q: 9.421185, mean_eps: 0.100000\n",
      "  67655/175000: episode: 1897, duration: 0.387s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.046 [0.000, 38.000], loss: 10.049200, mean_absolute_error: 0.681008, mean_q: 6.992331, mean_eps: 0.100000\n",
      "  67669/175000: episode: 1898, duration: 0.294s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 27.286 [18.000, 28.000], mean observation: 0.036 [0.000, 28.000], loss: 28.851449, mean_absolute_error: 1.099934, mean_q: 9.354906, mean_eps: 0.100000\n",
      "  67706/175000: episode: 1899, duration: 0.890s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 33.135 [28.000, 218.000], mean observation: 0.087 [0.000, 74.000], loss: 33.271668, mean_absolute_error: 0.822412, mean_q: 7.428931, mean_eps: 0.100000\n",
      "  67750/175000: episode: 1900, duration: 0.836s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 40.818 [6.000, 213.000], mean observation: 0.208 [0.000, 88.000], loss: 22.003748, mean_absolute_error: 0.713330, mean_q: 7.007607, mean_eps: 0.100000\n",
      "  67788/175000: episode: 1901, duration: 0.916s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 45.421 [28.000, 205.000], mean observation: 0.246 [0.000, 76.000], loss: 17.471564, mean_absolute_error: 0.901260, mean_q: 8.376204, mean_eps: 0.100000\n",
      "  67818/175000: episode: 1902, duration: 0.608s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 27.833 [23.000, 28.000], mean observation: 0.075 [0.000, 60.000], loss: 24.603057, mean_absolute_error: 1.009359, mean_q: 8.971950, mean_eps: 0.100000\n",
      "  67847/175000: episode: 1903, duration: 0.513s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 41.793 [4.000, 212.000], mean observation: 0.164 [0.000, 58.000], loss: 2.565078, mean_absolute_error: 0.697737, mean_q: 7.632745, mean_eps: 0.100000\n",
      "  67877/175000: episode: 1904, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 73.733 [28.000, 198.000], mean observation: 0.156 [0.000, 60.000], loss: 62.095998, mean_absolute_error: 1.078769, mean_q: 8.494900, mean_eps: 0.100000\n",
      "  67908/175000: episode: 1905, duration: 0.675s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 39.677 [28.000, 179.000], mean observation: 0.177 [0.000, 62.000], loss: 0.176998, mean_absolute_error: 0.618530, mean_q: 7.067373, mean_eps: 0.100000\n",
      "  67942/175000: episode: 1906, duration: 0.656s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 29.618 [24.000, 87.000], mean observation: 0.098 [0.000, 68.000], loss: 24.715383, mean_absolute_error: 0.793784, mean_q: 7.496963, mean_eps: 0.100000\n",
      "  67993/175000: episode: 1907, duration: 1.120s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.627 [28.000, 191.000], mean observation: 0.206 [0.000, 102.000], loss: 7.036411, mean_absolute_error: 0.737053, mean_q: 7.653635, mean_eps: 0.100000\n",
      "  68026/175000: episode: 1908, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 33.909 [28.000, 137.000], mean observation: 0.102 [0.000, 66.000], loss: 43.450889, mean_absolute_error: 1.039777, mean_q: 8.588052, mean_eps: 0.100000\n",
      "  68051/175000: episode: 1909, duration: 0.470s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 34.320 [28.000, 119.000], mean observation: 0.083 [0.000, 50.000], loss: 9.254199, mean_absolute_error: 0.724712, mean_q: 7.717972, mean_eps: 0.100000\n",
      "  68097/175000: episode: 1910, duration: 1.020s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 37.043 [14.000, 185.000], mean observation: 0.248 [0.000, 92.000], loss: 2.801768, mean_absolute_error: 0.512120, mean_q: 6.316751, mean_eps: 0.100000\n",
      "  68147/175000: episode: 1911, duration: 0.875s, episode steps: 50, steps per second: 57, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 66.400 [10.000, 162.000], mean observation: 0.283 [0.000, 100.000], loss: 20.900746, mean_absolute_error: 0.839268, mean_q: 7.989657, mean_eps: 0.100000\n",
      "  68179/175000: episode: 1912, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.075 [0.000, 64.000], loss: 0.552173, mean_absolute_error: 0.606236, mean_q: 6.993151, mean_eps: 0.100000\n",
      "  68201/175000: episode: 1913, duration: 0.449s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 49.955 [28.000, 156.000], mean observation: 0.068 [0.000, 44.000], loss: 0.311253, mean_absolute_error: 0.542627, mean_q: 6.757397, mean_eps: 0.100000\n",
      "  68242/175000: episode: 1914, duration: 0.920s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 44.293 [28.000, 219.000], mean observation: 0.173 [0.000, 82.000], loss: 7.072388, mean_absolute_error: 0.612668, mean_q: 6.991983, mean_eps: 0.100000\n",
      "  68273/175000: episode: 1915, duration: 0.558s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.806 [28.000, 221.000], mean observation: 0.339 [0.000, 62.000], loss: 23.600712, mean_absolute_error: 0.970241, mean_q: 8.862361, mean_eps: 0.100000\n",
      "  68314/175000: episode: 1916, duration: 0.933s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.341 [16.000, 221.000], mean observation: 0.293 [0.000, 82.000], loss: 8.589952, mean_absolute_error: 0.615400, mean_q: 6.616575, mean_eps: 0.100000\n",
      "  68358/175000: episode: 1917, duration: 0.867s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.682 [16.000, 205.000], mean observation: 0.233 [0.000, 88.000], loss: 0.302839, mean_absolute_error: 0.482656, mean_q: 5.882338, mean_eps: 0.100000\n",
      "  68396/175000: episode: 1918, duration: 0.732s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 38.868 [28.000, 150.000], mean observation: 0.163 [0.000, 76.000], loss: 42.921102, mean_absolute_error: 0.792126, mean_q: 6.854482, mean_eps: 0.100000\n",
      "  68444/175000: episode: 1919, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 100.375 [3.000, 169.000], mean observation: 0.419 [0.000, 96.000], loss: 1.797355, mean_absolute_error: 0.513231, mean_q: 6.260532, mean_eps: 0.100000\n",
      "  68486/175000: episode: 1920, duration: 0.794s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.071 [4.000, 189.000], mean observation: 0.318 [0.000, 84.000], loss: 3.327609, mean_absolute_error: 0.573407, mean_q: 6.568008, mean_eps: 0.100000\n",
      "  68500/175000: episode: 1921, duration: 0.275s, episode steps: 14, steps per second: 51, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 139.500 [49.000, 207.000], mean observation: 0.058 [0.000, 28.000], loss: 43.852686, mean_absolute_error: 0.955673, mean_q: 8.010092, mean_eps: 0.100000\n",
      "  68521/175000: episode: 1922, duration: 0.486s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 142.286 [49.000, 162.000], mean observation: 0.115 [0.000, 42.000], loss: 13.718457, mean_absolute_error: 0.657702, mean_q: 6.843891, mean_eps: 0.100000\n",
      "  68549/175000: episode: 1923, duration: 0.525s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 130.607 [37.000, 190.000], mean observation: 0.179 [0.000, 56.000], loss: 0.136723, mean_absolute_error: 0.461942, mean_q: 5.969129, mean_eps: 0.100000\n",
      "  68585/175000: episode: 1924, duration: 0.661s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.083 [0.000, 222.000], mean observation: 0.231 [0.000, 72.000], loss: 5.736752, mean_absolute_error: 0.739198, mean_q: 7.724895, mean_eps: 0.100000\n",
      "  68618/175000: episode: 1925, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 155.000 [28.000, 218.000], mean observation: 0.192 [0.000, 66.000], loss: 0.271793, mean_absolute_error: 0.594327, mean_q: 6.839884, mean_eps: 0.100000\n",
      "  68651/175000: episode: 1926, duration: 0.560s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 131.818 [28.000, 162.000], mean observation: 0.168 [0.000, 66.000], loss: 0.393567, mean_absolute_error: 0.597456, mean_q: 6.856568, mean_eps: 0.100000\n",
      "  68671/175000: episode: 1927, duration: 0.386s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 102.450 [3.000, 162.000], mean observation: 0.096 [0.000, 40.000], loss: 10.776922, mean_absolute_error: 0.748207, mean_q: 7.674607, mean_eps: 0.100000\n",
      "  68713/175000: episode: 1928, duration: 0.825s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.405 [17.000, 162.000], mean observation: 0.272 [0.000, 84.000], loss: 0.190120, mean_absolute_error: 0.431858, mean_q: 5.702335, mean_eps: 0.100000\n",
      "  68756/175000: episode: 1929, duration: 0.797s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 78.884 [27.000, 181.000], mean observation: 0.351 [0.000, 86.000], loss: 8.616553, mean_absolute_error: 0.817207, mean_q: 8.178877, mean_eps: 0.100000\n",
      "  68796/175000: episode: 1930, duration: 0.849s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 57.050 [27.000, 197.000], mean observation: 0.225 [0.000, 80.000], loss: 0.404349, mean_absolute_error: 0.562706, mean_q: 6.553903, mean_eps: 0.100000\n",
      "  68819/175000: episode: 1931, duration: 0.464s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 159.000 [159.000, 159.000], mean observation: 0.055 [0.000, 46.000], loss: 0.266152, mean_absolute_error: 0.442035, mean_q: 5.611116, mean_eps: 0.100000\n",
      "  68861/175000: episode: 1932, duration: 0.816s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.214 [12.000, 159.000], mean observation: 0.210 [0.000, 84.000], loss: 14.569751, mean_absolute_error: 0.754743, mean_q: 7.472973, mean_eps: 0.100000\n",
      "  68875/175000: episode: 1933, duration: 0.248s, episode steps: 14, steps per second: 56, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 140.286 [28.000, 159.000], mean observation: 0.040 [0.000, 28.000], loss: 3.224275, mean_absolute_error: 0.466789, mean_q: 6.073028, mean_eps: 0.100000\n",
      "  68905/175000: episode: 1934, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.533 [28.000, 162.000], mean observation: 0.106 [0.000, 60.000], loss: 21.340880, mean_absolute_error: 0.883254, mean_q: 8.042334, mean_eps: 0.100000\n",
      "  68944/175000: episode: 1935, duration: 0.697s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.923 [28.000, 159.000], mean observation: 0.189 [0.000, 78.000], loss: 16.394016, mean_absolute_error: 0.772829, mean_q: 7.479815, mean_eps: 0.100000\n",
      "  68982/175000: episode: 1936, duration: 0.717s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.395 [19.000, 221.000], mean observation: 0.248 [0.000, 76.000], loss: 1.015388, mean_absolute_error: 0.719007, mean_q: 7.721008, mean_eps: 0.100000\n",
      "  69016/175000: episode: 1937, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.000 [17.000, 216.000], mean observation: 0.275 [0.000, 68.000], loss: 1.160196, mean_absolute_error: 0.576301, mean_q: 6.572734, mean_eps: 0.100000\n",
      "  69053/175000: episode: 1938, duration: 0.725s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 32.459 [28.000, 193.000], mean observation: 0.090 [0.000, 74.000], loss: 46.998812, mean_absolute_error: 0.868708, mean_q: 7.294531, mean_eps: 0.100000\n",
      "  69074/175000: episode: 1939, duration: 0.369s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 33.476 [19.000, 129.000], mean observation: 0.069 [0.000, 42.000], loss: 2.716102, mean_absolute_error: 0.784520, mean_q: 8.164009, mean_eps: 0.100000\n",
      "  69128/175000: episode: 1940, duration: 1.089s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 40.167 [28.000, 218.000], mean observation: 0.308 [0.000, 108.000], loss: 31.891799, mean_absolute_error: 0.908008, mean_q: 8.053359, mean_eps: 0.100000\n",
      "  69173/175000: episode: 1941, duration: 1.035s, episode steps: 45, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 78.244 [28.000, 194.000], mean observation: 0.216 [0.000, 90.000], loss: 0.195603, mean_absolute_error: 0.532884, mean_q: 6.349533, mean_eps: 0.100000\n",
      "  69219/175000: episode: 1942, duration: 0.913s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 39.826 [25.000, 222.000], mean observation: 0.258 [0.000, 92.000], loss: 0.189822, mean_absolute_error: 0.436755, mean_q: 5.805037, mean_eps: 0.100000\n",
      "  69261/175000: episode: 1943, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 45.429 [28.000, 213.000], mean observation: 0.249 [0.000, 84.000], loss: 20.326131, mean_absolute_error: 0.879573, mean_q: 8.231557, mean_eps: 0.100000\n",
      "  69321/175000: episode: 1944, duration: 1.115s, episode steps: 60, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 95.183 [12.000, 223.000], mean observation: 0.313 [0.000, 120.000], loss: 15.066345, mean_absolute_error: 0.689148, mean_q: 7.068304, mean_eps: 0.100000\n",
      "  69354/175000: episode: 1945, duration: 0.620s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 32.212 [28.000, 167.000], mean observation: 0.101 [0.000, 66.000], loss: 0.195290, mean_absolute_error: 0.542307, mean_q: 6.360543, mean_eps: 0.100000\n",
      "  69383/175000: episode: 1946, duration: 0.576s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 50.966 [28.000, 99.000], mean observation: 0.150 [0.000, 58.000], loss: 41.417271, mean_absolute_error: 1.018701, mean_q: 8.487657, mean_eps: 0.100000\n",
      "  69412/175000: episode: 1947, duration: 0.615s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 41.414 [28.000, 209.000], mean observation: 0.121 [0.000, 58.000], loss: 0.160936, mean_absolute_error: 0.442292, mean_q: 5.687459, mean_eps: 0.100000\n",
      "  69445/175000: episode: 1948, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 46.364 [6.000, 124.000], mean observation: 0.155 [0.000, 66.000], loss: 0.222845, mean_absolute_error: 0.456355, mean_q: 5.778299, mean_eps: 0.100000\n",
      "  69468/175000: episode: 1949, duration: 0.475s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 37.087 [28.000, 164.000], mean observation: 0.062 [0.000, 46.000], loss: 0.200834, mean_absolute_error: 0.443159, mean_q: 5.669045, mean_eps: 0.100000\n",
      "  69519/175000: episode: 1950, duration: 0.977s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 72.216 [27.000, 197.000], mean observation: 0.255 [0.000, 102.000], loss: 0.158397, mean_absolute_error: 0.427802, mean_q: 5.607324, mean_eps: 0.100000\n",
      "  69558/175000: episode: 1951, duration: 0.705s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 43.026 [28.000, 221.000], mean observation: 0.099 [0.000, 78.000], loss: 15.341600, mean_absolute_error: 0.727131, mean_q: 7.325326, mean_eps: 0.100000\n",
      "  69599/175000: episode: 1952, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 37.146 [28.000, 201.000], mean observation: 0.287 [0.000, 82.000], loss: 37.591396, mean_absolute_error: 1.170558, mean_q: 9.695232, mean_eps: 0.100000\n",
      "  69628/175000: episode: 1953, duration: 0.681s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 42.034 [26.000, 214.000], mean observation: 0.161 [0.000, 58.000], loss: 1.787801, mean_absolute_error: 0.620106, mean_q: 6.849864, mean_eps: 0.100000\n",
      "  69655/175000: episode: 1954, duration: 0.577s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.148 [1.000, 140.000], mean observation: 0.130 [0.000, 54.000], loss: 0.470374, mean_absolute_error: 0.458221, mean_q: 5.844123, mean_eps: 0.100000\n",
      "  69691/175000: episode: 1955, duration: 0.780s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 33.222 [28.000, 216.000], mean observation: 0.118 [0.000, 72.000], loss: 0.162512, mean_absolute_error: 0.436598, mean_q: 5.693503, mean_eps: 0.100000\n",
      "  69721/175000: episode: 1956, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 29.200 [11.000, 83.000], mean observation: 0.120 [0.000, 60.000], loss: 24.401439, mean_absolute_error: 0.828659, mean_q: 7.629613, mean_eps: 0.100000\n",
      "  69773/175000: episode: 1957, duration: 0.913s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.154 [16.000, 207.000], mean observation: 0.284 [0.000, 104.000], loss: 3.292269, mean_absolute_error: 0.845887, mean_q: 8.443717, mean_eps: 0.100000\n",
      "  69799/175000: episode: 1958, duration: 0.456s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 106.769 [28.000, 171.000], mean observation: 0.155 [0.000, 52.000], loss: 14.431132, mean_absolute_error: 0.925549, mean_q: 8.786179, mean_eps: 0.100000\n",
      "  69814/175000: episode: 1959, duration: 0.294s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 68.533 [28.000, 182.000], mean observation: 0.093 [0.000, 30.000], loss: 6.981705, mean_absolute_error: 0.767856, mean_q: 8.080081, mean_eps: 0.100000\n",
      "  69857/175000: episode: 1960, duration: 0.839s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.186 [9.000, 203.000], mean observation: 0.382 [0.000, 86.000], loss: 12.161770, mean_absolute_error: 0.701393, mean_q: 7.293131, mean_eps: 0.100000\n",
      "  69879/175000: episode: 1961, duration: 0.382s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 57.227 [1.000, 179.000], mean observation: 0.139 [0.000, 44.000], loss: 43.941704, mean_absolute_error: 0.901881, mean_q: 7.890256, mean_eps: 0.100000\n",
      "  69906/175000: episode: 1962, duration: 0.530s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.148 [13.000, 159.000], mean observation: 0.174 [0.000, 54.000], loss: 0.162521, mean_absolute_error: 0.450044, mean_q: 6.107947, mean_eps: 0.100000\n",
      "  69932/175000: episode: 1963, duration: 0.523s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.423 [28.000, 159.000], mean observation: 0.221 [0.000, 52.000], loss: 0.220846, mean_absolute_error: 0.461159, mean_q: 6.112054, mean_eps: 0.100000\n",
      "  69945/175000: episode: 1964, duration: 0.289s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 116.923 [27.000, 222.000], mean observation: 0.077 [0.000, 26.000], loss: 0.186221, mean_absolute_error: 0.481419, mean_q: 6.409954, mean_eps: 0.100000\n",
      "  69977/175000: episode: 1965, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 115.031 [7.000, 206.000], mean observation: 0.297 [0.000, 64.000], loss: 37.961377, mean_absolute_error: 0.926731, mean_q: 8.362082, mean_eps: 0.100000\n",
      "  70003/175000: episode: 1966, duration: 0.485s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 51.962 [28.000, 156.000], mean observation: 0.169 [0.000, 52.000], loss: 15.742711, mean_absolute_error: 0.849136, mean_q: 8.388871, mean_eps: 0.100000\n",
      "  70054/175000: episode: 1967, duration: 0.956s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 96.804 [26.000, 217.000], mean observation: 0.606 [0.000, 102.000], loss: 0.496093, mean_absolute_error: 0.468042, mean_q: 6.351323, mean_eps: 0.100000\n",
      "  70083/175000: episode: 1968, duration: 0.519s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.138 [28.000, 216.000], mean observation: 0.144 [0.000, 58.000], loss: 24.148362, mean_absolute_error: 0.873238, mean_q: 8.402736, mean_eps: 0.100000\n",
      "  70126/175000: episode: 1969, duration: 0.881s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.884 [28.000, 216.000], mean observation: 0.241 [0.000, 86.000], loss: 10.637898, mean_absolute_error: 0.688719, mean_q: 7.496452, mean_eps: 0.100000\n",
      "  70165/175000: episode: 1970, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 39.051 [27.000, 220.000], mean observation: 0.162 [0.000, 78.000], loss: 5.051785, mean_absolute_error: 0.575026, mean_q: 6.928570, mean_eps: 0.100000\n",
      "  70214/175000: episode: 1971, duration: 0.940s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.857 [20.000, 205.000], mean observation: 0.346 [0.000, 98.000], loss: 18.405020, mean_absolute_error: 0.709957, mean_q: 7.452708, mean_eps: 0.100000\n",
      "  70247/175000: episode: 1972, duration: 0.613s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 107.333 [18.000, 186.000], mean observation: 0.380 [0.000, 66.000], loss: 4.479396, mean_absolute_error: 0.605742, mean_q: 7.213696, mean_eps: 0.100000\n",
      "  70285/175000: episode: 1973, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.816 [28.000, 222.000], mean observation: 0.145 [0.000, 76.000], loss: 23.613859, mean_absolute_error: 0.859884, mean_q: 8.296809, mean_eps: 0.100000\n",
      "  70321/175000: episode: 1974, duration: 0.753s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.889 [57.000, 174.000], mean observation: 0.260 [0.000, 72.000], loss: 26.812950, mean_absolute_error: 0.791376, mean_q: 7.776662, mean_eps: 0.100000\n",
      "  70354/175000: episode: 1975, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 142.303 [106.000, 213.000], mean observation: 0.244 [0.000, 66.000], loss: 15.388385, mean_absolute_error: 0.790894, mean_q: 8.187651, mean_eps: 0.100000\n",
      "  70391/175000: episode: 1976, duration: 0.684s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 136.595 [87.000, 174.000], mean observation: 0.154 [0.000, 74.000], loss: 48.067522, mean_absolute_error: 1.030046, mean_q: 9.043692, mean_eps: 0.100000\n",
      "  70429/175000: episode: 1977, duration: 0.766s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 133.474 [63.000, 223.000], mean observation: 0.351 [0.000, 76.000], loss: 0.185765, mean_absolute_error: 0.458520, mean_q: 6.422886, mean_eps: 0.100000\n",
      "  70456/175000: episode: 1978, duration: 0.571s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 147.074 [90.000, 162.000], mean observation: 0.136 [0.000, 54.000], loss: 0.715841, mean_absolute_error: 0.579084, mean_q: 7.193834, mean_eps: 0.100000\n",
      "  70523/175000: episode: 1979, duration: 1.383s, episode steps: 67, steps per second: 48, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 84.343 [5.000, 194.000], mean observation: 0.657 [0.000, 134.000], loss: 0.202777, mean_absolute_error: 0.514504, mean_q: 6.825099, mean_eps: 0.100000\n",
      "  70555/175000: episode: 1980, duration: 0.686s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 86.406 [24.000, 191.000], mean observation: 0.291 [0.000, 64.000], loss: 0.201859, mean_absolute_error: 0.475869, mean_q: 6.531876, mean_eps: 0.100000\n",
      "  70590/175000: episode: 1981, duration: 0.865s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 45.114 [18.000, 192.000], mean observation: 0.181 [0.000, 70.000], loss: 20.128438, mean_absolute_error: 0.777186, mean_q: 7.996857, mean_eps: 0.100000\n",
      "  70626/175000: episode: 1982, duration: 0.692s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 26.333 [3.000, 27.000], mean observation: 0.118 [0.000, 72.000], loss: 0.322647, mean_absolute_error: 0.491164, mean_q: 6.746839, mean_eps: 0.100000\n",
      "  70653/175000: episode: 1983, duration: 0.517s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 65.037 [8.000, 156.000], mean observation: 0.255 [0.000, 54.000], loss: 0.412332, mean_absolute_error: 0.470689, mean_q: 6.741336, mean_eps: 0.100000\n",
      "  70685/175000: episode: 1984, duration: 0.615s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 87.031 [27.000, 208.000], mean observation: 0.176 [0.000, 64.000], loss: 60.672685, mean_absolute_error: 0.995984, mean_q: 8.547337, mean_eps: 0.100000\n",
      "  70713/175000: episode: 1985, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 168.679 [18.000, 215.000], mean observation: 0.131 [0.000, 56.000], loss: 2.491030, mean_absolute_error: 0.473131, mean_q: 6.557728, mean_eps: 0.100000\n",
      "  70760/175000: episode: 1986, duration: 0.944s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.617 [30.000, 224.000], mean observation: 0.362 [0.000, 94.000], loss: 67.359250, mean_absolute_error: 0.965163, mean_q: 7.937852, mean_eps: 0.100000\n",
      "  70800/175000: episode: 1987, duration: 0.830s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.625 [26.000, 219.000], mean observation: 0.363 [0.000, 80.000], loss: 7.906099, mean_absolute_error: 0.579543, mean_q: 7.129366, mean_eps: 0.100000\n",
      "  70831/175000: episode: 1988, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.000 [21.000, 186.000], mean observation: 0.213 [0.000, 62.000], loss: 68.943553, mean_absolute_error: 1.027774, mean_q: 8.477954, mean_eps: 0.100000\n",
      "  70865/175000: episode: 1989, duration: 0.807s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.088 [17.000, 162.000], mean observation: 0.123 [0.000, 68.000], loss: 40.807526, mean_absolute_error: 1.028268, mean_q: 9.424409, mean_eps: 0.100000\n",
      "  70889/175000: episode: 1990, duration: 0.474s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 158.000 [150.000, 162.000], mean observation: 0.088 [0.000, 48.000], loss: 26.106470, mean_absolute_error: 0.961043, mean_q: 9.473010, mean_eps: 0.100000\n",
      "  70911/175000: episode: 1991, duration: 0.420s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 160.864 [156.000, 215.000], mean observation: 0.082 [0.000, 44.000], loss: 0.198607, mean_absolute_error: 0.449929, mean_q: 6.660967, mean_eps: 0.100000\n",
      "  70936/175000: episode: 1992, duration: 0.519s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 132.880 [8.000, 162.000], mean observation: 0.124 [0.000, 50.000], loss: 0.248109, mean_absolute_error: 0.454054, mean_q: 6.707075, mean_eps: 0.100000\n",
      "  70970/175000: episode: 1993, duration: 0.785s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 128.882 [12.000, 184.000], mean observation: 0.199 [0.000, 68.000], loss: 58.021243, mean_absolute_error: 0.956553, mean_q: 8.461681, mean_eps: 0.100000\n",
      "  71023/175000: episode: 1994, duration: 0.987s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.491 [32.000, 197.000], mean observation: 0.443 [0.000, 106.000], loss: 9.688795, mean_absolute_error: 0.572354, mean_q: 7.187525, mean_eps: 0.100000\n",
      "  71058/175000: episode: 1995, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.543 [9.000, 194.000], mean observation: 0.322 [0.000, 70.000], loss: 71.083899, mean_absolute_error: 1.270061, mean_q: 10.234366, mean_eps: 0.100000\n",
      "  71081/175000: episode: 1996, duration: 0.454s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 153.304 [46.000, 162.000], mean observation: 0.099 [0.000, 46.000], loss: 0.082831, mean_absolute_error: 0.427180, mean_q: 6.417583, mean_eps: 0.100000\n",
      "  71122/175000: episode: 1997, duration: 0.873s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.293 [28.000, 224.000], mean observation: 0.392 [0.000, 82.000], loss: 5.093147, mean_absolute_error: 0.563185, mean_q: 7.295025, mean_eps: 0.100000\n",
      "  71150/175000: episode: 1998, duration: 0.609s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.179 [48.000, 162.000], mean observation: 0.171 [0.000, 56.000], loss: 0.299434, mean_absolute_error: 0.449243, mean_q: 6.808192, mean_eps: 0.100000\n",
      "  71191/175000: episode: 1999, duration: 0.834s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.390 [28.000, 190.000], mean observation: 0.246 [0.000, 82.000], loss: 9.930498, mean_absolute_error: 0.581227, mean_q: 7.249050, mean_eps: 0.100000\n",
      "  71209/175000: episode: 2000, duration: 0.364s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 68.167 [42.000, 101.000], mean observation: 0.069 [0.000, 36.000], loss: 0.088960, mean_absolute_error: 0.428833, mean_q: 6.615179, mean_eps: 0.100000\n",
      "  71230/175000: episode: 2001, duration: 0.502s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 74.429 [48.000, 162.000], mean observation: 0.065 [0.000, 42.000], loss: 6.220993, mean_absolute_error: 0.693858, mean_q: 8.425265, mean_eps: 0.100000\n",
      "  71274/175000: episode: 2002, duration: 1.112s, episode steps: 44, steps per second: 40, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 80.159 [18.000, 221.000], mean observation: 0.495 [0.000, 88.000], loss: 0.312669, mean_absolute_error: 0.457292, mean_q: 6.910391, mean_eps: 0.100000\n",
      "  71315/175000: episode: 2003, duration: 0.992s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.073 [52.000, 162.000], mean observation: 0.266 [0.000, 82.000], loss: 38.827936, mean_absolute_error: 0.949397, mean_q: 8.816987, mean_eps: 0.100000\n",
      "  71359/175000: episode: 2004, duration: 1.015s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.886 [3.000, 221.000], mean observation: 0.551 [0.000, 88.000], loss: 54.222634, mean_absolute_error: 0.816923, mean_q: 7.421530, mean_eps: 0.100000\n",
      "  71409/175000: episode: 2005, duration: 1.148s, episode steps: 50, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 57.840 [28.000, 156.000], mean observation: 0.228 [0.000, 100.000], loss: 17.726636, mean_absolute_error: 0.688141, mean_q: 7.741936, mean_eps: 0.100000\n",
      "  71459/175000: episode: 2006, duration: 0.939s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.600 [25.000, 221.000], mean observation: 0.560 [0.000, 100.000], loss: 21.345601, mean_absolute_error: 0.803786, mean_q: 8.523852, mean_eps: 0.100000\n",
      "  71511/175000: episode: 2007, duration: 0.967s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 84.404 [28.000, 224.000], mean observation: 0.360 [0.000, 104.000], loss: 24.182031, mean_absolute_error: 0.698756, mean_q: 7.583088, mean_eps: 0.100000\n",
      "  71545/175000: episode: 2008, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 105.559 [42.000, 164.000], mean observation: 0.284 [0.000, 68.000], loss: 50.186569, mean_absolute_error: 1.034043, mean_q: 9.170237, mean_eps: 0.100000\n",
      "  71576/175000: episode: 2009, duration: 0.676s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 128.871 [39.000, 188.000], mean observation: 0.148 [0.000, 62.000], loss: 0.108311, mean_absolute_error: 0.415444, mean_q: 6.292987, mean_eps: 0.100000\n",
      "  71604/175000: episode: 2010, duration: 0.660s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.607 [28.000, 169.000], mean observation: 0.238 [0.000, 56.000], loss: 27.217320, mean_absolute_error: 0.890172, mean_q: 8.811085, mean_eps: 0.100000\n",
      "  71652/175000: episode: 2011, duration: 0.905s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 103.896 [1.000, 215.000], mean observation: 0.424 [0.000, 96.000], loss: 19.169100, mean_absolute_error: 0.711988, mean_q: 7.833720, mean_eps: 0.100000\n",
      "  71672/175000: episode: 2012, duration: 0.462s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 57.550 [15.000, 116.000], mean observation: 0.109 [0.000, 40.000], loss: 0.147691, mean_absolute_error: 0.403733, mean_q: 6.107743, mean_eps: 0.100000\n",
      "  71726/175000: episode: 2013, duration: 1.025s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 39.111 [28.000, 203.000], mean observation: 0.296 [0.000, 108.000], loss: 7.655484, mean_absolute_error: 0.555476, mean_q: 7.164510, mean_eps: 0.100000\n",
      "  71766/175000: episode: 2014, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 32.200 [28.000, 170.000], mean observation: 0.167 [0.000, 80.000], loss: 11.092187, mean_absolute_error: 0.594174, mean_q: 7.334002, mean_eps: 0.100000\n",
      "  71790/175000: episode: 2015, duration: 0.453s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 29.583 [28.000, 66.000], mean observation: 0.058 [0.000, 48.000], loss: 28.480382, mean_absolute_error: 0.742846, mean_q: 7.837541, mean_eps: 0.100000\n",
      "  71831/175000: episode: 2016, duration: 0.726s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.829 [15.000, 184.000], mean observation: 0.214 [0.000, 82.000], loss: 0.261590, mean_absolute_error: 0.408973, mean_q: 6.326092, mean_eps: 0.100000\n",
      "  71874/175000: episode: 2017, duration: 0.821s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.512 [10.000, 198.000], mean observation: 0.176 [0.000, 86.000], loss: 0.197684, mean_absolute_error: 0.419277, mean_q: 6.362025, mean_eps: 0.100000\n",
      "  71892/175000: episode: 2018, duration: 0.331s, episode steps: 18, steps per second: 54, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 71.667 [16.000, 179.000], mean observation: 0.085 [0.000, 36.000], loss: 0.177105, mean_absolute_error: 0.436178, mean_q: 6.420413, mean_eps: 0.100000\n",
      "  71926/175000: episode: 2019, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.941 [28.000, 172.000], mean observation: 0.163 [0.000, 68.000], loss: 46.710728, mean_absolute_error: 0.911557, mean_q: 8.250647, mean_eps: 0.100000\n",
      "  71952/175000: episode: 2020, duration: 0.479s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 136.538 [2.000, 184.000], mean observation: 0.128 [0.000, 52.000], loss: 0.262109, mean_absolute_error: 0.428044, mean_q: 6.137931, mean_eps: 0.100000\n",
      "  71990/175000: episode: 2021, duration: 0.795s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.526 [48.000, 197.000], mean observation: 0.335 [0.000, 76.000], loss: 2.249248, mean_absolute_error: 0.565084, mean_q: 7.100899, mean_eps: 0.100000\n",
      "  72010/175000: episode: 2022, duration: 0.380s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.950 [28.000, 160.000], mean observation: 0.131 [0.000, 40.000], loss: 0.242126, mean_absolute_error: 0.475458, mean_q: 6.626996, mean_eps: 0.100000\n",
      "  72029/175000: episode: 2023, duration: 0.353s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 138.632 [82.000, 202.000], mean observation: 0.084 [0.000, 38.000], loss: 0.436977, mean_absolute_error: 0.408453, mean_q: 6.049500, mean_eps: 0.100000\n",
      "  72081/175000: episode: 2024, duration: 0.950s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 122.038 [28.000, 195.000], mean observation: 0.593 [0.000, 104.000], loss: 3.344451, mean_absolute_error: 0.605340, mean_q: 7.467801, mean_eps: 0.100000\n",
      "  72103/175000: episode: 2025, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 113.727 [28.000, 221.000], mean observation: 0.107 [0.000, 44.000], loss: 58.174918, mean_absolute_error: 0.939103, mean_q: 8.204029, mean_eps: 0.100000\n",
      "  72118/175000: episode: 2026, duration: 0.286s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 80.133 [28.000, 159.000], mean observation: 0.103 [0.000, 30.000], loss: 0.128058, mean_absolute_error: 0.488101, mean_q: 6.747097, mean_eps: 0.100000\n",
      "  72161/175000: episode: 2027, duration: 0.811s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 91.860 [5.000, 221.000], mean observation: 0.347 [0.000, 86.000], loss: 28.641353, mean_absolute_error: 0.664141, mean_q: 6.977469, mean_eps: 0.100000\n",
      "  72182/175000: episode: 2028, duration: 0.362s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 30.143 [28.000, 73.000], mean observation: 0.081 [0.000, 42.000], loss: 17.059841, mean_absolute_error: 0.749483, mean_q: 7.935649, mean_eps: 0.100000\n",
      "  72233/175000: episode: 2029, duration: 1.006s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.078 [28.000, 142.000], mean observation: 0.255 [0.000, 102.000], loss: 26.540258, mean_absolute_error: 0.877562, mean_q: 8.626237, mean_eps: 0.100000\n",
      "  72264/175000: episode: 2030, duration: 0.553s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 31.710 [28.000, 87.000], mean observation: 0.083 [0.000, 62.000], loss: 0.419431, mean_absolute_error: 0.654969, mean_q: 7.855711, mean_eps: 0.100000\n",
      "  72292/175000: episode: 2031, duration: 0.554s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 57.607 [28.000, 156.000], mean observation: 0.094 [0.000, 56.000], loss: 0.193870, mean_absolute_error: 0.503504, mean_q: 6.986860, mean_eps: 0.100000\n",
      "  72310/175000: episode: 2032, duration: 0.386s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 40.611 [28.000, 221.000], mean observation: 0.059 [0.000, 36.000], loss: 31.663639, mean_absolute_error: 0.814930, mean_q: 8.063615, mean_eps: 0.100000\n",
      "  72355/175000: episode: 2033, duration: 0.811s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.756 [28.000, 156.000], mean observation: 0.225 [0.000, 90.000], loss: 0.281008, mean_absolute_error: 0.465506, mean_q: 6.581272, mean_eps: 0.100000\n",
      "  72376/175000: episode: 2034, duration: 0.470s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 92.095 [28.000, 156.000], mean observation: 0.147 [0.000, 42.000], loss: 0.596430, mean_absolute_error: 0.435522, mean_q: 6.380845, mean_eps: 0.100000\n",
      "  72428/175000: episode: 2035, duration: 1.014s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.135 [2.000, 198.000], mean observation: 0.523 [0.000, 104.000], loss: 0.955163, mean_absolute_error: 0.489629, mean_q: 6.636609, mean_eps: 0.100000\n",
      "  72461/175000: episode: 2036, duration: 0.689s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.818 [28.000, 195.000], mean observation: 0.370 [0.000, 66.000], loss: 28.274342, mean_absolute_error: 0.868250, mean_q: 8.431501, mean_eps: 0.100000\n",
      "  72482/175000: episode: 2037, duration: 0.401s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 78.952 [52.000, 195.000], mean observation: 0.096 [0.000, 42.000], loss: 4.058864, mean_absolute_error: 0.695522, mean_q: 7.986350, mean_eps: 0.100000\n",
      "  72511/175000: episode: 2038, duration: 0.551s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 92.345 [52.000, 169.000], mean observation: 0.087 [0.000, 58.000], loss: 24.107835, mean_absolute_error: 0.747545, mean_q: 7.911914, mean_eps: 0.100000\n",
      "  72547/175000: episode: 2039, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 77.000 [28.000, 198.000], mean observation: 0.147 [0.000, 72.000], loss: 59.006803, mean_absolute_error: 1.140728, mean_q: 9.569599, mean_eps: 0.100000\n",
      "  72595/175000: episode: 2040, duration: 0.912s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.667 [52.000, 218.000], mean observation: 0.225 [0.000, 96.000], loss: 0.223585, mean_absolute_error: 0.446091, mean_q: 6.395349, mean_eps: 0.100000\n",
      "  72621/175000: episode: 2041, duration: 0.554s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 52.308 [52.000, 60.000], mean observation: 0.078 [0.000, 52.000], loss: 38.377376, mean_absolute_error: 0.944583, mean_q: 8.743134, mean_eps: 0.100000\n",
      "  72640/175000: episode: 2042, duration: 0.373s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 58.737 [43.000, 179.000], mean observation: 0.062 [0.000, 38.000], loss: 0.210517, mean_absolute_error: 0.687841, mean_q: 7.944205, mean_eps: 0.100000\n",
      "  72691/175000: episode: 2043, duration: 1.036s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.353 [27.000, 220.000], mean observation: 0.474 [0.000, 102.000], loss: 5.490421, mean_absolute_error: 0.742543, mean_q: 8.320737, mean_eps: 0.100000\n",
      "  72710/175000: episode: 2044, duration: 0.362s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 90.684 [52.000, 148.000], mean observation: 0.075 [0.000, 38.000], loss: 0.204100, mean_absolute_error: 0.464885, mean_q: 6.625752, mean_eps: 0.100000\n",
      "  72739/175000: episode: 2045, duration: 0.607s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 101.655 [52.000, 210.000], mean observation: 0.169 [0.000, 58.000], loss: 0.341372, mean_absolute_error: 0.454625, mean_q: 6.364294, mean_eps: 0.100000\n",
      "  72756/175000: episode: 2046, duration: 0.353s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 52.000 [52.000, 52.000], mean observation: 0.041 [0.000, 34.000], loss: 0.132209, mean_absolute_error: 0.742731, mean_q: 8.444944, mean_eps: 0.100000\n",
      "  72790/175000: episode: 2047, duration: 0.687s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.618 [28.000, 202.000], mean observation: 0.211 [0.000, 68.000], loss: 0.146076, mean_absolute_error: 0.418029, mean_q: 5.962342, mean_eps: 0.100000\n",
      "  72818/175000: episode: 2048, duration: 0.607s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 105.929 [52.000, 172.000], mean observation: 0.120 [0.000, 56.000], loss: 0.560693, mean_absolute_error: 0.466193, mean_q: 6.565517, mean_eps: 0.100000\n",
      "  72856/175000: episode: 2049, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.421 [5.000, 215.000], mean observation: 0.176 [0.000, 76.000], loss: 19.448649, mean_absolute_error: 0.929703, mean_q: 9.075288, mean_eps: 0.100000\n",
      "  72873/175000: episode: 2050, duration: 0.357s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 30.824 [28.000, 76.000], mean observation: 0.043 [0.000, 34.000], loss: 20.203324, mean_absolute_error: 0.743455, mean_q: 7.590290, mean_eps: 0.100000\n",
      "  72921/175000: episode: 2051, duration: 1.140s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.750 [28.000, 202.000], mean observation: 0.218 [0.000, 96.000], loss: 46.238581, mean_absolute_error: 0.839522, mean_q: 7.797550, mean_eps: 0.100000\n",
      "  72958/175000: episode: 2052, duration: 0.805s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 69.054 [28.000, 158.000], mean observation: 0.143 [0.000, 74.000], loss: 0.288312, mean_absolute_error: 0.547841, mean_q: 7.142111, mean_eps: 0.100000\n",
      "  72979/175000: episode: 2053, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 35.619 [10.000, 134.000], mean observation: 0.072 [0.000, 42.000], loss: 0.212969, mean_absolute_error: 0.418065, mean_q: 5.984916, mean_eps: 0.100000\n",
      "  73023/175000: episode: 2054, duration: 1.117s, episode steps: 44, steps per second: 39, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.432 [17.000, 211.000], mean observation: 0.348 [0.000, 88.000], loss: 0.521061, mean_absolute_error: 0.473381, mean_q: 6.584459, mean_eps: 0.100000\n",
      "  73060/175000: episode: 2055, duration: 0.782s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 63.459 [27.000, 127.000], mean observation: 0.179 [0.000, 74.000], loss: 15.063974, mean_absolute_error: 0.610542, mean_q: 6.958544, mean_eps: 0.100000\n",
      "  73098/175000: episode: 2056, duration: 0.765s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.447 [27.000, 202.000], mean observation: 0.175 [0.000, 76.000], loss: 0.207236, mean_absolute_error: 0.429518, mean_q: 6.127132, mean_eps: 0.100000\n",
      "  73132/175000: episode: 2057, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.912 [32.000, 162.000], mean observation: 0.205 [0.000, 68.000], loss: 38.144862, mean_absolute_error: 0.917679, mean_q: 8.279915, mean_eps: 0.100000\n",
      "  73187/175000: episode: 2058, duration: 1.264s, episode steps: 55, steps per second: 44, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 119.873 [5.000, 224.000], mean observation: 0.612 [0.000, 110.000], loss: 1.958109, mean_absolute_error: 0.525473, mean_q: 6.723226, mean_eps: 0.100000\n",
      "  73223/175000: episode: 2059, duration: 0.850s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 65.056 [6.000, 190.000], mean observation: 0.236 [0.000, 72.000], loss: 6.998519, mean_absolute_error: 0.724665, mean_q: 7.945472, mean_eps: 0.100000\n",
      "  73255/175000: episode: 2060, duration: 0.790s, episode steps: 32, steps per second: 40, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.688 [5.000, 200.000], mean observation: 0.252 [0.000, 64.000], loss: 4.017838, mean_absolute_error: 0.741579, mean_q: 8.052058, mean_eps: 0.100000\n",
      "  73289/175000: episode: 2061, duration: 0.825s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.559 [32.000, 127.000], mean observation: 0.120 [0.000, 68.000], loss: 10.073140, mean_absolute_error: 0.611022, mean_q: 7.018516, mean_eps: 0.100000\n",
      "  73337/175000: episode: 2062, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 76.646 [30.000, 197.000], mean observation: 0.258 [0.000, 96.000], loss: 15.039535, mean_absolute_error: 0.791658, mean_q: 8.214256, mean_eps: 0.100000\n",
      "  73375/175000: episode: 2063, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.895 [28.000, 183.000], mean observation: 0.346 [0.000, 76.000], loss: 20.454757, mean_absolute_error: 0.998246, mean_q: 9.710449, mean_eps: 0.100000\n",
      "  73411/175000: episode: 2064, duration: 0.852s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.583 [32.000, 183.000], mean observation: 0.357 [0.000, 72.000], loss: 0.840178, mean_absolute_error: 0.564770, mean_q: 7.096956, mean_eps: 0.100000\n",
      "  73473/175000: episode: 2065, duration: 1.442s, episode steps: 62, steps per second: 43, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 103.726 [15.000, 193.000], mean observation: 0.543 [0.000, 124.000], loss: 31.169109, mean_absolute_error: 0.788867, mean_q: 7.929210, mean_eps: 0.100000\n",
      "  73511/175000: episode: 2066, duration: 0.809s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.579 [28.000, 221.000], mean observation: 0.392 [0.000, 76.000], loss: 82.951673, mean_absolute_error: 1.234468, mean_q: 9.476587, mean_eps: 0.100000\n",
      "  73542/175000: episode: 2067, duration: 0.648s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 38.839 [28.000, 164.000], mean observation: 0.113 [0.000, 62.000], loss: 6.212882, mean_absolute_error: 0.949057, mean_q: 10.060990, mean_eps: 0.100000\n",
      "  73580/175000: episode: 2068, duration: 0.769s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.474 [23.000, 221.000], mean observation: 0.349 [0.000, 76.000], loss: 11.696059, mean_absolute_error: 0.802643, mean_q: 8.816039, mean_eps: 0.100000\n",
      "  73605/175000: episode: 2069, duration: 0.486s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 61.160 [28.000, 156.000], mean observation: 0.167 [0.000, 50.000], loss: 0.202799, mean_absolute_error: 0.419449, mean_q: 6.331637, mean_eps: 0.100000\n",
      "  73646/175000: episode: 2070, duration: 0.963s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 64.439 [1.000, 168.000], mean observation: 0.371 [0.000, 82.000], loss: 0.225945, mean_absolute_error: 0.519600, mean_q: 7.229581, mean_eps: 0.100000\n",
      "  73692/175000: episode: 2071, duration: 0.891s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 73.761 [6.000, 193.000], mean observation: 0.401 [0.000, 92.000], loss: 3.365559, mean_absolute_error: 0.562560, mean_q: 7.239211, mean_eps: 0.100000\n",
      "  73737/175000: episode: 2072, duration: 0.910s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 84.089 [7.000, 210.000], mean observation: 0.509 [0.000, 90.000], loss: 10.567385, mean_absolute_error: 0.713563, mean_q: 8.207515, mean_eps: 0.100000\n",
      "  73769/175000: episode: 2073, duration: 0.571s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 37.312 [23.000, 112.000], mean observation: 0.160 [0.000, 64.000], loss: 14.475796, mean_absolute_error: 0.786973, mean_q: 8.606670, mean_eps: 0.100000\n",
      "  73805/175000: episode: 2074, duration: 0.694s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 52.472 [28.000, 191.000], mean observation: 0.152 [0.000, 72.000], loss: 49.390125, mean_absolute_error: 1.078755, mean_q: 9.902657, mean_eps: 0.100000\n",
      "  73851/175000: episode: 2075, duration: 0.854s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.891 [28.000, 223.000], mean observation: 0.369 [0.000, 92.000], loss: 14.386261, mean_absolute_error: 0.624939, mean_q: 7.671123, mean_eps: 0.100000\n",
      "  73892/175000: episode: 2076, duration: 0.907s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.927 [11.000, 207.000], mean observation: 0.253 [0.000, 82.000], loss: 11.481354, mean_absolute_error: 0.726398, mean_q: 8.550265, mean_eps: 0.100000\n",
      "  73934/175000: episode: 2077, duration: 1.029s, episode steps: 42, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.833 [28.000, 167.000], mean observation: 0.272 [0.000, 84.000], loss: 13.221429, mean_absolute_error: 0.614981, mean_q: 7.624047, mean_eps: 0.100000\n",
      "  73974/175000: episode: 2078, duration: 0.831s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 76.800 [13.000, 210.000], mean observation: 0.271 [0.000, 80.000], loss: 9.865552, mean_absolute_error: 0.750157, mean_q: 8.887114, mean_eps: 0.100000\n",
      "  74016/175000: episode: 2079, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.071 [24.000, 206.000], mean observation: 0.294 [0.000, 84.000], loss: 0.165477, mean_absolute_error: 0.507413, mean_q: 7.564474, mean_eps: 0.100000\n",
      "  74048/175000: episode: 2080, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 40.406 [28.000, 211.000], mean observation: 0.112 [0.000, 64.000], loss: 0.173138, mean_absolute_error: 0.464868, mean_q: 7.115989, mean_eps: 0.100000\n",
      "  74093/175000: episode: 2081, duration: 0.940s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.022 [28.000, 162.000], mean observation: 0.319 [0.000, 90.000], loss: 10.707790, mean_absolute_error: 0.788719, mean_q: 9.231096, mean_eps: 0.100000\n",
      "  74130/175000: episode: 2082, duration: 0.882s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 88.838 [27.000, 182.000], mean observation: 0.336 [0.000, 74.000], loss: 53.208896, mean_absolute_error: 0.955792, mean_q: 9.111463, mean_eps: 0.100000\n",
      "  74160/175000: episode: 2083, duration: 0.673s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 80.700 [24.000, 156.000], mean observation: 0.196 [0.000, 60.000], loss: 0.709134, mean_absolute_error: 0.510950, mean_q: 7.433999, mean_eps: 0.100000\n",
      "  74211/175000: episode: 2084, duration: 1.129s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 75.824 [1.000, 178.000], mean observation: 0.465 [0.000, 102.000], loss: 1.050743, mean_absolute_error: 0.563426, mean_q: 7.872248, mean_eps: 0.100000\n",
      "  74262/175000: episode: 2085, duration: 1.158s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 83.275 [24.000, 188.000], mean observation: 0.456 [0.000, 102.000], loss: 0.183508, mean_absolute_error: 0.479838, mean_q: 7.331786, mean_eps: 0.100000\n",
      "  74309/175000: episode: 2086, duration: 0.897s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.574 [24.000, 184.000], mean observation: 0.328 [0.000, 94.000], loss: 22.010669, mean_absolute_error: 0.739687, mean_q: 8.463032, mean_eps: 0.100000\n",
      "  74336/175000: episode: 2087, duration: 0.571s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 53.444 [24.000, 220.000], mean observation: 0.142 [0.000, 54.000], loss: 0.218715, mean_absolute_error: 0.437498, mean_q: 6.965949, mean_eps: 0.100000\n",
      "  74357/175000: episode: 2088, duration: 0.480s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 72.476 [24.000, 158.000], mean observation: 0.079 [0.000, 42.000], loss: 1.336567, mean_absolute_error: 0.652108, mean_q: 8.607960, mean_eps: 0.100000\n",
      "  74393/175000: episode: 2089, duration: 0.727s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 88.778 [24.000, 184.000], mean observation: 0.143 [0.000, 72.000], loss: 62.207638, mean_absolute_error: 0.983523, mean_q: 8.879408, mean_eps: 0.100000\n",
      "  74431/175000: episode: 2090, duration: 0.719s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.105 [10.000, 207.000], mean observation: 0.200 [0.000, 76.000], loss: 4.331852, mean_absolute_error: 0.612021, mean_q: 7.885777, mean_eps: 0.100000\n",
      "  74478/175000: episode: 2091, duration: 0.965s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 64.638 [1.000, 118.000], mean observation: 0.184 [0.000, 94.000], loss: 4.207999, mean_absolute_error: 0.564672, mean_q: 7.493568, mean_eps: 0.100000\n",
      "  74537/175000: episode: 2092, duration: 1.297s, episode steps: 59, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 77.203 [24.000, 216.000], mean observation: 0.270 [0.000, 118.000], loss: 0.238198, mean_absolute_error: 0.541534, mean_q: 7.518997, mean_eps: 0.100000\n",
      "  74577/175000: episode: 2093, duration: 0.901s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.475 [24.000, 168.000], mean observation: 0.157 [0.000, 80.000], loss: 14.203260, mean_absolute_error: 0.675987, mean_q: 7.957335, mean_eps: 0.100000\n",
      "  74629/175000: episode: 2094, duration: 1.000s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.615 [24.000, 216.000], mean observation: 0.476 [0.000, 104.000], loss: 8.371088, mean_absolute_error: 0.681222, mean_q: 8.259612, mean_eps: 0.100000\n",
      "  74667/175000: episode: 2095, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.158 [24.000, 153.000], mean observation: 0.263 [0.000, 76.000], loss: 30.419809, mean_absolute_error: 0.837920, mean_q: 8.686300, mean_eps: 0.100000\n",
      "  74704/175000: episode: 2096, duration: 0.759s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.378 [6.000, 214.000], mean observation: 0.279 [0.000, 74.000], loss: 22.592114, mean_absolute_error: 0.813910, mean_q: 8.873389, mean_eps: 0.100000\n",
      "  74737/175000: episode: 2097, duration: 0.632s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.091 [14.000, 210.000], mean observation: 0.263 [0.000, 66.000], loss: 17.791956, mean_absolute_error: 0.658025, mean_q: 7.742673, mean_eps: 0.100000\n",
      "  74770/175000: episode: 2098, duration: 0.608s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.667 [28.000, 101.000], mean observation: 0.157 [0.000, 66.000], loss: 42.796578, mean_absolute_error: 0.895255, mean_q: 8.873816, mean_eps: 0.100000\n",
      "  74797/175000: episode: 2099, duration: 0.575s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 78.815 [28.000, 213.000], mean observation: 0.123 [0.000, 54.000], loss: 9.722585, mean_absolute_error: 0.636391, mean_q: 7.849809, mean_eps: 0.100000\n",
      "  74831/175000: episode: 2100, duration: 0.587s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 68.235 [0.000, 188.000], mean observation: 0.248 [0.000, 68.000], loss: 17.009051, mean_absolute_error: 0.639848, mean_q: 7.700007, mean_eps: 0.100000\n",
      "  74845/175000: episode: 2101, duration: 0.307s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 73.500 [27.000, 156.000], mean observation: 0.058 [0.000, 28.000], loss: 22.876418, mean_absolute_error: 1.126375, mean_q: 11.092598, mean_eps: 0.100000\n",
      "  74873/175000: episode: 2102, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 81.357 [27.000, 191.000], mean observation: 0.195 [0.000, 56.000], loss: 0.251250, mean_absolute_error: 0.464581, mean_q: 7.158204, mean_eps: 0.100000\n",
      "  74911/175000: episode: 2103, duration: 0.719s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 80.079 [9.000, 213.000], mean observation: 0.265 [0.000, 76.000], loss: 55.061305, mean_absolute_error: 0.883846, mean_q: 8.377126, mean_eps: 0.100000\n",
      "  74924/175000: episode: 2104, duration: 0.307s, episode steps: 13, steps per second: 42, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 112.615 [27.000, 211.000], mean observation: 0.073 [0.000, 26.000], loss: 76.378081, mean_absolute_error: 1.140056, mean_q: 9.599924, mean_eps: 0.100000\n",
      "  74944/175000: episode: 2105, duration: 0.423s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 106.500 [27.000, 216.000], mean observation: 0.120 [0.000, 40.000], loss: 0.243478, mean_absolute_error: 0.424941, mean_q: 6.908809, mean_eps: 0.100000\n",
      "  74987/175000: episode: 2106, duration: 0.819s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.674 [8.000, 219.000], mean observation: 0.457 [0.000, 86.000], loss: 28.778007, mean_absolute_error: 0.933060, mean_q: 9.856596, mean_eps: 0.100000\n",
      "  75027/175000: episode: 2107, duration: 0.704s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 130.450 [27.000, 221.000], mean observation: 0.348 [0.000, 80.000], loss: 20.528145, mean_absolute_error: 0.936835, mean_q: 10.143052, mean_eps: 0.100000\n",
      "  75076/175000: episode: 2108, duration: 0.956s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 165.592 [24.000, 223.000], mean observation: 0.483 [0.000, 98.000], loss: 0.198875, mean_absolute_error: 0.591158, mean_q: 8.233751, mean_eps: 0.100000\n",
      "  75117/175000: episode: 2109, duration: 0.774s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.537 [24.000, 200.000], mean observation: 0.401 [0.000, 82.000], loss: 0.202714, mean_absolute_error: 0.421458, mean_q: 7.062115, mean_eps: 0.100000\n",
      "  75158/175000: episode: 2110, duration: 0.771s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.902 [27.000, 184.000], mean observation: 0.243 [0.000, 82.000], loss: 36.237370, mean_absolute_error: 0.826992, mean_q: 8.908750, mean_eps: 0.100000\n",
      "  75201/175000: episode: 2111, duration: 0.844s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.186 [27.000, 221.000], mean observation: 0.288 [0.000, 86.000], loss: 0.134776, mean_absolute_error: 0.429249, mean_q: 7.042576, mean_eps: 0.100000\n",
      "  75257/175000: episode: 2112, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 162.071 [99.000, 192.000], mean observation: 0.494 [0.000, 112.000], loss: 39.054513, mean_absolute_error: 0.697134, mean_q: 7.752271, mean_eps: 0.100000\n",
      "  75290/175000: episode: 2113, duration: 0.584s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.606 [5.000, 206.000], mean observation: 0.210 [0.000, 66.000], loss: 22.518527, mean_absolute_error: 0.668935, mean_q: 8.050912, mean_eps: 0.100000\n",
      "  75328/175000: episode: 2114, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 84.132 [5.000, 204.000], mean observation: 0.256 [0.000, 76.000], loss: 38.471984, mean_absolute_error: 0.935315, mean_q: 9.705127, mean_eps: 0.100000\n",
      "  75350/175000: episode: 2115, duration: 0.420s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.053 [0.000, 44.000], loss: 1.556375, mean_absolute_error: 0.679649, mean_q: 8.692017, mean_eps: 0.100000\n",
      "  75394/175000: episode: 2116, duration: 0.820s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 126.318 [5.000, 198.000], mean observation: 0.439 [0.000, 88.000], loss: 0.180207, mean_absolute_error: 0.466022, mean_q: 7.430006, mean_eps: 0.100000\n",
      "  75414/175000: episode: 2117, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 137.450 [5.000, 192.000], mean observation: 0.131 [0.000, 40.000], loss: 20.941091, mean_absolute_error: 0.746671, mean_q: 8.732468, mean_eps: 0.100000\n",
      "  75461/175000: episode: 2118, duration: 0.880s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.787 [9.000, 209.000], mean observation: 0.552 [0.000, 94.000], loss: 12.712013, mean_absolute_error: 0.568101, mean_q: 7.566642, mean_eps: 0.100000\n",
      "  75497/175000: episode: 2119, duration: 0.624s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.528 [9.000, 224.000], mean observation: 0.303 [0.000, 72.000], loss: 0.181839, mean_absolute_error: 0.428035, mean_q: 6.994840, mean_eps: 0.100000\n",
      "  75521/175000: episode: 2120, duration: 0.503s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 140.667 [4.000, 216.000], mean observation: 0.153 [0.000, 48.000], loss: 21.054639, mean_absolute_error: 0.922953, mean_q: 9.880095, mean_eps: 0.100000\n",
      "  75565/175000: episode: 2121, duration: 0.882s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.136 [0.000, 198.000], mean observation: 0.385 [0.000, 88.000], loss: 46.219030, mean_absolute_error: 0.863714, mean_q: 8.443392, mean_eps: 0.100000\n",
      "  75587/175000: episode: 2122, duration: 0.380s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 150.545 [39.000, 192.000], mean observation: 0.092 [0.000, 44.000], loss: 0.199981, mean_absolute_error: 0.423729, mean_q: 6.673979, mean_eps: 0.100000\n",
      "  75629/175000: episode: 2123, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.333 [9.000, 193.000], mean observation: 0.298 [0.000, 84.000], loss: 35.244883, mean_absolute_error: 0.791815, mean_q: 8.328273, mean_eps: 0.100000\n",
      "  75657/175000: episode: 2124, duration: 0.493s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 16.214 [9.000, 211.000], mean observation: 0.067 [0.000, 56.000], loss: 17.014893, mean_absolute_error: 0.701870, mean_q: 8.214667, mean_eps: 0.100000\n",
      "  75708/175000: episode: 2125, duration: 0.997s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 125.765 [16.000, 192.000], mean observation: 0.571 [0.000, 102.000], loss: 0.177466, mean_absolute_error: 0.446255, mean_q: 6.805083, mean_eps: 0.100000\n",
      "  75741/175000: episode: 2126, duration: 0.726s, episode steps: 33, steps per second: 45, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 183.455 [86.000, 216.000], mean observation: 0.162 [0.000, 66.000], loss: 25.328467, mean_absolute_error: 0.676055, mean_q: 7.404281, mean_eps: 0.100000\n",
      "  75782/175000: episode: 2127, duration: 0.813s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 179.195 [30.000, 195.000], mean observation: 0.178 [0.000, 82.000], loss: 24.573752, mean_absolute_error: 0.798031, mean_q: 8.418841, mean_eps: 0.100000\n",
      "  75814/175000: episode: 2128, duration: 0.648s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.281 [7.000, 209.000], mean observation: 0.151 [0.000, 64.000], loss: 31.929518, mean_absolute_error: 0.907474, mean_q: 9.005359, mean_eps: 0.100000\n",
      "  75860/175000: episode: 2129, duration: 0.921s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.587 [7.000, 192.000], mean observation: 0.395 [0.000, 92.000], loss: 5.543322, mean_absolute_error: 0.594448, mean_q: 7.636609, mean_eps: 0.100000\n",
      "  75903/175000: episode: 2130, duration: 0.851s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 77.233 [3.000, 207.000], mean observation: 0.399 [0.000, 86.000], loss: 4.858162, mean_absolute_error: 0.567993, mean_q: 7.322278, mean_eps: 0.100000\n",
      "  75939/175000: episode: 2131, duration: 0.660s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 30.139 [6.000, 156.000], mean observation: 0.178 [0.000, 72.000], loss: 10.288419, mean_absolute_error: 0.661223, mean_q: 8.208113, mean_eps: 0.100000\n",
      "  75965/175000: episode: 2132, duration: 0.557s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.269 [1.000, 211.000], mean observation: 0.263 [0.000, 52.000], loss: 0.443100, mean_absolute_error: 0.469831, mean_q: 6.888450, mean_eps: 0.100000\n",
      "  76000/175000: episode: 2133, duration: 0.638s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.143 [5.000, 207.000], mean observation: 0.378 [0.000, 70.000], loss: 13.761317, mean_absolute_error: 0.658232, mean_q: 7.599225, mean_eps: 0.100000\n",
      "  76038/175000: episode: 2134, duration: 0.789s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.368 [5.000, 207.000], mean observation: 0.186 [0.000, 76.000], loss: 4.835594, mean_absolute_error: 0.604857, mean_q: 7.543602, mean_eps: 0.100000\n",
      "  76085/175000: episode: 2135, duration: 0.879s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 19.936 [5.000, 162.000], mean observation: 0.115 [0.000, 94.000], loss: 8.172147, mean_absolute_error: 0.586279, mean_q: 7.286747, mean_eps: 0.100000\n",
      "  76123/175000: episode: 2136, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.842 [5.000, 184.000], mean observation: 0.178 [0.000, 76.000], loss: 18.647166, mean_absolute_error: 0.649121, mean_q: 7.471844, mean_eps: 0.100000\n",
      "  76165/175000: episode: 2137, duration: 0.870s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 38.190 [5.000, 224.000], mean observation: 0.215 [0.000, 84.000], loss: 3.284527, mean_absolute_error: 0.576931, mean_q: 7.688480, mean_eps: 0.100000\n",
      "  76191/175000: episode: 2138, duration: 0.446s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 10.115 [5.000, 138.000], mean observation: 0.062 [0.000, 52.000], loss: 13.536180, mean_absolute_error: 0.722805, mean_q: 8.386852, mean_eps: 0.100000\n",
      "  76216/175000: episode: 2139, duration: 0.580s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 98.400 [5.000, 223.000], mean observation: 0.113 [0.000, 50.000], loss: 0.252849, mean_absolute_error: 0.686350, mean_q: 8.995576, mean_eps: 0.100000\n",
      "  76237/175000: episode: 2140, duration: 0.425s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.571 [38.000, 209.000], mean observation: 0.076 [0.000, 42.000], loss: 39.186298, mean_absolute_error: 1.033189, mean_q: 10.053548, mean_eps: 0.100000\n",
      "  76290/175000: episode: 2141, duration: 1.091s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 135.038 [6.000, 207.000], mean observation: 0.329 [0.000, 106.000], loss: 0.202758, mean_absolute_error: 0.417748, mean_q: 6.654563, mean_eps: 0.100000\n",
      "  76330/175000: episode: 2142, duration: 0.900s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.675 [6.000, 156.000], mean observation: 0.322 [0.000, 80.000], loss: 0.323174, mean_absolute_error: 0.429176, mean_q: 6.834097, mean_eps: 0.100000\n",
      "  76375/175000: episode: 2143, duration: 1.024s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.356 [6.000, 194.000], mean observation: 0.317 [0.000, 90.000], loss: 14.685728, mean_absolute_error: 0.977567, mean_q: 10.315063, mean_eps: 0.100000\n",
      "  76408/175000: episode: 2144, duration: 0.634s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.667 [9.000, 149.000], mean observation: 0.120 [0.000, 66.000], loss: 0.510898, mean_absolute_error: 0.441856, mean_q: 6.675969, mean_eps: 0.100000\n",
      "  76452/175000: episode: 2145, duration: 1.047s, episode steps: 44, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 56.977 [28.000, 216.000], mean observation: 0.363 [0.000, 88.000], loss: 0.543929, mean_absolute_error: 0.444342, mean_q: 6.732817, mean_eps: 0.100000\n",
      "  76483/175000: episode: 2146, duration: 0.640s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.516 [0.000, 223.000], mean observation: 0.264 [0.000, 62.000], loss: 115.437568, mean_absolute_error: 1.416984, mean_q: 10.061588, mean_eps: 0.100000\n",
      "  76519/175000: episode: 2147, duration: 0.712s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 145.722 [28.000, 223.000], mean observation: 0.188 [0.000, 72.000], loss: 73.210287, mean_absolute_error: 1.057393, mean_q: 9.062605, mean_eps: 0.100000\n",
      "  76557/175000: episode: 2148, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.026 [20.000, 223.000], mean observation: 0.280 [0.000, 76.000], loss: 0.691993, mean_absolute_error: 0.589984, mean_q: 8.068216, mean_eps: 0.100000\n",
      "  76585/175000: episode: 2149, duration: 0.537s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 208.929 [30.000, 223.000], mean observation: 0.105 [0.000, 56.000], loss: 54.024885, mean_absolute_error: 1.031992, mean_q: 9.467903, mean_eps: 0.100000\n",
      "  76605/175000: episode: 2150, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 77.800 [27.000, 223.000], mean observation: 0.113 [0.000, 40.000], loss: 79.632421, mean_absolute_error: 1.023101, mean_q: 8.416183, mean_eps: 0.100000\n",
      "  76643/175000: episode: 2151, duration: 0.811s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.684 [27.000, 223.000], mean observation: 0.107 [0.000, 76.000], loss: 0.712083, mean_absolute_error: 0.551284, mean_q: 7.588836, mean_eps: 0.100000\n",
      "  76672/175000: episode: 2152, duration: 0.652s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.103 [88.000, 220.000], mean observation: 0.096 [0.000, 58.000], loss: 7.786076, mean_absolute_error: 0.847034, mean_q: 9.629600, mean_eps: 0.100000\n",
      "  76711/175000: episode: 2153, duration: 0.832s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.615 [2.000, 220.000], mean observation: 0.171 [0.000, 78.000], loss: 32.257761, mean_absolute_error: 0.980149, mean_q: 9.849585, mean_eps: 0.100000\n",
      "  76742/175000: episode: 2154, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 98.323 [46.000, 150.000], mean observation: 0.153 [0.000, 62.000], loss: 10.060100, mean_absolute_error: 0.659364, mean_q: 8.237534, mean_eps: 0.100000\n",
      "  76792/175000: episode: 2155, duration: 0.999s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 116.820 [5.000, 220.000], mean observation: 0.299 [0.000, 100.000], loss: 0.274815, mean_absolute_error: 0.444674, mean_q: 6.834672, mean_eps: 0.100000\n",
      "  76825/175000: episode: 2156, duration: 0.667s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.545 [5.000, 150.000], mean observation: 0.188 [0.000, 66.000], loss: 0.216729, mean_absolute_error: 0.476397, mean_q: 7.083346, mean_eps: 0.100000\n",
      "  76851/175000: episode: 2157, duration: 0.468s, episode steps: 26, steps per second: 56, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 147.154 [82.000, 210.000], mean observation: 0.147 [0.000, 52.000], loss: 8.108506, mean_absolute_error: 0.718392, mean_q: 8.549023, mean_eps: 0.100000\n",
      "  76887/175000: episode: 2158, duration: 0.637s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 141.278 [32.000, 150.000], mean observation: 0.179 [0.000, 72.000], loss: 28.430931, mean_absolute_error: 0.968350, mean_q: 9.868832, mean_eps: 0.100000\n",
      "  76916/175000: episode: 2159, duration: 0.565s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 160.690 [88.000, 195.000], mean observation: 0.201 [0.000, 58.000], loss: 0.236208, mean_absolute_error: 0.456334, mean_q: 6.976318, mean_eps: 0.100000\n",
      "  76948/175000: episode: 2160, duration: 0.636s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 162.156 [141.000, 195.000], mean observation: 0.144 [0.000, 64.000], loss: 5.660075, mean_absolute_error: 0.548124, mean_q: 7.748206, mean_eps: 0.100000\n",
      "  76992/175000: episode: 2161, duration: 0.852s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.568 [16.000, 217.000], mean observation: 0.349 [0.000, 88.000], loss: 0.209744, mean_absolute_error: 0.445171, mean_q: 6.795978, mean_eps: 0.100000\n",
      "  77034/175000: episode: 2162, duration: 0.806s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.333 [23.000, 223.000], mean observation: 0.478 [0.000, 84.000], loss: 7.402396, mean_absolute_error: 0.620228, mean_q: 8.006081, mean_eps: 0.100000\n",
      "  77074/175000: episode: 2163, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 139.025 [28.000, 191.000], mean observation: 0.274 [0.000, 80.000], loss: 0.109844, mean_absolute_error: 0.446134, mean_q: 6.971526, mean_eps: 0.100000\n",
      "  77097/175000: episode: 2164, duration: 0.438s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 36.087 [27.000, 160.000], mean observation: 0.062 [0.000, 46.000], loss: 62.294985, mean_absolute_error: 0.950172, mean_q: 8.712871, mean_eps: 0.100000\n",
      "  77129/175000: episode: 2165, duration: 0.557s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 32.719 [25.000, 212.000], mean observation: 0.125 [0.000, 64.000], loss: 17.243136, mean_absolute_error: 0.858167, mean_q: 9.453856, mean_eps: 0.100000\n",
      "  77145/175000: episode: 2166, duration: 0.296s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 28.062 [27.000, 44.000], mean observation: 0.052 [0.000, 32.000], loss: 115.014564, mean_absolute_error: 1.261837, mean_q: 9.041242, mean_eps: 0.100000\n",
      "  77172/175000: episode: 2167, duration: 0.494s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 36.481 [27.000, 164.000], mean observation: 0.110 [0.000, 54.000], loss: 74.394396, mean_absolute_error: 1.018408, mean_q: 8.734990, mean_eps: 0.100000\n",
      "  77208/175000: episode: 2168, duration: 0.714s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 40.500 [27.000, 206.000], mean observation: 0.123 [0.000, 72.000], loss: 2.531445, mean_absolute_error: 0.473801, mean_q: 7.141344, mean_eps: 0.100000\n",
      "  77246/175000: episode: 2169, duration: 0.705s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 60.342 [8.000, 188.000], mean observation: 0.170 [0.000, 76.000], loss: 1.574884, mean_absolute_error: 0.493013, mean_q: 7.740530, mean_eps: 0.100000\n",
      "  77276/175000: episode: 2170, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 41.267 [14.000, 200.000], mean observation: 0.142 [0.000, 60.000], loss: 0.176147, mean_absolute_error: 0.452461, mean_q: 7.374690, mean_eps: 0.100000\n",
      "  77317/175000: episode: 2171, duration: 0.798s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 46.488 [1.000, 213.000], mean observation: 0.229 [0.000, 82.000], loss: 0.379213, mean_absolute_error: 0.431936, mean_q: 6.922673, mean_eps: 0.100000\n",
      "  77337/175000: episode: 2172, duration: 0.351s, episode steps: 20, steps per second: 57, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.048 [0.000, 40.000], loss: 63.273503, mean_absolute_error: 0.983350, mean_q: 9.067514, mean_eps: 0.100000\n",
      "  77380/175000: episode: 2173, duration: 0.799s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.209 [14.000, 204.000], mean observation: 0.308 [0.000, 86.000], loss: 11.176889, mean_absolute_error: 0.635595, mean_q: 7.914006, mean_eps: 0.100000\n",
      "  77445/175000: episode: 2174, duration: 1.203s, episode steps: 65, steps per second: 54, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 122.862 [16.000, 213.000], mean observation: 0.555 [0.000, 130.000], loss: 0.735650, mean_absolute_error: 0.509370, mean_q: 7.486817, mean_eps: 0.100000\n",
      "  77473/175000: episode: 2175, duration: 0.498s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 35.429 [28.000, 172.000], mean observation: 0.071 [0.000, 56.000], loss: 1.355174, mean_absolute_error: 0.465550, mean_q: 7.187298, mean_eps: 0.100000\n",
      "  77513/175000: episode: 2176, duration: 0.739s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.375 [3.000, 183.000], mean observation: 0.196 [0.000, 80.000], loss: 3.566906, mean_absolute_error: 0.611506, mean_q: 8.177759, mean_eps: 0.100000\n",
      "  77547/175000: episode: 2177, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 34.000 [28.000, 155.000], mean observation: 0.115 [0.000, 68.000], loss: 1.389392, mean_absolute_error: 0.454019, mean_q: 7.085069, mean_eps: 0.100000\n",
      "  77572/175000: episode: 2178, duration: 0.601s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 33.080 [28.000, 155.000], mean observation: 0.061 [0.000, 50.000], loss: 35.547517, mean_absolute_error: 0.825602, mean_q: 8.680229, mean_eps: 0.100000\n",
      "  77607/175000: episode: 2179, duration: 0.734s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.343 [27.000, 195.000], mean observation: 0.111 [0.000, 70.000], loss: 11.015883, mean_absolute_error: 0.625325, mean_q: 7.971013, mean_eps: 0.100000\n",
      "  77649/175000: episode: 2180, duration: 0.827s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.238 [6.000, 195.000], mean observation: 0.154 [0.000, 84.000], loss: 3.164064, mean_absolute_error: 0.584075, mean_q: 8.157425, mean_eps: 0.100000\n",
      "  77678/175000: episode: 2181, duration: 0.636s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 73.379 [24.000, 195.000], mean observation: 0.121 [0.000, 58.000], loss: 42.341274, mean_absolute_error: 0.849007, mean_q: 8.903147, mean_eps: 0.100000\n",
      "  77712/175000: episode: 2182, duration: 0.794s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.559 [24.000, 223.000], mean observation: 0.300 [0.000, 68.000], loss: 34.273825, mean_absolute_error: 0.943134, mean_q: 9.549789, mean_eps: 0.100000\n",
      "  77736/175000: episode: 2183, duration: 0.624s, episode steps: 24, steps per second: 38, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 79.750 [22.000, 216.000], mean observation: 0.120 [0.000, 48.000], loss: 97.919363, mean_absolute_error: 1.684376, mean_q: 13.469561, mean_eps: 0.100000\n",
      "  77771/175000: episode: 2184, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.743 [28.000, 224.000], mean observation: 0.155 [0.000, 70.000], loss: 54.975084, mean_absolute_error: 0.964089, mean_q: 9.210756, mean_eps: 0.100000\n",
      "  77810/175000: episode: 2185, duration: 1.022s, episode steps: 39, steps per second: 38, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 38.231 [28.000, 204.000], mean observation: 0.164 [0.000, 78.000], loss: 0.138686, mean_absolute_error: 0.446661, mean_q: 7.325199, mean_eps: 0.100000\n",
      "  77845/175000: episode: 2186, duration: 0.721s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 38.343 [28.000, 129.000], mean observation: 0.128 [0.000, 70.000], loss: 11.593333, mean_absolute_error: 0.641663, mean_q: 8.346280, mean_eps: 0.100000\n",
      "  77894/175000: episode: 2187, duration: 1.051s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 36.000 [1.000, 200.000], mean observation: 0.249 [0.000, 98.000], loss: 0.176602, mean_absolute_error: 0.452020, mean_q: 7.328081, mean_eps: 0.100000\n",
      "  77932/175000: episode: 2188, duration: 0.826s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 36.316 [28.000, 184.000], mean observation: 0.118 [0.000, 76.000], loss: 52.836832, mean_absolute_error: 0.990418, mean_q: 9.539212, mean_eps: 0.100000\n",
      "  77969/175000: episode: 2189, duration: 0.868s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 55.081 [28.000, 156.000], mean observation: 0.160 [0.000, 74.000], loss: 0.117582, mean_absolute_error: 0.441994, mean_q: 7.125803, mean_eps: 0.100000\n",
      "  77995/175000: episode: 2190, duration: 0.524s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 28.962 [23.000, 58.000], mean observation: 0.093 [0.000, 52.000], loss: 0.105037, mean_absolute_error: 0.449165, mean_q: 7.076525, mean_eps: 0.100000\n",
      "  78033/175000: episode: 2191, duration: 0.915s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 33.237 [19.000, 208.000], mean observation: 0.194 [0.000, 76.000], loss: 16.916457, mean_absolute_error: 0.669497, mean_q: 8.408713, mean_eps: 0.100000\n",
      "  78068/175000: episode: 2192, duration: 0.807s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 55.914 [28.000, 199.000], mean observation: 0.165 [0.000, 70.000], loss: 13.215365, mean_absolute_error: 0.671666, mean_q: 8.263766, mean_eps: 0.100000\n",
      "  78123/175000: episode: 2193, duration: 1.211s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 68.127 [8.000, 174.000], mean observation: 0.434 [0.000, 110.000], loss: 18.926288, mean_absolute_error: 0.729053, mean_q: 8.880575, mean_eps: 0.100000\n",
      "  78167/175000: episode: 2194, duration: 0.923s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 45.614 [28.000, 210.000], mean observation: 0.149 [0.000, 88.000], loss: 0.216022, mean_absolute_error: 0.454904, mean_q: 7.592348, mean_eps: 0.100000\n",
      "  78200/175000: episode: 2195, duration: 0.662s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.061 [28.000, 221.000], mean observation: 0.119 [0.000, 66.000], loss: 82.017513, mean_absolute_error: 1.151015, mean_q: 9.756381, mean_eps: 0.100000\n",
      "  78243/175000: episode: 2196, duration: 1.033s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.860 [28.000, 182.000], mean observation: 0.363 [0.000, 86.000], loss: 23.958486, mean_absolute_error: 0.781681, mean_q: 9.128661, mean_eps: 0.100000\n",
      "  78276/175000: episode: 2197, duration: 0.816s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 39.788 [28.000, 210.000], mean observation: 0.143 [0.000, 66.000], loss: 31.026194, mean_absolute_error: 0.756215, mean_q: 8.665394, mean_eps: 0.100000\n",
      "  78314/175000: episode: 2198, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 56.026 [4.000, 190.000], mean observation: 0.211 [0.000, 76.000], loss: 47.205837, mean_absolute_error: 1.054040, mean_q: 10.296949, mean_eps: 0.100000\n",
      "  78348/175000: episode: 2199, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 39.971 [15.000, 223.000], mean observation: 0.171 [0.000, 68.000], loss: 0.395055, mean_absolute_error: 0.529247, mean_q: 8.138852, mean_eps: 0.100000\n",
      "  78387/175000: episode: 2200, duration: 0.830s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.385 [28.000, 156.000], mean observation: 0.158 [0.000, 78.000], loss: 0.190257, mean_absolute_error: 0.449384, mean_q: 7.559505, mean_eps: 0.100000\n",
      "  78412/175000: episode: 2201, duration: 0.636s, episode steps: 25, steps per second: 39, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 144.560 [28.000, 182.000], mean observation: 0.159 [0.000, 50.000], loss: 0.160526, mean_absolute_error: 0.461221, mean_q: 7.675017, mean_eps: 0.100000\n",
      "  78442/175000: episode: 2202, duration: 0.718s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.400 [28.000, 156.000], mean observation: 0.183 [0.000, 60.000], loss: 0.184858, mean_absolute_error: 0.439083, mean_q: 7.543046, mean_eps: 0.100000\n",
      "  78456/175000: episode: 2203, duration: 0.314s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 111.786 [5.000, 156.000], mean observation: 0.079 [0.000, 28.000], loss: 0.192802, mean_absolute_error: 0.423685, mean_q: 7.316387, mean_eps: 0.100000\n",
      "  78492/175000: episode: 2204, duration: 0.889s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.000 [28.000, 221.000], mean observation: 0.258 [0.000, 72.000], loss: 0.224769, mean_absolute_error: 0.470106, mean_q: 7.687743, mean_eps: 0.100000\n",
      "  78546/175000: episode: 2205, duration: 1.141s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 81.815 [16.000, 210.000], mean observation: 0.533 [0.000, 108.000], loss: 63.450094, mean_absolute_error: 1.007199, mean_q: 9.703346, mean_eps: 0.100000\n",
      "  78580/175000: episode: 2206, duration: 0.669s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.118 [20.000, 156.000], mean observation: 0.232 [0.000, 68.000], loss: 5.693702, mean_absolute_error: 0.556739, mean_q: 8.614956, mean_eps: 0.100000\n",
      "  78614/175000: episode: 2207, duration: 0.827s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.765 [28.000, 197.000], mean observation: 0.232 [0.000, 68.000], loss: 0.441715, mean_absolute_error: 0.471305, mean_q: 7.887000, mean_eps: 0.100000\n",
      "  78636/175000: episode: 2208, duration: 0.525s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 33.364 [18.000, 156.000], mean observation: 0.054 [0.000, 44.000], loss: 89.791389, mean_absolute_error: 1.375810, mean_q: 11.672538, mean_eps: 0.100000\n",
      "  78656/175000: episode: 2209, duration: 0.538s, episode steps: 20, steps per second: 37, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 80.150 [28.000, 213.000], mean observation: 0.070 [0.000, 40.000], loss: 0.233866, mean_absolute_error: 0.498832, mean_q: 8.290585, mean_eps: 0.100000\n",
      "  78709/175000: episode: 2210, duration: 1.227s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.509 [4.000, 196.000], mean observation: 0.321 [0.000, 106.000], loss: 32.586582, mean_absolute_error: 0.712966, mean_q: 8.373075, mean_eps: 0.100000\n",
      "  78751/175000: episode: 2211, duration: 0.918s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 53.238 [28.000, 218.000], mean observation: 0.239 [0.000, 84.000], loss: 45.787474, mean_absolute_error: 0.798941, mean_q: 8.361064, mean_eps: 0.100000\n",
      "  78782/175000: episode: 2212, duration: 0.590s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 29.742 [6.000, 104.000], mean observation: 0.104 [0.000, 62.000], loss: 27.366097, mean_absolute_error: 0.924074, mean_q: 9.883640, mean_eps: 0.100000\n",
      "  78817/175000: episode: 2213, duration: 0.822s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 30.086 [5.000, 156.000], mean observation: 0.110 [0.000, 70.000], loss: 0.165632, mean_absolute_error: 0.487688, mean_q: 7.544779, mean_eps: 0.100000\n",
      "  78855/175000: episode: 2214, duration: 0.783s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 91.605 [5.000, 208.000], mean observation: 0.215 [0.000, 76.000], loss: 0.166075, mean_absolute_error: 0.454920, mean_q: 7.487658, mean_eps: 0.100000\n",
      "  78900/175000: episode: 2215, duration: 1.062s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 145.556 [28.000, 208.000], mean observation: 0.363 [0.000, 90.000], loss: 29.943659, mean_absolute_error: 1.068066, mean_q: 10.990147, mean_eps: 0.100000\n",
      "  78953/175000: episode: 2216, duration: 1.151s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 122.736 [3.000, 208.000], mean observation: 0.385 [0.000, 106.000], loss: 11.012434, mean_absolute_error: 0.627889, mean_q: 8.486992, mean_eps: 0.100000\n",
      "  78978/175000: episode: 2217, duration: 0.494s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 81.560 [28.000, 199.000], mean observation: 0.169 [0.000, 50.000], loss: 0.351670, mean_absolute_error: 0.505625, mean_q: 7.767486, mean_eps: 0.100000\n",
      "  79034/175000: episode: 2218, duration: 1.257s, episode steps: 56, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 103.571 [28.000, 202.000], mean observation: 0.404 [0.000, 112.000], loss: 0.312094, mean_absolute_error: 0.466396, mean_q: 7.714925, mean_eps: 0.100000\n",
      "  79052/175000: episode: 2219, duration: 0.366s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 124.556 [50.000, 156.000], mean observation: 0.086 [0.000, 36.000], loss: 0.237135, mean_absolute_error: 0.470813, mean_q: 7.808732, mean_eps: 0.100000\n",
      "  79081/175000: episode: 2220, duration: 0.687s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 140.483 [80.000, 180.000], mean observation: 0.173 [0.000, 58.000], loss: 4.128229, mean_absolute_error: 0.648788, mean_q: 8.964659, mean_eps: 0.100000\n",
      "  79105/175000: episode: 2221, duration: 0.521s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 128.083 [21.000, 156.000], mean observation: 0.143 [0.000, 48.000], loss: 0.463751, mean_absolute_error: 0.455952, mean_q: 7.419898, mean_eps: 0.100000\n",
      "  79128/175000: episode: 2222, duration: 0.517s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 116.000 [89.000, 156.000], mean observation: 0.080 [0.000, 46.000], loss: 0.110446, mean_absolute_error: 0.455558, mean_q: 7.460796, mean_eps: 0.100000\n",
      "  79160/175000: episode: 2223, duration: 0.738s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 155.906 [49.000, 222.000], mean observation: 0.176 [0.000, 64.000], loss: 0.150432, mean_absolute_error: 0.532357, mean_q: 8.005572, mean_eps: 0.100000\n",
      "  79196/175000: episode: 2224, duration: 0.719s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 133.972 [86.000, 213.000], mean observation: 0.206 [0.000, 72.000], loss: 0.175303, mean_absolute_error: 0.505218, mean_q: 7.788980, mean_eps: 0.100000\n",
      "  79237/175000: episode: 2225, duration: 0.859s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 110.415 [1.000, 215.000], mean observation: 0.232 [0.000, 82.000], loss: 0.598613, mean_absolute_error: 0.515775, mean_q: 7.920213, mean_eps: 0.100000\n",
      "  79271/175000: episode: 2226, duration: 0.637s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 121.324 [0.000, 199.000], mean observation: 0.222 [0.000, 68.000], loss: 2.398636, mean_absolute_error: 0.491376, mean_q: 7.549493, mean_eps: 0.100000\n",
      "  79293/175000: episode: 2227, duration: 0.506s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 119.318 [79.000, 195.000], mean observation: 0.093 [0.000, 44.000], loss: 1.911274, mean_absolute_error: 0.502614, mean_q: 7.484316, mean_eps: 0.100000\n",
      "  79331/175000: episode: 2228, duration: 0.807s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.816 [1.000, 195.000], mean observation: 0.204 [0.000, 76.000], loss: 1.301912, mean_absolute_error: 0.548386, mean_q: 7.980261, mean_eps: 0.100000\n",
      "  79359/175000: episode: 2229, duration: 0.583s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 110.643 [79.000, 156.000], mean observation: 0.133 [0.000, 56.000], loss: 19.659858, mean_absolute_error: 0.613328, mean_q: 8.148609, mean_eps: 0.100000\n",
      "  79409/175000: episode: 2230, duration: 1.100s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.280 [1.000, 223.000], mean observation: 0.453 [0.000, 100.000], loss: 2.192434, mean_absolute_error: 0.541264, mean_q: 8.073777, mean_eps: 0.100000\n",
      "  79456/175000: episode: 2231, duration: 1.019s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 117.404 [5.000, 223.000], mean observation: 0.399 [0.000, 94.000], loss: 1.172098, mean_absolute_error: 0.616564, mean_q: 8.441530, mean_eps: 0.100000\n",
      "  79489/175000: episode: 2232, duration: 0.845s, episode steps: 33, steps per second: 39, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 98.242 [20.000, 156.000], mean observation: 0.204 [0.000, 66.000], loss: 1.216089, mean_absolute_error: 0.520261, mean_q: 8.024208, mean_eps: 0.100000\n",
      "  79525/175000: episode: 2233, duration: 0.646s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 111.472 [56.000, 222.000], mean observation: 0.153 [0.000, 72.000], loss: 18.690110, mean_absolute_error: 0.705975, mean_q: 8.669885, mean_eps: 0.100000\n",
      "  79570/175000: episode: 2234, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.578 [72.000, 223.000], mean observation: 0.309 [0.000, 90.000], loss: 0.940149, mean_absolute_error: 0.514604, mean_q: 7.939281, mean_eps: 0.100000\n",
      "  79582/175000: episode: 2235, duration: 0.303s, episode steps: 12, steps per second: 40, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 83.500 [38.000, 133.000], mean observation: 0.051 [0.000, 24.000], loss: 0.317822, mean_absolute_error: 0.518857, mean_q: 7.918485, mean_eps: 0.100000\n",
      "  79625/175000: episode: 2236, duration: 0.999s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.488 [9.000, 220.000], mean observation: 0.211 [0.000, 86.000], loss: 31.933171, mean_absolute_error: 0.777915, mean_q: 8.860588, mean_eps: 0.100000\n",
      "  79647/175000: episode: 2237, duration: 0.408s, episode steps: 22, steps per second: 54, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 84.682 [9.000, 187.000], mean observation: 0.092 [0.000, 44.000], loss: 2.666606, mean_absolute_error: 0.519168, mean_q: 7.828578, mean_eps: 0.100000\n",
      "  79692/175000: episode: 2238, duration: 0.881s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 113.044 [43.000, 218.000], mean observation: 0.312 [0.000, 90.000], loss: 53.902602, mean_absolute_error: 0.874585, mean_q: 8.784642, mean_eps: 0.100000\n",
      "  79755/175000: episode: 2239, duration: 1.188s, episode steps: 63, steps per second: 53, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.175 [1.000, 208.000], mean observation: 0.580 [0.000, 126.000], loss: 28.682343, mean_absolute_error: 0.715294, mean_q: 8.308714, mean_eps: 0.100000\n",
      "  79783/175000: episode: 2240, duration: 0.487s, episode steps: 28, steps per second: 58, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.643 [15.000, 194.000], mean observation: 0.194 [0.000, 56.000], loss: 1.864996, mean_absolute_error: 0.524641, mean_q: 7.645294, mean_eps: 0.100000\n",
      "  79823/175000: episode: 2241, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 132.675 [40.000, 222.000], mean observation: 0.347 [0.000, 80.000], loss: 1.426082, mean_absolute_error: 0.542908, mean_q: 7.908337, mean_eps: 0.100000\n",
      "  79853/175000: episode: 2242, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 139.667 [79.000, 174.000], mean observation: 0.140 [0.000, 60.000], loss: 1.638730, mean_absolute_error: 0.542296, mean_q: 7.835054, mean_eps: 0.100000\n",
      "  79886/175000: episode: 2243, duration: 0.769s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 156.273 [133.000, 206.000], mean observation: 0.127 [0.000, 66.000], loss: 1.179507, mean_absolute_error: 0.563339, mean_q: 8.134776, mean_eps: 0.100000\n",
      "  79941/175000: episode: 2244, duration: 1.068s, episode steps: 55, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 81.800 [7.000, 200.000], mean observation: 0.504 [0.000, 110.000], loss: 1.186482, mean_absolute_error: 0.541680, mean_q: 7.998979, mean_eps: 0.100000\n",
      "  79978/175000: episode: 2245, duration: 0.785s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.595 [64.000, 215.000], mean observation: 0.261 [0.000, 74.000], loss: 20.035264, mean_absolute_error: 0.737103, mean_q: 8.658447, mean_eps: 0.100000\n",
      "  79994/175000: episode: 2246, duration: 0.403s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 132.312 [52.000, 150.000], mean observation: 0.054 [0.000, 32.000], loss: 3.057430, mean_absolute_error: 0.559828, mean_q: 8.142532, mean_eps: 0.100000\n",
      "  80019/175000: episode: 2247, duration: 0.523s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 150.000 [150.000, 150.000], mean observation: 0.059 [0.000, 50.000], loss: 0.382101, mean_absolute_error: 0.526319, mean_q: 7.585968, mean_eps: 0.100000\n",
      "  80066/175000: episode: 2248, duration: 0.982s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.149 [64.000, 195.000], mean observation: 0.357 [0.000, 94.000], loss: 261.454629, mean_absolute_error: 2.125601, mean_q: 10.953390, mean_eps: 0.100000\n",
      "  80114/175000: episode: 2249, duration: 1.025s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 62.375 [14.000, 182.000], mean observation: 0.285 [0.000, 96.000], loss: 1.129966, mean_absolute_error: 0.529637, mean_q: 7.566607, mean_eps: 0.100000\n",
      "  80154/175000: episode: 2250, duration: 0.762s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.925 [10.000, 168.000], mean observation: 0.357 [0.000, 80.000], loss: 0.999701, mean_absolute_error: 0.497608, mean_q: 7.605030, mean_eps: 0.100000\n",
      "  80185/175000: episode: 2251, duration: 0.609s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.871 [90.000, 165.000], mean observation: 0.122 [0.000, 62.000], loss: 31.467738, mean_absolute_error: 0.821418, mean_q: 8.855335, mean_eps: 0.100000\n",
      "  80227/175000: episode: 2252, duration: 0.752s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 147.429 [45.000, 176.000], mean observation: 0.227 [0.000, 84.000], loss: 109.680364, mean_absolute_error: 1.382328, mean_q: 10.441626, mean_eps: 0.100000\n",
      "  80273/175000: episode: 2253, duration: 0.880s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 148.891 [28.000, 215.000], mean observation: 0.369 [0.000, 92.000], loss: 0.661962, mean_absolute_error: 0.507988, mean_q: 7.383546, mean_eps: 0.100000\n",
      "  80321/175000: episode: 2254, duration: 0.863s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.583 [24.000, 218.000], mean observation: 0.302 [0.000, 96.000], loss: 1.076435, mean_absolute_error: 0.543955, mean_q: 7.636250, mean_eps: 0.100000\n",
      "  80339/175000: episode: 2255, duration: 0.417s, episode steps: 18, steps per second: 43, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 99.111 [28.000, 156.000], mean observation: 0.070 [0.000, 36.000], loss: 0.802519, mean_absolute_error: 0.529700, mean_q: 7.432351, mean_eps: 0.100000\n",
      "  80376/175000: episode: 2256, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 35.865 [28.000, 219.000], mean observation: 0.093 [0.000, 74.000], loss: 0.539173, mean_absolute_error: 0.497699, mean_q: 7.351336, mean_eps: 0.100000\n",
      "  80433/175000: episode: 2257, duration: 1.073s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 73.807 [0.000, 209.000], mean observation: 0.499 [0.000, 114.000], loss: 36.250597, mean_absolute_error: 0.837207, mean_q: 8.793617, mean_eps: 0.100000\n",
      "  80482/175000: episode: 2258, duration: 0.876s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 94.694 [2.000, 205.000], mean observation: 0.513 [0.000, 98.000], loss: 26.543971, mean_absolute_error: 0.725681, mean_q: 8.325041, mean_eps: 0.100000\n",
      "  80522/175000: episode: 2259, duration: 0.718s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 60.150 [8.000, 190.000], mean observation: 0.339 [0.000, 80.000], loss: 15.765971, mean_absolute_error: 0.708457, mean_q: 8.771997, mean_eps: 0.100000\n",
      "  80558/175000: episode: 2260, duration: 0.665s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.750 [28.000, 156.000], mean observation: 0.197 [0.000, 72.000], loss: 79.759160, mean_absolute_error: 0.998049, mean_q: 8.654291, mean_eps: 0.100000\n",
      "  80595/175000: episode: 2261, duration: 0.818s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.486 [15.000, 194.000], mean observation: 0.216 [0.000, 74.000], loss: 0.849509, mean_absolute_error: 0.508128, mean_q: 7.659420, mean_eps: 0.100000\n",
      "  80636/175000: episode: 2262, duration: 0.999s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.268 [48.000, 197.000], mean observation: 0.209 [0.000, 82.000], loss: 94.810754, mean_absolute_error: 1.180119, mean_q: 9.591144, mean_eps: 0.100000\n",
      "  80649/175000: episode: 2263, duration: 0.328s, episode steps: 13, steps per second: 40, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 101.692 [45.000, 209.000], mean observation: 0.050 [0.000, 26.000], loss: 1.588889, mean_absolute_error: 0.488250, mean_q: 7.573715, mean_eps: 0.100000\n",
      "  80662/175000: episode: 2264, duration: 0.302s, episode steps: 13, steps per second: 43, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 96.769 [18.000, 206.000], mean observation: 0.072 [0.000, 26.000], loss: 1.176036, mean_absolute_error: 0.474814, mean_q: 7.516917, mean_eps: 0.100000\n",
      "  80692/175000: episode: 2265, duration: 0.770s, episode steps: 30, steps per second: 39, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.167 [9.000, 156.000], mean observation: 0.136 [0.000, 60.000], loss: 228.598221, mean_absolute_error: 1.890374, mean_q: 10.530818, mean_eps: 0.100000\n",
      "  80704/175000: episode: 2266, duration: 0.350s, episode steps: 12, steps per second: 34, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 99.333 [39.000, 162.000], mean observation: 0.062 [0.000, 24.000], loss: 321.302792, mean_absolute_error: 2.335838, mean_q: 10.804901, mean_eps: 0.100000\n",
      "  80730/175000: episode: 2267, duration: 0.682s, episode steps: 26, steps per second: 38, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.115 [34.000, 156.000], mean observation: 0.119 [0.000, 52.000], loss: 0.724404, mean_absolute_error: 0.495313, mean_q: 7.697802, mean_eps: 0.100000\n",
      "  80767/175000: episode: 2268, duration: 0.816s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.865 [37.000, 211.000], mean observation: 0.202 [0.000, 74.000], loss: 31.887927, mean_absolute_error: 0.650462, mean_q: 7.846342, mean_eps: 0.100000\n",
      "  80822/175000: episode: 2269, duration: 1.355s, episode steps: 55, steps per second: 41, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 69.345 [48.000, 191.000], mean observation: 0.247 [0.000, 110.000], loss: 1.948117, mean_absolute_error: 0.522037, mean_q: 8.084062, mean_eps: 0.100000\n",
      "  80879/175000: episode: 2270, duration: 1.476s, episode steps: 57, steps per second: 39, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 95.158 [24.000, 213.000], mean observation: 0.529 [0.000, 114.000], loss: 1.755997, mean_absolute_error: 0.542194, mean_q: 8.261411, mean_eps: 0.100000\n",
      "  80925/175000: episode: 2271, duration: 1.091s, episode steps: 46, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 79.739 [24.000, 171.000], mean observation: 0.345 [0.000, 92.000], loss: 134.943597, mean_absolute_error: 1.192280, mean_q: 8.494850, mean_eps: 0.100000\n",
      "  80951/175000: episode: 2272, duration: 0.563s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 87.077 [42.000, 212.000], mean observation: 0.128 [0.000, 52.000], loss: 31.327596, mean_absolute_error: 0.852400, mean_q: 9.339862, mean_eps: 0.100000\n",
      "  80966/175000: episode: 2273, duration: 0.364s, episode steps: 15, steps per second: 41, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 42.000 [42.000, 42.000], mean observation: 0.037 [0.000, 30.000], loss: 2.258407, mean_absolute_error: 0.533881, mean_q: 7.897177, mean_eps: 0.100000\n",
      "  81007/175000: episode: 2274, duration: 0.799s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 40.049 [24.000, 149.000], mean observation: 0.291 [0.000, 82.000], loss: 1.220303, mean_absolute_error: 0.505851, mean_q: 7.918692, mean_eps: 0.100000\n",
      "  81028/175000: episode: 2275, duration: 0.453s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 48.667 [44.000, 142.000], mean observation: 0.051 [0.000, 42.000], loss: 1.713806, mean_absolute_error: 0.477196, mean_q: 7.522077, mean_eps: 0.100000\n",
      "  81068/175000: episode: 2276, duration: 0.851s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 46.900 [16.000, 156.000], mean observation: 0.143 [0.000, 80.000], loss: 92.350814, mean_absolute_error: 1.024010, mean_q: 8.671646, mean_eps: 0.100000\n",
      "  81098/175000: episode: 2277, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 51.267 [11.000, 197.000], mean observation: 0.218 [0.000, 60.000], loss: 2.724673, mean_absolute_error: 0.513887, mean_q: 8.051216, mean_eps: 0.100000\n",
      "  81131/175000: episode: 2278, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.515 [28.000, 148.000], mean observation: 0.227 [0.000, 66.000], loss: 1.562858, mean_absolute_error: 0.517422, mean_q: 8.043687, mean_eps: 0.100000\n",
      "  81164/175000: episode: 2279, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 93.333 [27.000, 224.000], mean observation: 0.271 [0.000, 66.000], loss: 271.019101, mean_absolute_error: 2.205657, mean_q: 11.848255, mean_eps: 0.100000\n",
      "  81214/175000: episode: 2280, duration: 0.970s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.720 [1.000, 186.000], mean observation: 0.558 [0.000, 100.000], loss: 51.282329, mean_absolute_error: 0.946867, mean_q: 9.952374, mean_eps: 0.100000\n",
      "  81240/175000: episode: 2281, duration: 0.486s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 86.192 [24.000, 223.000], mean observation: 0.211 [0.000, 52.000], loss: 247.888628, mean_absolute_error: 1.808000, mean_q: 9.685314, mean_eps: 0.100000\n",
      "  81286/175000: episode: 2282, duration: 0.920s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 131.478 [27.000, 223.000], mean observation: 0.471 [0.000, 92.000], loss: 2.405603, mean_absolute_error: 0.634984, mean_q: 9.370491, mean_eps: 0.100000\n",
      "  81325/175000: episode: 2283, duration: 0.748s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.923 [27.000, 223.000], mean observation: 0.241 [0.000, 78.000], loss: 55.914715, mean_absolute_error: 1.026908, mean_q: 10.441189, mean_eps: 0.100000\n",
      "  81344/175000: episode: 2284, duration: 0.361s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 119.842 [27.000, 223.000], mean observation: 0.125 [0.000, 38.000], loss: 108.498397, mean_absolute_error: 1.334061, mean_q: 11.166432, mean_eps: 0.100000\n",
      "  81378/175000: episode: 2285, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 163.559 [27.000, 223.000], mean observation: 0.306 [0.000, 68.000], loss: 0.576087, mean_absolute_error: 0.528783, mean_q: 8.438046, mean_eps: 0.100000\n",
      "  81410/175000: episode: 2286, duration: 0.702s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 150.594 [27.000, 223.000], mean observation: 0.267 [0.000, 64.000], loss: 0.705052, mean_absolute_error: 0.506387, mean_q: 8.165869, mean_eps: 0.100000\n",
      "  81455/175000: episode: 2287, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 164.756 [27.000, 223.000], mean observation: 0.290 [0.000, 90.000], loss: 140.357413, mean_absolute_error: 1.392869, mean_q: 10.047480, mean_eps: 0.100000\n",
      "  81480/175000: episode: 2288, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 140.440 [59.000, 223.000], mean observation: 0.144 [0.000, 50.000], loss: 34.542221, mean_absolute_error: 0.884064, mean_q: 10.114592, mean_eps: 0.100000\n",
      "  81525/175000: episode: 2289, duration: 0.898s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 163.289 [28.000, 223.000], mean observation: 0.468 [0.000, 90.000], loss: 0.171881, mean_absolute_error: 0.511246, mean_q: 8.109496, mean_eps: 0.100000\n",
      "  81576/175000: episode: 2290, duration: 1.025s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 135.725 [2.000, 223.000], mean observation: 0.313 [0.000, 102.000], loss: 72.853015, mean_absolute_error: 1.075435, mean_q: 9.929765, mean_eps: 0.100000\n",
      "  81620/175000: episode: 2291, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.136 [29.000, 223.000], mean observation: 0.228 [0.000, 88.000], loss: 0.538010, mean_absolute_error: 0.536387, mean_q: 8.527173, mean_eps: 0.100000\n",
      "  81657/175000: episode: 2292, duration: 0.726s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 150.270 [3.000, 223.000], mean observation: 0.227 [0.000, 74.000], loss: 0.305670, mean_absolute_error: 0.509534, mean_q: 8.231634, mean_eps: 0.100000\n",
      "  81687/175000: episode: 2293, duration: 0.565s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 162.400 [25.000, 223.000], mean observation: 0.221 [0.000, 60.000], loss: 0.562382, mean_absolute_error: 0.511621, mean_q: 8.202224, mean_eps: 0.100000\n",
      "  81728/175000: episode: 2294, duration: 0.779s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 140.707 [6.000, 223.000], mean observation: 0.309 [0.000, 82.000], loss: 1.661126, mean_absolute_error: 0.556778, mean_q: 8.784200, mean_eps: 0.100000\n",
      "  81751/175000: episode: 2295, duration: 0.448s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 139.652 [27.000, 180.000], mean observation: 0.097 [0.000, 46.000], loss: 1.203378, mean_absolute_error: 0.499523, mean_q: 7.876977, mean_eps: 0.100000\n",
      "  81778/175000: episode: 2296, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 152.481 [27.000, 223.000], mean observation: 0.124 [0.000, 54.000], loss: 171.243228, mean_absolute_error: 1.672262, mean_q: 11.163108, mean_eps: 0.100000\n",
      "  81816/175000: episode: 2297, duration: 0.711s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 69.211 [27.000, 198.000], mean observation: 0.107 [0.000, 76.000], loss: 0.190280, mean_absolute_error: 0.514809, mean_q: 7.982120, mean_eps: 0.100000\n",
      "  81857/175000: episode: 2298, duration: 0.811s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.244 [11.000, 220.000], mean observation: 0.263 [0.000, 82.000], loss: 4.019058, mean_absolute_error: 0.570017, mean_q: 8.193092, mean_eps: 0.100000\n",
      "  81890/175000: episode: 2299, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 119.667 [6.000, 198.000], mean observation: 0.131 [0.000, 66.000], loss: 0.541779, mean_absolute_error: 0.526310, mean_q: 7.850963, mean_eps: 0.100000\n",
      "  81929/175000: episode: 2300, duration: 0.699s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.949 [6.000, 223.000], mean observation: 0.315 [0.000, 78.000], loss: 1.670649, mean_absolute_error: 0.532651, mean_q: 7.796836, mean_eps: 0.100000\n",
      "  81964/175000: episode: 2301, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.743 [59.000, 223.000], mean observation: 0.347 [0.000, 70.000], loss: 0.511451, mean_absolute_error: 0.558406, mean_q: 8.048564, mean_eps: 0.100000\n",
      "  82000/175000: episode: 2302, duration: 0.717s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 200.167 [41.000, 223.000], mean observation: 0.155 [0.000, 72.000], loss: 1.643429, mean_absolute_error: 0.531228, mean_q: 7.578003, mean_eps: 0.100000\n",
      "  82059/175000: episode: 2303, duration: 1.165s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.542 [1.000, 223.000], mean observation: 0.458 [0.000, 118.000], loss: 35.116144, mean_absolute_error: 0.765921, mean_q: 8.252524, mean_eps: 0.100000\n",
      "  82099/175000: episode: 2304, duration: 0.741s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 211.650 [65.000, 223.000], mean observation: 0.129 [0.000, 80.000], loss: 112.074974, mean_absolute_error: 1.181547, mean_q: 9.053073, mean_eps: 0.100000\n",
      "  82117/175000: episode: 2305, duration: 0.354s, episode steps: 18, steps per second: 51, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 179.778 [133.000, 223.000], mean observation: 0.066 [0.000, 36.000], loss: 1.593845, mean_absolute_error: 0.526190, mean_q: 7.726810, mean_eps: 0.100000\n",
      "  82149/175000: episode: 2306, duration: 0.570s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 223.000 [223.000, 223.000], mean observation: 0.075 [0.000, 64.000], loss: 1.234744, mean_absolute_error: 0.534654, mean_q: 7.714791, mean_eps: 0.100000\n",
      "  82178/175000: episode: 2307, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 110.241 [1.000, 223.000], mean observation: 0.183 [0.000, 58.000], loss: 1.053337, mean_absolute_error: 0.532726, mean_q: 7.641684, mean_eps: 0.100000\n",
      "  82209/175000: episode: 2308, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.871 [1.000, 223.000], mean observation: 0.241 [0.000, 62.000], loss: 1.131379, mean_absolute_error: 0.544431, mean_q: 7.885838, mean_eps: 0.100000\n",
      "  82249/175000: episode: 2309, duration: 0.834s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 151.300 [70.000, 215.000], mean observation: 0.309 [0.000, 80.000], loss: 48.953373, mean_absolute_error: 0.892838, mean_q: 8.943767, mean_eps: 0.100000\n",
      "  82286/175000: episode: 2310, duration: 0.646s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.811 [5.000, 169.000], mean observation: 0.201 [0.000, 74.000], loss: 2.534349, mean_absolute_error: 0.509758, mean_q: 7.326066, mean_eps: 0.100000\n",
      "  82315/175000: episode: 2311, duration: 0.511s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 149.655 [27.000, 172.000], mean observation: 0.129 [0.000, 58.000], loss: 195.689216, mean_absolute_error: 1.582989, mean_q: 9.177767, mean_eps: 0.100000\n",
      "  82346/175000: episode: 2312, duration: 0.546s, episode steps: 31, steps per second: 57, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 153.323 [11.000, 169.000], mean observation: 0.137 [0.000, 62.000], loss: 172.743365, mean_absolute_error: 1.579216, mean_q: 9.849679, mean_eps: 0.100000\n",
      "  82383/175000: episode: 2313, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 145.162 [19.000, 181.000], mean observation: 0.221 [0.000, 74.000], loss: 1.376037, mean_absolute_error: 0.530791, mean_q: 7.705863, mean_eps: 0.100000\n",
      "  82414/175000: episode: 2314, duration: 0.623s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.387 [92.000, 182.000], mean observation: 0.194 [0.000, 62.000], loss: 0.762293, mean_absolute_error: 0.546065, mean_q: 7.802207, mean_eps: 0.100000\n",
      "  82448/175000: episode: 2315, duration: 0.708s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.235 [27.000, 169.000], mean observation: 0.187 [0.000, 68.000], loss: 0.284908, mean_absolute_error: 0.543817, mean_q: 7.685466, mean_eps: 0.100000\n",
      "  82499/175000: episode: 2316, duration: 1.017s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 135.373 [13.000, 222.000], mean observation: 0.387 [0.000, 102.000], loss: 10.664764, mean_absolute_error: 0.671133, mean_q: 8.327747, mean_eps: 0.100000\n",
      "  82538/175000: episode: 2317, duration: 0.773s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.744 [9.000, 204.000], mean observation: 0.351 [0.000, 78.000], loss: 90.913439, mean_absolute_error: 1.182654, mean_q: 9.615427, mean_eps: 0.100000\n",
      "  82573/175000: episode: 2318, duration: 0.677s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.371 [28.000, 218.000], mean observation: 0.262 [0.000, 70.000], loss: 0.584730, mean_absolute_error: 0.536489, mean_q: 7.856609, mean_eps: 0.100000\n",
      "  82587/175000: episode: 2319, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 47.500 [15.000, 219.000], mean observation: 0.070 [0.000, 28.000], loss: 0.334392, mean_absolute_error: 0.494889, mean_q: 7.413419, mean_eps: 0.100000\n",
      "  82617/175000: episode: 2320, duration: 0.582s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 108.367 [8.000, 217.000], mean observation: 0.214 [0.000, 60.000], loss: 2.786084, mean_absolute_error: 0.533704, mean_q: 7.808841, mean_eps: 0.100000\n",
      "  82641/175000: episode: 2321, duration: 0.463s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 69.833 [17.000, 163.000], mean observation: 0.061 [0.000, 48.000], loss: 132.464080, mean_absolute_error: 1.331848, mean_q: 9.493225, mean_eps: 0.100000\n",
      "  82675/175000: episode: 2322, duration: 0.575s, episode steps: 34, steps per second: 59, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.941 [68.000, 223.000], mean observation: 0.287 [0.000, 68.000], loss: 0.899178, mean_absolute_error: 0.561415, mean_q: 8.194729, mean_eps: 0.100000\n",
      "  82713/175000: episode: 2323, duration: 0.701s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.553 [68.000, 221.000], mean observation: 0.248 [0.000, 76.000], loss: 1.887676, mean_absolute_error: 0.532344, mean_q: 7.666017, mean_eps: 0.100000\n",
      "  82741/175000: episode: 2324, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.500 [68.000, 200.000], mean observation: 0.124 [0.000, 56.000], loss: 0.467168, mean_absolute_error: 0.551680, mean_q: 8.049722, mean_eps: 0.100000\n",
      "  82778/175000: episode: 2325, duration: 0.649s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.243 [15.000, 200.000], mean observation: 0.192 [0.000, 74.000], loss: 0.426242, mean_absolute_error: 0.495565, mean_q: 7.310665, mean_eps: 0.100000\n",
      "  82819/175000: episode: 2326, duration: 0.715s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 142.439 [15.000, 211.000], mean observation: 0.266 [0.000, 82.000], loss: 143.537064, mean_absolute_error: 1.305758, mean_q: 8.940401, mean_eps: 0.100000\n",
      "  82849/175000: episode: 2327, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 171.533 [37.000, 199.000], mean observation: 0.161 [0.000, 60.000], loss: 0.821319, mean_absolute_error: 0.506530, mean_q: 7.397968, mean_eps: 0.100000\n",
      "  82854/175000: episode: 2328, duration: 0.074s, episode steps: 5, steps per second: 67, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 182.000 [182.000, 182.000], mean observation: 0.015 [0.000, 10.000], loss: 460.187836, mean_absolute_error: 3.713093, mean_q: 16.965183, mean_eps: 0.100000\n",
      "  82910/175000: episode: 2329, duration: 1.084s, episode steps: 56, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 115.143 [15.000, 182.000], mean observation: 0.307 [0.000, 112.000], loss: 0.939602, mean_absolute_error: 0.508354, mean_q: 7.486415, mean_eps: 0.100000\n",
      "  82944/175000: episode: 2330, duration: 0.721s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.971 [15.000, 182.000], mean observation: 0.222 [0.000, 68.000], loss: 88.422049, mean_absolute_error: 1.058512, mean_q: 8.795320, mean_eps: 0.100000\n",
      "  82972/175000: episode: 2331, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.750 [33.000, 135.000], mean observation: 0.076 [0.000, 56.000], loss: 0.405171, mean_absolute_error: 0.487651, mean_q: 7.480550, mean_eps: 0.100000\n",
      "  82998/175000: episode: 2332, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 136.654 [32.000, 213.000], mean observation: 0.115 [0.000, 52.000], loss: 0.341096, mean_absolute_error: 0.495024, mean_q: 7.726760, mean_eps: 0.100000\n",
      "  83028/175000: episode: 2333, duration: 0.577s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.500 [60.000, 144.000], mean observation: 0.088 [0.000, 60.000], loss: 65.719908, mean_absolute_error: 0.995980, mean_q: 9.447239, mean_eps: 0.100000\n",
      "  83045/175000: episode: 2334, duration: 0.346s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 101.647 [15.000, 156.000], mean observation: 0.081 [0.000, 34.000], loss: 0.545043, mean_absolute_error: 0.495876, mean_q: 7.617129, mean_eps: 0.100000\n",
      "  83058/175000: episode: 2335, duration: 0.221s, episode steps: 13, steps per second: 59, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 81.846 [15.000, 156.000], mean observation: 0.054 [0.000, 26.000], loss: 0.235409, mean_absolute_error: 0.504330, mean_q: 8.040500, mean_eps: 0.100000\n",
      "  83098/175000: episode: 2336, duration: 0.743s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.500 [15.000, 211.000], mean observation: 0.337 [0.000, 80.000], loss: 0.397747, mean_absolute_error: 0.503111, mean_q: 7.495123, mean_eps: 0.100000\n",
      "  83149/175000: episode: 2337, duration: 0.936s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.706 [15.000, 195.000], mean observation: 0.553 [0.000, 102.000], loss: 1.177814, mean_absolute_error: 0.502065, mean_q: 7.480797, mean_eps: 0.100000\n",
      "  83174/175000: episode: 2338, duration: 0.437s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.059 [0.000, 50.000], loss: 2.127638, mean_absolute_error: 0.509353, mean_q: 7.460679, mean_eps: 0.100000\n",
      "  83214/175000: episode: 2339, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 34.875 [28.000, 197.000], mean observation: 0.150 [0.000, 80.000], loss: 109.231477, mean_absolute_error: 1.270067, mean_q: 9.574532, mean_eps: 0.100000\n",
      "  83245/175000: episode: 2340, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.355 [28.000, 200.000], mean observation: 0.174 [0.000, 62.000], loss: 61.101968, mean_absolute_error: 1.128536, mean_q: 10.164102, mean_eps: 0.100000\n",
      "  83293/175000: episode: 2341, duration: 1.008s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.833 [0.000, 223.000], mean observation: 0.451 [0.000, 96.000], loss: 0.235643, mean_absolute_error: 0.516577, mean_q: 7.508363, mean_eps: 0.100000\n",
      "  83320/175000: episode: 2342, duration: 0.537s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 75.074 [0.000, 223.000], mean observation: 0.096 [0.000, 54.000], loss: 1.696787, mean_absolute_error: 0.522945, mean_q: 7.402268, mean_eps: 0.100000\n",
      "  83364/175000: episode: 2343, duration: 0.864s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 145.886 [5.000, 223.000], mean observation: 0.332 [0.000, 88.000], loss: 228.634925, mean_absolute_error: 1.838291, mean_q: 9.947520, mean_eps: 0.100000\n",
      "  83411/175000: episode: 2344, duration: 0.965s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 128.638 [28.000, 223.000], mean observation: 0.154 [0.000, 94.000], loss: 23.432440, mean_absolute_error: 0.729799, mean_q: 8.471066, mean_eps: 0.100000\n",
      "  83450/175000: episode: 2345, duration: 0.804s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.949 [28.000, 223.000], mean observation: 0.135 [0.000, 78.000], loss: 102.576674, mean_absolute_error: 1.088993, mean_q: 8.963563, mean_eps: 0.100000\n",
      "  83478/175000: episode: 2346, duration: 0.589s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.071 [27.000, 223.000], mean observation: 0.105 [0.000, 56.000], loss: 5.696153, mean_absolute_error: 0.512692, mean_q: 7.737380, mean_eps: 0.100000\n",
      "  83524/175000: episode: 2347, duration: 0.924s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.587 [7.000, 223.000], mean observation: 0.213 [0.000, 92.000], loss: 83.336926, mean_absolute_error: 1.009851, mean_q: 9.058766, mean_eps: 0.100000\n",
      "  83539/175000: episode: 2348, duration: 0.352s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 39.400 [28.000, 199.000], mean observation: 0.047 [0.000, 30.000], loss: 228.216620, mean_absolute_error: 1.859628, mean_q: 10.456377, mean_eps: 0.100000\n",
      "  83573/175000: episode: 2349, duration: 0.638s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 44.294 [28.000, 208.000], mean observation: 0.102 [0.000, 68.000], loss: 19.468740, mean_absolute_error: 0.758192, mean_q: 9.181904, mean_eps: 0.100000\n",
      "  83619/175000: episode: 2350, duration: 0.859s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 39.674 [28.000, 219.000], mean observation: 0.172 [0.000, 92.000], loss: 59.297549, mean_absolute_error: 1.051384, mean_q: 10.310574, mean_eps: 0.100000\n",
      "  83661/175000: episode: 2351, duration: 0.867s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.333 [28.000, 179.000], mean observation: 0.250 [0.000, 84.000], loss: 0.865862, mean_absolute_error: 0.546517, mean_q: 8.176727, mean_eps: 0.100000\n",
      "  83696/175000: episode: 2352, duration: 0.711s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.657 [5.000, 223.000], mean observation: 0.204 [0.000, 70.000], loss: 0.240497, mean_absolute_error: 0.528378, mean_q: 7.929050, mean_eps: 0.100000\n",
      "  83737/175000: episode: 2353, duration: 0.862s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 158.098 [28.000, 223.000], mean observation: 0.196 [0.000, 82.000], loss: 0.929256, mean_absolute_error: 0.554451, mean_q: 7.869608, mean_eps: 0.100000\n",
      "  83774/175000: episode: 2354, duration: 0.775s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 186.432 [27.000, 223.000], mean observation: 0.200 [0.000, 74.000], loss: 0.342663, mean_absolute_error: 0.581748, mean_q: 8.210847, mean_eps: 0.100000\n",
      "  83805/175000: episode: 2355, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 217.290 [46.000, 223.000], mean observation: 0.107 [0.000, 62.000], loss: 0.219888, mean_absolute_error: 0.508743, mean_q: 7.594464, mean_eps: 0.100000\n",
      "  83850/175000: episode: 2356, duration: 1.009s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 180.311 [68.000, 223.000], mean observation: 0.295 [0.000, 90.000], loss: 0.276066, mean_absolute_error: 0.643240, mean_q: 8.742993, mean_eps: 0.100000\n",
      "  83890/175000: episode: 2357, duration: 0.792s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 158.025 [24.000, 223.000], mean observation: 0.449 [0.000, 80.000], loss: 24.025770, mean_absolute_error: 0.760533, mean_q: 8.916865, mean_eps: 0.100000\n",
      "  83931/175000: episode: 2358, duration: 0.789s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 155.049 [8.000, 218.000], mean observation: 0.413 [0.000, 82.000], loss: 0.459577, mean_absolute_error: 0.534029, mean_q: 7.968855, mean_eps: 0.100000\n",
      "  83969/175000: episode: 2359, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.158 [68.000, 218.000], mean observation: 0.190 [0.000, 76.000], loss: 0.168151, mean_absolute_error: 0.500608, mean_q: 7.595205, mean_eps: 0.100000\n",
      "  84018/175000: episode: 2360, duration: 1.035s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 173.490 [27.000, 218.000], mean observation: 0.326 [0.000, 98.000], loss: 0.223335, mean_absolute_error: 0.515616, mean_q: 7.590151, mean_eps: 0.100000\n",
      "  84063/175000: episode: 2361, duration: 0.891s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 199.978 [84.000, 223.000], mean observation: 0.313 [0.000, 90.000], loss: 86.470508, mean_absolute_error: 0.985437, mean_q: 8.281868, mean_eps: 0.100000\n",
      "  84117/175000: episode: 2362, duration: 1.118s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 170.741 [3.000, 223.000], mean observation: 0.444 [0.000, 108.000], loss: 0.426277, mean_absolute_error: 0.494625, mean_q: 7.427762, mean_eps: 0.100000\n",
      "  84154/175000: episode: 2363, duration: 0.706s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 167.135 [44.000, 223.000], mean observation: 0.324 [0.000, 74.000], loss: 0.337741, mean_absolute_error: 0.496702, mean_q: 7.420149, mean_eps: 0.100000\n",
      "  84202/175000: episode: 2364, duration: 0.955s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 161.000 [1.000, 220.000], mean observation: 0.313 [0.000, 96.000], loss: 11.916905, mean_absolute_error: 0.807459, mean_q: 9.693801, mean_eps: 0.100000\n",
      "  84249/175000: episode: 2365, duration: 0.877s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 156.340 [38.000, 218.000], mean observation: 0.326 [0.000, 94.000], loss: 186.101196, mean_absolute_error: 1.659124, mean_q: 10.126737, mean_eps: 0.100000\n",
      "  84267/175000: episode: 2366, duration: 0.294s, episode steps: 18, steps per second: 61, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 210.556 [84.000, 218.000], mean observation: 0.050 [0.000, 36.000], loss: 0.388232, mean_absolute_error: 0.542229, mean_q: 8.059820, mean_eps: 0.100000\n",
      "  84301/175000: episode: 2367, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 197.559 [1.000, 223.000], mean observation: 0.165 [0.000, 68.000], loss: 270.351977, mean_absolute_error: 2.034372, mean_q: 10.132759, mean_eps: 0.100000\n",
      "  84328/175000: episode: 2368, duration: 0.616s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 194.074 [28.000, 218.000], mean observation: 0.155 [0.000, 54.000], loss: 0.168713, mean_absolute_error: 0.490623, mean_q: 7.434614, mean_eps: 0.100000\n",
      "  84369/175000: episode: 2369, duration: 0.798s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.951 [1.000, 220.000], mean observation: 0.184 [0.000, 82.000], loss: 83.896624, mean_absolute_error: 1.274865, mean_q: 10.901841, mean_eps: 0.100000\n",
      "  84412/175000: episode: 2370, duration: 0.831s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.256 [27.000, 218.000], mean observation: 0.243 [0.000, 86.000], loss: 1.244318, mean_absolute_error: 0.617157, mean_q: 8.617051, mean_eps: 0.100000\n",
      "  84443/175000: episode: 2371, duration: 0.638s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 187.581 [21.000, 223.000], mean observation: 0.213 [0.000, 62.000], loss: 25.786994, mean_absolute_error: 0.814673, mean_q: 9.413703, mean_eps: 0.100000\n",
      "  84459/175000: episode: 2372, duration: 0.334s, episode steps: 16, steps per second: 48, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 203.250 [83.000, 218.000], mean observation: 0.045 [0.000, 32.000], loss: 0.173598, mean_absolute_error: 0.527142, mean_q: 7.953900, mean_eps: 0.100000\n",
      "  84506/175000: episode: 2373, duration: 0.924s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.979 [0.000, 218.000], mean observation: 0.320 [0.000, 94.000], loss: 0.658444, mean_absolute_error: 0.488220, mean_q: 7.613235, mean_eps: 0.100000\n",
      "  84549/175000: episode: 2374, duration: 0.849s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 208.837 [75.000, 218.000], mean observation: 0.140 [0.000, 86.000], loss: 30.296594, mean_absolute_error: 0.877173, mean_q: 9.723721, mean_eps: 0.100000\n",
      "  84588/175000: episode: 2375, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 185.128 [101.000, 223.000], mean observation: 0.180 [0.000, 78.000], loss: 4.857646, mean_absolute_error: 0.482784, mean_q: 7.426526, mean_eps: 0.100000\n",
      "  84637/175000: episode: 2376, duration: 0.973s, episode steps: 49, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.449 [32.000, 218.000], mean observation: 0.289 [0.000, 98.000], loss: 3.738774, mean_absolute_error: 0.691816, mean_q: 9.086033, mean_eps: 0.100000\n",
      "  84672/175000: episode: 2377, duration: 0.673s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.343 [5.000, 223.000], mean observation: 0.394 [0.000, 70.000], loss: 0.426208, mean_absolute_error: 0.499956, mean_q: 7.714679, mean_eps: 0.100000\n",
      "  84702/175000: episode: 2378, duration: 0.593s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.967 [64.000, 223.000], mean observation: 0.189 [0.000, 60.000], loss: 0.161432, mean_absolute_error: 0.534716, mean_q: 7.904541, mean_eps: 0.100000\n",
      "  84735/175000: episode: 2379, duration: 0.589s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 170.909 [20.000, 224.000], mean observation: 0.200 [0.000, 66.000], loss: 69.041360, mean_absolute_error: 0.957280, mean_q: 9.103711, mean_eps: 0.100000\n",
      "  84782/175000: episode: 2380, duration: 1.010s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.277 [15.000, 223.000], mean observation: 0.469 [0.000, 94.000], loss: 135.799779, mean_absolute_error: 1.192082, mean_q: 8.475022, mean_eps: 0.100000\n",
      "  84824/175000: episode: 2381, duration: 0.828s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.262 [15.000, 223.000], mean observation: 0.292 [0.000, 84.000], loss: 0.488996, mean_absolute_error: 0.491069, mean_q: 7.744233, mean_eps: 0.100000\n",
      "  84861/175000: episode: 2382, duration: 0.724s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.189 [15.000, 223.000], mean observation: 0.163 [0.000, 74.000], loss: 0.309252, mean_absolute_error: 0.486607, mean_q: 7.577953, mean_eps: 0.100000\n",
      "  84914/175000: episode: 2383, duration: 1.014s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 160.830 [15.000, 223.000], mean observation: 0.387 [0.000, 106.000], loss: 71.322881, mean_absolute_error: 0.919406, mean_q: 8.721355, mean_eps: 0.100000\n",
      "  84959/175000: episode: 2384, duration: 0.828s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.889 [12.000, 223.000], mean observation: 0.374 [0.000, 90.000], loss: 0.257376, mean_absolute_error: 0.460419, mean_q: 7.574424, mean_eps: 0.100000\n",
      "  84999/175000: episode: 2385, duration: 0.737s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 122.475 [3.000, 223.000], mean observation: 0.441 [0.000, 80.000], loss: 0.503868, mean_absolute_error: 0.615087, mean_q: 8.818742, mean_eps: 0.100000\n",
      "  85047/175000: episode: 2386, duration: 0.882s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.979 [15.000, 223.000], mean observation: 0.406 [0.000, 96.000], loss: 0.398621, mean_absolute_error: 0.488316, mean_q: 7.689701, mean_eps: 0.100000\n",
      "  85087/175000: episode: 2387, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 118.225 [15.000, 207.000], mean observation: 0.268 [0.000, 80.000], loss: 0.266324, mean_absolute_error: 0.483692, mean_q: 7.608816, mean_eps: 0.100000\n",
      "  85130/175000: episode: 2388, duration: 0.860s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 121.581 [3.000, 207.000], mean observation: 0.301 [0.000, 86.000], loss: 148.351097, mean_absolute_error: 1.385244, mean_q: 9.896146, mean_eps: 0.100000\n",
      "  85175/175000: episode: 2389, duration: 0.881s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.111 [15.000, 207.000], mean observation: 0.247 [0.000, 90.000], loss: 0.300842, mean_absolute_error: 0.601552, mean_q: 8.775654, mean_eps: 0.100000\n",
      "  85203/175000: episode: 2390, duration: 0.598s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 134.071 [103.000, 207.000], mean observation: 0.144 [0.000, 56.000], loss: 221.067263, mean_absolute_error: 1.643610, mean_q: 9.339982, mean_eps: 0.100000\n",
      "  85227/175000: episode: 2391, duration: 0.431s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 155.625 [103.000, 207.000], mean observation: 0.101 [0.000, 48.000], loss: 0.311048, mean_absolute_error: 0.474615, mean_q: 7.604729, mean_eps: 0.100000\n",
      "  85265/175000: episode: 2392, duration: 0.734s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.579 [22.000, 207.000], mean observation: 0.225 [0.000, 76.000], loss: 0.422915, mean_absolute_error: 0.469359, mean_q: 7.741198, mean_eps: 0.100000\n",
      "  85314/175000: episode: 2393, duration: 0.847s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.020 [17.000, 207.000], mean observation: 0.564 [0.000, 98.000], loss: 0.778063, mean_absolute_error: 0.473179, mean_q: 7.765555, mean_eps: 0.100000\n",
      "  85334/175000: episode: 2394, duration: 0.357s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 90.700 [40.000, 145.000], mean observation: 0.150 [0.000, 40.000], loss: 293.412511, mean_absolute_error: 2.052716, mean_q: 9.860659, mean_eps: 0.100000\n",
      "  85385/175000: episode: 2395, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.392 [40.000, 192.000], mean observation: 0.176 [0.000, 102.000], loss: 0.363388, mean_absolute_error: 0.468193, mean_q: 7.637370, mean_eps: 0.100000\n",
      "  85423/175000: episode: 2396, duration: 0.700s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.263 [40.000, 202.000], mean observation: 0.108 [0.000, 76.000], loss: 0.692890, mean_absolute_error: 0.449498, mean_q: 7.349309, mean_eps: 0.100000\n",
      "  85460/175000: episode: 2397, duration: 0.686s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 121.973 [22.000, 156.000], mean observation: 0.419 [0.000, 74.000], loss: 0.277199, mean_absolute_error: 0.489896, mean_q: 7.823641, mean_eps: 0.100000\n",
      "  85484/175000: episode: 2398, duration: 0.517s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 121.833 [40.000, 177.000], mean observation: 0.221 [0.000, 48.000], loss: 0.394105, mean_absolute_error: 0.473568, mean_q: 7.372667, mean_eps: 0.100000\n",
      "  85505/175000: episode: 2399, duration: 0.469s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.333 [68.000, 177.000], mean observation: 0.121 [0.000, 42.000], loss: 1.007458, mean_absolute_error: 0.486273, mean_q: 7.193049, mean_eps: 0.100000\n",
      "  85556/175000: episode: 2400, duration: 1.000s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.000 [20.000, 223.000], mean observation: 0.370 [0.000, 102.000], loss: 2.183699, mean_absolute_error: 0.479711, mean_q: 7.350345, mean_eps: 0.100000\n",
      "  85588/175000: episode: 2401, duration: 0.695s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 48.969 [9.000, 224.000], mean observation: 0.147 [0.000, 64.000], loss: 2.469721, mean_absolute_error: 0.501028, mean_q: 7.393282, mean_eps: 0.100000\n",
      "  85603/175000: episode: 2402, duration: 0.291s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 49.267 [43.000, 137.000], mean observation: 0.037 [0.000, 30.000], loss: 1.703716, mean_absolute_error: 0.470723, mean_q: 7.366169, mean_eps: 0.100000\n",
      "  85637/175000: episode: 2403, duration: 0.615s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.000 [36.000, 223.000], mean observation: 0.274 [0.000, 68.000], loss: 0.844587, mean_absolute_error: 0.469012, mean_q: 7.326246, mean_eps: 0.100000\n",
      "  85689/175000: episode: 2404, duration: 0.942s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 139.635 [2.000, 223.000], mean observation: 0.597 [0.000, 104.000], loss: 56.499636, mean_absolute_error: 0.864046, mean_q: 8.502220, mean_eps: 0.100000\n",
      "  85718/175000: episode: 2405, duration: 0.561s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.345 [5.000, 223.000], mean observation: 0.197 [0.000, 58.000], loss: 1.071922, mean_absolute_error: 0.477396, mean_q: 7.278421, mean_eps: 0.100000\n",
      "  85735/175000: episode: 2406, duration: 0.318s, episode steps: 17, steps per second: 53, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 88.000 [88.000, 88.000], mean observation: 0.041 [0.000, 34.000], loss: 191.099921, mean_absolute_error: 1.682241, mean_q: 10.099900, mean_eps: 0.100000\n",
      "  85751/175000: episode: 2407, duration: 0.310s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 110.875 [88.000, 149.000], mean observation: 0.044 [0.000, 32.000], loss: 0.510797, mean_absolute_error: 0.471428, mean_q: 7.223167, mean_eps: 0.100000\n",
      "  85788/175000: episode: 2408, duration: 0.784s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.784 [17.000, 223.000], mean observation: 0.164 [0.000, 74.000], loss: 0.261373, mean_absolute_error: 0.483488, mean_q: 7.343756, mean_eps: 0.100000\n",
      "  85823/175000: episode: 2409, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.000 [5.000, 223.000], mean observation: 0.313 [0.000, 70.000], loss: 0.287084, mean_absolute_error: 0.492282, mean_q: 7.538275, mean_eps: 0.100000\n",
      "  85843/175000: episode: 2410, duration: 0.428s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 108.000 [5.000, 192.000], mean observation: 0.105 [0.000, 40.000], loss: 0.170405, mean_absolute_error: 0.464787, mean_q: 7.195507, mean_eps: 0.100000\n",
      "  85874/175000: episode: 2411, duration: 0.662s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 133.871 [5.000, 191.000], mean observation: 0.133 [0.000, 62.000], loss: 1.638072, mean_absolute_error: 0.472543, mean_q: 7.155114, mean_eps: 0.100000\n",
      "  85918/175000: episode: 2412, duration: 0.874s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.636 [5.000, 217.000], mean observation: 0.232 [0.000, 88.000], loss: 1.608796, mean_absolute_error: 0.468678, mean_q: 7.078382, mean_eps: 0.100000\n",
      "  85960/175000: episode: 2413, duration: 0.924s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 70.690 [18.000, 195.000], mean observation: 0.540 [0.000, 84.000], loss: 90.704927, mean_absolute_error: 1.143265, mean_q: 9.640649, mean_eps: 0.100000\n",
      "  85984/175000: episode: 2414, duration: 0.511s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 116.333 [15.000, 191.000], mean observation: 0.090 [0.000, 48.000], loss: 1.338267, mean_absolute_error: 0.458372, mean_q: 7.240319, mean_eps: 0.100000\n",
      "  86021/175000: episode: 2415, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.649 [4.000, 223.000], mean observation: 0.280 [0.000, 74.000], loss: 3.160050, mean_absolute_error: 0.477376, mean_q: 7.721332, mean_eps: 0.100000\n",
      "  86061/175000: episode: 2416, duration: 0.748s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 203.375 [68.000, 223.000], mean observation: 0.101 [0.000, 80.000], loss: 106.286729, mean_absolute_error: 1.072012, mean_q: 8.818841, mean_eps: 0.100000\n",
      "  86091/175000: episode: 2417, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 59.733 [15.000, 184.000], mean observation: 0.141 [0.000, 60.000], loss: 0.401683, mean_absolute_error: 0.594890, mean_q: 8.564712, mean_eps: 0.100000\n",
      "  86142/175000: episode: 2418, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 51.686 [5.000, 223.000], mean observation: 0.258 [0.000, 102.000], loss: 0.174512, mean_absolute_error: 0.455412, mean_q: 7.574765, mean_eps: 0.100000\n",
      "  86151/175000: episode: 2419, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 37.444 [15.000, 217.000], mean observation: 0.037 [0.000, 18.000], loss: 0.924348, mean_absolute_error: 0.434718, mean_q: 7.300991, mean_eps: 0.100000\n",
      "  86182/175000: episode: 2420, duration: 0.632s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 46.742 [15.000, 202.000], mean observation: 0.129 [0.000, 62.000], loss: 0.362198, mean_absolute_error: 0.526134, mean_q: 8.181639, mean_eps: 0.100000\n",
      "  86220/175000: episode: 2421, duration: 0.737s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.342 [5.000, 222.000], mean observation: 0.311 [0.000, 76.000], loss: 71.028234, mean_absolute_error: 0.919723, mean_q: 8.836143, mean_eps: 0.100000\n",
      "  86256/175000: episode: 2422, duration: 0.711s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 79.444 [15.000, 202.000], mean observation: 0.241 [0.000, 72.000], loss: 0.959331, mean_absolute_error: 0.442230, mean_q: 7.485656, mean_eps: 0.100000\n",
      "  86296/175000: episode: 2423, duration: 0.767s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 120.000 [15.000, 218.000], mean observation: 0.332 [0.000, 80.000], loss: 1.143529, mean_absolute_error: 0.452455, mean_q: 7.473840, mean_eps: 0.100000\n",
      "  86331/175000: episode: 2424, duration: 0.646s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 127.143 [28.000, 218.000], mean observation: 0.179 [0.000, 70.000], loss: 1.521738, mean_absolute_error: 0.448189, mean_q: 7.345949, mean_eps: 0.100000\n",
      "  86370/175000: episode: 2425, duration: 0.773s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.026 [15.000, 222.000], mean observation: 0.395 [0.000, 78.000], loss: 3.430302, mean_absolute_error: 0.487808, mean_q: 7.700622, mean_eps: 0.100000\n",
      "  86406/175000: episode: 2426, duration: 0.656s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.611 [15.000, 222.000], mean observation: 0.245 [0.000, 72.000], loss: 0.934616, mean_absolute_error: 0.476229, mean_q: 7.402344, mean_eps: 0.100000\n",
      "  86432/175000: episode: 2427, duration: 0.525s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 56.500 [15.000, 223.000], mean observation: 0.109 [0.000, 52.000], loss: 58.430319, mean_absolute_error: 0.976192, mean_q: 9.419541, mean_eps: 0.100000\n",
      "  86454/175000: episode: 2428, duration: 0.425s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 183.455 [15.000, 223.000], mean observation: 0.091 [0.000, 44.000], loss: 107.316300, mean_absolute_error: 1.203134, mean_q: 9.451817, mean_eps: 0.100000\n",
      "  86482/175000: episode: 2429, duration: 0.525s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.250 [15.000, 223.000], mean observation: 0.130 [0.000, 56.000], loss: 0.711521, mean_absolute_error: 0.509542, mean_q: 7.731705, mean_eps: 0.100000\n",
      "  86518/175000: episode: 2430, duration: 0.642s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.028 [71.000, 223.000], mean observation: 0.407 [0.000, 72.000], loss: 7.684815, mean_absolute_error: 0.665193, mean_q: 8.812607, mean_eps: 0.100000\n",
      "  86561/175000: episode: 2431, duration: 0.816s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.279 [15.000, 202.000], mean observation: 0.400 [0.000, 86.000], loss: 0.879149, mean_absolute_error: 0.735741, mean_q: 9.817051, mean_eps: 0.100000\n",
      "  86600/175000: episode: 2432, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 77.462 [15.000, 191.000], mean observation: 0.263 [0.000, 78.000], loss: 47.118861, mean_absolute_error: 0.819167, mean_q: 8.746654, mean_eps: 0.100000\n",
      "  86627/175000: episode: 2433, duration: 0.521s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.667 [15.000, 194.000], mean observation: 0.250 [0.000, 54.000], loss: 0.567962, mean_absolute_error: 0.456736, mean_q: 7.571638, mean_eps: 0.100000\n",
      "  86655/175000: episode: 2434, duration: 0.511s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.000 [15.000, 191.000], mean observation: 0.266 [0.000, 56.000], loss: 191.755778, mean_absolute_error: 1.712254, mean_q: 10.845588, mean_eps: 0.100000\n",
      "  86681/175000: episode: 2435, duration: 0.499s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 59.462 [28.000, 213.000], mean observation: 0.112 [0.000, 52.000], loss: 0.676653, mean_absolute_error: 0.436485, mean_q: 7.459019, mean_eps: 0.100000\n",
      "  86732/175000: episode: 2436, duration: 1.003s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 36.118 [15.000, 218.000], mean observation: 0.136 [0.000, 102.000], loss: 30.358702, mean_absolute_error: 0.698266, mean_q: 8.696458, mean_eps: 0.100000\n",
      "  86785/175000: episode: 2437, duration: 1.025s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 80.792 [7.000, 165.000], mean observation: 0.309 [0.000, 106.000], loss: 1.412508, mean_absolute_error: 0.466263, mean_q: 8.083090, mean_eps: 0.100000\n",
      "  86833/175000: episode: 2438, duration: 0.877s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 163.146 [28.000, 223.000], mean observation: 0.360 [0.000, 96.000], loss: 0.256018, mean_absolute_error: 0.461373, mean_q: 8.144729, mean_eps: 0.100000\n",
      "  86874/175000: episode: 2439, duration: 0.726s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.780 [28.000, 223.000], mean observation: 0.335 [0.000, 82.000], loss: 91.730851, mean_absolute_error: 1.014890, mean_q: 9.348973, mean_eps: 0.100000\n",
      "  86917/175000: episode: 2440, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 159.302 [28.000, 223.000], mean observation: 0.145 [0.000, 86.000], loss: 31.875166, mean_absolute_error: 0.743064, mean_q: 9.200145, mean_eps: 0.100000\n",
      "  86954/175000: episode: 2441, duration: 0.751s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 123.135 [57.000, 223.000], mean observation: 0.234 [0.000, 74.000], loss: 0.243292, mean_absolute_error: 0.467146, mean_q: 8.226702, mean_eps: 0.100000\n",
      "  87008/175000: episode: 2442, duration: 1.034s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 121.333 [7.000, 223.000], mean observation: 0.332 [0.000, 108.000], loss: 107.705480, mean_absolute_error: 1.061421, mean_q: 9.384270, mean_eps: 0.100000\n",
      "  87042/175000: episode: 2443, duration: 0.653s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 51.471 [11.000, 195.000], mean observation: 0.240 [0.000, 68.000], loss: 63.963748, mean_absolute_error: 0.734944, mean_q: 7.991534, mean_eps: 0.100000\n",
      "  87076/175000: episode: 2444, duration: 0.671s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 46.912 [28.000, 201.000], mean observation: 0.138 [0.000, 68.000], loss: 55.659641, mean_absolute_error: 0.887467, mean_q: 9.356426, mean_eps: 0.100000\n",
      "  87112/175000: episode: 2445, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 45.250 [6.000, 217.000], mean observation: 0.149 [0.000, 72.000], loss: 61.248158, mean_absolute_error: 1.075916, mean_q: 10.711115, mean_eps: 0.100000\n",
      "  87145/175000: episode: 2446, duration: 0.721s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 55.970 [10.000, 217.000], mean observation: 0.275 [0.000, 66.000], loss: 1.392606, mean_absolute_error: 0.722529, mean_q: 10.214268, mean_eps: 0.100000\n",
      "  87180/175000: episode: 2447, duration: 0.717s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 34.000 [28.000, 126.000], mean observation: 0.121 [0.000, 70.000], loss: 0.337358, mean_absolute_error: 0.485109, mean_q: 8.418480, mean_eps: 0.100000\n",
      "  87204/175000: episode: 2448, duration: 0.517s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.057 [0.000, 48.000], loss: 0.301298, mean_absolute_error: 0.498044, mean_q: 8.665406, mean_eps: 0.100000\n",
      "  87227/175000: episode: 2449, duration: 0.488s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.217 [6.000, 214.000], mean observation: 0.096 [0.000, 46.000], loss: 210.466108, mean_absolute_error: 1.413286, mean_q: 8.416291, mean_eps: 0.100000\n",
      "  87273/175000: episode: 2450, duration: 0.913s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.130 [27.000, 214.000], mean observation: 0.489 [0.000, 92.000], loss: 0.352379, mean_absolute_error: 0.596461, mean_q: 9.209276, mean_eps: 0.100000\n",
      "  87306/175000: episode: 2451, duration: 0.663s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.515 [28.000, 191.000], mean observation: 0.229 [0.000, 66.000], loss: 0.290795, mean_absolute_error: 0.470726, mean_q: 7.801361, mean_eps: 0.100000\n",
      "  87349/175000: episode: 2452, duration: 0.834s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.465 [5.000, 191.000], mean observation: 0.274 [0.000, 86.000], loss: 6.681667, mean_absolute_error: 0.674203, mean_q: 9.189968, mean_eps: 0.100000\n",
      "  87370/175000: episode: 2453, duration: 0.402s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 36.143 [28.000, 199.000], mean observation: 0.060 [0.000, 42.000], loss: 0.181966, mean_absolute_error: 0.526135, mean_q: 8.305267, mean_eps: 0.100000\n",
      "  87388/175000: episode: 2454, duration: 0.389s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 39.944 [28.000, 221.000], mean observation: 0.066 [0.000, 36.000], loss: 0.438113, mean_absolute_error: 0.519587, mean_q: 8.156442, mean_eps: 0.100000\n",
      "  87448/175000: episode: 2455, duration: 1.267s, episode steps: 60, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 134.517 [5.000, 215.000], mean observation: 0.502 [0.000, 120.000], loss: 52.255486, mean_absolute_error: 0.910677, mean_q: 9.529406, mean_eps: 0.100000\n",
      "  87473/175000: episode: 2456, duration: 0.564s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 29.800 [19.000, 82.000], mean observation: 0.074 [0.000, 50.000], loss: 182.327471, mean_absolute_error: 1.487294, mean_q: 9.450946, mean_eps: 0.100000\n",
      "  87485/175000: episode: 2457, duration: 0.246s, episode steps: 12, steps per second: 49, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 26.000 [4.000, 28.000], mean observation: 0.035 [0.000, 24.000], loss: 0.107792, mean_absolute_error: 0.542578, mean_q: 8.769643, mean_eps: 0.100000\n",
      "  87519/175000: episode: 2458, duration: 0.689s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 29.824 [21.000, 97.000], mean observation: 0.083 [0.000, 68.000], loss: 0.374673, mean_absolute_error: 0.656632, mean_q: 9.407515, mean_eps: 0.100000\n",
      "  87553/175000: episode: 2459, duration: 0.691s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.382 [7.000, 204.000], mean observation: 0.166 [0.000, 68.000], loss: 335.300790, mean_absolute_error: 2.295189, mean_q: 10.650076, mean_eps: 0.100000\n",
      "  87587/175000: episode: 2460, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.265 [28.000, 218.000], mean observation: 0.121 [0.000, 68.000], loss: 0.230373, mean_absolute_error: 0.463567, mean_q: 7.856064, mean_eps: 0.100000\n",
      "  87622/175000: episode: 2461, duration: 0.728s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.629 [28.000, 218.000], mean observation: 0.093 [0.000, 70.000], loss: 140.716263, mean_absolute_error: 1.430814, mean_q: 10.709095, mean_eps: 0.100000\n",
      "  87653/175000: episode: 2462, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 105.710 [7.000, 218.000], mean observation: 0.209 [0.000, 62.000], loss: 0.546300, mean_absolute_error: 0.474800, mean_q: 8.069906, mean_eps: 0.100000\n",
      "  87685/175000: episode: 2463, duration: 0.668s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 28.844 [28.000, 55.000], mean observation: 0.122 [0.000, 64.000], loss: 0.187287, mean_absolute_error: 0.485710, mean_q: 8.057287, mean_eps: 0.100000\n",
      "  87716/175000: episode: 2464, duration: 0.656s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 115.290 [14.000, 218.000], mean observation: 0.171 [0.000, 62.000], loss: 170.020862, mean_absolute_error: 1.856954, mean_q: 13.173429, mean_eps: 0.100000\n",
      "  87731/175000: episode: 2465, duration: 0.302s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 93.067 [28.000, 191.000], mean observation: 0.072 [0.000, 30.000], loss: 0.181074, mean_absolute_error: 0.783600, mean_q: 10.495355, mean_eps: 0.100000\n",
      "  87752/175000: episode: 2466, duration: 0.457s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 93.524 [5.000, 215.000], mean observation: 0.149 [0.000, 42.000], loss: 0.114107, mean_absolute_error: 0.454149, mean_q: 7.650917, mean_eps: 0.100000\n",
      "  87790/175000: episode: 2467, duration: 0.805s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 70.842 [26.000, 218.000], mean observation: 0.284 [0.000, 76.000], loss: 5.101196, mean_absolute_error: 0.660815, mean_q: 9.254169, mean_eps: 0.100000\n",
      "  87826/175000: episode: 2468, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 51.333 [28.000, 218.000], mean observation: 0.102 [0.000, 72.000], loss: 0.202190, mean_absolute_error: 0.517713, mean_q: 8.059381, mean_eps: 0.100000\n",
      "  87856/175000: episode: 2469, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 91.967 [5.000, 218.000], mean observation: 0.202 [0.000, 60.000], loss: 0.372613, mean_absolute_error: 0.487457, mean_q: 7.810173, mean_eps: 0.100000\n",
      "  87906/175000: episode: 2470, duration: 0.990s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.840 [27.000, 217.000], mean observation: 0.425 [0.000, 100.000], loss: 170.757646, mean_absolute_error: 1.703331, mean_q: 11.705861, mean_eps: 0.100000\n",
      "  87946/175000: episode: 2471, duration: 0.882s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 87.300 [0.000, 221.000], mean observation: 0.295 [0.000, 80.000], loss: 0.127544, mean_absolute_error: 0.493402, mean_q: 7.715311, mean_eps: 0.100000\n",
      "  87985/175000: episode: 2472, duration: 0.753s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.667 [28.000, 218.000], mean observation: 0.245 [0.000, 78.000], loss: 1.314322, mean_absolute_error: 0.634030, mean_q: 8.953956, mean_eps: 0.100000\n",
      "  88032/175000: episode: 2473, duration: 0.955s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.489 [28.000, 184.000], mean observation: 0.245 [0.000, 94.000], loss: 134.685053, mean_absolute_error: 1.597686, mean_q: 12.104947, mean_eps: 0.100000\n",
      "  88066/175000: episode: 2474, duration: 0.705s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 33.088 [28.000, 201.000], mean observation: 0.111 [0.000, 68.000], loss: 1.377372, mean_absolute_error: 0.685115, mean_q: 9.565354, mean_eps: 0.100000\n",
      "  88110/175000: episode: 2475, duration: 0.886s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 97.091 [28.000, 191.000], mean observation: 0.183 [0.000, 88.000], loss: 81.414192, mean_absolute_error: 0.962248, mean_q: 8.830287, mean_eps: 0.100000\n",
      "  88150/175000: episode: 2476, duration: 0.850s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.750 [39.000, 192.000], mean observation: 0.305 [0.000, 80.000], loss: 0.383325, mean_absolute_error: 0.461920, mean_q: 7.833321, mean_eps: 0.100000\n",
      "  88199/175000: episode: 2477, duration: 0.935s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 89.571 [3.000, 204.000], mean observation: 0.478 [0.000, 98.000], loss: 0.475822, mean_absolute_error: 0.510838, mean_q: 8.559826, mean_eps: 0.100000\n",
      "  88238/175000: episode: 2478, duration: 0.838s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.231 [27.000, 211.000], mean observation: 0.427 [0.000, 78.000], loss: 123.190334, mean_absolute_error: 1.219605, mean_q: 10.077643, mean_eps: 0.100000\n",
      "  88276/175000: episode: 2479, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.105 [9.000, 224.000], mean observation: 0.397 [0.000, 76.000], loss: 0.207319, mean_absolute_error: 0.489523, mean_q: 8.220252, mean_eps: 0.100000\n",
      "  88330/175000: episode: 2480, duration: 1.098s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 88.222 [6.000, 177.000], mean observation: 0.603 [0.000, 108.000], loss: 2.950122, mean_absolute_error: 0.486906, mean_q: 8.105371, mean_eps: 0.100000\n",
      "  88369/175000: episode: 2481, duration: 0.822s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.154 [2.000, 191.000], mean observation: 0.255 [0.000, 78.000], loss: 0.348317, mean_absolute_error: 0.517231, mean_q: 8.544375, mean_eps: 0.100000\n",
      "  88423/175000: episode: 2482, duration: 1.061s, episode steps: 54, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.370 [7.000, 221.000], mean observation: 0.684 [0.000, 108.000], loss: 100.681623, mean_absolute_error: 1.118051, mean_q: 9.762084, mean_eps: 0.100000\n",
      "  88461/175000: episode: 2483, duration: 0.785s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.132 [9.000, 184.000], mean observation: 0.105 [0.000, 76.000], loss: 0.356365, mean_absolute_error: 0.466876, mean_q: 7.817116, mean_eps: 0.100000\n",
      "  88488/175000: episode: 2484, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 163.222 [15.000, 184.000], mean observation: 0.079 [0.000, 54.000], loss: 105.352841, mean_absolute_error: 1.378496, mean_q: 11.709401, mean_eps: 0.100000\n",
      "  88527/175000: episode: 2485, duration: 0.757s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.385 [5.000, 191.000], mean observation: 0.333 [0.000, 78.000], loss: 0.285723, mean_absolute_error: 0.441191, mean_q: 7.730347, mean_eps: 0.100000\n",
      "  88565/175000: episode: 2486, duration: 0.815s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.658 [5.000, 202.000], mean observation: 0.275 [0.000, 76.000], loss: 98.533644, mean_absolute_error: 1.143638, mean_q: 10.171649, mean_eps: 0.100000\n",
      "  88618/175000: episode: 2487, duration: 1.135s, episode steps: 53, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 80.547 [5.000, 191.000], mean observation: 0.669 [0.000, 106.000], loss: 3.764429, mean_absolute_error: 0.480014, mean_q: 7.978821, mean_eps: 0.100000\n",
      "  88649/175000: episode: 2488, duration: 0.684s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 25.065 [15.000, 221.000], mean observation: 0.114 [0.000, 62.000], loss: 118.993635, mean_absolute_error: 1.360903, mean_q: 11.021153, mean_eps: 0.100000\n",
      "  88676/175000: episode: 2489, duration: 0.618s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 25.074 [15.000, 139.000], mean observation: 0.065 [0.000, 54.000], loss: 204.974618, mean_absolute_error: 1.862041, mean_q: 12.028647, mean_eps: 0.100000\n",
      "  88719/175000: episode: 2490, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 23.512 [15.000, 136.000], mean observation: 0.182 [0.000, 86.000], loss: 149.481693, mean_absolute_error: 1.247917, mean_q: 8.860095, mean_eps: 0.100000\n",
      "  88763/175000: episode: 2491, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.045 [15.000, 220.000], mean observation: 0.604 [0.000, 88.000], loss: 0.321108, mean_absolute_error: 0.437211, mean_q: 7.494477, mean_eps: 0.100000\n",
      "  88799/175000: episode: 2492, duration: 0.665s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 42.278 [27.000, 218.000], mean observation: 0.128 [0.000, 72.000], loss: 61.720729, mean_absolute_error: 0.901873, mean_q: 9.182836, mean_eps: 0.100000\n",
      "  88821/175000: episode: 2493, duration: 0.510s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 27.409 [15.000, 28.000], mean observation: 0.082 [0.000, 44.000], loss: 30.219776, mean_absolute_error: 0.833341, mean_q: 9.687392, mean_eps: 0.100000\n",
      "  88864/175000: episode: 2494, duration: 0.855s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 54.093 [27.000, 144.000], mean observation: 0.172 [0.000, 86.000], loss: 19.271317, mean_absolute_error: 0.705968, mean_q: 9.028720, mean_eps: 0.100000\n",
      "  88907/175000: episode: 2495, duration: 0.907s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 52.163 [28.000, 217.000], mean observation: 0.251 [0.000, 86.000], loss: 73.408814, mean_absolute_error: 0.908958, mean_q: 8.832241, mean_eps: 0.100000\n",
      "  88944/175000: episode: 2496, duration: 0.880s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 41.054 [12.000, 187.000], mean observation: 0.139 [0.000, 74.000], loss: 173.764122, mean_absolute_error: 1.571386, mean_q: 10.732727, mean_eps: 0.100000\n",
      "  88986/175000: episode: 2497, duration: 0.906s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 49.333 [28.000, 191.000], mean observation: 0.309 [0.000, 84.000], loss: 106.776105, mean_absolute_error: 1.096987, mean_q: 9.178472, mean_eps: 0.100000\n",
      "  89036/175000: episode: 2498, duration: 1.046s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 39.680 [28.000, 199.000], mean observation: 0.187 [0.000, 100.000], loss: 141.631939, mean_absolute_error: 1.345455, mean_q: 9.998538, mean_eps: 0.100000\n",
      "  89082/175000: episode: 2499, duration: 0.977s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.196 [28.000, 223.000], mean observation: 0.394 [0.000, 92.000], loss: 22.289813, mean_absolute_error: 0.689195, mean_q: 8.875698, mean_eps: 0.100000\n",
      "  89108/175000: episode: 2500, duration: 0.559s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 29.692 [10.000, 90.000], mean observation: 0.079 [0.000, 52.000], loss: 91.476906, mean_absolute_error: 1.107698, mean_q: 10.012434, mean_eps: 0.100000\n",
      "  89132/175000: episode: 2501, duration: 0.522s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 27.042 [5.000, 28.000], mean observation: 0.079 [0.000, 48.000], loss: 0.123159, mean_absolute_error: 0.479782, mean_q: 7.979429, mean_eps: 0.100000\n",
      "  89173/175000: episode: 2502, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 38.171 [1.000, 205.000], mean observation: 0.151 [0.000, 82.000], loss: 130.261043, mean_absolute_error: 1.196901, mean_q: 9.099287, mean_eps: 0.100000\n",
      "  89200/175000: episode: 2503, duration: 0.595s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 27.444 [4.000, 37.000], mean observation: 0.088 [0.000, 54.000], loss: 0.153827, mean_absolute_error: 0.481446, mean_q: 8.143456, mean_eps: 0.100000\n",
      "  89243/175000: episode: 2504, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 56.744 [20.000, 207.000], mean observation: 0.197 [0.000, 86.000], loss: 0.150690, mean_absolute_error: 0.477538, mean_q: 7.979189, mean_eps: 0.100000\n",
      "  89278/175000: episode: 2505, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.714 [6.000, 211.000], mean observation: 0.236 [0.000, 70.000], loss: 0.178568, mean_absolute_error: 0.489373, mean_q: 8.080861, mean_eps: 0.100000\n",
      "  89314/175000: episode: 2506, duration: 0.771s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.056 [27.000, 157.000], mean observation: 0.132 [0.000, 72.000], loss: 1.289481, mean_absolute_error: 0.652418, mean_q: 9.409492, mean_eps: 0.100000\n",
      "  89348/175000: episode: 2507, duration: 0.704s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 32.294 [28.000, 105.000], mean observation: 0.082 [0.000, 68.000], loss: 164.497487, mean_absolute_error: 1.385516, mean_q: 9.378280, mean_eps: 0.100000\n",
      "  89392/175000: episode: 2508, duration: 0.936s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 40.364 [10.000, 224.000], mean observation: 0.178 [0.000, 88.000], loss: 118.697136, mean_absolute_error: 1.103524, mean_q: 8.825823, mean_eps: 0.100000\n",
      "  89415/175000: episode: 2509, duration: 0.516s, episode steps: 23, steps per second: 45, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.055 [0.000, 46.000], loss: 165.677947, mean_absolute_error: 1.467498, mean_q: 10.162690, mean_eps: 0.100000\n",
      "  89432/175000: episode: 2510, duration: 0.394s, episode steps: 17, steps per second: 43, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 26.647 [5.000, 28.000], mean observation: 0.044 [0.000, 34.000], loss: 318.478566, mean_absolute_error: 2.275914, mean_q: 11.188061, mean_eps: 0.100000\n",
      "  89472/175000: episode: 2511, duration: 0.848s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 55.125 [3.000, 214.000], mean observation: 0.245 [0.000, 80.000], loss: 0.202605, mean_absolute_error: 0.477499, mean_q: 7.984460, mean_eps: 0.100000\n",
      "  89502/175000: episode: 2512, duration: 0.590s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 33.600 [28.000, 196.000], mean observation: 0.109 [0.000, 60.000], loss: 68.262215, mean_absolute_error: 0.958536, mean_q: 9.462478, mean_eps: 0.100000\n",
      "  89525/175000: episode: 2513, duration: 0.501s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 48.652 [28.000, 173.000], mean observation: 0.101 [0.000, 46.000], loss: 86.194959, mean_absolute_error: 1.092569, mean_q: 10.185052, mean_eps: 0.100000\n",
      "  89567/175000: episode: 2514, duration: 0.861s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 61.071 [28.000, 201.000], mean observation: 0.206 [0.000, 84.000], loss: 0.293213, mean_absolute_error: 0.491627, mean_q: 8.292763, mean_eps: 0.100000\n",
      "  89602/175000: episode: 2515, duration: 0.763s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 50.714 [16.000, 159.000], mean observation: 0.197 [0.000, 70.000], loss: 1.323099, mean_absolute_error: 0.628705, mean_q: 9.252047, mean_eps: 0.100000\n",
      "  89646/175000: episode: 2516, duration: 0.972s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.750 [1.000, 191.000], mean observation: 0.344 [0.000, 88.000], loss: 35.975392, mean_absolute_error: 0.740514, mean_q: 8.906117, mean_eps: 0.100000\n",
      "  89688/175000: episode: 2517, duration: 0.913s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 32.167 [11.000, 190.000], mean observation: 0.191 [0.000, 84.000], loss: 154.681045, mean_absolute_error: 1.270988, mean_q: 8.981814, mean_eps: 0.100000\n",
      "  89717/175000: episode: 2518, duration: 0.761s, episode steps: 29, steps per second: 38, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 32.655 [28.000, 163.000], mean observation: 0.071 [0.000, 58.000], loss: 0.089293, mean_absolute_error: 0.424132, mean_q: 7.649418, mean_eps: 0.100000\n",
      "  89759/175000: episode: 2519, duration: 0.897s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 58.619 [28.000, 224.000], mean observation: 0.249 [0.000, 84.000], loss: 109.748577, mean_absolute_error: 1.219790, mean_q: 10.222801, mean_eps: 0.100000\n",
      "  89778/175000: episode: 2520, duration: 0.380s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 30.053 [2.000, 90.000], mean observation: 0.066 [0.000, 38.000], loss: 0.215381, mean_absolute_error: 0.461781, mean_q: 8.077061, mean_eps: 0.100000\n",
      "  89813/175000: episode: 2521, duration: 0.722s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 58.686 [2.000, 162.000], mean observation: 0.148 [0.000, 70.000], loss: 43.858338, mean_absolute_error: 0.842482, mean_q: 9.818489, mean_eps: 0.100000\n",
      "  89849/175000: episode: 2522, duration: 0.730s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 46.167 [20.000, 188.000], mean observation: 0.147 [0.000, 72.000], loss: 167.295672, mean_absolute_error: 1.382909, mean_q: 9.675838, mean_eps: 0.100000\n",
      "  89886/175000: episode: 2523, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 48.000 [0.000, 169.000], mean observation: 0.318 [0.000, 74.000], loss: 43.555726, mean_absolute_error: 0.795541, mean_q: 9.439155, mean_eps: 0.100000\n",
      "  89904/175000: episode: 2524, duration: 0.344s, episode steps: 18, steps per second: 52, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 49.278 [28.000, 216.000], mean observation: 0.093 [0.000, 36.000], loss: 0.156962, mean_absolute_error: 0.452929, mean_q: 8.237008, mean_eps: 0.100000\n",
      "  89943/175000: episode: 2525, duration: 0.824s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.949 [28.000, 206.000], mean observation: 0.320 [0.000, 78.000], loss: 0.187642, mean_absolute_error: 0.444638, mean_q: 8.127959, mean_eps: 0.100000\n",
      "  89958/175000: episode: 2526, duration: 0.316s, episode steps: 15, steps per second: 47, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 37.133 [28.000, 121.000], mean observation: 0.075 [0.000, 30.000], loss: 0.123442, mean_absolute_error: 0.459617, mean_q: 8.260682, mean_eps: 0.100000\n",
      "  89994/175000: episode: 2527, duration: 0.744s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.417 [28.000, 213.000], mean observation: 0.251 [0.000, 72.000], loss: 11.235247, mean_absolute_error: 0.506012, mean_q: 8.325526, mean_eps: 0.100000\n",
      "  90039/175000: episode: 2528, duration: 1.004s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 74.222 [14.000, 218.000], mean observation: 0.266 [0.000, 90.000], loss: 0.328830, mean_absolute_error: 0.447939, mean_q: 8.245423, mean_eps: 0.100000\n",
      "  90091/175000: episode: 2529, duration: 1.125s, episode steps: 52, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 31.885 [5.000, 135.000], mean observation: 0.254 [0.000, 104.000], loss: 0.749147, mean_absolute_error: 0.410726, mean_q: 7.765131, mean_eps: 0.100000\n",
      "  90125/175000: episode: 2530, duration: 0.738s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.618 [0.000, 194.000], mean observation: 0.130 [0.000, 68.000], loss: 0.305219, mean_absolute_error: 0.426771, mean_q: 7.892429, mean_eps: 0.100000\n",
      "  90171/175000: episode: 2531, duration: 0.986s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 44.413 [28.000, 187.000], mean observation: 0.335 [0.000, 92.000], loss: 0.489302, mean_absolute_error: 0.429559, mean_q: 7.870919, mean_eps: 0.100000\n",
      "  90199/175000: episode: 2532, duration: 0.579s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 90.107 [1.000, 202.000], mean observation: 0.226 [0.000, 56.000], loss: 0.349463, mean_absolute_error: 0.442523, mean_q: 8.014250, mean_eps: 0.100000\n",
      "  90231/175000: episode: 2533, duration: 0.709s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 27.812 [22.000, 28.000], mean observation: 0.083 [0.000, 64.000], loss: 0.181964, mean_absolute_error: 0.475160, mean_q: 8.503743, mean_eps: 0.100000\n",
      "  90254/175000: episode: 2534, duration: 0.520s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 33.913 [28.000, 164.000], mean observation: 0.061 [0.000, 46.000], loss: 30.760811, mean_absolute_error: 0.835777, mean_q: 10.196575, mean_eps: 0.100000\n",
      "  90280/175000: episode: 2535, duration: 0.624s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 33.231 [28.000, 93.000], mean observation: 0.077 [0.000, 52.000], loss: 0.967019, mean_absolute_error: 0.462669, mean_q: 8.280913, mean_eps: 0.100000\n",
      "  90327/175000: episode: 2536, duration: 0.945s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 30.872 [28.000, 136.000], mean observation: 0.137 [0.000, 94.000], loss: 11.313858, mean_absolute_error: 0.630717, mean_q: 9.192530, mean_eps: 0.100000\n",
      "  90343/175000: episode: 2537, duration: 0.361s, episode steps: 16, steps per second: 44, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 80.250 [28.000, 208.000], mean observation: 0.058 [0.000, 32.000], loss: 0.588186, mean_absolute_error: 0.450371, mean_q: 7.943981, mean_eps: 0.100000\n",
      "  90363/175000: episode: 2538, duration: 0.429s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 60.000 [9.000, 170.000], mean observation: 0.096 [0.000, 40.000], loss: 6.115019, mean_absolute_error: 0.778873, mean_q: 10.603823, mean_eps: 0.100000\n",
      "  90392/175000: episode: 2539, duration: 0.633s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 63.448 [5.000, 200.000], mean observation: 0.149 [0.000, 58.000], loss: 0.207457, mean_absolute_error: 0.485488, mean_q: 8.138580, mean_eps: 0.100000\n",
      "  90431/175000: episode: 2540, duration: 0.760s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.590 [5.000, 223.000], mean observation: 0.258 [0.000, 78.000], loss: 0.853054, mean_absolute_error: 0.491272, mean_q: 8.044104, mean_eps: 0.100000\n",
      "  90480/175000: episode: 2541, duration: 1.087s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.113 [0.000, 98.000], loss: 37.113200, mean_absolute_error: 0.765344, mean_q: 9.185124, mean_eps: 0.100000\n",
      "  90515/175000: episode: 2542, duration: 0.699s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.771 [5.000, 184.000], mean observation: 0.156 [0.000, 70.000], loss: 0.297489, mean_absolute_error: 0.479290, mean_q: 8.327115, mean_eps: 0.100000\n",
      "  90539/175000: episode: 2543, duration: 0.446s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 39.125 [28.000, 203.000], mean observation: 0.064 [0.000, 48.000], loss: 0.666204, mean_absolute_error: 0.479747, mean_q: 8.271269, mean_eps: 0.100000\n",
      "  90572/175000: episode: 2544, duration: 0.823s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 49.667 [0.000, 191.000], mean observation: 0.177 [0.000, 66.000], loss: 104.131850, mean_absolute_error: 1.269978, mean_q: 10.951070, mean_eps: 0.100000\n",
      "  90612/175000: episode: 2545, duration: 1.078s, episode steps: 40, steps per second: 37, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 45.575 [4.000, 223.000], mean observation: 0.149 [0.000, 80.000], loss: 116.780242, mean_absolute_error: 1.096693, mean_q: 9.215930, mean_eps: 0.100000\n",
      "  90650/175000: episode: 2546, duration: 0.737s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 42.763 [17.000, 221.000], mean observation: 0.170 [0.000, 76.000], loss: 80.366693, mean_absolute_error: 0.976119, mean_q: 9.712427, mean_eps: 0.100000\n",
      "  90677/175000: episode: 2547, duration: 0.493s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 64.815 [28.000, 213.000], mean observation: 0.117 [0.000, 54.000], loss: 0.721420, mean_absolute_error: 0.493532, mean_q: 8.798186, mean_eps: 0.100000\n",
      "  90699/175000: episode: 2548, duration: 0.507s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 67.318 [28.000, 182.000], mean observation: 0.128 [0.000, 44.000], loss: 0.198848, mean_absolute_error: 0.549228, mean_q: 9.155834, mean_eps: 0.100000\n",
      "  90740/175000: episode: 2549, duration: 0.914s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 67.195 [20.000, 211.000], mean observation: 0.283 [0.000, 82.000], loss: 0.993531, mean_absolute_error: 0.495960, mean_q: 9.020991, mean_eps: 0.100000\n",
      "  90781/175000: episode: 2550, duration: 0.836s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 47.634 [28.000, 200.000], mean observation: 0.194 [0.000, 82.000], loss: 0.292733, mean_absolute_error: 0.483521, mean_q: 8.378194, mean_eps: 0.100000\n",
      "  90808/175000: episode: 2551, duration: 0.491s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.000 [28.000, 217.000], mean observation: 0.068 [0.000, 54.000], loss: 187.388275, mean_absolute_error: 1.586623, mean_q: 10.801030, mean_eps: 0.100000\n",
      "  90831/175000: episode: 2552, duration: 0.498s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.055 [0.000, 46.000], loss: 0.232829, mean_absolute_error: 0.531504, mean_q: 8.635347, mean_eps: 0.100000\n",
      "  90860/175000: episode: 2553, duration: 0.588s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 33.966 [28.000, 152.000], mean observation: 0.092 [0.000, 58.000], loss: 0.389186, mean_absolute_error: 0.521662, mean_q: 8.638499, mean_eps: 0.100000\n",
      "  90906/175000: episode: 2554, duration: 0.856s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 56.261 [1.000, 202.000], mean observation: 0.378 [0.000, 92.000], loss: 51.902451, mean_absolute_error: 0.853236, mean_q: 9.608098, mean_eps: 0.100000\n",
      "  90948/175000: episode: 2555, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 51.000 [15.000, 202.000], mean observation: 0.164 [0.000, 84.000], loss: 0.167460, mean_absolute_error: 0.515722, mean_q: 8.687827, mean_eps: 0.100000\n",
      "  90995/175000: episode: 2556, duration: 0.932s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.894 [28.000, 218.000], mean observation: 0.384 [0.000, 94.000], loss: 0.363861, mean_absolute_error: 0.554025, mean_q: 8.988152, mean_eps: 0.100000\n",
      "  91034/175000: episode: 2557, duration: 0.956s, episode steps: 39, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 33.333 [20.000, 216.000], mean observation: 0.150 [0.000, 78.000], loss: 0.338341, mean_absolute_error: 0.548090, mean_q: 8.952398, mean_eps: 0.100000\n",
      "  91084/175000: episode: 2558, duration: 1.308s, episode steps: 50, steps per second: 38, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 61.380 [7.000, 214.000], mean observation: 0.383 [0.000, 100.000], loss: 0.197598, mean_absolute_error: 0.524419, mean_q: 8.692111, mean_eps: 0.100000\n",
      "  91116/175000: episode: 2559, duration: 0.694s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.875 [28.000, 191.000], mean observation: 0.122 [0.000, 64.000], loss: 0.475331, mean_absolute_error: 0.510297, mean_q: 8.655839, mean_eps: 0.100000\n",
      "  91139/175000: episode: 2560, duration: 0.494s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 71.478 [18.000, 215.000], mean observation: 0.112 [0.000, 46.000], loss: 109.885882, mean_absolute_error: 1.491342, mean_q: 12.995377, mean_eps: 0.100000\n",
      "  91162/175000: episode: 2561, duration: 0.425s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 33.913 [28.000, 164.000], mean observation: 0.078 [0.000, 46.000], loss: 181.044353, mean_absolute_error: 1.636409, mean_q: 11.059922, mean_eps: 0.100000\n",
      "  91213/175000: episode: 2562, duration: 1.068s, episode steps: 51, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.196 [24.000, 210.000], mean observation: 0.482 [0.000, 102.000], loss: 51.958943, mean_absolute_error: 0.878424, mean_q: 9.706241, mean_eps: 0.100000\n",
      "  91236/175000: episode: 2563, duration: 0.513s, episode steps: 23, steps per second: 45, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.522 [28.000, 180.000], mean observation: 0.090 [0.000, 46.000], loss: 0.103833, mean_absolute_error: 0.509826, mean_q: 8.135394, mean_eps: 0.100000\n",
      "  91264/175000: episode: 2564, duration: 0.654s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 96.250 [0.000, 138.000], mean observation: 0.108 [0.000, 56.000], loss: 50.566508, mean_absolute_error: 0.934885, mean_q: 9.972462, mean_eps: 0.100000\n",
      "  91281/175000: episode: 2565, duration: 0.438s, episode steps: 17, steps per second: 39, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 131.941 [3.000, 215.000], mean observation: 0.047 [0.000, 34.000], loss: 0.690664, mean_absolute_error: 0.568325, mean_q: 8.644353, mean_eps: 0.100000\n",
      "  91318/175000: episode: 2566, duration: 0.837s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 80.351 [25.000, 191.000], mean observation: 0.252 [0.000, 74.000], loss: 0.132755, mean_absolute_error: 0.498475, mean_q: 8.482059, mean_eps: 0.100000\n",
      "  91348/175000: episode: 2567, duration: 0.707s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 125.867 [28.000, 202.000], mean observation: 0.199 [0.000, 60.000], loss: 0.358406, mean_absolute_error: 0.486137, mean_q: 8.442306, mean_eps: 0.100000\n",
      "  91371/175000: episode: 2568, duration: 0.460s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 32.783 [25.000, 141.000], mean observation: 0.059 [0.000, 46.000], loss: 24.964533, mean_absolute_error: 0.872554, mean_q: 10.861216, mean_eps: 0.100000\n",
      "  91424/175000: episode: 2569, duration: 1.182s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 117.038 [14.000, 191.000], mean observation: 0.598 [0.000, 106.000], loss: 28.119395, mean_absolute_error: 0.758762, mean_q: 9.969551, mean_eps: 0.100000\n",
      "  91466/175000: episode: 2570, duration: 1.084s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.167 [28.000, 214.000], mean observation: 0.413 [0.000, 84.000], loss: 0.213772, mean_absolute_error: 0.471276, mean_q: 8.387483, mean_eps: 0.100000\n",
      "  91504/175000: episode: 2571, duration: 0.816s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 90.789 [27.000, 216.000], mean observation: 0.227 [0.000, 76.000], loss: 0.373571, mean_absolute_error: 0.506600, mean_q: 8.803195, mean_eps: 0.100000\n",
      "  91526/175000: episode: 2572, duration: 0.502s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 92.864 [28.000, 222.000], mean observation: 0.105 [0.000, 44.000], loss: 0.203286, mean_absolute_error: 0.511491, mean_q: 8.666027, mean_eps: 0.100000\n",
      "  91581/175000: episode: 2573, duration: 1.073s, episode steps: 55, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 124.927 [1.000, 210.000], mean observation: 0.438 [0.000, 110.000], loss: 0.185893, mean_absolute_error: 0.500128, mean_q: 8.328458, mean_eps: 0.100000\n",
      "  91606/175000: episode: 2574, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 176.000 [41.000, 206.000], mean observation: 0.104 [0.000, 50.000], loss: 1.545211, mean_absolute_error: 0.460536, mean_q: 8.141892, mean_eps: 0.100000\n",
      "  91648/175000: episode: 2575, duration: 0.850s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.119 [15.000, 213.000], mean observation: 0.198 [0.000, 84.000], loss: 33.108100, mean_absolute_error: 0.764044, mean_q: 9.412255, mean_eps: 0.100000\n",
      "  91683/175000: episode: 2576, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.971 [28.000, 191.000], mean observation: 0.110 [0.000, 70.000], loss: 0.317897, mean_absolute_error: 0.484414, mean_q: 7.935574, mean_eps: 0.100000\n",
      "  91718/175000: episode: 2577, duration: 0.722s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 32.029 [1.000, 196.000], mean observation: 0.086 [0.000, 70.000], loss: 63.853783, mean_absolute_error: 0.941754, mean_q: 9.483491, mean_eps: 0.100000\n",
      "  91761/175000: episode: 2578, duration: 0.874s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.116 [11.000, 191.000], mean observation: 0.262 [0.000, 86.000], loss: 56.377844, mean_absolute_error: 0.849585, mean_q: 9.014160, mean_eps: 0.100000\n",
      "  91792/175000: episode: 2579, duration: 0.602s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.161 [28.000, 191.000], mean observation: 0.214 [0.000, 62.000], loss: 148.470901, mean_absolute_error: 1.346236, mean_q: 9.872989, mean_eps: 0.100000\n",
      "  91828/175000: episode: 2580, duration: 0.805s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 37.111 [28.000, 221.000], mean observation: 0.090 [0.000, 72.000], loss: 0.318782, mean_absolute_error: 0.498834, mean_q: 7.993105, mean_eps: 0.100000\n",
      "  91850/175000: episode: 2581, duration: 0.438s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 36.273 [24.000, 214.000], mean observation: 0.074 [0.000, 44.000], loss: 0.162072, mean_absolute_error: 0.523920, mean_q: 8.219584, mean_eps: 0.100000\n",
      "  91869/175000: episode: 2582, duration: 0.379s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 69.053 [28.000, 213.000], mean observation: 0.056 [0.000, 38.000], loss: 96.440649, mean_absolute_error: 1.258200, mean_q: 11.068695, mean_eps: 0.100000\n",
      "  91915/175000: episode: 2583, duration: 0.893s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.543 [28.000, 223.000], mean observation: 0.479 [0.000, 92.000], loss: 0.229147, mean_absolute_error: 0.503619, mean_q: 7.960469, mean_eps: 0.100000\n",
      "  91964/175000: episode: 2584, duration: 1.008s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 73.694 [28.000, 191.000], mean observation: 0.312 [0.000, 98.000], loss: 79.255207, mean_absolute_error: 1.021071, mean_q: 9.399439, mean_eps: 0.100000\n",
      "  92009/175000: episode: 2585, duration: 0.952s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 127.667 [28.000, 218.000], mean observation: 0.335 [0.000, 90.000], loss: 18.571357, mean_absolute_error: 0.732262, mean_q: 9.241104, mean_eps: 0.100000\n",
      "  92050/175000: episode: 2586, duration: 0.770s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.122 [28.000, 179.000], mean observation: 0.246 [0.000, 82.000], loss: 0.237260, mean_absolute_error: 0.557944, mean_q: 8.323836, mean_eps: 0.100000\n",
      "  92097/175000: episode: 2587, duration: 0.997s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 43.468 [0.000, 191.000], mean observation: 0.176 [0.000, 94.000], loss: 12.377049, mean_absolute_error: 0.702233, mean_q: 9.285543, mean_eps: 0.100000\n",
      "  92127/175000: episode: 2588, duration: 0.568s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 75.433 [26.000, 213.000], mean observation: 0.272 [0.000, 60.000], loss: 0.484811, mean_absolute_error: 0.475744, mean_q: 7.846240, mean_eps: 0.100000\n",
      "  92174/175000: episode: 2589, duration: 0.958s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.106 [3.000, 197.000], mean observation: 0.293 [0.000, 94.000], loss: 0.309456, mean_absolute_error: 0.463460, mean_q: 7.851255, mean_eps: 0.100000\n",
      "  92197/175000: episode: 2590, duration: 0.478s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 45.783 [15.000, 191.000], mean observation: 0.062 [0.000, 46.000], loss: 0.356239, mean_absolute_error: 0.478493, mean_q: 8.067826, mean_eps: 0.100000\n",
      "  92230/175000: episode: 2591, duration: 0.772s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 29.121 [4.000, 89.000], mean observation: 0.172 [0.000, 66.000], loss: 0.131082, mean_absolute_error: 0.486684, mean_q: 7.960241, mean_eps: 0.100000\n",
      "  92253/175000: episode: 2592, duration: 0.605s, episode steps: 23, steps per second: 38, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.870 [3.000, 215.000], mean observation: 0.089 [0.000, 46.000], loss: 0.220348, mean_absolute_error: 0.469314, mean_q: 7.872440, mean_eps: 0.100000\n",
      "  92302/175000: episode: 2593, duration: 0.944s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.245 [24.000, 222.000], mean observation: 0.508 [0.000, 98.000], loss: 0.247568, mean_absolute_error: 0.474916, mean_q: 7.852905, mean_eps: 0.100000\n",
      "  92339/175000: episode: 2594, duration: 0.690s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 58.541 [2.000, 191.000], mean observation: 0.231 [0.000, 74.000], loss: 0.755310, mean_absolute_error: 0.447067, mean_q: 7.609951, mean_eps: 0.100000\n",
      "  92366/175000: episode: 2595, duration: 0.608s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 50.593 [15.000, 191.000], mean observation: 0.203 [0.000, 54.000], loss: 114.739338, mean_absolute_error: 1.223713, mean_q: 10.307020, mean_eps: 0.100000\n",
      "  92411/175000: episode: 2596, duration: 0.971s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 57.200 [8.000, 214.000], mean observation: 0.451 [0.000, 90.000], loss: 0.488740, mean_absolute_error: 0.465606, mean_q: 7.908947, mean_eps: 0.100000\n",
      "  92454/175000: episode: 2597, duration: 1.014s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 59.116 [15.000, 146.000], mean observation: 0.210 [0.000, 86.000], loss: 0.841012, mean_absolute_error: 0.476374, mean_q: 7.992984, mean_eps: 0.100000\n",
      "  92469/175000: episode: 2598, duration: 0.304s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 32.000 [27.000, 99.000], mean observation: 0.045 [0.000, 30.000], loss: 0.242352, mean_absolute_error: 0.552831, mean_q: 8.484477, mean_eps: 0.100000\n",
      "  92510/175000: episode: 2599, duration: 0.863s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 28.659 [22.000, 80.000], mean observation: 0.132 [0.000, 82.000], loss: 108.779817, mean_absolute_error: 1.249596, mean_q: 10.634142, mean_eps: 0.100000\n",
      "  92548/175000: episode: 2600, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.658 [0.000, 213.000], mean observation: 0.110 [0.000, 76.000], loss: 49.399566, mean_absolute_error: 0.833397, mean_q: 9.290748, mean_eps: 0.100000\n",
      "  92592/175000: episode: 2601, duration: 0.911s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 42.545 [27.000, 173.000], mean observation: 0.227 [0.000, 88.000], loss: 0.311120, mean_absolute_error: 0.450952, mean_q: 7.928614, mean_eps: 0.100000\n",
      "  92615/175000: episode: 2602, duration: 0.428s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 50.000 [27.000, 135.000], mean observation: 0.093 [0.000, 46.000], loss: 0.987925, mean_absolute_error: 0.480254, mean_q: 8.015009, mean_eps: 0.100000\n",
      "  92638/175000: episode: 2603, duration: 0.565s, episode steps: 23, steps per second: 41, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 29.348 [28.000, 59.000], mean observation: 0.083 [0.000, 46.000], loss: 0.513874, mean_absolute_error: 0.542746, mean_q: 8.373893, mean_eps: 0.100000\n",
      "  92660/175000: episode: 2604, duration: 0.586s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 99.500 [15.000, 180.000], mean observation: 0.114 [0.000, 44.000], loss: 13.352303, mean_absolute_error: 0.823793, mean_q: 10.334949, mean_eps: 0.100000\n",
      "  92708/175000: episode: 2605, duration: 0.968s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 53.104 [28.000, 158.000], mean observation: 0.247 [0.000, 96.000], loss: 72.903461, mean_absolute_error: 0.931908, mean_q: 8.771240, mean_eps: 0.100000\n",
      "  92722/175000: episode: 2606, duration: 0.398s, episode steps: 14, steps per second: 35, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 94.000 [28.000, 158.000], mean observation: 0.077 [0.000, 28.000], loss: 0.111682, mean_absolute_error: 0.491693, mean_q: 7.509734, mean_eps: 0.100000\n",
      "  92768/175000: episode: 2607, duration: 1.119s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.870 [28.000, 158.000], mean observation: 0.267 [0.000, 92.000], loss: 0.176688, mean_absolute_error: 0.480803, mean_q: 7.519406, mean_eps: 0.100000\n",
      "  92810/175000: episode: 2608, duration: 1.099s, episode steps: 42, steps per second: 38, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 150.143 [63.000, 184.000], mean observation: 0.162 [0.000, 84.000], loss: 7.726036, mean_absolute_error: 0.622621, mean_q: 8.695966, mean_eps: 0.100000\n",
      "  92841/175000: episode: 2609, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.710 [24.000, 224.000], mean observation: 0.143 [0.000, 62.000], loss: 0.182998, mean_absolute_error: 0.455031, mean_q: 7.443510, mean_eps: 0.100000\n",
      "  92882/175000: episode: 2610, duration: 0.871s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.561 [4.000, 184.000], mean observation: 0.224 [0.000, 82.000], loss: 26.638462, mean_absolute_error: 0.722092, mean_q: 8.725658, mean_eps: 0.100000\n",
      "  92925/175000: episode: 2611, duration: 0.969s, episode steps: 43, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 128.674 [7.000, 210.000], mean observation: 0.262 [0.000, 86.000], loss: 61.131991, mean_absolute_error: 0.875181, mean_q: 8.799260, mean_eps: 0.100000\n",
      "  92957/175000: episode: 2612, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 153.719 [42.000, 158.000], mean observation: 0.106 [0.000, 64.000], loss: 1.464188, mean_absolute_error: 0.657490, mean_q: 9.399095, mean_eps: 0.100000\n",
      "  93000/175000: episode: 2613, duration: 0.953s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.442 [5.000, 222.000], mean observation: 0.367 [0.000, 86.000], loss: 0.181267, mean_absolute_error: 0.452003, mean_q: 7.451328, mean_eps: 0.100000\n",
      "  93043/175000: episode: 2614, duration: 0.898s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.791 [7.000, 209.000], mean observation: 0.238 [0.000, 86.000], loss: 0.280961, mean_absolute_error: 0.463824, mean_q: 7.625679, mean_eps: 0.100000\n",
      "  93098/175000: episode: 2615, duration: 1.157s, episode steps: 55, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 103.345 [7.000, 185.000], mean observation: 0.396 [0.000, 110.000], loss: 0.857593, mean_absolute_error: 0.494214, mean_q: 7.955762, mean_eps: 0.100000\n",
      "  93127/175000: episode: 2616, duration: 0.671s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 60.069 [24.000, 184.000], mean observation: 0.178 [0.000, 58.000], loss: 0.588092, mean_absolute_error: 0.423253, mean_q: 7.351550, mean_eps: 0.100000\n",
      "  93183/175000: episode: 2617, duration: 1.106s, episode steps: 56, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 87.232 [27.000, 222.000], mean observation: 0.439 [0.000, 112.000], loss: 70.678249, mean_absolute_error: 0.865230, mean_q: 8.572177, mean_eps: 0.100000\n",
      "  93211/175000: episode: 2618, duration: 0.566s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 120.786 [8.000, 184.000], mean observation: 0.145 [0.000, 56.000], loss: 0.178109, mean_absolute_error: 0.453321, mean_q: 7.641269, mean_eps: 0.100000\n",
      "  93263/175000: episode: 2619, duration: 1.184s, episode steps: 52, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.654 [29.000, 184.000], mean observation: 0.527 [0.000, 104.000], loss: 13.609320, mean_absolute_error: 0.600344, mean_q: 8.483439, mean_eps: 0.100000\n",
      "  93284/175000: episode: 2620, duration: 0.454s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 115.524 [21.000, 184.000], mean observation: 0.146 [0.000, 42.000], loss: 0.126691, mean_absolute_error: 0.424606, mean_q: 7.458640, mean_eps: 0.100000\n",
      "  93330/175000: episode: 2621, duration: 1.000s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 85.957 [34.000, 221.000], mean observation: 0.376 [0.000, 92.000], loss: 0.160555, mean_absolute_error: 0.438259, mean_q: 7.499789, mean_eps: 0.100000\n",
      "  93361/175000: episode: 2622, duration: 0.669s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 102.387 [44.000, 184.000], mean observation: 0.213 [0.000, 62.000], loss: 43.168153, mean_absolute_error: 0.835112, mean_q: 9.552144, mean_eps: 0.100000\n",
      "  93379/175000: episode: 2623, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 85.333 [44.000, 158.000], mean observation: 0.077 [0.000, 36.000], loss: 0.314701, mean_absolute_error: 0.429428, mean_q: 7.410194, mean_eps: 0.100000\n",
      "  93420/175000: episode: 2624, duration: 0.987s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.146 [44.000, 221.000], mean observation: 0.243 [0.000, 82.000], loss: 0.331254, mean_absolute_error: 0.422467, mean_q: 7.299936, mean_eps: 0.100000\n",
      "  93475/175000: episode: 2625, duration: 1.218s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 128.236 [49.000, 221.000], mean observation: 0.339 [0.000, 110.000], loss: 24.939055, mean_absolute_error: 0.652533, mean_q: 8.503656, mean_eps: 0.100000\n",
      "  93516/175000: episode: 2626, duration: 0.794s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.366 [13.000, 215.000], mean observation: 0.271 [0.000, 82.000], loss: 104.951461, mean_absolute_error: 1.262937, mean_q: 10.879559, mean_eps: 0.100000\n",
      "  93571/175000: episode: 2627, duration: 1.188s, episode steps: 55, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 143.109 [14.000, 215.000], mean observation: 0.424 [0.000, 110.000], loss: 0.413063, mean_absolute_error: 0.448230, mean_q: 7.621946, mean_eps: 0.100000\n",
      "  93606/175000: episode: 2628, duration: 0.718s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.086 [48.000, 176.000], mean observation: 0.221 [0.000, 70.000], loss: 0.177896, mean_absolute_error: 0.459565, mean_q: 7.519699, mean_eps: 0.100000\n",
      "  93636/175000: episode: 2629, duration: 0.604s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 111.300 [13.000, 207.000], mean observation: 0.256 [0.000, 60.000], loss: 0.199268, mean_absolute_error: 0.458388, mean_q: 7.409367, mean_eps: 0.100000\n",
      "  93682/175000: episode: 2630, duration: 0.957s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.391 [5.000, 191.000], mean observation: 0.365 [0.000, 92.000], loss: 83.172939, mean_absolute_error: 1.204850, mean_q: 11.157078, mean_eps: 0.100000\n",
      "  93728/175000: episode: 2631, duration: 1.131s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.826 [48.000, 213.000], mean observation: 0.431 [0.000, 92.000], loss: 92.947047, mean_absolute_error: 1.004113, mean_q: 8.700242, mean_eps: 0.100000\n",
      "  93761/175000: episode: 2632, duration: 0.749s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 121.030 [8.000, 207.000], mean observation: 0.191 [0.000, 66.000], loss: 0.607103, mean_absolute_error: 0.467487, mean_q: 7.730361, mean_eps: 0.100000\n",
      "  93802/175000: episode: 2633, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.293 [42.000, 202.000], mean observation: 0.266 [0.000, 82.000], loss: 4.840403, mean_absolute_error: 0.607730, mean_q: 8.959512, mean_eps: 0.100000\n",
      "  93826/175000: episode: 2634, duration: 0.557s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 147.167 [101.000, 217.000], mean observation: 0.090 [0.000, 48.000], loss: 0.175338, mean_absolute_error: 0.430320, mean_q: 7.485181, mean_eps: 0.100000\n",
      "  93856/175000: episode: 2635, duration: 0.644s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 134.033 [101.000, 179.000], mean observation: 0.140 [0.000, 60.000], loss: 70.257506, mean_absolute_error: 0.964964, mean_q: 9.543589, mean_eps: 0.100000\n",
      "  93909/175000: episode: 2636, duration: 1.105s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.547 [30.000, 207.000], mean observation: 0.309 [0.000, 106.000], loss: 96.289977, mean_absolute_error: 1.082722, mean_q: 9.780517, mean_eps: 0.100000\n",
      "  93944/175000: episode: 2637, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 147.971 [101.000, 176.000], mean observation: 0.141 [0.000, 70.000], loss: 55.368640, mean_absolute_error: 0.861213, mean_q: 9.433362, mean_eps: 0.100000\n",
      "  93971/175000: episode: 2638, duration: 0.514s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 140.333 [24.000, 178.000], mean observation: 0.210 [0.000, 54.000], loss: 0.219565, mean_absolute_error: 0.457563, mean_q: 8.280657, mean_eps: 0.100000\n",
      "  93984/175000: episode: 2639, duration: 0.287s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 148.308 [15.000, 220.000], mean observation: 0.082 [0.000, 26.000], loss: 205.989818, mean_absolute_error: 1.894198, mean_q: 12.745317, mean_eps: 0.100000\n",
      "  94015/175000: episode: 2640, duration: 0.648s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 129.742 [88.000, 221.000], mean observation: 0.166 [0.000, 62.000], loss: 0.219028, mean_absolute_error: 0.423460, mean_q: 7.759926, mean_eps: 0.100000\n",
      "  94040/175000: episode: 2641, duration: 0.590s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 153.680 [24.000, 222.000], mean observation: 0.174 [0.000, 50.000], loss: 0.210605, mean_absolute_error: 0.434503, mean_q: 8.050773, mean_eps: 0.100000\n",
      "  94077/175000: episode: 2642, duration: 0.800s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.486 [38.000, 222.000], mean observation: 0.216 [0.000, 74.000], loss: 0.150479, mean_absolute_error: 0.413220, mean_q: 7.649956, mean_eps: 0.100000\n",
      "  94107/175000: episode: 2643, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.500 [41.000, 222.000], mean observation: 0.188 [0.000, 60.000], loss: 0.259949, mean_absolute_error: 0.440954, mean_q: 7.940844, mean_eps: 0.100000\n",
      "  94138/175000: episode: 2644, duration: 0.630s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 105.419 [58.000, 222.000], mean observation: 0.192 [0.000, 62.000], loss: 99.444502, mean_absolute_error: 1.071318, mean_q: 9.635205, mean_eps: 0.100000\n",
      "  94186/175000: episode: 2645, duration: 0.982s, episode steps: 48, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.854 [41.000, 222.000], mean observation: 0.367 [0.000, 96.000], loss: 0.177829, mean_absolute_error: 0.410181, mean_q: 7.397466, mean_eps: 0.100000\n",
      "  94232/175000: episode: 2646, duration: 0.928s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.935 [13.000, 158.000], mean observation: 0.363 [0.000, 92.000], loss: 0.173680, mean_absolute_error: 0.435568, mean_q: 7.705984, mean_eps: 0.100000\n",
      "  94256/175000: episode: 2647, duration: 0.588s, episode steps: 24, steps per second: 41, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 143.792 [21.000, 202.000], mean observation: 0.127 [0.000, 48.000], loss: 0.121214, mean_absolute_error: 0.418449, mean_q: 7.490832, mean_eps: 0.100000\n",
      "  94285/175000: episode: 2648, duration: 0.649s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.517 [16.000, 169.000], mean observation: 0.259 [0.000, 58.000], loss: 0.133251, mean_absolute_error: 0.439996, mean_q: 7.871873, mean_eps: 0.100000\n",
      "  94329/175000: episode: 2649, duration: 0.908s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.023 [35.000, 189.000], mean observation: 0.379 [0.000, 88.000], loss: 0.385164, mean_absolute_error: 0.484467, mean_q: 8.204824, mean_eps: 0.100000\n",
      "  94369/175000: episode: 2650, duration: 0.793s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 175.375 [68.000, 223.000], mean observation: 0.268 [0.000, 80.000], loss: 0.301629, mean_absolute_error: 0.416696, mean_q: 7.539675, mean_eps: 0.100000\n",
      "  94403/175000: episode: 2651, duration: 0.651s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 183.000 [33.000, 223.000], mean observation: 0.212 [0.000, 68.000], loss: 112.148092, mean_absolute_error: 1.132997, mean_q: 9.509701, mean_eps: 0.100000\n",
      "  94437/175000: episode: 2652, duration: 0.862s, episode steps: 34, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.676 [64.000, 222.000], mean observation: 0.213 [0.000, 68.000], loss: 0.165410, mean_absolute_error: 0.451743, mean_q: 7.816226, mean_eps: 0.100000\n",
      "  94495/175000: episode: 2653, duration: 1.180s, episode steps: 58, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 155.931 [20.000, 222.000], mean observation: 0.394 [0.000, 116.000], loss: 92.161341, mean_absolute_error: 1.086635, mean_q: 9.949508, mean_eps: 0.100000\n",
      "  94518/175000: episode: 2654, duration: 0.491s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 138.652 [1.000, 222.000], mean observation: 0.123 [0.000, 46.000], loss: 0.116155, mean_absolute_error: 0.444264, mean_q: 7.901022, mean_eps: 0.100000\n",
      "  94555/175000: episode: 2655, duration: 0.822s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 178.216 [1.000, 222.000], mean observation: 0.366 [0.000, 74.000], loss: 0.102941, mean_absolute_error: 0.431569, mean_q: 7.711719, mean_eps: 0.100000\n",
      "  94598/175000: episode: 2656, duration: 0.892s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 182.953 [1.000, 222.000], mean observation: 0.472 [0.000, 86.000], loss: 0.147134, mean_absolute_error: 0.417954, mean_q: 7.500859, mean_eps: 0.100000\n",
      "  94645/175000: episode: 2657, duration: 1.019s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 136.936 [0.000, 215.000], mean observation: 0.563 [0.000, 94.000], loss: 0.148671, mean_absolute_error: 0.414794, mean_q: 7.451059, mean_eps: 0.100000\n",
      "  94663/175000: episode: 2658, duration: 0.332s, episode steps: 18, steps per second: 54, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 85.389 [68.000, 197.000], mean observation: 0.101 [0.000, 36.000], loss: 0.149447, mean_absolute_error: 0.429837, mean_q: 7.619736, mean_eps: 0.100000\n",
      "  94703/175000: episode: 2659, duration: 0.772s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 100.725 [51.000, 215.000], mean observation: 0.153 [0.000, 80.000], loss: 89.658526, mean_absolute_error: 0.968591, mean_q: 8.912697, mean_eps: 0.100000\n",
      "  94734/175000: episode: 2660, duration: 0.653s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.419 [64.000, 216.000], mean observation: 0.273 [0.000, 62.000], loss: 0.109458, mean_absolute_error: 0.407849, mean_q: 7.372326, mean_eps: 0.100000\n",
      "  94775/175000: episode: 2661, duration: 0.883s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.756 [4.000, 213.000], mean observation: 0.494 [0.000, 82.000], loss: 0.204195, mean_absolute_error: 0.440588, mean_q: 7.725591, mean_eps: 0.100000\n",
      "  94795/175000: episode: 2662, duration: 0.413s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 114.200 [32.000, 215.000], mean observation: 0.141 [0.000, 40.000], loss: 0.219120, mean_absolute_error: 0.417066, mean_q: 7.288403, mean_eps: 0.100000\n",
      "  94838/175000: episode: 2663, duration: 0.904s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 70.907 [12.000, 215.000], mean observation: 0.532 [0.000, 86.000], loss: 1.177757, mean_absolute_error: 0.602987, mean_q: 9.093394, mean_eps: 0.100000\n",
      "  94875/175000: episode: 2664, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 168.811 [32.000, 223.000], mean observation: 0.238 [0.000, 74.000], loss: 43.606074, mean_absolute_error: 0.794446, mean_q: 9.425553, mean_eps: 0.100000\n",
      "  94895/175000: episode: 2665, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 168.850 [126.000, 215.000], mean observation: 0.094 [0.000, 40.000], loss: 198.018938, mean_absolute_error: 1.637680, mean_q: 10.655843, mean_eps: 0.100000\n",
      "  94941/175000: episode: 2666, duration: 0.875s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 160.783 [80.000, 215.000], mean observation: 0.448 [0.000, 92.000], loss: 0.219420, mean_absolute_error: 0.437414, mean_q: 7.853035, mean_eps: 0.100000\n",
      "  94978/175000: episode: 2667, duration: 0.638s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 163.784 [69.000, 221.000], mean observation: 0.270 [0.000, 74.000], loss: 0.609073, mean_absolute_error: 0.437203, mean_q: 7.745652, mean_eps: 0.100000\n",
      "  95009/175000: episode: 2668, duration: 0.595s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 178.774 [70.000, 221.000], mean observation: 0.216 [0.000, 62.000], loss: 83.737986, mean_absolute_error: 1.022289, mean_q: 10.015918, mean_eps: 0.100000\n",
      "  95046/175000: episode: 2669, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.243 [81.000, 221.000], mean observation: 0.318 [0.000, 74.000], loss: 0.130739, mean_absolute_error: 0.434723, mean_q: 7.935575, mean_eps: 0.100000\n",
      "  95094/175000: episode: 2670, duration: 0.950s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.396 [36.000, 223.000], mean observation: 0.577 [0.000, 96.000], loss: 12.538541, mean_absolute_error: 0.628339, mean_q: 9.173884, mean_eps: 0.100000\n",
      "  95150/175000: episode: 2671, duration: 1.029s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 146.679 [46.000, 222.000], mean observation: 0.604 [0.000, 112.000], loss: 0.671952, mean_absolute_error: 0.420170, mean_q: 7.765429, mean_eps: 0.100000\n",
      "  95179/175000: episode: 2672, duration: 0.558s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 122.172 [64.000, 208.000], mean observation: 0.137 [0.000, 58.000], loss: 63.234288, mean_absolute_error: 0.982224, mean_q: 10.692667, mean_eps: 0.100000\n",
      "  95215/175000: episode: 2673, duration: 0.791s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 130.861 [58.000, 208.000], mean observation: 0.295 [0.000, 72.000], loss: 0.221102, mean_absolute_error: 0.431742, mean_q: 8.149587, mean_eps: 0.100000\n",
      "  95254/175000: episode: 2674, duration: 0.822s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.641 [16.000, 208.000], mean observation: 0.243 [0.000, 78.000], loss: 0.421392, mean_absolute_error: 0.421757, mean_q: 8.128333, mean_eps: 0.100000\n",
      "  95304/175000: episode: 2675, duration: 1.013s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.360 [34.000, 208.000], mean observation: 0.390 [0.000, 100.000], loss: 0.246644, mean_absolute_error: 0.550317, mean_q: 9.213711, mean_eps: 0.100000\n",
      "  95344/175000: episode: 2676, duration: 0.882s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 83.250 [44.000, 208.000], mean observation: 0.286 [0.000, 80.000], loss: 48.442046, mean_absolute_error: 0.853038, mean_q: 9.714022, mean_eps: 0.100000\n",
      "  95389/175000: episode: 2677, duration: 0.912s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 91.644 [38.000, 213.000], mean observation: 0.541 [0.000, 90.000], loss: 87.043234, mean_absolute_error: 1.133862, mean_q: 10.419504, mean_eps: 0.100000\n",
      "  95441/175000: episode: 2678, duration: 1.114s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.404 [12.000, 177.000], mean observation: 0.417 [0.000, 104.000], loss: 26.418772, mean_absolute_error: 0.703021, mean_q: 8.742313, mean_eps: 0.100000\n",
      "  95487/175000: episode: 2679, duration: 0.919s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 120.783 [13.000, 208.000], mean observation: 0.393 [0.000, 92.000], loss: 0.440909, mean_absolute_error: 0.464040, mean_q: 7.394078, mean_eps: 0.100000\n",
      "  95535/175000: episode: 2680, duration: 0.919s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.042 [4.000, 177.000], mean observation: 0.468 [0.000, 96.000], loss: 85.844832, mean_absolute_error: 0.954213, mean_q: 8.563770, mean_eps: 0.100000\n",
      "  95579/175000: episode: 2681, duration: 0.855s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.864 [12.000, 177.000], mean observation: 0.264 [0.000, 88.000], loss: 79.559955, mean_absolute_error: 1.093020, mean_q: 10.268416, mean_eps: 0.100000\n",
      "  95616/175000: episode: 2682, duration: 0.786s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 124.973 [12.000, 158.000], mean observation: 0.121 [0.000, 74.000], loss: 0.138657, mean_absolute_error: 0.431922, mean_q: 7.297833, mean_eps: 0.100000\n",
      "  95636/175000: episode: 2683, duration: 0.440s, episode steps: 20, steps per second: 45, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 86.550 [46.000, 170.000], mean observation: 0.099 [0.000, 40.000], loss: 0.175921, mean_absolute_error: 0.441593, mean_q: 7.582382, mean_eps: 0.100000\n",
      "  95679/175000: episode: 2684, duration: 0.818s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.093 [17.000, 199.000], mean observation: 0.309 [0.000, 86.000], loss: 0.202087, mean_absolute_error: 0.473118, mean_q: 7.765982, mean_eps: 0.100000\n",
      "  95700/175000: episode: 2685, duration: 0.424s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 105.714 [46.000, 190.000], mean observation: 0.117 [0.000, 42.000], loss: 0.219643, mean_absolute_error: 0.456892, mean_q: 7.444852, mean_eps: 0.100000\n",
      "  95732/175000: episode: 2686, duration: 0.707s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.500 [46.000, 158.000], mean observation: 0.159 [0.000, 64.000], loss: 175.160716, mean_absolute_error: 1.656050, mean_q: 11.320895, mean_eps: 0.100000\n",
      "  95772/175000: episode: 2687, duration: 0.846s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.775 [1.000, 218.000], mean observation: 0.304 [0.000, 80.000], loss: 0.319634, mean_absolute_error: 0.482123, mean_q: 7.584286, mean_eps: 0.100000\n",
      "  95830/175000: episode: 2688, duration: 1.229s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.000 [0.000, 201.000], mean observation: 0.635 [0.000, 116.000], loss: 89.660380, mean_absolute_error: 1.084128, mean_q: 9.579416, mean_eps: 0.100000\n",
      "  95880/175000: episode: 2689, duration: 1.052s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 84.060 [0.000, 219.000], mean observation: 0.407 [0.000, 100.000], loss: 0.211295, mean_absolute_error: 0.452557, mean_q: 7.416929, mean_eps: 0.100000\n",
      "  95915/175000: episode: 2690, duration: 0.712s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.343 [24.000, 191.000], mean observation: 0.323 [0.000, 70.000], loss: 0.385304, mean_absolute_error: 0.443817, mean_q: 7.585098, mean_eps: 0.100000\n",
      "  95948/175000: episode: 2691, duration: 0.717s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 146.364 [0.000, 184.000], mean observation: 0.153 [0.000, 66.000], loss: 0.210580, mean_absolute_error: 0.442918, mean_q: 7.459562, mean_eps: 0.100000\n",
      "  95990/175000: episode: 2692, duration: 0.873s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.857 [15.000, 204.000], mean observation: 0.363 [0.000, 84.000], loss: 0.766490, mean_absolute_error: 0.448790, mean_q: 7.534603, mean_eps: 0.100000\n",
      "  96021/175000: episode: 2693, duration: 0.628s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 68.129 [16.000, 184.000], mean observation: 0.265 [0.000, 62.000], loss: 60.214545, mean_absolute_error: 0.926897, mean_q: 9.206897, mean_eps: 0.100000\n",
      "  96043/175000: episode: 2694, duration: 0.379s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 105.000 [14.000, 184.000], mean observation: 0.097 [0.000, 44.000], loss: 0.118817, mean_absolute_error: 0.415220, mean_q: 7.063527, mean_eps: 0.100000\n",
      "  96057/175000: episode: 2695, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 128.429 [16.000, 184.000], mean observation: 0.062 [0.000, 28.000], loss: 0.182980, mean_absolute_error: 0.461077, mean_q: 7.387852, mean_eps: 0.100000\n",
      "  96080/175000: episode: 2696, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 106.609 [1.000, 185.000], mean observation: 0.114 [0.000, 46.000], loss: 0.522889, mean_absolute_error: 0.473056, mean_q: 7.538762, mean_eps: 0.100000\n",
      "  96133/175000: episode: 2697, duration: 1.097s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.358 [0.000, 184.000], mean observation: 0.507 [0.000, 106.000], loss: 0.262604, mean_absolute_error: 0.449434, mean_q: 7.153570, mean_eps: 0.100000\n",
      "  96172/175000: episode: 2698, duration: 0.855s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.590 [16.000, 199.000], mean observation: 0.289 [0.000, 78.000], loss: 0.121039, mean_absolute_error: 0.436453, mean_q: 7.073071, mean_eps: 0.100000\n",
      "  96206/175000: episode: 2699, duration: 0.724s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.176 [8.000, 177.000], mean observation: 0.206 [0.000, 68.000], loss: 90.258796, mean_absolute_error: 1.041096, mean_q: 8.998828, mean_eps: 0.100000\n",
      "  96224/175000: episode: 2700, duration: 0.460s, episode steps: 18, steps per second: 39, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 99.222 [51.000, 177.000], mean observation: 0.109 [0.000, 36.000], loss: 45.906425, mean_absolute_error: 1.092210, mean_q: 11.300214, mean_eps: 0.100000\n",
      "  96258/175000: episode: 2701, duration: 0.745s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.235 [51.000, 221.000], mean observation: 0.209 [0.000, 68.000], loss: 3.566904, mean_absolute_error: 0.618674, mean_q: 8.598240, mean_eps: 0.100000\n",
      "  96307/175000: episode: 2702, duration: 0.954s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.510 [33.000, 208.000], mean observation: 0.398 [0.000, 98.000], loss: 31.126335, mean_absolute_error: 0.741951, mean_q: 8.546781, mean_eps: 0.100000\n",
      "  96350/175000: episode: 2703, duration: 0.868s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.581 [0.000, 160.000], mean observation: 0.201 [0.000, 86.000], loss: 0.276985, mean_absolute_error: 0.475897, mean_q: 7.391874, mean_eps: 0.100000\n",
      "  96388/175000: episode: 2704, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 98.132 [0.000, 209.000], mean observation: 0.304 [0.000, 76.000], loss: 0.168369, mean_absolute_error: 0.472552, mean_q: 7.361056, mean_eps: 0.100000\n",
      "  96443/175000: episode: 2705, duration: 1.044s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 68.855 [0.000, 166.000], mean observation: 0.305 [0.000, 110.000], loss: 102.534063, mean_absolute_error: 1.166002, mean_q: 9.702524, mean_eps: 0.100000\n",
      "  96465/175000: episode: 2706, duration: 0.476s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 107.318 [17.000, 189.000], mean observation: 0.124 [0.000, 44.000], loss: 0.106163, mean_absolute_error: 0.469524, mean_q: 7.403372, mean_eps: 0.100000\n",
      "  96496/175000: episode: 2707, duration: 0.575s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 101.097 [4.000, 209.000], mean observation: 0.272 [0.000, 62.000], loss: 0.262463, mean_absolute_error: 0.492295, mean_q: 7.523671, mean_eps: 0.100000\n",
      "  96511/175000: episode: 2708, duration: 0.280s, episode steps: 15, steps per second: 54, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 146.600 [52.000, 193.000], mean observation: 0.050 [0.000, 30.000], loss: 0.185672, mean_absolute_error: 0.441484, mean_q: 7.071141, mean_eps: 0.100000\n",
      "  96532/175000: episode: 2709, duration: 0.420s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.048 [52.000, 200.000], mean observation: 0.118 [0.000, 42.000], loss: 0.197361, mean_absolute_error: 0.452136, mean_q: 7.416527, mean_eps: 0.100000\n",
      "  96561/175000: episode: 2710, duration: 0.697s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 115.621 [5.000, 200.000], mean observation: 0.183 [0.000, 58.000], loss: 0.280811, mean_absolute_error: 0.463752, mean_q: 7.320551, mean_eps: 0.100000\n",
      "  96594/175000: episode: 2711, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.515 [25.000, 200.000], mean observation: 0.308 [0.000, 66.000], loss: 0.160143, mean_absolute_error: 0.459209, mean_q: 7.363111, mean_eps: 0.100000\n",
      "  96629/175000: episode: 2712, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 171.343 [7.000, 215.000], mean observation: 0.117 [0.000, 70.000], loss: 0.174361, mean_absolute_error: 0.466759, mean_q: 7.228808, mean_eps: 0.100000\n",
      "  96682/175000: episode: 2713, duration: 0.920s, episode steps: 53, steps per second: 58, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 99.415 [8.000, 218.000], mean observation: 0.339 [0.000, 106.000], loss: 0.187287, mean_absolute_error: 0.451751, mean_q: 7.142952, mean_eps: 0.100000\n",
      "  96700/175000: episode: 2714, duration: 0.376s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 116.278 [0.000, 215.000], mean observation: 0.102 [0.000, 36.000], loss: 0.212100, mean_absolute_error: 0.428837, mean_q: 6.959096, mean_eps: 0.100000\n",
      "  96745/175000: episode: 2715, duration: 0.929s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 151.978 [51.000, 215.000], mean observation: 0.261 [0.000, 90.000], loss: 0.152197, mean_absolute_error: 0.430086, mean_q: 6.795277, mean_eps: 0.100000\n",
      "  96781/175000: episode: 2716, duration: 0.671s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 86.917 [0.000, 215.000], mean observation: 0.332 [0.000, 72.000], loss: 84.120888, mean_absolute_error: 1.002022, mean_q: 8.447370, mean_eps: 0.100000\n",
      "  96816/175000: episode: 2717, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.657 [0.000, 207.000], mean observation: 0.299 [0.000, 70.000], loss: 149.702731, mean_absolute_error: 1.312736, mean_q: 8.796304, mean_eps: 0.100000\n",
      "  96869/175000: episode: 2718, duration: 1.032s, episode steps: 53, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 157.000 [16.000, 215.000], mean observation: 0.197 [0.000, 106.000], loss: 0.242280, mean_absolute_error: 0.442301, mean_q: 6.982033, mean_eps: 0.100000\n",
      "  96913/175000: episode: 2719, duration: 0.796s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.500 [0.000, 215.000], mean observation: 0.374 [0.000, 88.000], loss: 0.257361, mean_absolute_error: 0.442260, mean_q: 6.820365, mean_eps: 0.100000\n",
      "  96950/175000: episode: 2720, duration: 0.655s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 128.162 [0.000, 215.000], mean observation: 0.316 [0.000, 74.000], loss: 65.568778, mean_absolute_error: 0.929880, mean_q: 8.542952, mean_eps: 0.100000\n",
      "  96967/175000: episode: 2721, duration: 0.313s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 104.235 [0.000, 216.000], mean observation: 0.089 [0.000, 34.000], loss: 0.264831, mean_absolute_error: 0.428714, mean_q: 6.508184, mean_eps: 0.100000\n",
      "  97006/175000: episode: 2722, duration: 0.723s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.179 [0.000, 209.000], mean observation: 0.503 [0.000, 78.000], loss: 0.212909, mean_absolute_error: 0.431635, mean_q: 6.673202, mean_eps: 0.100000\n",
      "  97046/175000: episode: 2723, duration: 0.762s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.300 [12.000, 218.000], mean observation: 0.587 [0.000, 80.000], loss: 0.154932, mean_absolute_error: 0.428189, mean_q: 6.485694, mean_eps: 0.100000\n",
      "  97095/175000: episode: 2724, duration: 0.923s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.796 [1.000, 209.000], mean observation: 0.500 [0.000, 98.000], loss: 15.428116, mean_absolute_error: 0.652740, mean_q: 7.922495, mean_eps: 0.100000\n",
      "  97123/175000: episode: 2725, duration: 0.515s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.857 [0.000, 218.000], mean observation: 0.180 [0.000, 56.000], loss: 0.155373, mean_absolute_error: 0.440431, mean_q: 6.593488, mean_eps: 0.100000\n",
      "  97150/175000: episode: 2726, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 37.852 [16.000, 169.000], mean observation: 0.086 [0.000, 54.000], loss: 0.332507, mean_absolute_error: 0.440248, mean_q: 6.602644, mean_eps: 0.100000\n",
      "  97211/175000: episode: 2727, duration: 1.070s, episode steps: 61, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 137.803 [16.000, 221.000], mean observation: 0.853 [0.000, 122.000], loss: 62.623522, mean_absolute_error: 1.092869, mean_q: 10.280415, mean_eps: 0.100000\n",
      "  97259/175000: episode: 2728, duration: 0.864s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.417 [7.000, 209.000], mean observation: 0.670 [0.000, 96.000], loss: 32.786629, mean_absolute_error: 0.709953, mean_q: 7.818552, mean_eps: 0.100000\n",
      "  97314/175000: episode: 2729, duration: 0.995s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 144.236 [6.000, 224.000], mean observation: 0.619 [0.000, 110.000], loss: 61.928668, mean_absolute_error: 0.866070, mean_q: 8.040003, mean_eps: 0.100000\n",
      "  97353/175000: episode: 2730, duration: 0.820s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.205 [68.000, 218.000], mean observation: 0.363 [0.000, 78.000], loss: 0.348806, mean_absolute_error: 0.466960, mean_q: 6.866944, mean_eps: 0.100000\n",
      "  97381/175000: episode: 2731, duration: 0.497s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 135.893 [41.000, 207.000], mean observation: 0.206 [0.000, 56.000], loss: 0.225452, mean_absolute_error: 0.471229, mean_q: 6.840784, mean_eps: 0.100000\n",
      "  97409/175000: episode: 2732, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 132.214 [68.000, 158.000], mean observation: 0.134 [0.000, 56.000], loss: 0.155347, mean_absolute_error: 0.480585, mean_q: 7.132651, mean_eps: 0.100000\n",
      "  97441/175000: episode: 2733, duration: 0.586s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 131.688 [4.000, 213.000], mean observation: 0.218 [0.000, 64.000], loss: 0.577683, mean_absolute_error: 0.464277, mean_q: 6.691432, mean_eps: 0.100000\n",
      "  97470/175000: episode: 2734, duration: 0.502s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 133.069 [5.000, 222.000], mean observation: 0.217 [0.000, 58.000], loss: 1.838380, mean_absolute_error: 0.743286, mean_q: 9.565054, mean_eps: 0.100000\n",
      "  97497/175000: episode: 2735, duration: 0.602s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 169.593 [98.000, 218.000], mean observation: 0.160 [0.000, 54.000], loss: 0.542681, mean_absolute_error: 0.495977, mean_q: 7.499184, mean_eps: 0.100000\n",
      "  97524/175000: episode: 2736, duration: 0.504s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 38.111 [27.000, 204.000], mean observation: 0.067 [0.000, 54.000], loss: 54.304406, mean_absolute_error: 1.016885, mean_q: 10.341667, mean_eps: 0.100000\n",
      "  97569/175000: episode: 2737, duration: 0.970s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.333 [7.000, 212.000], mean observation: 0.304 [0.000, 90.000], loss: 0.277988, mean_absolute_error: 0.485522, mean_q: 7.378133, mean_eps: 0.100000\n",
      "  97609/175000: episode: 2738, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 29.300 [27.000, 119.000], mean observation: 0.093 [0.000, 80.000], loss: 0.249097, mean_absolute_error: 0.481978, mean_q: 7.180239, mean_eps: 0.100000\n",
      "  97654/175000: episode: 2739, duration: 0.777s, episode steps: 45, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 46.978 [5.000, 208.000], mean observation: 0.295 [0.000, 90.000], loss: 0.545713, mean_absolute_error: 0.459562, mean_q: 6.734743, mean_eps: 0.100000\n",
      "  97696/175000: episode: 2740, duration: 0.768s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.429 [5.000, 200.000], mean observation: 0.453 [0.000, 84.000], loss: 0.589505, mean_absolute_error: 0.479132, mean_q: 6.978572, mean_eps: 0.100000\n",
      "  97743/175000: episode: 2741, duration: 0.881s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 69.021 [5.000, 215.000], mean observation: 0.513 [0.000, 94.000], loss: 7.168214, mean_absolute_error: 0.498188, mean_q: 6.749841, mean_eps: 0.100000\n",
      "  97795/175000: episode: 2742, duration: 0.965s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 63.481 [5.000, 221.000], mean observation: 0.591 [0.000, 104.000], loss: 0.238504, mean_absolute_error: 0.469326, mean_q: 6.805117, mean_eps: 0.100000\n",
      "  97807/175000: episode: 2743, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 140.583 [52.000, 215.000], mean observation: 0.080 [0.000, 24.000], loss: 0.340945, mean_absolute_error: 0.490197, mean_q: 6.985286, mean_eps: 0.100000\n",
      "  97857/175000: episode: 2744, duration: 0.920s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.580 [5.000, 215.000], mean observation: 0.739 [0.000, 100.000], loss: 0.192144, mean_absolute_error: 0.496079, mean_q: 7.277159, mean_eps: 0.100000\n",
      "  97891/175000: episode: 2745, duration: 0.605s, episode steps: 34, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.294 [5.000, 184.000], mean observation: 0.216 [0.000, 68.000], loss: 190.113508, mean_absolute_error: 1.943953, mean_q: 13.175636, mean_eps: 0.100000\n",
      "  97936/175000: episode: 2746, duration: 0.967s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.978 [7.000, 222.000], mean observation: 0.297 [0.000, 90.000], loss: 25.714665, mean_absolute_error: 0.751099, mean_q: 8.872542, mean_eps: 0.100000\n",
      "  97991/175000: episode: 2747, duration: 1.018s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 109.400 [27.000, 222.000], mean observation: 0.498 [0.000, 110.000], loss: 0.564085, mean_absolute_error: 0.503028, mean_q: 7.631963, mean_eps: 0.100000\n",
      "  98019/175000: episode: 2748, duration: 0.529s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 33.571 [27.000, 211.000], mean observation: 0.104 [0.000, 56.000], loss: 0.722214, mean_absolute_error: 0.515562, mean_q: 8.078150, mean_eps: 0.100000\n",
      "  98067/175000: episode: 2749, duration: 0.957s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 149.146 [16.000, 222.000], mean observation: 0.484 [0.000, 96.000], loss: 0.325153, mean_absolute_error: 0.493672, mean_q: 7.639015, mean_eps: 0.100000\n",
      "  98108/175000: episode: 2750, duration: 0.791s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 49.902 [27.000, 218.000], mean observation: 0.124 [0.000, 82.000], loss: 92.084763, mean_absolute_error: 1.254175, mean_q: 11.120115, mean_eps: 0.100000\n",
      "  98144/175000: episode: 2751, duration: 0.823s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 55.000 [27.000, 221.000], mean observation: 0.096 [0.000, 72.000], loss: 47.123081, mean_absolute_error: 0.858449, mean_q: 9.300199, mean_eps: 0.100000\n",
      "  98170/175000: episode: 2752, duration: 0.487s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 30.077 [27.000, 107.000], mean observation: 0.063 [0.000, 52.000], loss: 0.243889, mean_absolute_error: 0.491724, mean_q: 7.683935, mean_eps: 0.100000\n",
      "  98194/175000: episode: 2753, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.875 [27.000, 191.000], mean observation: 0.122 [0.000, 48.000], loss: 0.134129, mean_absolute_error: 0.480053, mean_q: 7.586378, mean_eps: 0.100000\n",
      "  98230/175000: episode: 2754, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 140.333 [81.000, 191.000], mean observation: 0.268 [0.000, 72.000], loss: 0.332161, mean_absolute_error: 0.486940, mean_q: 8.072876, mean_eps: 0.100000\n",
      "  98252/175000: episode: 2755, duration: 0.430s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 150.136 [81.000, 191.000], mean observation: 0.121 [0.000, 44.000], loss: 0.300163, mean_absolute_error: 0.496346, mean_q: 7.883311, mean_eps: 0.100000\n",
      "  98298/175000: episode: 2756, duration: 0.836s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 130.978 [3.000, 210.000], mean observation: 0.446 [0.000, 92.000], loss: 1.405910, mean_absolute_error: 0.619877, mean_q: 9.149370, mean_eps: 0.100000\n",
      "  98344/175000: episode: 2757, duration: 0.865s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.957 [62.000, 210.000], mean observation: 0.388 [0.000, 92.000], loss: 27.700159, mean_absolute_error: 0.717141, mean_q: 8.782033, mean_eps: 0.100000\n",
      "  98376/175000: episode: 2758, duration: 0.638s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 136.500 [6.000, 210.000], mean observation: 0.245 [0.000, 64.000], loss: 0.303019, mean_absolute_error: 0.447997, mean_q: 7.589820, mean_eps: 0.100000\n",
      "  98406/175000: episode: 2759, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 163.800 [81.000, 212.000], mean observation: 0.303 [0.000, 60.000], loss: 0.574139, mean_absolute_error: 0.438741, mean_q: 7.252894, mean_eps: 0.100000\n",
      "  98434/175000: episode: 2760, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 146.643 [76.000, 210.000], mean observation: 0.195 [0.000, 56.000], loss: 0.828101, mean_absolute_error: 0.452937, mean_q: 7.197519, mean_eps: 0.100000\n",
      "  98473/175000: episode: 2761, duration: 0.826s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.154 [0.000, 210.000], mean observation: 0.446 [0.000, 78.000], loss: 0.167214, mean_absolute_error: 0.445717, mean_q: 7.061623, mean_eps: 0.100000\n",
      "  98515/175000: episode: 2762, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 127.881 [0.000, 215.000], mean observation: 0.617 [0.000, 84.000], loss: 0.148594, mean_absolute_error: 0.455688, mean_q: 7.432096, mean_eps: 0.100000\n",
      "  98563/175000: episode: 2763, duration: 0.850s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 152.042 [11.000, 215.000], mean observation: 0.640 [0.000, 96.000], loss: 4.143004, mean_absolute_error: 1.038326, mean_q: 13.031509, mean_eps: 0.100000\n",
      "  98582/175000: episode: 2764, duration: 0.375s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 45.368 [27.000, 160.000], mean observation: 0.048 [0.000, 38.000], loss: 0.222588, mean_absolute_error: 0.456793, mean_q: 7.756647, mean_eps: 0.100000\n",
      "  98609/175000: episode: 2765, duration: 0.522s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 180.370 [81.000, 219.000], mean observation: 0.085 [0.000, 54.000], loss: 0.358855, mean_absolute_error: 0.480757, mean_q: 7.872363, mean_eps: 0.100000\n",
      "  98643/175000: episode: 2766, duration: 0.567s, episode steps: 34, steps per second: 60, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.324 [9.000, 221.000], mean observation: 0.167 [0.000, 68.000], loss: 0.375419, mean_absolute_error: 0.459665, mean_q: 7.332923, mean_eps: 0.100000\n",
      "  98685/175000: episode: 2767, duration: 0.769s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 54.595 [27.000, 210.000], mean observation: 0.111 [0.000, 84.000], loss: 2.317168, mean_absolute_error: 0.597983, mean_q: 8.606059, mean_eps: 0.100000\n",
      "  98716/175000: episode: 2768, duration: 0.586s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 53.452 [27.000, 210.000], mean observation: 0.087 [0.000, 62.000], loss: 0.639960, mean_absolute_error: 0.701299, mean_q: 9.718486, mean_eps: 0.100000\n",
      "  98764/175000: episode: 2769, duration: 0.919s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.021 [8.000, 214.000], mean observation: 0.479 [0.000, 96.000], loss: 0.321453, mean_absolute_error: 0.461114, mean_q: 7.271057, mean_eps: 0.100000\n",
      "  98794/175000: episode: 2770, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 157.833 [81.000, 210.000], mean observation: 0.307 [0.000, 60.000], loss: 0.551561, mean_absolute_error: 0.469386, mean_q: 6.907839, mean_eps: 0.100000\n",
      "  98843/175000: episode: 2771, duration: 0.842s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 152.531 [12.000, 212.000], mean observation: 0.691 [0.000, 98.000], loss: 0.617272, mean_absolute_error: 0.484824, mean_q: 7.436174, mean_eps: 0.100000\n",
      "  98867/175000: episode: 2772, duration: 0.434s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 119.125 [35.000, 210.000], mean observation: 0.155 [0.000, 48.000], loss: 0.193117, mean_absolute_error: 0.469157, mean_q: 7.350373, mean_eps: 0.100000\n",
      "  98884/175000: episode: 2773, duration: 0.375s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 127.647 [48.000, 210.000], mean observation: 0.141 [0.000, 34.000], loss: 0.198453, mean_absolute_error: 0.452718, mean_q: 7.018484, mean_eps: 0.100000\n",
      "  98934/175000: episode: 2774, duration: 0.899s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 162.660 [48.000, 210.000], mean observation: 0.321 [0.000, 100.000], loss: 9.697454, mean_absolute_error: 0.874041, mean_q: 10.594786, mean_eps: 0.100000\n",
      "  98974/175000: episode: 2775, duration: 0.704s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 146.150 [45.000, 210.000], mean observation: 0.269 [0.000, 80.000], loss: 0.519242, mean_absolute_error: 0.458349, mean_q: 6.935048, mean_eps: 0.100000\n",
      "  99007/175000: episode: 2776, duration: 0.577s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.727 [10.000, 139.000], mean observation: 0.138 [0.000, 66.000], loss: 0.122174, mean_absolute_error: 0.448547, mean_q: 6.802012, mean_eps: 0.100000\n",
      "  99026/175000: episode: 2777, duration: 0.390s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 104.579 [28.000, 196.000], mean observation: 0.070 [0.000, 38.000], loss: 1.657517, mean_absolute_error: 0.771724, mean_q: 10.162837, mean_eps: 0.100000\n",
      "  99074/175000: episode: 2778, duration: 0.924s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 101.292 [1.000, 210.000], mean observation: 0.381 [0.000, 96.000], loss: 0.212211, mean_absolute_error: 0.447137, mean_q: 7.017683, mean_eps: 0.100000\n",
      "  99127/175000: episode: 2779, duration: 0.933s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 158.358 [33.000, 213.000], mean observation: 0.683 [0.000, 106.000], loss: 1.083692, mean_absolute_error: 0.576062, mean_q: 8.256540, mean_eps: 0.100000\n",
      "  99160/175000: episode: 2780, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.939 [35.000, 210.000], mean observation: 0.318 [0.000, 66.000], loss: 0.434948, mean_absolute_error: 0.455120, mean_q: 7.093408, mean_eps: 0.100000\n",
      "  99192/175000: episode: 2781, duration: 0.622s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 161.250 [76.000, 210.000], mean observation: 0.223 [0.000, 64.000], loss: 0.352982, mean_absolute_error: 0.474639, mean_q: 7.124758, mean_eps: 0.100000\n",
      "  99220/175000: episode: 2782, duration: 0.619s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.643 [13.000, 210.000], mean observation: 0.230 [0.000, 56.000], loss: 0.324982, mean_absolute_error: 0.714821, mean_q: 9.148006, mean_eps: 0.100000\n",
      "  99275/175000: episode: 2783, duration: 1.033s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 131.927 [10.000, 208.000], mean observation: 0.570 [0.000, 110.000], loss: 0.218475, mean_absolute_error: 0.485654, mean_q: 7.197667, mean_eps: 0.100000\n",
      "  99306/175000: episode: 2784, duration: 0.577s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 86.387 [6.000, 209.000], mean observation: 0.303 [0.000, 62.000], loss: 0.319607, mean_absolute_error: 0.468535, mean_q: 7.011353, mean_eps: 0.100000\n",
      "  99330/175000: episode: 2785, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 54.583 [1.000, 209.000], mean observation: 0.138 [0.000, 48.000], loss: 0.937125, mean_absolute_error: 0.744365, mean_q: 9.637947, mean_eps: 0.100000\n",
      "  99386/175000: episode: 2786, duration: 1.166s, episode steps: 56, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 47.714 [3.000, 209.000], mean observation: 0.956 [0.000, 112.000], loss: 0.299903, mean_absolute_error: 0.462228, mean_q: 7.020946, mean_eps: 0.100000\n",
      "  99417/175000: episode: 2787, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 149.548 [3.000, 205.000], mean observation: 0.408 [0.000, 62.000], loss: 0.166891, mean_absolute_error: 0.456172, mean_q: 6.925194, mean_eps: 0.100000\n",
      "  99465/175000: episode: 2788, duration: 0.869s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 100.146 [10.000, 210.000], mean observation: 0.587 [0.000, 96.000], loss: 14.522673, mean_absolute_error: 0.639091, mean_q: 8.027593, mean_eps: 0.100000\n",
      "  99501/175000: episode: 2789, duration: 0.632s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.056 [4.000, 210.000], mean observation: 0.502 [0.000, 72.000], loss: 2.619223, mean_absolute_error: 0.647947, mean_q: 8.792446, mean_eps: 0.100000\n",
      "  99540/175000: episode: 2790, duration: 0.702s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.128 [14.000, 209.000], mean observation: 0.309 [0.000, 78.000], loss: 0.160003, mean_absolute_error: 0.452525, mean_q: 6.959724, mean_eps: 0.100000\n",
      "  99574/175000: episode: 2791, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 145.059 [30.000, 209.000], mean observation: 0.439 [0.000, 68.000], loss: 0.126137, mean_absolute_error: 0.446532, mean_q: 6.971879, mean_eps: 0.100000\n",
      "  99625/175000: episode: 2792, duration: 0.961s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 158.333 [43.000, 209.000], mean observation: 0.561 [0.000, 102.000], loss: 0.377687, mean_absolute_error: 0.593075, mean_q: 8.379195, mean_eps: 0.100000\n",
      "  99668/175000: episode: 2793, duration: 0.797s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.419 [12.000, 209.000], mean observation: 0.500 [0.000, 86.000], loss: 13.461345, mean_absolute_error: 1.020798, mean_q: 11.686267, mean_eps: 0.100000\n",
      "  99695/175000: episode: 2794, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 133.444 [4.000, 222.000], mean observation: 0.253 [0.000, 54.000], loss: 44.250924, mean_absolute_error: 1.133060, mean_q: 11.456789, mean_eps: 0.100000\n",
      "  99743/175000: episode: 2795, duration: 0.849s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.812 [40.000, 222.000], mean observation: 0.565 [0.000, 96.000], loss: 36.225963, mean_absolute_error: 0.898555, mean_q: 9.843169, mean_eps: 0.100000\n",
      "  99778/175000: episode: 2796, duration: 0.661s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.371 [1.000, 176.000], mean observation: 0.291 [0.000, 70.000], loss: 0.141129, mean_absolute_error: 0.462901, mean_q: 7.434818, mean_eps: 0.100000\n",
      "  99809/175000: episode: 2797, duration: 0.598s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 134.161 [34.000, 176.000], mean observation: 0.256 [0.000, 62.000], loss: 1.256940, mean_absolute_error: 0.664388, mean_q: 9.315355, mean_eps: 0.100000\n",
      "  99869/175000: episode: 2798, duration: 1.116s, episode steps: 60, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 106.950 [6.000, 176.000], mean observation: 0.750 [0.000, 120.000], loss: 30.169064, mean_absolute_error: 0.817985, mean_q: 9.542542, mean_eps: 0.100000\n",
      "  99900/175000: episode: 2799, duration: 0.565s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 78.968 [24.000, 203.000], mean observation: 0.282 [0.000, 62.000], loss: 0.109688, mean_absolute_error: 0.434947, mean_q: 7.167051, mean_eps: 0.100000\n",
      "  99938/175000: episode: 2800, duration: 0.766s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.526 [37.000, 177.000], mean observation: 0.277 [0.000, 76.000], loss: 4.769007, mean_absolute_error: 0.626603, mean_q: 8.992407, mean_eps: 0.100000\n",
      "  99984/175000: episode: 2801, duration: 0.834s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.022 [9.000, 209.000], mean observation: 0.501 [0.000, 92.000], loss: 2.324854, mean_absolute_error: 0.765520, mean_q: 10.115243, mean_eps: 0.100000\n",
      " 100023/175000: episode: 2802, duration: 0.765s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.000 [55.000, 55.000], mean observation: 0.090 [0.000, 78.000], loss: 0.172368, mean_absolute_error: 0.440038, mean_q: 7.219145, mean_eps: 0.100000\n",
      " 100067/175000: episode: 2803, duration: 0.789s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.318 [43.000, 221.000], mean observation: 0.297 [0.000, 88.000], loss: 0.182791, mean_absolute_error: 0.440052, mean_q: 7.544099, mean_eps: 0.100000\n",
      " 100102/175000: episode: 2804, duration: 0.637s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 112.429 [6.000, 207.000], mean observation: 0.260 [0.000, 70.000], loss: 0.243009, mean_absolute_error: 0.452381, mean_q: 7.522562, mean_eps: 0.100000\n",
      " 100143/175000: episode: 2805, duration: 0.715s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.317 [12.000, 207.000], mean observation: 0.355 [0.000, 82.000], loss: 0.198933, mean_absolute_error: 0.444874, mean_q: 7.480418, mean_eps: 0.100000\n",
      " 100179/175000: episode: 2806, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.694 [24.000, 213.000], mean observation: 0.265 [0.000, 72.000], loss: 0.156094, mean_absolute_error: 0.445117, mean_q: 7.484464, mean_eps: 0.100000\n",
      " 100213/175000: episode: 2807, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 153.559 [55.000, 213.000], mean observation: 0.279 [0.000, 68.000], loss: 0.201039, mean_absolute_error: 0.434640, mean_q: 7.348600, mean_eps: 0.100000\n",
      " 100239/175000: episode: 2808, duration: 0.455s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 155.731 [49.000, 213.000], mean observation: 0.239 [0.000, 52.000], loss: 0.665891, mean_absolute_error: 0.438511, mean_q: 7.360682, mean_eps: 0.100000\n",
      " 100284/175000: episode: 2809, duration: 0.884s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 175.956 [3.000, 213.000], mean observation: 0.256 [0.000, 90.000], loss: 9.713488, mean_absolute_error: 0.767258, mean_q: 9.968536, mean_eps: 0.100000\n",
      " 100332/175000: episode: 2810, duration: 0.955s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 113.646 [48.000, 176.000], mean observation: 0.354 [0.000, 96.000], loss: 22.544215, mean_absolute_error: 0.677348, mean_q: 8.647352, mean_eps: 0.100000\n",
      " 100356/175000: episode: 2811, duration: 0.499s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 113.208 [48.000, 208.000], mean observation: 0.107 [0.000, 48.000], loss: 0.856483, mean_absolute_error: 0.489053, mean_q: 7.319405, mean_eps: 0.100000\n",
      " 100399/175000: episode: 2812, duration: 0.814s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.581 [16.000, 212.000], mean observation: 0.526 [0.000, 86.000], loss: 0.462267, mean_absolute_error: 0.457240, mean_q: 6.892063, mean_eps: 0.100000\n",
      " 100413/175000: episode: 2813, duration: 0.288s, episode steps: 14, steps per second: 49, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 125.429 [16.000, 208.000], mean observation: 0.064 [0.000, 28.000], loss: 0.469645, mean_absolute_error: 0.479130, mean_q: 7.156640, mean_eps: 0.100000\n",
      " 100448/175000: episode: 2814, duration: 0.662s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.914 [9.000, 208.000], mean observation: 0.191 [0.000, 70.000], loss: 0.260135, mean_absolute_error: 0.431493, mean_q: 7.025555, mean_eps: 0.100000\n",
      " 100475/175000: episode: 2815, duration: 0.477s, episode steps: 27, steps per second: 57, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 154.815 [16.000, 208.000], mean observation: 0.134 [0.000, 54.000], loss: 0.161510, mean_absolute_error: 0.443092, mean_q: 7.383690, mean_eps: 0.100000\n",
      " 100495/175000: episode: 2816, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 153.300 [55.000, 211.000], mean observation: 0.106 [0.000, 40.000], loss: 0.629245, mean_absolute_error: 0.457143, mean_q: 7.530973, mean_eps: 0.100000\n",
      " 100522/175000: episode: 2817, duration: 0.513s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 148.778 [50.000, 196.000], mean observation: 0.166 [0.000, 54.000], loss: 12.241233, mean_absolute_error: 0.719283, mean_q: 9.466365, mean_eps: 0.100000\n",
      " 100564/175000: episode: 2818, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.929 [22.000, 192.000], mean observation: 0.487 [0.000, 84.000], loss: 3.725436, mean_absolute_error: 0.629243, mean_q: 9.033531, mean_eps: 0.100000\n",
      " 100582/175000: episode: 2819, duration: 0.358s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 141.722 [53.000, 192.000], mean observation: 0.078 [0.000, 36.000], loss: 0.296717, mean_absolute_error: 0.437102, mean_q: 7.787465, mean_eps: 0.100000\n",
      " 100621/175000: episode: 2820, duration: 0.747s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 148.513 [7.000, 191.000], mean observation: 0.298 [0.000, 78.000], loss: 9.054726, mean_absolute_error: 0.640270, mean_q: 9.227747, mean_eps: 0.100000\n",
      " 100665/175000: episode: 2821, duration: 0.791s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.591 [32.000, 203.000], mean observation: 0.485 [0.000, 88.000], loss: 0.232247, mean_absolute_error: 0.460570, mean_q: 7.565595, mean_eps: 0.100000\n",
      " 100696/175000: episode: 2822, duration: 0.550s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 143.323 [41.000, 207.000], mean observation: 0.213 [0.000, 62.000], loss: 5.841234, mean_absolute_error: 0.457441, mean_q: 7.319233, mean_eps: 0.100000\n",
      " 100707/175000: episode: 2823, duration: 0.229s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 96.000 [96.000, 96.000], mean observation: 0.028 [0.000, 22.000], loss: 0.470756, mean_absolute_error: 0.446985, mean_q: 7.455961, mean_eps: 0.100000\n",
      " 100750/175000: episode: 2824, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.326 [67.000, 182.000], mean observation: 0.209 [0.000, 86.000], loss: 0.171224, mean_absolute_error: 0.445279, mean_q: 7.647472, mean_eps: 0.100000\n",
      " 100805/175000: episode: 2825, duration: 0.986s, episode steps: 55, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 142.164 [20.000, 193.000], mean observation: 0.522 [0.000, 110.000], loss: 12.490714, mean_absolute_error: 0.614234, mean_q: 8.751237, mean_eps: 0.100000\n",
      " 100844/175000: episode: 2826, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.564 [13.000, 158.000], mean observation: 0.279 [0.000, 78.000], loss: 54.806545, mean_absolute_error: 1.070880, mean_q: 11.665834, mean_eps: 0.100000\n",
      " 100891/175000: episode: 2827, duration: 0.864s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 90.255 [0.000, 205.000], mean observation: 0.339 [0.000, 94.000], loss: 0.208943, mean_absolute_error: 0.470919, mean_q: 8.312669, mean_eps: 0.100000\n",
      " 100937/175000: episode: 2828, duration: 0.819s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 142.109 [28.000, 215.000], mean observation: 0.312 [0.000, 92.000], loss: 2.214883, mean_absolute_error: 0.597724, mean_q: 9.445506, mean_eps: 0.100000\n",
      " 100977/175000: episode: 2829, duration: 0.716s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 110.675 [23.000, 208.000], mean observation: 0.374 [0.000, 80.000], loss: 1.479273, mean_absolute_error: 0.486127, mean_q: 8.108702, mean_eps: 0.100000\n",
      " 101000/175000: episode: 2830, duration: 0.432s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 115.000 [28.000, 208.000], mean observation: 0.125 [0.000, 46.000], loss: 0.156833, mean_absolute_error: 0.447553, mean_q: 7.612701, mean_eps: 0.100000\n",
      " 101020/175000: episode: 2831, duration: 0.408s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 136.300 [42.000, 208.000], mean observation: 0.111 [0.000, 40.000], loss: 101.658739, mean_absolute_error: 1.305761, mean_q: 11.912534, mean_eps: 0.100000\n",
      " 101051/175000: episode: 2832, duration: 0.594s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 155.774 [48.000, 219.000], mean observation: 0.261 [0.000, 62.000], loss: 0.341641, mean_absolute_error: 0.677404, mean_q: 9.887049, mean_eps: 0.100000\n",
      " 101070/175000: episode: 2833, duration: 0.394s, episode steps: 19, steps per second: 48, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 110.421 [105.000, 208.000], mean observation: 0.074 [0.000, 38.000], loss: 0.191397, mean_absolute_error: 0.472016, mean_q: 8.252823, mean_eps: 0.100000\n",
      " 101098/175000: episode: 2834, duration: 0.497s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 102.286 [39.000, 105.000], mean observation: 0.078 [0.000, 56.000], loss: 0.369775, mean_absolute_error: 0.457381, mean_q: 7.801770, mean_eps: 0.100000\n",
      " 101142/175000: episode: 2835, duration: 0.795s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.341 [12.000, 208.000], mean observation: 0.225 [0.000, 88.000], loss: 0.255793, mean_absolute_error: 0.603488, mean_q: 9.270238, mean_eps: 0.100000\n",
      " 101166/175000: episode: 2836, duration: 0.449s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 122.292 [50.000, 208.000], mean observation: 0.113 [0.000, 48.000], loss: 0.450686, mean_absolute_error: 0.460737, mean_q: 7.841183, mean_eps: 0.100000\n",
      " 101222/175000: episode: 2837, duration: 0.991s, episode steps: 56, steps per second: 57, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 102.304 [1.000, 220.000], mean observation: 0.793 [0.000, 112.000], loss: 0.685578, mean_absolute_error: 0.714734, mean_q: 10.392127, mean_eps: 0.100000\n",
      " 101272/175000: episode: 2838, duration: 0.921s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.200 [28.000, 160.000], mean observation: 0.317 [0.000, 100.000], loss: 2.313345, mean_absolute_error: 0.666171, mean_q: 9.696039, mean_eps: 0.100000\n",
      " 101311/175000: episode: 2839, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 87.846 [0.000, 210.000], mean observation: 0.283 [0.000, 78.000], loss: 2.669873, mean_absolute_error: 0.669820, mean_q: 9.673558, mean_eps: 0.100000\n",
      " 101350/175000: episode: 2840, duration: 0.739s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.513 [28.000, 209.000], mean observation: 0.309 [0.000, 78.000], loss: 0.561429, mean_absolute_error: 0.661264, mean_q: 9.681194, mean_eps: 0.100000\n",
      " 101371/175000: episode: 2841, duration: 0.364s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 76.952 [49.000, 153.000], mean observation: 0.088 [0.000, 42.000], loss: 0.127179, mean_absolute_error: 0.517369, mean_q: 8.306534, mean_eps: 0.100000\n",
      " 101411/175000: episode: 2842, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 68.125 [28.000, 161.000], mean observation: 0.293 [0.000, 80.000], loss: 0.184843, mean_absolute_error: 0.620227, mean_q: 9.373603, mean_eps: 0.100000\n",
      " 101444/175000: episode: 2843, duration: 0.626s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.788 [41.000, 215.000], mean observation: 0.153 [0.000, 66.000], loss: 2.999820, mean_absolute_error: 0.838526, mean_q: 11.240796, mean_eps: 0.100000\n",
      " 101466/175000: episode: 2844, duration: 0.429s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 165.000 [165.000, 165.000], mean observation: 0.053 [0.000, 44.000], loss: 1.459269, mean_absolute_error: 0.475212, mean_q: 7.573398, mean_eps: 0.100000\n",
      " 101492/175000: episode: 2845, duration: 0.497s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 158.269 [38.000, 165.000], mean observation: 0.112 [0.000, 52.000], loss: 2.513429, mean_absolute_error: 0.460678, mean_q: 7.388777, mean_eps: 0.100000\n",
      " 101528/175000: episode: 2846, duration: 0.709s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 154.194 [21.000, 165.000], mean observation: 0.156 [0.000, 72.000], loss: 2.117489, mean_absolute_error: 0.603674, mean_q: 8.608778, mean_eps: 0.100000\n",
      " 101558/175000: episode: 2847, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.633 [22.000, 213.000], mean observation: 0.185 [0.000, 60.000], loss: 25.950384, mean_absolute_error: 0.843861, mean_q: 9.757482, mean_eps: 0.100000\n",
      " 101583/175000: episode: 2848, duration: 0.449s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.240 [4.000, 208.000], mean observation: 0.175 [0.000, 50.000], loss: 0.285319, mean_absolute_error: 0.782011, mean_q: 10.451342, mean_eps: 0.100000\n",
      " 101612/175000: episode: 2849, duration: 0.556s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.862 [10.000, 191.000], mean observation: 0.221 [0.000, 58.000], loss: 0.251973, mean_absolute_error: 0.530597, mean_q: 8.005509, mean_eps: 0.100000\n",
      " 101669/175000: episode: 2850, duration: 1.054s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.684 [10.000, 222.000], mean observation: 0.685 [0.000, 114.000], loss: 1.336367, mean_absolute_error: 0.564730, mean_q: 8.363436, mean_eps: 0.100000\n",
      " 101714/175000: episode: 2851, duration: 0.789s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.378 [12.000, 214.000], mean observation: 0.518 [0.000, 90.000], loss: 30.752351, mean_absolute_error: 0.772430, mean_q: 8.924259, mean_eps: 0.100000\n",
      " 101757/175000: episode: 2852, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 123.186 [20.000, 214.000], mean observation: 0.454 [0.000, 86.000], loss: 1.346444, mean_absolute_error: 0.541661, mean_q: 7.595024, mean_eps: 0.100000\n",
      " 101780/175000: episode: 2853, duration: 0.433s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 118.957 [10.000, 174.000], mean observation: 0.250 [0.000, 46.000], loss: 2.084932, mean_absolute_error: 0.599528, mean_q: 7.869846, mean_eps: 0.100000\n",
      " 101819/175000: episode: 2854, duration: 0.738s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.077 [10.000, 216.000], mean observation: 0.450 [0.000, 78.000], loss: 0.717845, mean_absolute_error: 0.604312, mean_q: 7.903569, mean_eps: 0.100000\n",
      " 101860/175000: episode: 2855, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.415 [5.000, 186.000], mean observation: 0.471 [0.000, 82.000], loss: 4.175936, mean_absolute_error: 0.762095, mean_q: 9.518015, mean_eps: 0.100000\n",
      " 101922/175000: episode: 2856, duration: 1.094s, episode steps: 62, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 121.565 [11.000, 189.000], mean observation: 0.631 [0.000, 124.000], loss: 3.310966, mean_absolute_error: 0.781470, mean_q: 10.113445, mean_eps: 0.100000\n",
      " 101946/175000: episode: 2857, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 116.417 [41.000, 192.000], mean observation: 0.214 [0.000, 48.000], loss: 0.267065, mean_absolute_error: 0.544034, mean_q: 7.859580, mean_eps: 0.100000\n",
      " 101992/175000: episode: 2858, duration: 0.859s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 111.087 [59.000, 208.000], mean observation: 0.696 [0.000, 92.000], loss: 0.495252, mean_absolute_error: 0.546813, mean_q: 7.945515, mean_eps: 0.100000\n",
      " 102039/175000: episode: 2859, duration: 0.887s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.617 [24.000, 222.000], mean observation: 0.463 [0.000, 94.000], loss: 13.724522, mean_absolute_error: 0.733316, mean_q: 9.160885, mean_eps: 0.100000\n",
      " 102083/175000: episode: 2860, duration: 0.790s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.341 [17.000, 220.000], mean observation: 0.487 [0.000, 88.000], loss: 9.010930, mean_absolute_error: 0.717670, mean_q: 9.244735, mean_eps: 0.100000\n",
      " 102120/175000: episode: 2861, duration: 0.692s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 123.027 [44.000, 195.000], mean observation: 0.478 [0.000, 74.000], loss: 0.410172, mean_absolute_error: 0.509116, mean_q: 7.791613, mean_eps: 0.100000\n",
      " 102146/175000: episode: 2862, duration: 0.503s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.654 [46.000, 217.000], mean observation: 0.148 [0.000, 52.000], loss: 44.906553, mean_absolute_error: 1.141123, mean_q: 11.695591, mean_eps: 0.100000\n",
      " 102171/175000: episode: 2863, duration: 0.438s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 143.760 [89.000, 158.000], mean observation: 0.131 [0.000, 50.000], loss: 38.544283, mean_absolute_error: 0.958606, mean_q: 10.483649, mean_eps: 0.100000\n",
      " 102203/175000: episode: 2864, duration: 0.612s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 123.969 [44.000, 202.000], mean observation: 0.275 [0.000, 64.000], loss: 45.489848, mean_absolute_error: 0.977235, mean_q: 10.195415, mean_eps: 0.100000\n",
      " 102233/175000: episode: 2865, duration: 0.589s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 192.467 [89.000, 213.000], mean observation: 0.088 [0.000, 60.000], loss: 0.311652, mean_absolute_error: 0.701303, mean_q: 9.131301, mean_eps: 0.100000\n",
      " 102272/175000: episode: 2866, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 176.846 [17.000, 221.000], mean observation: 0.286 [0.000, 78.000], loss: 0.180030, mean_absolute_error: 0.471198, mean_q: 6.743860, mean_eps: 0.100000\n",
      " 102307/175000: episode: 2867, duration: 0.641s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.657 [64.000, 213.000], mean observation: 0.332 [0.000, 70.000], loss: 0.154427, mean_absolute_error: 0.475276, mean_q: 7.181630, mean_eps: 0.100000\n",
      " 102343/175000: episode: 2868, duration: 0.688s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.694 [37.000, 197.000], mean observation: 0.303 [0.000, 72.000], loss: 0.209078, mean_absolute_error: 0.447148, mean_q: 7.191907, mean_eps: 0.100000\n",
      " 102374/175000: episode: 2869, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 129.065 [37.000, 173.000], mean observation: 0.202 [0.000, 62.000], loss: 5.562380, mean_absolute_error: 0.677054, mean_q: 9.130232, mean_eps: 0.100000\n",
      " 102413/175000: episode: 2870, duration: 0.749s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.333 [1.000, 173.000], mean observation: 0.360 [0.000, 78.000], loss: 27.376190, mean_absolute_error: 0.752189, mean_q: 9.148122, mean_eps: 0.100000\n",
      " 102442/175000: episode: 2871, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.138 [48.000, 213.000], mean observation: 0.188 [0.000, 58.000], loss: 9.526202, mean_absolute_error: 0.709968, mean_q: 9.413848, mean_eps: 0.100000\n",
      " 102451/175000: episode: 2872, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 152.000 [152.000, 152.000], mean observation: 0.024 [0.000, 18.000], loss: 0.119649, mean_absolute_error: 0.449125, mean_q: 7.437580, mean_eps: 0.100000\n",
      " 102490/175000: episode: 2873, duration: 0.723s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.615 [31.000, 185.000], mean observation: 0.323 [0.000, 78.000], loss: 0.278907, mean_absolute_error: 0.519218, mean_q: 7.984105, mean_eps: 0.100000\n",
      " 102522/175000: episode: 2874, duration: 0.581s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 145.594 [23.000, 222.000], mean observation: 0.216 [0.000, 64.000], loss: 0.187016, mean_absolute_error: 0.418581, mean_q: 7.360218, mean_eps: 0.100000\n",
      " 102558/175000: episode: 2875, duration: 0.646s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.806 [9.000, 178.000], mean observation: 0.206 [0.000, 72.000], loss: 0.383353, mean_absolute_error: 0.426744, mean_q: 7.113334, mean_eps: 0.100000\n",
      " 102588/175000: episode: 2876, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.133 [9.000, 152.000], mean observation: 0.170 [0.000, 60.000], loss: 26.283891, mean_absolute_error: 0.532518, mean_q: 7.123005, mean_eps: 0.100000\n",
      " 102603/175000: episode: 2877, duration: 0.353s, episode steps: 15, steps per second: 42, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 154.800 [66.000, 223.000], mean observation: 0.107 [0.000, 30.000], loss: 0.105562, mean_absolute_error: 0.430928, mean_q: 7.172877, mean_eps: 0.100000\n",
      " 102639/175000: episode: 2878, duration: 0.679s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 134.083 [0.000, 213.000], mean observation: 0.236 [0.000, 72.000], loss: 0.777493, mean_absolute_error: 0.429147, mean_q: 7.524368, mean_eps: 0.100000\n",
      " 102681/175000: episode: 2879, duration: 0.850s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.071 [24.000, 224.000], mean observation: 0.524 [0.000, 84.000], loss: 12.642941, mean_absolute_error: 0.628353, mean_q: 8.535334, mean_eps: 0.100000\n",
      " 102725/175000: episode: 2880, duration: 0.843s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.659 [0.000, 208.000], mean observation: 0.528 [0.000, 88.000], loss: 0.170821, mean_absolute_error: 0.429026, mean_q: 7.010420, mean_eps: 0.100000\n",
      " 102744/175000: episode: 2881, duration: 0.394s, episode steps: 19, steps per second: 48, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 126.474 [5.000, 224.000], mean observation: 0.147 [0.000, 38.000], loss: 0.324172, mean_absolute_error: 0.430730, mean_q: 7.172357, mean_eps: 0.100000\n",
      " 102772/175000: episode: 2882, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 118.250 [7.000, 178.000], mean observation: 0.207 [0.000, 56.000], loss: 0.283510, mean_absolute_error: 0.431077, mean_q: 6.896158, mean_eps: 0.100000\n",
      " 102825/175000: episode: 2883, duration: 1.074s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 110.208 [2.000, 223.000], mean observation: 0.950 [0.000, 106.000], loss: 0.437086, mean_absolute_error: 0.453263, mean_q: 7.032318, mean_eps: 0.100000\n",
      " 102860/175000: episode: 2884, duration: 0.743s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.257 [7.000, 171.000], mean observation: 0.222 [0.000, 70.000], loss: 0.332612, mean_absolute_error: 0.474551, mean_q: 7.117369, mean_eps: 0.100000\n",
      " 102905/175000: episode: 2885, duration: 0.890s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.911 [39.000, 181.000], mean observation: 0.449 [0.000, 90.000], loss: 0.568498, mean_absolute_error: 0.657695, mean_q: 8.723546, mean_eps: 0.100000\n",
      " 102953/175000: episode: 2886, duration: 0.863s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.500 [2.000, 222.000], mean observation: 0.598 [0.000, 96.000], loss: 0.355129, mean_absolute_error: 0.511182, mean_q: 7.208681, mean_eps: 0.100000\n",
      " 102986/175000: episode: 2887, duration: 0.567s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.212 [15.000, 222.000], mean observation: 0.413 [0.000, 66.000], loss: 0.443917, mean_absolute_error: 0.517948, mean_q: 7.179989, mean_eps: 0.100000\n",
      " 103025/175000: episode: 2888, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.385 [9.000, 222.000], mean observation: 0.447 [0.000, 78.000], loss: 0.342047, mean_absolute_error: 0.510824, mean_q: 6.928340, mean_eps: 0.100000\n",
      " 103064/175000: episode: 2889, duration: 0.801s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 210.923 [158.000, 215.000], mean observation: 0.120 [0.000, 78.000], loss: 0.226518, mean_absolute_error: 0.545136, mean_q: 7.753164, mean_eps: 0.100000\n",
      " 103117/175000: episode: 2890, duration: 1.006s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 172.113 [58.000, 209.000], mean observation: 0.247 [0.000, 106.000], loss: 0.298221, mean_absolute_error: 0.525512, mean_q: 7.112500, mean_eps: 0.100000\n",
      " 103165/175000: episode: 2891, duration: 0.858s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 154.458 [2.000, 209.000], mean observation: 0.265 [0.000, 96.000], loss: 0.517411, mean_absolute_error: 0.496213, mean_q: 6.565011, mean_eps: 0.100000\n",
      " 103194/175000: episode: 2892, duration: 0.498s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 184.276 [2.000, 209.000], mean observation: 0.164 [0.000, 58.000], loss: 17.395701, mean_absolute_error: 0.798731, mean_q: 8.852291, mean_eps: 0.100000\n",
      " 103227/175000: episode: 2893, duration: 0.600s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.394 [39.000, 209.000], mean observation: 0.225 [0.000, 66.000], loss: 0.321522, mean_absolute_error: 0.502584, mean_q: 6.496521, mean_eps: 0.100000\n",
      " 103275/175000: episode: 2894, duration: 0.879s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.396 [2.000, 209.000], mean observation: 0.219 [0.000, 96.000], loss: 11.876591, mean_absolute_error: 0.686290, mean_q: 7.671491, mean_eps: 0.100000\n",
      " 103316/175000: episode: 2895, duration: 0.806s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 129.659 [16.000, 209.000], mean observation: 0.224 [0.000, 82.000], loss: 0.613878, mean_absolute_error: 0.492813, mean_q: 6.273957, mean_eps: 0.100000\n",
      " 103350/175000: episode: 2896, duration: 0.653s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.882 [15.000, 212.000], mean observation: 0.239 [0.000, 68.000], loss: 0.433879, mean_absolute_error: 0.487861, mean_q: 6.143910, mean_eps: 0.100000\n",
      " 103400/175000: episode: 2897, duration: 0.920s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.080 [15.000, 218.000], mean observation: 0.592 [0.000, 100.000], loss: 0.380017, mean_absolute_error: 0.526418, mean_q: 6.837446, mean_eps: 0.100000\n",
      " 103448/175000: episode: 2898, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.188 [4.000, 215.000], mean observation: 0.598 [0.000, 96.000], loss: 11.753638, mean_absolute_error: 0.703224, mean_q: 8.116050, mean_eps: 0.100000\n",
      " 103489/175000: episode: 2899, duration: 0.792s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 86.683 [15.000, 212.000], mean observation: 0.557 [0.000, 82.000], loss: 4.007226, mean_absolute_error: 0.529736, mean_q: 6.842635, mean_eps: 0.100000\n",
      " 103530/175000: episode: 2900, duration: 0.737s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.902 [5.000, 222.000], mean observation: 0.441 [0.000, 82.000], loss: 21.983079, mean_absolute_error: 0.753207, mean_q: 8.443795, mean_eps: 0.100000\n",
      " 103572/175000: episode: 2901, duration: 0.777s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 55.167 [5.000, 221.000], mean observation: 0.416 [0.000, 84.000], loss: 0.218809, mean_absolute_error: 0.486990, mean_q: 6.878811, mean_eps: 0.100000\n",
      " 103609/175000: episode: 2902, duration: 0.723s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.324 [11.000, 221.000], mean observation: 0.498 [0.000, 74.000], loss: 24.405241, mean_absolute_error: 0.755801, mean_q: 8.438253, mean_eps: 0.100000\n",
      " 103642/175000: episode: 2903, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 55.152 [11.000, 209.000], mean observation: 0.381 [0.000, 66.000], loss: 0.193206, mean_absolute_error: 0.475253, mean_q: 7.015525, mean_eps: 0.100000\n",
      " 103684/175000: episode: 2904, duration: 0.766s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.071 [32.000, 209.000], mean observation: 0.540 [0.000, 84.000], loss: 0.129233, mean_absolute_error: 0.457482, mean_q: 6.481644, mean_eps: 0.100000\n",
      " 103740/175000: episode: 2905, duration: 1.108s, episode steps: 56, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 77.714 [5.000, 209.000], mean observation: 0.511 [0.000, 112.000], loss: 0.194415, mean_absolute_error: 0.458909, mean_q: 6.632134, mean_eps: 0.100000\n",
      " 103785/175000: episode: 2906, duration: 0.877s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.067 [37.000, 211.000], mean observation: 0.575 [0.000, 90.000], loss: 0.249538, mean_absolute_error: 0.459255, mean_q: 6.673685, mean_eps: 0.100000\n",
      " 103803/175000: episode: 2907, duration: 0.383s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 130.500 [12.000, 209.000], mean observation: 0.102 [0.000, 36.000], loss: 0.567820, mean_absolute_error: 0.457910, mean_q: 6.470788, mean_eps: 0.100000\n",
      " 103819/175000: episode: 2908, duration: 0.290s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 124.375 [12.000, 209.000], mean observation: 0.134 [0.000, 32.000], loss: 0.157164, mean_absolute_error: 0.470558, mean_q: 6.777156, mean_eps: 0.100000\n",
      " 103872/175000: episode: 2909, duration: 1.061s, episode steps: 53, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.547 [12.000, 211.000], mean observation: 0.641 [0.000, 106.000], loss: 0.382692, mean_absolute_error: 0.461287, mean_q: 6.746661, mean_eps: 0.100000\n",
      " 103924/175000: episode: 2910, duration: 1.067s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.038 [12.000, 209.000], mean observation: 0.721 [0.000, 104.000], loss: 86.922699, mean_absolute_error: 1.090086, mean_q: 9.032417, mean_eps: 0.100000\n",
      " 103956/175000: episode: 2911, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 157.625 [19.000, 209.000], mean observation: 0.102 [0.000, 64.000], loss: 22.484183, mean_absolute_error: 0.739391, mean_q: 8.386106, mean_eps: 0.100000\n",
      " 103971/175000: episode: 2912, duration: 0.319s, episode steps: 15, steps per second: 47, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 133.867 [38.000, 184.000], mean observation: 0.129 [0.000, 30.000], loss: 49.518375, mean_absolute_error: 1.085731, mean_q: 10.585015, mean_eps: 0.100000\n",
      " 104034/175000: episode: 2913, duration: 1.428s, episode steps: 63, steps per second: 44, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 116.492 [0.000, 191.000], mean observation: 0.665 [0.000, 126.000], loss: 0.209764, mean_absolute_error: 0.473648, mean_q: 6.816828, mean_eps: 0.100000\n",
      " 104066/175000: episode: 2914, duration: 0.620s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.719 [0.000, 172.000], mean observation: 0.288 [0.000, 64.000], loss: 34.504862, mean_absolute_error: 1.026692, mean_q: 10.474197, mean_eps: 0.100000\n",
      " 104088/175000: episode: 2915, duration: 0.455s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 124.682 [8.000, 172.000], mean observation: 0.188 [0.000, 44.000], loss: 0.322209, mean_absolute_error: 0.472966, mean_q: 6.529872, mean_eps: 0.100000\n",
      " 104124/175000: episode: 2916, duration: 0.662s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 135.667 [12.000, 191.000], mean observation: 0.375 [0.000, 72.000], loss: 0.665499, mean_absolute_error: 0.486049, mean_q: 6.882285, mean_eps: 0.100000\n",
      " 104153/175000: episode: 2917, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.310 [10.000, 211.000], mean observation: 0.217 [0.000, 58.000], loss: 6.220413, mean_absolute_error: 0.692413, mean_q: 8.534625, mean_eps: 0.100000\n",
      " 104197/175000: episode: 2918, duration: 0.793s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.409 [12.000, 211.000], mean observation: 0.563 [0.000, 88.000], loss: 0.138598, mean_absolute_error: 0.459767, mean_q: 6.311599, mean_eps: 0.100000\n",
      " 104225/175000: episode: 2919, duration: 0.507s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.500 [16.000, 211.000], mean observation: 0.316 [0.000, 56.000], loss: 0.134713, mean_absolute_error: 0.462328, mean_q: 6.514800, mean_eps: 0.100000\n",
      " 104273/175000: episode: 2920, duration: 0.864s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.000 [59.000, 211.000], mean observation: 0.579 [0.000, 96.000], loss: 22.746791, mean_absolute_error: 0.817736, mean_q: 8.996379, mean_eps: 0.100000\n",
      " 104306/175000: episode: 2921, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 92.909 [16.000, 217.000], mean observation: 0.214 [0.000, 66.000], loss: 0.652250, mean_absolute_error: 0.446266, mean_q: 6.482268, mean_eps: 0.100000\n",
      " 104351/175000: episode: 2922, duration: 0.853s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 89.333 [16.000, 202.000], mean observation: 0.385 [0.000, 90.000], loss: 0.168390, mean_absolute_error: 0.437772, mean_q: 6.444962, mean_eps: 0.100000\n",
      " 104392/175000: episode: 2923, duration: 0.843s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.122 [32.000, 211.000], mean observation: 0.387 [0.000, 82.000], loss: 0.160588, mean_absolute_error: 0.423469, mean_q: 6.694170, mean_eps: 0.100000\n",
      " 104437/175000: episode: 2924, duration: 0.929s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 99.600 [14.000, 191.000], mean observation: 0.586 [0.000, 90.000], loss: 15.026196, mean_absolute_error: 0.619967, mean_q: 7.860030, mean_eps: 0.100000\n",
      " 104495/175000: episode: 2925, duration: 1.049s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 141.897 [48.000, 201.000], mean observation: 0.565 [0.000, 116.000], loss: 2.517892, mean_absolute_error: 0.421403, mean_q: 6.452517, mean_eps: 0.100000\n",
      " 104516/175000: episode: 2926, duration: 0.509s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.381 [1.000, 182.000], mean observation: 0.106 [0.000, 42.000], loss: 0.148706, mean_absolute_error: 0.433364, mean_q: 6.652026, mean_eps: 0.100000\n",
      " 104569/175000: episode: 2927, duration: 1.141s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 116.660 [22.000, 190.000], mean observation: 0.459 [0.000, 106.000], loss: 0.124979, mean_absolute_error: 0.435074, mean_q: 6.727763, mean_eps: 0.100000\n",
      " 104596/175000: episode: 2928, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 100.926 [4.000, 158.000], mean observation: 0.316 [0.000, 54.000], loss: 0.148232, mean_absolute_error: 0.423721, mean_q: 6.321519, mean_eps: 0.100000\n",
      " 104626/175000: episode: 2929, duration: 0.608s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.733 [27.000, 220.000], mean observation: 0.247 [0.000, 60.000], loss: 25.866568, mean_absolute_error: 0.758013, mean_q: 8.664650, mean_eps: 0.100000\n",
      " 104664/175000: episode: 2930, duration: 0.774s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 153.500 [3.000, 222.000], mean observation: 0.368 [0.000, 76.000], loss: 13.204240, mean_absolute_error: 0.675840, mean_q: 8.369831, mean_eps: 0.100000\n",
      " 104705/175000: episode: 2931, duration: 0.783s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.927 [48.000, 202.000], mean observation: 0.455 [0.000, 82.000], loss: 2.014952, mean_absolute_error: 0.570373, mean_q: 7.628856, mean_eps: 0.100000\n",
      " 104746/175000: episode: 2932, duration: 0.713s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.293 [59.000, 220.000], mean observation: 0.384 [0.000, 82.000], loss: 0.475417, mean_absolute_error: 0.437564, mean_q: 6.570168, mean_eps: 0.100000\n",
      " 104780/175000: episode: 2933, duration: 0.668s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.353 [58.000, 216.000], mean observation: 0.162 [0.000, 68.000], loss: 0.205053, mean_absolute_error: 0.474166, mean_q: 6.701890, mean_eps: 0.100000\n",
      " 104798/175000: episode: 2934, duration: 0.367s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 91.611 [0.000, 169.000], mean observation: 0.080 [0.000, 36.000], loss: 0.224077, mean_absolute_error: 0.459133, mean_q: 6.325145, mean_eps: 0.100000\n",
      " 104836/175000: episode: 2935, duration: 0.711s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 106.711 [50.000, 220.000], mean observation: 0.159 [0.000, 76.000], loss: 0.206219, mean_absolute_error: 0.481002, mean_q: 6.806272, mean_eps: 0.100000\n",
      " 104860/175000: episode: 2936, duration: 0.479s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.833 [44.000, 201.000], mean observation: 0.197 [0.000, 48.000], loss: 0.290246, mean_absolute_error: 0.490363, mean_q: 6.771654, mean_eps: 0.100000\n",
      " 104895/175000: episode: 2937, duration: 0.653s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 88.371 [1.000, 158.000], mean observation: 0.260 [0.000, 70.000], loss: 0.221520, mean_absolute_error: 0.506274, mean_q: 6.592922, mean_eps: 0.100000\n",
      " 104912/175000: episode: 2938, duration: 0.373s, episode steps: 17, steps per second: 46, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 99.765 [10.000, 158.000], mean observation: 0.081 [0.000, 34.000], loss: 1.095235, mean_absolute_error: 0.502118, mean_q: 6.454820, mean_eps: 0.100000\n",
      " 104945/175000: episode: 2939, duration: 0.620s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.636 [0.000, 181.000], mean observation: 0.288 [0.000, 66.000], loss: 0.679284, mean_absolute_error: 0.539573, mean_q: 6.779442, mean_eps: 0.100000\n",
      " 104982/175000: episode: 2940, duration: 0.655s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.054 [32.000, 158.000], mean observation: 0.336 [0.000, 74.000], loss: 24.343652, mean_absolute_error: 0.996951, mean_q: 10.061372, mean_eps: 0.100000\n",
      " 105017/175000: episode: 2941, duration: 0.636s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.571 [30.000, 216.000], mean observation: 0.273 [0.000, 70.000], loss: 0.279075, mean_absolute_error: 0.540796, mean_q: 6.599501, mean_eps: 0.100000\n",
      " 105049/175000: episode: 2942, duration: 0.588s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 64.594 [8.000, 213.000], mean observation: 0.272 [0.000, 64.000], loss: 1.933668, mean_absolute_error: 0.708297, mean_q: 8.264946, mean_eps: 0.100000\n",
      " 105097/175000: episode: 2943, duration: 0.925s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.917 [10.000, 216.000], mean observation: 0.552 [0.000, 96.000], loss: 14.678691, mean_absolute_error: 0.700070, mean_q: 7.676481, mean_eps: 0.100000\n",
      " 105134/175000: episode: 2944, duration: 0.644s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 122.730 [3.000, 176.000], mean observation: 0.397 [0.000, 74.000], loss: 0.112931, mean_absolute_error: 0.517904, mean_q: 6.574821, mean_eps: 0.100000\n",
      " 105179/175000: episode: 2945, duration: 0.798s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.000 [32.000, 176.000], mean observation: 0.429 [0.000, 90.000], loss: 16.608874, mean_absolute_error: 0.711637, mean_q: 7.921662, mean_eps: 0.100000\n",
      " 105229/175000: episode: 2946, duration: 0.957s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.420 [10.000, 170.000], mean observation: 0.676 [0.000, 100.000], loss: 0.948922, mean_absolute_error: 0.465988, mean_q: 6.387570, mean_eps: 0.100000\n",
      " 105278/175000: episode: 2947, duration: 0.906s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.592 [10.000, 220.000], mean observation: 0.567 [0.000, 98.000], loss: 2.180895, mean_absolute_error: 0.475273, mean_q: 6.160555, mean_eps: 0.100000\n",
      " 105311/175000: episode: 2948, duration: 0.579s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.030 [9.000, 213.000], mean observation: 0.314 [0.000, 66.000], loss: 4.023802, mean_absolute_error: 0.538946, mean_q: 6.264539, mean_eps: 0.100000\n",
      " 105351/175000: episode: 2949, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 87.125 [9.000, 219.000], mean observation: 0.481 [0.000, 80.000], loss: 5.761849, mean_absolute_error: 0.723438, mean_q: 7.784512, mean_eps: 0.100000\n",
      " 105384/175000: episode: 2950, duration: 0.692s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.242 [9.000, 186.000], mean observation: 0.391 [0.000, 66.000], loss: 3.521423, mean_absolute_error: 0.568218, mean_q: 6.305157, mean_eps: 0.100000\n",
      " 105413/175000: episode: 2951, duration: 0.689s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 104.000 [7.000, 160.000], mean observation: 0.344 [0.000, 58.000], loss: 38.126684, mean_absolute_error: 0.745118, mean_q: 6.557919, mean_eps: 0.100000\n",
      " 105444/175000: episode: 2952, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 85.710 [7.000, 135.000], mean observation: 0.254 [0.000, 62.000], loss: 0.247037, mean_absolute_error: 0.505826, mean_q: 6.075346, mean_eps: 0.100000\n",
      " 105491/175000: episode: 2953, duration: 0.854s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 96.043 [7.000, 220.000], mean observation: 0.606 [0.000, 94.000], loss: 0.183476, mean_absolute_error: 0.487536, mean_q: 6.172086, mean_eps: 0.100000\n",
      " 105522/175000: episode: 2954, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 62.419 [7.000, 188.000], mean observation: 0.226 [0.000, 62.000], loss: 0.361204, mean_absolute_error: 0.470152, mean_q: 5.954836, mean_eps: 0.100000\n",
      " 105567/175000: episode: 2955, duration: 0.789s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.111 [7.000, 213.000], mean observation: 0.555 [0.000, 90.000], loss: 0.891010, mean_absolute_error: 0.469359, mean_q: 5.955306, mean_eps: 0.100000\n",
      " 105610/175000: episode: 2956, duration: 0.802s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.349 [7.000, 197.000], mean observation: 0.424 [0.000, 86.000], loss: 2.585620, mean_absolute_error: 0.477999, mean_q: 5.992763, mean_eps: 0.100000\n",
      " 105644/175000: episode: 2957, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.206 [7.000, 202.000], mean observation: 0.270 [0.000, 68.000], loss: 0.124014, mean_absolute_error: 0.467532, mean_q: 6.243754, mean_eps: 0.100000\n",
      " 105668/175000: episode: 2958, duration: 0.500s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 90.167 [7.000, 162.000], mean observation: 0.156 [0.000, 48.000], loss: 0.592887, mean_absolute_error: 0.709290, mean_q: 8.587264, mean_eps: 0.100000\n",
      " 105688/175000: episode: 2959, duration: 0.421s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 111.250 [37.000, 152.000], mean observation: 0.107 [0.000, 40.000], loss: 4.298558, mean_absolute_error: 0.780340, mean_q: 8.967929, mean_eps: 0.100000\n",
      " 105725/175000: episode: 2960, duration: 0.726s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 112.865 [5.000, 152.000], mean observation: 0.206 [0.000, 74.000], loss: 0.165288, mean_absolute_error: 0.463686, mean_q: 6.447522, mean_eps: 0.100000\n",
      " 105755/175000: episode: 2961, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 123.900 [37.000, 196.000], mean observation: 0.229 [0.000, 60.000], loss: 0.108935, mean_absolute_error: 0.448960, mean_q: 6.366829, mean_eps: 0.100000\n",
      " 105795/175000: episode: 2962, duration: 0.805s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 137.500 [37.000, 222.000], mean observation: 0.447 [0.000, 80.000], loss: 0.352262, mean_absolute_error: 0.609626, mean_q: 7.851747, mean_eps: 0.100000\n",
      " 105809/175000: episode: 2963, duration: 0.289s, episode steps: 14, steps per second: 49, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 66.429 [59.000, 111.000], mean observation: 0.053 [0.000, 28.000], loss: 0.076211, mean_absolute_error: 0.458471, mean_q: 6.241454, mean_eps: 0.100000\n",
      " 105845/175000: episode: 2964, duration: 0.678s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 71.667 [19.000, 183.000], mean observation: 0.316 [0.000, 72.000], loss: 0.182588, mean_absolute_error: 0.433720, mean_q: 5.764679, mean_eps: 0.100000\n",
      " 105882/175000: episode: 2965, duration: 0.641s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 73.135 [0.000, 224.000], mean observation: 0.418 [0.000, 74.000], loss: 8.981838, mean_absolute_error: 0.473176, mean_q: 5.914842, mean_eps: 0.100000\n",
      " 105911/175000: episode: 2966, duration: 0.593s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.586 [43.000, 181.000], mean observation: 0.235 [0.000, 58.000], loss: 0.149450, mean_absolute_error: 0.416657, mean_q: 6.009015, mean_eps: 0.100000\n",
      " 105947/175000: episode: 2967, duration: 0.667s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.861 [59.000, 216.000], mean observation: 0.341 [0.000, 72.000], loss: 0.140816, mean_absolute_error: 0.411027, mean_q: 5.844673, mean_eps: 0.100000\n",
      " 105982/175000: episode: 2968, duration: 0.635s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 126.886 [59.000, 216.000], mean observation: 0.329 [0.000, 70.000], loss: 6.571156, mean_absolute_error: 0.620314, mean_q: 7.584690, mean_eps: 0.100000\n",
      " 106030/175000: episode: 2969, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.083 [41.000, 194.000], mean observation: 0.416 [0.000, 96.000], loss: 0.124049, mean_absolute_error: 0.400757, mean_q: 5.932628, mean_eps: 0.100000\n",
      " 106085/175000: episode: 2970, duration: 1.038s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 76.055 [0.000, 185.000], mean observation: 0.736 [0.000, 110.000], loss: 22.333752, mean_absolute_error: 0.613352, mean_q: 7.046766, mean_eps: 0.100000\n",
      " 106120/175000: episode: 2971, duration: 0.624s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.914 [0.000, 208.000], mean observation: 0.333 [0.000, 70.000], loss: 0.195947, mean_absolute_error: 0.406474, mean_q: 6.057287, mean_eps: 0.100000\n",
      " 106157/175000: episode: 2972, duration: 0.691s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.324 [0.000, 208.000], mean observation: 0.303 [0.000, 74.000], loss: 15.609679, mean_absolute_error: 0.814580, mean_q: 9.070808, mean_eps: 0.100000\n",
      " 106187/175000: episode: 2973, duration: 0.551s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 126.667 [28.000, 208.000], mean observation: 0.204 [0.000, 60.000], loss: 0.122127, mean_absolute_error: 0.418563, mean_q: 6.161015, mean_eps: 0.100000\n",
      " 106215/175000: episode: 2974, duration: 0.503s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 153.786 [28.000, 208.000], mean observation: 0.193 [0.000, 56.000], loss: 0.167932, mean_absolute_error: 0.406373, mean_q: 5.929690, mean_eps: 0.100000\n",
      " 106236/175000: episode: 2975, duration: 0.456s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 106.619 [62.000, 158.000], mean observation: 0.142 [0.000, 42.000], loss: 0.347987, mean_absolute_error: 0.406790, mean_q: 6.169731, mean_eps: 0.100000\n",
      " 106279/175000: episode: 2976, duration: 0.811s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.535 [48.000, 207.000], mean observation: 0.584 [0.000, 86.000], loss: 0.280071, mean_absolute_error: 0.550881, mean_q: 7.269007, mean_eps: 0.100000\n",
      " 106309/175000: episode: 2977, duration: 0.578s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.533 [43.000, 148.000], mean observation: 0.194 [0.000, 60.000], loss: 32.850687, mean_absolute_error: 1.093619, mean_q: 10.742387, mean_eps: 0.100000\n",
      " 106357/175000: episode: 2978, duration: 0.852s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 126.208 [32.000, 204.000], mean observation: 0.423 [0.000, 96.000], loss: 6.304744, mean_absolute_error: 0.572409, mean_q: 7.249691, mean_eps: 0.100000\n",
      " 106415/175000: episode: 2979, duration: 1.011s, episode steps: 58, steps per second: 57, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 111.207 [14.000, 158.000], mean observation: 0.417 [0.000, 116.000], loss: 0.116277, mean_absolute_error: 0.407226, mean_q: 5.933018, mean_eps: 0.100000\n",
      " 106441/175000: episode: 2980, duration: 0.512s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.077 [82.000, 158.000], mean observation: 0.131 [0.000, 52.000], loss: 0.665143, mean_absolute_error: 0.640205, mean_q: 8.087831, mean_eps: 0.100000\n",
      " 106488/175000: episode: 2981, duration: 0.859s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.191 [32.000, 184.000], mean observation: 0.389 [0.000, 94.000], loss: 1.688653, mean_absolute_error: 0.518818, mean_q: 7.134580, mean_eps: 0.100000\n",
      " 106528/175000: episode: 2982, duration: 0.763s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.250 [53.000, 195.000], mean observation: 0.424 [0.000, 80.000], loss: 2.892140, mean_absolute_error: 0.395053, mean_q: 5.618135, mean_eps: 0.100000\n",
      " 106558/175000: episode: 2983, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.433 [49.000, 218.000], mean observation: 0.268 [0.000, 60.000], loss: 0.106391, mean_absolute_error: 0.394782, mean_q: 5.897903, mean_eps: 0.100000\n",
      " 106602/175000: episode: 2984, duration: 0.833s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.659 [46.000, 208.000], mean observation: 0.330 [0.000, 88.000], loss: 22.972019, mean_absolute_error: 0.797225, mean_q: 8.677292, mean_eps: 0.100000\n",
      " 106652/175000: episode: 2985, duration: 0.929s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.440 [33.000, 222.000], mean observation: 0.401 [0.000, 100.000], loss: 2.201744, mean_absolute_error: 0.410210, mean_q: 5.819708, mean_eps: 0.100000\n",
      " 106682/175000: episode: 2986, duration: 0.579s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.867 [29.000, 208.000], mean observation: 0.202 [0.000, 60.000], loss: 0.267574, mean_absolute_error: 0.407795, mean_q: 5.948827, mean_eps: 0.100000\n",
      " 106719/175000: episode: 2987, duration: 0.686s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 188.919 [9.000, 208.000], mean observation: 0.112 [0.000, 74.000], loss: 38.421964, mean_absolute_error: 0.745982, mean_q: 7.525387, mean_eps: 0.100000\n",
      " 106759/175000: episode: 2988, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.375 [33.000, 208.000], mean observation: 0.313 [0.000, 80.000], loss: 0.160507, mean_absolute_error: 0.394615, mean_q: 5.822921, mean_eps: 0.100000\n",
      " 106808/175000: episode: 2989, duration: 1.034s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.490 [33.000, 224.000], mean observation: 0.570 [0.000, 98.000], loss: 2.088851, mean_absolute_error: 0.550265, mean_q: 7.244041, mean_eps: 0.100000\n",
      " 106854/175000: episode: 2990, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 123.413 [27.000, 223.000], mean observation: 0.506 [0.000, 92.000], loss: 0.190738, mean_absolute_error: 0.387891, mean_q: 5.848193, mean_eps: 0.100000\n",
      " 106902/175000: episode: 2991, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.833 [43.000, 196.000], mean observation: 0.336 [0.000, 96.000], loss: 0.111724, mean_absolute_error: 0.391840, mean_q: 6.016524, mean_eps: 0.100000\n",
      " 106933/175000: episode: 2992, duration: 0.570s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 141.226 [43.000, 183.000], mean observation: 0.150 [0.000, 62.000], loss: 0.167017, mean_absolute_error: 0.368067, mean_q: 5.720748, mean_eps: 0.100000\n",
      " 106959/175000: episode: 2993, duration: 0.452s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.500 [5.000, 183.000], mean observation: 0.161 [0.000, 52.000], loss: 0.316488, mean_absolute_error: 0.372623, mean_q: 5.990868, mean_eps: 0.100000\n",
      " 106995/175000: episode: 2994, duration: 0.701s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 149.194 [10.000, 208.000], mean observation: 0.216 [0.000, 72.000], loss: 0.131806, mean_absolute_error: 0.378837, mean_q: 5.758494, mean_eps: 0.100000\n",
      " 107048/175000: episode: 2995, duration: 0.998s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 153.038 [34.000, 208.000], mean observation: 0.468 [0.000, 106.000], loss: 0.819842, mean_absolute_error: 0.384681, mean_q: 5.760172, mean_eps: 0.100000\n",
      " 107094/175000: episode: 2996, duration: 0.828s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 151.283 [0.000, 212.000], mean observation: 0.400 [0.000, 92.000], loss: 0.220232, mean_absolute_error: 0.538472, mean_q: 6.862434, mean_eps: 0.100000\n",
      " 107149/175000: episode: 2997, duration: 1.038s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 151.309 [18.000, 219.000], mean observation: 0.265 [0.000, 110.000], loss: 10.642714, mean_absolute_error: 0.581063, mean_q: 7.076061, mean_eps: 0.100000\n",
      " 107193/175000: episode: 2998, duration: 0.788s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 127.568 [7.000, 187.000], mean observation: 0.329 [0.000, 88.000], loss: 0.219706, mean_absolute_error: 0.415936, mean_q: 5.985616, mean_eps: 0.100000\n",
      " 107231/175000: episode: 2999, duration: 0.681s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 119.026 [8.000, 224.000], mean observation: 0.305 [0.000, 76.000], loss: 0.181544, mean_absolute_error: 0.414686, mean_q: 5.925844, mean_eps: 0.100000\n",
      " 107262/175000: episode: 3000, duration: 0.559s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 148.581 [79.000, 215.000], mean observation: 0.204 [0.000, 62.000], loss: 0.233432, mean_absolute_error: 0.418757, mean_q: 5.925479, mean_eps: 0.100000\n",
      " 107279/175000: episode: 3001, duration: 0.305s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 131.765 [40.000, 169.000], mean observation: 0.077 [0.000, 34.000], loss: 15.463165, mean_absolute_error: 0.912639, mean_q: 9.651766, mean_eps: 0.100000\n",
      " 107312/175000: episode: 3002, duration: 0.659s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 109.879 [0.000, 208.000], mean observation: 0.293 [0.000, 66.000], loss: 0.101852, mean_absolute_error: 0.420473, mean_q: 5.638636, mean_eps: 0.100000\n",
      " 107346/175000: episode: 3003, duration: 0.651s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.941 [18.000, 171.000], mean observation: 0.258 [0.000, 68.000], loss: 0.263007, mean_absolute_error: 0.422283, mean_q: 5.671825, mean_eps: 0.100000\n",
      " 107391/175000: episode: 3004, duration: 0.793s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.156 [0.000, 171.000], mean observation: 0.329 [0.000, 90.000], loss: 0.435030, mean_absolute_error: 0.578476, mean_q: 7.096866, mean_eps: 0.100000\n",
      " 107438/175000: episode: 3005, duration: 0.848s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 139.532 [78.000, 171.000], mean observation: 0.311 [0.000, 94.000], loss: 21.129962, mean_absolute_error: 0.810639, mean_q: 8.286182, mean_eps: 0.100000\n",
      " 107469/175000: episode: 3006, duration: 0.570s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 155.323 [79.000, 180.000], mean observation: 0.197 [0.000, 62.000], loss: 0.128931, mean_absolute_error: 0.667660, mean_q: 7.765375, mean_eps: 0.100000\n",
      " 107485/175000: episode: 3007, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 133.438 [57.000, 188.000], mean observation: 0.132 [0.000, 32.000], loss: 0.119271, mean_absolute_error: 0.456044, mean_q: 5.817832, mean_eps: 0.100000\n",
      " 107505/175000: episode: 3008, duration: 0.367s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 121.950 [20.000, 195.000], mean observation: 0.151 [0.000, 40.000], loss: 37.614394, mean_absolute_error: 1.263503, mean_q: 11.907045, mean_eps: 0.100000\n",
      " 107553/175000: episode: 3009, duration: 0.854s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.667 [37.000, 188.000], mean observation: 0.477 [0.000, 96.000], loss: 0.115912, mean_absolute_error: 0.443357, mean_q: 5.700808, mean_eps: 0.100000\n",
      " 107583/175000: episode: 3010, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 122.167 [47.000, 212.000], mean observation: 0.238 [0.000, 60.000], loss: 0.098803, mean_absolute_error: 0.437913, mean_q: 5.656171, mean_eps: 0.100000\n",
      " 107638/175000: episode: 3011, duration: 1.018s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 155.764 [22.000, 218.000], mean observation: 0.805 [0.000, 110.000], loss: 8.900123, mean_absolute_error: 0.484649, mean_q: 5.779044, mean_eps: 0.100000\n",
      " 107677/175000: episode: 3012, duration: 0.691s, episode steps: 39, steps per second: 56, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 146.410 [91.000, 208.000], mean observation: 0.420 [0.000, 77.000], loss: 21.003176, mean_absolute_error: 0.681517, mean_q: 7.152808, mean_eps: 0.100000\n",
      " 107709/175000: episode: 3013, duration: 0.573s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 138.000 [18.000, 208.000], mean observation: 0.277 [0.000, 64.000], loss: 0.492364, mean_absolute_error: 0.434946, mean_q: 5.392498, mean_eps: 0.100000\n",
      " 107765/175000: episode: 3014, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 118.607 [33.000, 221.000], mean observation: 0.639 [0.000, 112.000], loss: 23.454002, mean_absolute_error: 0.661757, mean_q: 6.613264, mean_eps: 0.100000\n",
      " 107816/175000: episode: 3015, duration: 0.922s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 102.804 [21.000, 208.000], mean observation: 0.533 [0.000, 102.000], loss: 1.069664, mean_absolute_error: 0.604734, mean_q: 6.739196, mean_eps: 0.100000\n",
      " 107860/175000: episode: 3016, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.705 [5.000, 195.000], mean observation: 0.250 [0.000, 88.000], loss: 2.171535, mean_absolute_error: 0.657423, mean_q: 6.985899, mean_eps: 0.100000\n",
      " 107891/175000: episode: 3017, duration: 0.613s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.677 [7.000, 195.000], mean observation: 0.305 [0.000, 62.000], loss: 2.671145, mean_absolute_error: 0.874303, mean_q: 8.856764, mean_eps: 0.100000\n",
      " 107936/175000: episode: 3018, duration: 0.869s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 75.156 [11.000, 201.000], mean observation: 0.667 [0.000, 90.000], loss: 0.603497, mean_absolute_error: 0.512970, mean_q: 5.487448, mean_eps: 0.100000\n",
      " 107977/175000: episode: 3019, duration: 0.804s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.683 [27.000, 166.000], mean observation: 0.434 [0.000, 82.000], loss: 0.392295, mean_absolute_error: 0.514524, mean_q: 5.592135, mean_eps: 0.100000\n",
      " 108013/175000: episode: 3020, duration: 0.684s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.667 [11.000, 201.000], mean observation: 0.507 [0.000, 72.000], loss: 0.374273, mean_absolute_error: 0.542237, mean_q: 5.762522, mean_eps: 0.100000\n",
      " 108040/175000: episode: 3021, duration: 0.505s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 121.741 [11.000, 166.000], mean observation: 0.178 [0.000, 54.000], loss: 0.178562, mean_absolute_error: 0.552894, mean_q: 5.806014, mean_eps: 0.100000\n",
      " 108086/175000: episode: 3022, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.630 [1.000, 211.000], mean observation: 0.435 [0.000, 92.000], loss: 3.080057, mean_absolute_error: 0.649227, mean_q: 6.219114, mean_eps: 0.100000\n",
      " 108117/175000: episode: 3023, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 122.806 [33.000, 214.000], mean observation: 0.255 [0.000, 62.000], loss: 1.192570, mean_absolute_error: 0.632836, mean_q: 5.958154, mean_eps: 0.100000\n",
      " 108159/175000: episode: 3024, duration: 0.698s, episode steps: 42, steps per second: 60, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.429 [32.000, 203.000], mean observation: 0.479 [0.000, 84.000], loss: 0.343933, mean_absolute_error: 0.542489, mean_q: 5.339167, mean_eps: 0.100000\n",
      " 108206/175000: episode: 3025, duration: 0.881s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.426 [18.000, 219.000], mean observation: 0.510 [0.000, 94.000], loss: 0.221738, mean_absolute_error: 0.565954, mean_q: 5.601270, mean_eps: 0.100000\n",
      " 108235/175000: episode: 3026, duration: 0.536s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 118.000 [43.000, 202.000], mean observation: 0.127 [0.000, 58.000], loss: 0.317679, mean_absolute_error: 0.588395, mean_q: 5.975394, mean_eps: 0.100000\n",
      " 108284/175000: episode: 3027, duration: 0.937s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.633 [24.000, 203.000], mean observation: 0.602 [0.000, 98.000], loss: 5.681371, mean_absolute_error: 0.712372, mean_q: 7.124727, mean_eps: 0.100000\n",
      " 108313/175000: episode: 3028, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 112.034 [60.000, 224.000], mean observation: 0.241 [0.000, 58.000], loss: 0.604925, mean_absolute_error: 0.537184, mean_q: 5.431834, mean_eps: 0.100000\n",
      " 108346/175000: episode: 3029, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.030 [1.000, 221.000], mean observation: 0.184 [0.000, 66.000], loss: 0.396668, mean_absolute_error: 0.531478, mean_q: 4.984396, mean_eps: 0.100000\n",
      " 108365/175000: episode: 3030, duration: 0.358s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 165.789 [60.000, 222.000], mean observation: 0.126 [0.000, 38.000], loss: 0.263813, mean_absolute_error: 0.556448, mean_q: 5.346349, mean_eps: 0.100000\n",
      " 108399/175000: episode: 3031, duration: 0.571s, episode steps: 34, steps per second: 60, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 163.529 [48.000, 224.000], mean observation: 0.262 [0.000, 68.000], loss: 0.261824, mean_absolute_error: 0.538437, mean_q: 5.050739, mean_eps: 0.100000\n",
      " 108429/175000: episode: 3032, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 152.067 [2.000, 224.000], mean observation: 0.114 [0.000, 60.000], loss: 1.366489, mean_absolute_error: 0.555645, mean_q: 5.167786, mean_eps: 0.100000\n",
      " 108442/175000: episode: 3033, duration: 0.226s, episode steps: 13, steps per second: 58, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 186.385 [81.000, 202.000], mean observation: 0.047 [0.000, 26.000], loss: 1.160328, mean_absolute_error: 0.549814, mean_q: 4.837460, mean_eps: 0.100000\n",
      " 108481/175000: episode: 3034, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.821 [32.000, 224.000], mean observation: 0.273 [0.000, 78.000], loss: 0.840531, mean_absolute_error: 0.583795, mean_q: 4.854683, mean_eps: 0.100000\n",
      " 108505/175000: episode: 3035, duration: 0.419s, episode steps: 24, steps per second: 57, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 169.125 [29.000, 202.000], mean observation: 0.106 [0.000, 48.000], loss: 0.733510, mean_absolute_error: 0.748397, mean_q: 6.185853, mean_eps: 0.100000\n",
      " 108529/175000: episode: 3036, duration: 0.442s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 174.083 [60.000, 224.000], mean observation: 0.218 [0.000, 48.000], loss: 5.711268, mean_absolute_error: 0.636161, mean_q: 4.686682, mean_eps: 0.100000\n",
      " 108550/175000: episode: 3037, duration: 0.360s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 122.000 [59.000, 224.000], mean observation: 0.163 [0.000, 42.000], loss: 0.875847, mean_absolute_error: 0.644678, mean_q: 4.841585, mean_eps: 0.100000\n",
      " 108585/175000: episode: 3038, duration: 0.772s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.971 [15.000, 224.000], mean observation: 0.348 [0.000, 70.000], loss: 0.767883, mean_absolute_error: 0.663096, mean_q: 4.707691, mean_eps: 0.100000\n",
      " 108623/175000: episode: 3039, duration: 0.676s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.553 [11.000, 206.000], mean observation: 0.434 [0.000, 76.000], loss: 18.495750, mean_absolute_error: 0.880564, mean_q: 6.144940, mean_eps: 0.100000\n",
      " 108676/175000: episode: 3040, duration: 0.990s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 148.245 [38.000, 216.000], mean observation: 0.749 [0.000, 106.000], loss: 0.169319, mean_absolute_error: 0.659951, mean_q: 4.996997, mean_eps: 0.100000\n",
      " 108709/175000: episode: 3041, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 103.212 [23.000, 218.000], mean observation: 0.313 [0.000, 66.000], loss: 0.158406, mean_absolute_error: 0.619982, mean_q: 5.047389, mean_eps: 0.100000\n",
      " 108746/175000: episode: 3042, duration: 0.657s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 152.270 [32.000, 216.000], mean observation: 0.268 [0.000, 74.000], loss: 0.166098, mean_absolute_error: 0.602645, mean_q: 4.946201, mean_eps: 0.100000\n",
      " 108767/175000: episode: 3043, duration: 0.374s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 103.381 [10.000, 212.000], mean observation: 0.121 [0.000, 42.000], loss: 0.141257, mean_absolute_error: 0.594626, mean_q: 5.051074, mean_eps: 0.100000\n",
      " 108810/175000: episode: 3044, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 139.860 [1.000, 206.000], mean observation: 0.478 [0.000, 86.000], loss: 0.328338, mean_absolute_error: 0.582121, mean_q: 4.926721, mean_eps: 0.100000\n",
      " 108862/175000: episode: 3045, duration: 0.986s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.519 [0.000, 174.000], mean observation: 0.722 [0.000, 104.000], loss: 15.601877, mean_absolute_error: 0.826947, mean_q: 6.750968, mean_eps: 0.100000\n",
      " 108904/175000: episode: 3046, duration: 0.747s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.500 [7.000, 188.000], mean observation: 0.653 [0.000, 84.000], loss: 2.441762, mean_absolute_error: 0.684467, mean_q: 6.317075, mean_eps: 0.100000\n",
      " 108947/175000: episode: 3047, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.744 [0.000, 188.000], mean observation: 0.567 [0.000, 86.000], loss: 26.411034, mean_absolute_error: 0.953883, mean_q: 8.189942, mean_eps: 0.100000\n",
      " 108978/175000: episode: 3048, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 109.516 [1.000, 222.000], mean observation: 0.316 [0.000, 62.000], loss: 0.101698, mean_absolute_error: 0.504052, mean_q: 5.085995, mean_eps: 0.100000\n",
      " 108999/175000: episode: 3049, duration: 0.375s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 76.905 [44.000, 168.000], mean observation: 0.064 [0.000, 42.000], loss: 0.118189, mean_absolute_error: 0.490579, mean_q: 5.060850, mean_eps: 0.100000\n",
      " 109029/175000: episode: 3050, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 129.533 [1.000, 202.000], mean observation: 0.186 [0.000, 60.000], loss: 0.230841, mean_absolute_error: 0.505562, mean_q: 5.137655, mean_eps: 0.100000\n",
      " 109059/175000: episode: 3051, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 174.533 [59.000, 202.000], mean observation: 0.173 [0.000, 60.000], loss: 0.117466, mean_absolute_error: 0.483378, mean_q: 5.193119, mean_eps: 0.100000\n",
      " 109099/175000: episode: 3052, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 157.925 [28.000, 207.000], mean observation: 0.269 [0.000, 80.000], loss: 0.193399, mean_absolute_error: 0.470083, mean_q: 5.297915, mean_eps: 0.100000\n",
      " 109123/175000: episode: 3053, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.042 [1.000, 222.000], mean observation: 0.218 [0.000, 48.000], loss: 30.222127, mean_absolute_error: 0.823888, mean_q: 7.202619, mean_eps: 0.100000\n",
      " 109169/175000: episode: 3054, duration: 0.861s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.022 [6.000, 222.000], mean observation: 0.562 [0.000, 92.000], loss: 0.137150, mean_absolute_error: 0.463640, mean_q: 5.356966, mean_eps: 0.100000\n",
      " 109202/175000: episode: 3055, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 105.606 [39.000, 222.000], mean observation: 0.280 [0.000, 66.000], loss: 15.440144, mean_absolute_error: 0.690569, mean_q: 6.991062, mean_eps: 0.100000\n",
      " 109215/175000: episode: 3056, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 137.692 [2.000, 220.000], mean observation: 0.063 [0.000, 26.000], loss: 1.297575, mean_absolute_error: 0.476287, mean_q: 5.442477, mean_eps: 0.100000\n",
      " 109240/175000: episode: 3057, duration: 0.511s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 127.080 [59.000, 202.000], mean observation: 0.141 [0.000, 50.000], loss: 0.097686, mean_absolute_error: 0.450992, mean_q: 5.652814, mean_eps: 0.100000\n",
      " 109275/175000: episode: 3058, duration: 0.663s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.286 [59.000, 191.000], mean observation: 0.242 [0.000, 70.000], loss: 0.160651, mean_absolute_error: 0.471911, mean_q: 5.904065, mean_eps: 0.100000\n",
      " 109317/175000: episode: 3059, duration: 0.805s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.738 [59.000, 223.000], mean observation: 0.390 [0.000, 84.000], loss: 0.076765, mean_absolute_error: 0.437040, mean_q: 5.800677, mean_eps: 0.100000\n",
      " 109334/175000: episode: 3060, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 185.059 [59.000, 202.000], mean observation: 0.064 [0.000, 34.000], loss: 57.876043, mean_absolute_error: 1.413782, mean_q: 12.273187, mean_eps: 0.100000\n",
      " 109374/175000: episode: 3061, duration: 0.721s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 71.025 [0.000, 202.000], mean observation: 0.350 [0.000, 80.000], loss: 0.077382, mean_absolute_error: 0.446550, mean_q: 6.113369, mean_eps: 0.100000\n",
      " 109409/175000: episode: 3062, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.229 [3.000, 202.000], mean observation: 0.354 [0.000, 70.000], loss: 0.381258, mean_absolute_error: 0.436831, mean_q: 6.088694, mean_eps: 0.100000\n",
      " 109436/175000: episode: 3063, duration: 0.499s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 132.889 [0.000, 207.000], mean observation: 0.172 [0.000, 54.000], loss: 0.077799, mean_absolute_error: 0.418224, mean_q: 5.764527, mean_eps: 0.100000\n",
      " 109477/175000: episode: 3064, duration: 0.778s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.634 [39.000, 206.000], mean observation: 0.399 [0.000, 82.000], loss: 8.492031, mean_absolute_error: 0.592947, mean_q: 6.970044, mean_eps: 0.100000\n",
      " 109511/175000: episode: 3065, duration: 0.615s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.088 [43.000, 215.000], mean observation: 0.380 [0.000, 68.000], loss: 0.080554, mean_absolute_error: 0.418406, mean_q: 5.869442, mean_eps: 0.100000\n",
      " 109552/175000: episode: 3066, duration: 0.862s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.341 [21.000, 191.000], mean observation: 0.424 [0.000, 82.000], loss: 14.789354, mean_absolute_error: 0.754149, mean_q: 8.086734, mean_eps: 0.100000\n",
      " 109589/175000: episode: 3067, duration: 0.701s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.757 [22.000, 177.000], mean observation: 0.200 [0.000, 74.000], loss: 0.222254, mean_absolute_error: 0.438375, mean_q: 5.941677, mean_eps: 0.100000\n",
      " 109622/175000: episode: 3068, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 138.485 [1.000, 175.000], mean observation: 0.182 [0.000, 66.000], loss: 0.431495, mean_absolute_error: 0.432657, mean_q: 5.964406, mean_eps: 0.100000\n",
      " 109653/175000: episode: 3069, duration: 0.603s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.258 [1.000, 168.000], mean observation: 0.229 [0.000, 62.000], loss: 0.116667, mean_absolute_error: 0.423770, mean_q: 5.935530, mean_eps: 0.100000\n",
      " 109685/175000: episode: 3070, duration: 0.572s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 131.188 [10.000, 199.000], mean observation: 0.233 [0.000, 64.000], loss: 0.151036, mean_absolute_error: 0.433513, mean_q: 5.882959, mean_eps: 0.100000\n",
      " 109734/175000: episode: 3071, duration: 0.895s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 136.041 [93.000, 219.000], mean observation: 0.534 [0.000, 98.000], loss: 0.178085, mean_absolute_error: 0.416287, mean_q: 5.913128, mean_eps: 0.100000\n",
      " 109797/175000: episode: 3072, duration: 1.171s, episode steps: 63, steps per second: 54, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 129.524 [22.000, 193.000], mean observation: 0.642 [0.000, 126.000], loss: 1.859175, mean_absolute_error: 0.619034, mean_q: 7.670086, mean_eps: 0.100000\n",
      " 109832/175000: episode: 3073, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 135.143 [22.000, 206.000], mean observation: 0.306 [0.000, 70.000], loss: 0.274630, mean_absolute_error: 0.431150, mean_q: 6.066507, mean_eps: 0.100000\n",
      " 109849/175000: episode: 3074, duration: 0.378s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 129.412 [43.000, 206.000], mean observation: 0.108 [0.000, 34.000], loss: 0.282027, mean_absolute_error: 0.420860, mean_q: 5.917642, mean_eps: 0.100000\n",
      " 109875/175000: episode: 3075, duration: 0.450s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.654 [22.000, 206.000], mean observation: 0.240 [0.000, 52.000], loss: 0.288061, mean_absolute_error: 0.427065, mean_q: 6.066193, mean_eps: 0.100000\n",
      " 109908/175000: episode: 3076, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 122.606 [8.000, 175.000], mean observation: 0.378 [0.000, 66.000], loss: 0.274726, mean_absolute_error: 0.435313, mean_q: 6.073161, mean_eps: 0.100000\n",
      " 109938/175000: episode: 3077, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 120.567 [44.000, 220.000], mean observation: 0.131 [0.000, 60.000], loss: 0.179615, mean_absolute_error: 0.487273, mean_q: 6.275029, mean_eps: 0.100000\n",
      " 109967/175000: episode: 3078, duration: 0.515s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.862 [1.000, 203.000], mean observation: 0.161 [0.000, 58.000], loss: 0.225604, mean_absolute_error: 0.472074, mean_q: 6.183371, mean_eps: 0.100000\n",
      " 110030/175000: episode: 3079, duration: 1.223s, episode steps: 63, steps per second: 51, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.810 [1.000, 206.000], mean observation: 0.690 [0.000, 126.000], loss: 0.255372, mean_absolute_error: 0.476036, mean_q: 6.074331, mean_eps: 0.100000\n",
      " 110082/175000: episode: 3080, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 146.962 [1.000, 212.000], mean observation: 0.570 [0.000, 104.000], loss: 10.400927, mean_absolute_error: 0.731299, mean_q: 8.171459, mean_eps: 0.100000\n",
      " 110122/175000: episode: 3081, duration: 0.716s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 101.150 [30.000, 208.000], mean observation: 0.549 [0.000, 80.000], loss: 0.225848, mean_absolute_error: 0.487069, mean_q: 6.277799, mean_eps: 0.100000\n",
      " 110156/175000: episode: 3082, duration: 0.643s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.265 [1.000, 190.000], mean observation: 0.431 [0.000, 68.000], loss: 0.145204, mean_absolute_error: 0.486370, mean_q: 6.255746, mean_eps: 0.100000\n",
      " 110192/175000: episode: 3083, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.389 [1.000, 220.000], mean observation: 0.383 [0.000, 72.000], loss: 0.235224, mean_absolute_error: 0.502493, mean_q: 6.371379, mean_eps: 0.100000\n",
      " 110227/175000: episode: 3084, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.657 [1.000, 205.000], mean observation: 0.523 [0.000, 70.000], loss: 0.239526, mean_absolute_error: 0.488768, mean_q: 6.355213, mean_eps: 0.100000\n",
      " 110268/175000: episode: 3085, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.537 [1.000, 219.000], mean observation: 0.486 [0.000, 82.000], loss: 0.312868, mean_absolute_error: 0.495612, mean_q: 6.337172, mean_eps: 0.100000\n",
      " 110318/175000: episode: 3086, duration: 0.935s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 89.020 [22.000, 203.000], mean observation: 0.531 [0.000, 100.000], loss: 31.929172, mean_absolute_error: 0.767420, mean_q: 7.294113, mean_eps: 0.100000\n",
      " 110340/175000: episode: 3087, duration: 0.438s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 127.727 [83.000, 188.000], mean observation: 0.154 [0.000, 44.000], loss: 0.184372, mean_absolute_error: 0.507654, mean_q: 6.145593, mean_eps: 0.100000\n",
      " 110360/175000: episode: 3088, duration: 0.423s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 109.150 [65.000, 204.000], mean observation: 0.195 [0.000, 40.000], loss: 0.244945, mean_absolute_error: 0.523757, mean_q: 6.221249, mean_eps: 0.100000\n",
      " 110412/175000: episode: 3089, duration: 0.998s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.538 [8.000, 207.000], mean observation: 0.493 [0.000, 104.000], loss: 16.868416, mean_absolute_error: 0.578077, mean_q: 6.085587, mean_eps: 0.100000\n",
      " 110450/175000: episode: 3090, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.105 [25.000, 160.000], mean observation: 0.144 [0.000, 76.000], loss: 201.931228, mean_absolute_error: 1.502781, mean_q: 7.299170, mean_eps: 0.100000\n",
      " 110466/175000: episode: 3091, duration: 0.299s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 128.875 [103.000, 220.000], mean observation: 0.065 [0.000, 32.000], loss: 0.156734, mean_absolute_error: 0.481835, mean_q: 6.031261, mean_eps: 0.100000\n",
      " 110523/175000: episode: 3092, duration: 1.026s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.649 [15.000, 216.000], mean observation: 0.832 [0.000, 114.000], loss: 97.106483, mean_absolute_error: 1.208099, mean_q: 8.808702, mean_eps: 0.100000\n",
      " 110569/175000: episode: 3093, duration: 0.924s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.913 [3.000, 218.000], mean observation: 0.469 [0.000, 92.000], loss: 220.757660, mean_absolute_error: 1.569871, mean_q: 7.155326, mean_eps: 0.100000\n",
      " 110607/175000: episode: 3094, duration: 0.826s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.184 [46.000, 220.000], mean observation: 0.338 [0.000, 76.000], loss: 0.107767, mean_absolute_error: 0.458962, mean_q: 6.133392, mean_eps: 0.100000\n",
      " 110624/175000: episode: 3095, duration: 0.445s, episode steps: 17, steps per second: 38, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.529 [45.000, 202.000], mean observation: 0.074 [0.000, 34.000], loss: 0.147380, mean_absolute_error: 0.501486, mean_q: 6.224082, mean_eps: 0.100000\n",
      " 110643/175000: episode: 3096, duration: 0.476s, episode steps: 19, steps per second: 40, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 116.053 [45.000, 202.000], mean observation: 0.086 [0.000, 38.000], loss: 0.116430, mean_absolute_error: 0.496680, mean_q: 6.138400, mean_eps: 0.100000\n",
      " 110665/175000: episode: 3097, duration: 0.496s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 94.455 [48.000, 209.000], mean observation: 0.133 [0.000, 44.000], loss: 0.443196, mean_absolute_error: 0.519780, mean_q: 6.099052, mean_eps: 0.100000\n",
      " 110697/175000: episode: 3098, duration: 0.686s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 61.844 [45.000, 202.000], mean observation: 0.221 [0.000, 64.000], loss: 27.371769, mean_absolute_error: 0.621684, mean_q: 5.814986, mean_eps: 0.100000\n",
      " 110727/175000: episode: 3099, duration: 0.551s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 68.067 [45.000, 222.000], mean observation: 0.277 [0.000, 60.000], loss: 4.106171, mean_absolute_error: 0.723459, mean_q: 8.083295, mean_eps: 0.100000\n",
      " 110740/175000: episode: 3100, duration: 0.307s, episode steps: 13, steps per second: 42, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 91.000 [48.000, 202.000], mean observation: 0.083 [0.000, 26.000], loss: 82.332599, mean_absolute_error: 1.364053, mean_q: 10.495377, mean_eps: 0.100000\n",
      " 110770/175000: episode: 3101, duration: 0.645s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 79.533 [48.000, 202.000], mean observation: 0.189 [0.000, 60.000], loss: 0.206661, mean_absolute_error: 0.574399, mean_q: 6.321118, mean_eps: 0.100000\n",
      " 110808/175000: episode: 3102, duration: 0.763s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.921 [15.000, 168.000], mean observation: 0.339 [0.000, 76.000], loss: 4.605512, mean_absolute_error: 0.728935, mean_q: 7.649837, mean_eps: 0.100000\n",
      " 110833/175000: episode: 3103, duration: 0.504s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 79.760 [25.000, 217.000], mean observation: 0.145 [0.000, 50.000], loss: 0.109412, mean_absolute_error: 0.508061, mean_q: 6.007749, mean_eps: 0.100000\n",
      " 110883/175000: episode: 3104, duration: 0.901s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.860 [27.000, 208.000], mean observation: 0.353 [0.000, 100.000], loss: 0.168599, mean_absolute_error: 0.484347, mean_q: 6.069924, mean_eps: 0.100000\n",
      " 110919/175000: episode: 3105, duration: 0.688s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.861 [4.000, 168.000], mean observation: 0.170 [0.000, 72.000], loss: 0.355312, mean_absolute_error: 0.511728, mean_q: 6.029443, mean_eps: 0.100000\n",
      " 110961/175000: episode: 3106, duration: 0.801s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.833 [0.000, 195.000], mean observation: 0.330 [0.000, 84.000], loss: 0.125248, mean_absolute_error: 0.514821, mean_q: 5.896397, mean_eps: 0.100000\n",
      " 110977/175000: episode: 3107, duration: 0.305s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 109.688 [15.000, 168.000], mean observation: 0.053 [0.000, 32.000], loss: 0.123972, mean_absolute_error: 0.520860, mean_q: 5.892403, mean_eps: 0.100000\n",
      " 111005/175000: episode: 3108, duration: 0.539s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 84.429 [15.000, 192.000], mean observation: 0.191 [0.000, 56.000], loss: 0.150567, mean_absolute_error: 0.527112, mean_q: 6.072569, mean_eps: 0.100000\n",
      " 111035/175000: episode: 3109, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 29.467 [15.000, 155.000], mean observation: 0.081 [0.000, 60.000], loss: 0.383095, mean_absolute_error: 0.516290, mean_q: 5.973230, mean_eps: 0.100000\n",
      " 111065/175000: episode: 3110, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 89.067 [15.000, 217.000], mean observation: 0.262 [0.000, 60.000], loss: 32.446711, mean_absolute_error: 0.848730, mean_q: 7.880097, mean_eps: 0.100000\n",
      " 111090/175000: episode: 3111, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 87.200 [42.000, 181.000], mean observation: 0.208 [0.000, 50.000], loss: 0.571570, mean_absolute_error: 0.506644, mean_q: 5.943733, mean_eps: 0.100000\n",
      " 111131/175000: episode: 3112, duration: 0.716s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 158.024 [15.000, 222.000], mean observation: 0.480 [0.000, 82.000], loss: 0.159788, mean_absolute_error: 0.506187, mean_q: 5.990041, mean_eps: 0.100000\n",
      " 111177/175000: episode: 3113, duration: 1.042s, episode steps: 46, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 130.891 [5.000, 222.000], mean observation: 0.550 [0.000, 92.000], loss: 67.921060, mean_absolute_error: 0.914044, mean_q: 6.897966, mean_eps: 0.100000\n",
      " 111198/175000: episode: 3114, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 90.381 [5.000, 180.000], mean observation: 0.099 [0.000, 42.000], loss: 39.972871, mean_absolute_error: 0.963934, mean_q: 8.547364, mean_eps: 0.100000\n",
      " 111235/175000: episode: 3115, duration: 0.862s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.838 [5.000, 180.000], mean observation: 0.373 [0.000, 74.000], loss: 0.189391, mean_absolute_error: 0.479788, mean_q: 5.629890, mean_eps: 0.100000\n",
      " 111277/175000: episode: 3116, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.000 [18.000, 222.000], mean observation: 0.286 [0.000, 84.000], loss: 0.130658, mean_absolute_error: 0.481313, mean_q: 5.903411, mean_eps: 0.100000\n",
      " 111312/175000: episode: 3117, duration: 0.675s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.343 [18.000, 218.000], mean observation: 0.442 [0.000, 70.000], loss: 0.128734, mean_absolute_error: 0.489626, mean_q: 6.176365, mean_eps: 0.100000\n",
      " 111356/175000: episode: 3118, duration: 1.032s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.864 [18.000, 208.000], mean observation: 0.466 [0.000, 88.000], loss: 0.599487, mean_absolute_error: 0.571905, mean_q: 6.869402, mean_eps: 0.100000\n",
      " 111391/175000: episode: 3119, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.029 [1.000, 209.000], mean observation: 0.287 [0.000, 70.000], loss: 0.437350, mean_absolute_error: 0.478930, mean_q: 5.809193, mean_eps: 0.100000\n",
      " 111431/175000: episode: 3120, duration: 0.720s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 146.150 [1.000, 219.000], mean observation: 0.307 [0.000, 80.000], loss: 0.162104, mean_absolute_error: 0.471006, mean_q: 5.605887, mean_eps: 0.100000\n",
      " 111472/175000: episode: 3121, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 189.195 [24.000, 209.000], mean observation: 0.149 [0.000, 82.000], loss: 0.854433, mean_absolute_error: 0.560492, mean_q: 6.650680, mean_eps: 0.100000\n",
      " 111534/175000: episode: 3122, duration: 1.269s, episode steps: 62, steps per second: 49, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 120.355 [1.000, 209.000], mean observation: 0.742 [0.000, 124.000], loss: 0.275079, mean_absolute_error: 0.419964, mean_q: 5.499731, mean_eps: 0.100000\n",
      " 111576/175000: episode: 3123, duration: 0.857s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.381 [40.000, 210.000], mean observation: 0.262 [0.000, 84.000], loss: 0.200545, mean_absolute_error: 0.438304, mean_q: 5.719891, mean_eps: 0.100000\n",
      " 111629/175000: episode: 3124, duration: 1.007s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 119.717 [40.000, 183.000], mean observation: 0.191 [0.000, 106.000], loss: 1.213848, mean_absolute_error: 0.549909, mean_q: 6.527667, mean_eps: 0.100000\n",
      " 111652/175000: episode: 3125, duration: 0.449s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 100.739 [40.000, 202.000], mean observation: 0.124 [0.000, 46.000], loss: 0.291632, mean_absolute_error: 0.432602, mean_q: 5.525083, mean_eps: 0.100000\n",
      " 111673/175000: episode: 3126, duration: 0.451s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 118.571 [40.000, 183.000], mean observation: 0.075 [0.000, 42.000], loss: 0.371510, mean_absolute_error: 0.432240, mean_q: 5.608370, mean_eps: 0.100000\n",
      " 111729/175000: episode: 3127, duration: 1.028s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 89.964 [3.000, 191.000], mean observation: 0.640 [0.000, 112.000], loss: 0.347691, mean_absolute_error: 0.460465, mean_q: 5.589517, mean_eps: 0.100000\n",
      " 111756/175000: episode: 3128, duration: 0.487s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 137.333 [132.000, 196.000], mean observation: 0.069 [0.000, 54.000], loss: 0.254134, mean_absolute_error: 0.496025, mean_q: 5.835351, mean_eps: 0.100000\n",
      " 111789/175000: episode: 3129, duration: 0.618s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.364 [23.000, 206.000], mean observation: 0.281 [0.000, 66.000], loss: 0.233307, mean_absolute_error: 0.498840, mean_q: 6.060391, mean_eps: 0.100000\n",
      " 111819/175000: episode: 3130, duration: 0.569s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 135.633 [52.000, 208.000], mean observation: 0.209 [0.000, 60.000], loss: 0.349202, mean_absolute_error: 0.464541, mean_q: 5.777955, mean_eps: 0.100000\n",
      " 111861/175000: episode: 3131, duration: 0.797s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.881 [15.000, 220.000], mean observation: 0.235 [0.000, 84.000], loss: 1.436692, mean_absolute_error: 0.443701, mean_q: 5.545090, mean_eps: 0.100000\n",
      " 111883/175000: episode: 3132, duration: 0.374s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 106.455 [28.000, 208.000], mean observation: 0.201 [0.000, 44.000], loss: 5.049010, mean_absolute_error: 0.474179, mean_q: 5.729887, mean_eps: 0.100000\n",
      " 111911/175000: episode: 3133, duration: 0.528s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.607 [28.000, 166.000], mean observation: 0.186 [0.000, 56.000], loss: 3.800164, mean_absolute_error: 0.468536, mean_q: 5.374234, mean_eps: 0.100000\n",
      " 111947/175000: episode: 3134, duration: 0.765s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.944 [17.000, 158.000], mean observation: 0.362 [0.000, 72.000], loss: 7.665932, mean_absolute_error: 0.513308, mean_q: 5.586228, mean_eps: 0.100000\n",
      " 111983/175000: episode: 3135, duration: 0.803s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 52.611 [28.000, 206.000], mean observation: 0.304 [0.000, 72.000], loss: 6.973734, mean_absolute_error: 0.578127, mean_q: 5.841571, mean_eps: 0.100000\n",
      " 112018/175000: episode: 3136, duration: 0.716s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 56.914 [12.000, 174.000], mean observation: 0.379 [0.000, 70.000], loss: 7.012295, mean_absolute_error: 0.585158, mean_q: 5.670373, mean_eps: 0.100000\n",
      " 112078/175000: episode: 3137, duration: 1.089s, episode steps: 60, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 48.617 [28.000, 220.000], mean observation: 0.714 [0.000, 120.000], loss: 160.107564, mean_absolute_error: 1.494879, mean_q: 7.740641, mean_eps: 0.100000\n",
      " 112113/175000: episode: 3138, duration: 0.671s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.086 [1.000, 148.000], mean observation: 0.124 [0.000, 70.000], loss: 2.382023, mean_absolute_error: 0.794826, mean_q: 7.649232, mean_eps: 0.100000\n",
      " 112148/175000: episode: 3139, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.514 [1.000, 209.000], mean observation: 0.137 [0.000, 70.000], loss: 50.184493, mean_absolute_error: 1.002621, mean_q: 7.474495, mean_eps: 0.100000\n",
      " 112195/175000: episode: 3140, duration: 0.857s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.915 [1.000, 210.000], mean observation: 0.407 [0.000, 94.000], loss: 0.603047, mean_absolute_error: 0.567377, mean_q: 5.351023, mean_eps: 0.100000\n",
      " 112239/175000: episode: 3141, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 74.659 [1.000, 198.000], mean observation: 0.250 [0.000, 88.000], loss: 0.440775, mean_absolute_error: 0.547752, mean_q: 5.463658, mean_eps: 0.100000\n",
      " 112273/175000: episode: 3142, duration: 0.647s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.265 [1.000, 173.000], mean observation: 0.147 [0.000, 68.000], loss: 0.236540, mean_absolute_error: 0.525971, mean_q: 5.392970, mean_eps: 0.100000\n",
      " 112306/175000: episode: 3143, duration: 0.576s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.818 [28.000, 215.000], mean observation: 0.260 [0.000, 66.000], loss: 0.567507, mean_absolute_error: 0.487363, mean_q: 5.327946, mean_eps: 0.100000\n",
      " 112335/175000: episode: 3144, duration: 0.527s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 104.172 [1.000, 223.000], mean observation: 0.146 [0.000, 58.000], loss: 26.640369, mean_absolute_error: 0.804842, mean_q: 7.240169, mean_eps: 0.100000\n",
      " 112373/175000: episode: 3145, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.526 [3.000, 223.000], mean observation: 0.522 [0.000, 76.000], loss: 1.909914, mean_absolute_error: 0.634875, mean_q: 6.484745, mean_eps: 0.100000\n",
      " 112417/175000: episode: 3146, duration: 0.790s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.227 [13.000, 220.000], mean observation: 0.616 [0.000, 88.000], loss: 6.273082, mean_absolute_error: 0.673682, mean_q: 6.764297, mean_eps: 0.100000\n",
      " 112437/175000: episode: 3147, duration: 0.385s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 20.050 [1.000, 215.000], mean observation: 0.053 [0.000, 40.000], loss: 0.162268, mean_absolute_error: 0.507376, mean_q: 5.605628, mean_eps: 0.100000\n",
      " 112462/175000: episode: 3148, duration: 0.434s, episode steps: 25, steps per second: 58, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 74.760 [1.000, 214.000], mean observation: 0.205 [0.000, 50.000], loss: 0.457218, mean_absolute_error: 0.532900, mean_q: 5.747233, mean_eps: 0.100000\n",
      " 112504/175000: episode: 3149, duration: 0.830s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.190 [1.000, 218.000], mean observation: 0.536 [0.000, 84.000], loss: 33.220241, mean_absolute_error: 0.772074, mean_q: 6.861128, mean_eps: 0.100000\n",
      " 112559/175000: episode: 3150, duration: 1.029s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 56.709 [1.000, 179.000], mean observation: 0.397 [0.000, 110.000], loss: 45.772481, mean_absolute_error: 0.771568, mean_q: 6.560808, mean_eps: 0.100000\n",
      " 112617/175000: episode: 3151, duration: 1.064s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 56.448 [1.000, 169.000], mean observation: 0.402 [0.000, 116.000], loss: 144.695942, mean_absolute_error: 1.192419, mean_q: 6.531966, mean_eps: 0.100000\n",
      " 112656/175000: episode: 3152, duration: 0.707s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.000 [1.000, 206.000], mean observation: 0.443 [0.000, 78.000], loss: 72.302124, mean_absolute_error: 0.928836, mean_q: 7.090562, mean_eps: 0.100000\n",
      " 112703/175000: episode: 3153, duration: 0.838s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 57.638 [1.000, 206.000], mean observation: 0.555 [0.000, 94.000], loss: 136.203299, mean_absolute_error: 1.171508, mean_q: 6.587148, mean_eps: 0.100000\n",
      " 112731/175000: episode: 3154, duration: 0.520s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 107.393 [41.000, 199.000], mean observation: 0.154 [0.000, 56.000], loss: 0.302849, mean_absolute_error: 0.444053, mean_q: 5.659665, mean_eps: 0.100000\n",
      " 112776/175000: episode: 3155, duration: 0.862s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 63.578 [1.000, 205.000], mean observation: 0.400 [0.000, 90.000], loss: 0.268262, mean_absolute_error: 0.450125, mean_q: 5.668703, mean_eps: 0.100000\n",
      " 112815/175000: episode: 3156, duration: 0.747s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 31.256 [1.000, 176.000], mean observation: 0.345 [0.000, 78.000], loss: 0.559891, mean_absolute_error: 0.424395, mean_q: 5.333301, mean_eps: 0.100000\n",
      " 112857/175000: episode: 3157, duration: 0.783s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.405 [1.000, 211.000], mean observation: 0.273 [0.000, 84.000], loss: 0.374543, mean_absolute_error: 0.412971, mean_q: 5.293510, mean_eps: 0.100000\n",
      " 112895/175000: episode: 3158, duration: 0.666s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.132 [10.000, 209.000], mean observation: 0.221 [0.000, 76.000], loss: 25.249362, mean_absolute_error: 0.703715, mean_q: 6.956276, mean_eps: 0.100000\n",
      " 112927/175000: episode: 3159, duration: 0.601s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 61.875 [42.000, 184.000], mean observation: 0.138 [0.000, 64.000], loss: 0.156893, mean_absolute_error: 0.530782, mean_q: 6.206510, mean_eps: 0.100000\n",
      " 112972/175000: episode: 3160, duration: 0.876s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.711 [15.000, 209.000], mean observation: 0.371 [0.000, 90.000], loss: 11.227861, mean_absolute_error: 0.720939, mean_q: 7.562842, mean_eps: 0.100000\n",
      " 113020/175000: episode: 3161, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 72.000 [0.000, 208.000], mean observation: 0.218 [0.000, 96.000], loss: 121.309660, mean_absolute_error: 1.232235, mean_q: 7.704222, mean_eps: 0.100000\n",
      " 113047/175000: episode: 3162, duration: 0.545s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 43.556 [11.000, 215.000], mean observation: 0.070 [0.000, 54.000], loss: 0.305784, mean_absolute_error: 0.463024, mean_q: 5.193744, mean_eps: 0.100000\n",
      " 113087/175000: episode: 3163, duration: 0.748s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.175 [38.000, 181.000], mean observation: 0.099 [0.000, 80.000], loss: 0.146696, mean_absolute_error: 0.476226, mean_q: 5.593756, mean_eps: 0.100000\n",
      " 113122/175000: episode: 3164, duration: 0.671s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.457 [33.000, 177.000], mean observation: 0.144 [0.000, 70.000], loss: 335.215041, mean_absolute_error: 2.137258, mean_q: 7.062722, mean_eps: 0.100000\n",
      " 113162/175000: episode: 3165, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 38.075 [20.000, 59.000], mean observation: 0.144 [0.000, 80.000], loss: 0.205128, mean_absolute_error: 0.487849, mean_q: 5.637808, mean_eps: 0.100000\n",
      " 113205/175000: episode: 3166, duration: 0.792s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 38.000 [38.000, 38.000], mean observation: 0.099 [0.000, 86.000], loss: 223.074321, mean_absolute_error: 1.744447, mean_q: 8.272706, mean_eps: 0.100000\n",
      " 113268/175000: episode: 3167, duration: 1.108s, episode steps: 63, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 69.714 [1.000, 177.000], mean observation: 0.436 [0.000, 126.000], loss: 58.751739, mean_absolute_error: 0.851348, mean_q: 6.719086, mean_eps: 0.100000\n",
      " 113310/175000: episode: 3168, duration: 0.794s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 30.500 [1.000, 188.000], mean observation: 0.170 [0.000, 84.000], loss: 286.939103, mean_absolute_error: 1.976940, mean_q: 7.609518, mean_eps: 0.100000\n",
      " 113353/175000: episode: 3169, duration: 0.891s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.442 [1.000, 188.000], mean observation: 0.415 [0.000, 86.000], loss: 0.176314, mean_absolute_error: 0.534849, mean_q: 5.832171, mean_eps: 0.100000\n",
      " 113400/175000: episode: 3170, duration: 0.950s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.553 [21.000, 206.000], mean observation: 0.589 [0.000, 94.000], loss: 0.561268, mean_absolute_error: 0.569443, mean_q: 5.936355, mean_eps: 0.100000\n",
      " 113438/175000: episode: 3171, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.579 [1.000, 206.000], mean observation: 0.272 [0.000, 76.000], loss: 0.265465, mean_absolute_error: 0.525828, mean_q: 5.694280, mean_eps: 0.100000\n",
      " 113474/175000: episode: 3172, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.333 [1.000, 181.000], mean observation: 0.196 [0.000, 72.000], loss: 0.147585, mean_absolute_error: 0.560927, mean_q: 5.721726, mean_eps: 0.100000\n",
      " 113517/175000: episode: 3173, duration: 0.772s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.442 [1.000, 188.000], mean observation: 0.406 [0.000, 86.000], loss: 22.650054, mean_absolute_error: 0.919223, mean_q: 7.748175, mean_eps: 0.100000\n",
      " 113555/175000: episode: 3174, duration: 0.685s, episode steps: 38, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 42.474 [1.000, 212.000], mean observation: 0.151 [0.000, 76.000], loss: 24.527216, mean_absolute_error: 0.733358, mean_q: 5.613969, mean_eps: 0.100000\n",
      " 113585/175000: episode: 3175, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 53.500 [38.000, 148.000], mean observation: 0.079 [0.000, 60.000], loss: 2.538476, mean_absolute_error: 0.641410, mean_q: 5.411280, mean_eps: 0.100000\n",
      " 113616/175000: episode: 3176, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 44.452 [1.000, 72.000], mean observation: 0.096 [0.000, 62.000], loss: 0.201517, mean_absolute_error: 0.610147, mean_q: 5.470203, mean_eps: 0.100000\n",
      " 113643/175000: episode: 3177, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 45.481 [30.000, 68.000], mean observation: 0.085 [0.000, 54.000], loss: 0.750377, mean_absolute_error: 0.775559, mean_q: 7.148769, mean_eps: 0.100000\n",
      " 113699/175000: episode: 3178, duration: 1.008s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 107.339 [1.000, 188.000], mean observation: 0.605 [0.000, 112.000], loss: 371.869903, mean_absolute_error: 2.328253, mean_q: 6.304360, mean_eps: 0.100000\n",
      " 113726/175000: episode: 3179, duration: 0.493s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.741 [26.000, 177.000], mean observation: 0.197 [0.000, 54.000], loss: 0.208494, mean_absolute_error: 0.630295, mean_q: 5.979032, mean_eps: 0.100000\n",
      " 113756/175000: episode: 3180, duration: 0.564s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 61.800 [38.000, 194.000], mean observation: 0.122 [0.000, 60.000], loss: 0.094467, mean_absolute_error: 0.554767, mean_q: 5.526941, mean_eps: 0.100000\n",
      " 113779/175000: episode: 3181, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 81.130 [38.000, 205.000], mean observation: 0.129 [0.000, 46.000], loss: 665.606831, mean_absolute_error: 3.740754, mean_q: 7.951030, mean_eps: 0.100000\n",
      " 113831/175000: episode: 3182, duration: 0.930s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.500 [27.000, 223.000], mean observation: 0.820 [0.000, 104.000], loss: 0.217062, mean_absolute_error: 0.546334, mean_q: 5.465712, mean_eps: 0.100000\n",
      " 113860/175000: episode: 3183, duration: 0.589s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 57.000 [18.000, 221.000], mean observation: 0.141 [0.000, 58.000], loss: 805.660636, mean_absolute_error: 4.306668, mean_q: 7.308063, mean_eps: 0.100000\n",
      " 113907/175000: episode: 3184, duration: 0.978s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.298 [18.000, 221.000], mean observation: 0.472 [0.000, 94.000], loss: 0.530235, mean_absolute_error: 0.548095, mean_q: 5.396431, mean_eps: 0.100000\n",
      " 113948/175000: episode: 3185, duration: 0.777s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.707 [5.000, 176.000], mean observation: 0.278 [0.000, 82.000], loss: 11.329474, mean_absolute_error: 0.592874, mean_q: 5.193910, mean_eps: 0.100000\n",
      " 113986/175000: episode: 3186, duration: 0.735s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.816 [5.000, 221.000], mean observation: 0.302 [0.000, 76.000], loss: 200.197448, mean_absolute_error: 1.533890, mean_q: 6.281808, mean_eps: 0.100000\n",
      " 114044/175000: episode: 3187, duration: 1.083s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 115.776 [3.000, 221.000], mean observation: 0.528 [0.000, 116.000], loss: 1.809197, mean_absolute_error: 0.594267, mean_q: 5.850010, mean_eps: 0.100000\n",
      " 114099/175000: episode: 3188, duration: 1.035s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 108.636 [0.000, 223.000], mean observation: 0.676 [0.000, 110.000], loss: 0.782905, mean_absolute_error: 0.581335, mean_q: 5.745673, mean_eps: 0.100000\n",
      " 114139/175000: episode: 3189, duration: 0.769s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.075 [49.000, 178.000], mean observation: 0.468 [0.000, 80.000], loss: 1.847092, mean_absolute_error: 0.502579, mean_q: 4.972106, mean_eps: 0.100000\n",
      " 114180/175000: episode: 3190, duration: 0.782s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.049 [9.000, 212.000], mean observation: 0.421 [0.000, 82.000], loss: 84.181674, mean_absolute_error: 0.999743, mean_q: 6.143812, mean_eps: 0.100000\n",
      " 114209/175000: episode: 3191, duration: 0.569s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 138.103 [55.000, 215.000], mean observation: 0.270 [0.000, 58.000], loss: 0.152097, mean_absolute_error: 0.523498, mean_q: 5.092046, mean_eps: 0.100000\n",
      " 114243/175000: episode: 3192, duration: 0.645s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.471 [52.000, 221.000], mean observation: 0.399 [0.000, 68.000], loss: 99.153780, mean_absolute_error: 1.273274, mean_q: 8.071585, mean_eps: 0.100000\n",
      " 114285/175000: episode: 3193, duration: 0.822s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 140.548 [3.000, 217.000], mean observation: 0.359 [0.000, 84.000], loss: 0.309231, mean_absolute_error: 0.518911, mean_q: 4.974310, mean_eps: 0.100000\n",
      " 114309/175000: episode: 3194, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 146.250 [24.000, 222.000], mean observation: 0.228 [0.000, 48.000], loss: 0.096720, mean_absolute_error: 0.529159, mean_q: 5.298711, mean_eps: 0.100000\n",
      " 114348/175000: episode: 3195, duration: 0.750s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.949 [1.000, 222.000], mean observation: 0.279 [0.000, 78.000], loss: 19.279829, mean_absolute_error: 0.713260, mean_q: 6.232649, mean_eps: 0.100000\n",
      " 114377/175000: episode: 3196, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 117.690 [16.000, 222.000], mean observation: 0.307 [0.000, 58.000], loss: 0.755605, mean_absolute_error: 0.531463, mean_q: 5.614920, mean_eps: 0.100000\n",
      " 114408/175000: episode: 3197, duration: 0.560s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.774 [44.000, 215.000], mean observation: 0.170 [0.000, 62.000], loss: 131.062915, mean_absolute_error: 1.274157, mean_q: 7.250130, mean_eps: 0.100000\n",
      " 114445/175000: episode: 3198, duration: 0.865s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.054 [1.000, 222.000], mean observation: 0.524 [0.000, 74.000], loss: 3.206900, mean_absolute_error: 0.639533, mean_q: 6.759229, mean_eps: 0.100000\n",
      " 114478/175000: episode: 3199, duration: 0.581s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 118.242 [44.000, 178.000], mean observation: 0.260 [0.000, 66.000], loss: 50.425157, mean_absolute_error: 0.922945, mean_q: 7.247900, mean_eps: 0.100000\n",
      " 114526/175000: episode: 3200, duration: 0.886s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.021 [27.000, 169.000], mean observation: 0.416 [0.000, 96.000], loss: 122.338362, mean_absolute_error: 1.292740, mean_q: 7.953076, mean_eps: 0.100000\n",
      " 114558/175000: episode: 3201, duration: 0.593s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.688 [6.000, 169.000], mean observation: 0.179 [0.000, 64.000], loss: 121.340917, mean_absolute_error: 1.221889, mean_q: 7.392531, mean_eps: 0.100000\n",
      " 114570/175000: episode: 3202, duration: 0.227s, episode steps: 12, steps per second: 53, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 107.000 [55.000, 224.000], mean observation: 0.097 [0.000, 24.000], loss: 0.125959, mean_absolute_error: 0.526508, mean_q: 5.759363, mean_eps: 0.100000\n",
      " 114605/175000: episode: 3203, duration: 0.653s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.543 [28.000, 169.000], mean observation: 0.230 [0.000, 70.000], loss: 53.081876, mean_absolute_error: 0.902941, mean_q: 7.351235, mean_eps: 0.100000\n",
      " 114644/175000: episode: 3204, duration: 0.804s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.538 [29.000, 185.000], mean observation: 0.388 [0.000, 78.000], loss: 16.184331, mean_absolute_error: 0.737104, mean_q: 7.311669, mean_eps: 0.100000\n",
      " 114680/175000: episode: 3205, duration: 0.723s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.472 [22.000, 213.000], mean observation: 0.301 [0.000, 72.000], loss: 0.247713, mean_absolute_error: 0.507381, mean_q: 5.760230, mean_eps: 0.100000\n",
      " 114723/175000: episode: 3206, duration: 0.894s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.721 [38.000, 210.000], mean observation: 0.358 [0.000, 86.000], loss: 82.270686, mean_absolute_error: 1.127481, mean_q: 8.267279, mean_eps: 0.100000\n",
      " 114757/175000: episode: 3207, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 89.176 [18.000, 169.000], mean observation: 0.219 [0.000, 68.000], loss: 0.136463, mean_absolute_error: 0.528520, mean_q: 5.685848, mean_eps: 0.100000\n",
      " 114785/175000: episode: 3208, duration: 0.531s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 37.857 [1.000, 215.000], mean observation: 0.125 [0.000, 56.000], loss: 0.115184, mean_absolute_error: 0.558333, mean_q: 5.809732, mean_eps: 0.100000\n",
      " 114820/175000: episode: 3209, duration: 0.631s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.857 [1.000, 208.000], mean observation: 0.350 [0.000, 70.000], loss: 7.388201, mean_absolute_error: 0.747445, mean_q: 7.256573, mean_eps: 0.100000\n",
      " 114873/175000: episode: 3210, duration: 1.037s, episode steps: 53, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 93.604 [1.000, 222.000], mean observation: 0.514 [0.000, 106.000], loss: 319.012830, mean_absolute_error: 2.251450, mean_q: 8.537870, mean_eps: 0.100000\n",
      " 114914/175000: episode: 3211, duration: 0.707s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 76.146 [7.000, 208.000], mean observation: 0.387 [0.000, 82.000], loss: 0.106049, mean_absolute_error: 0.548817, mean_q: 5.699363, mean_eps: 0.100000\n",
      " 114956/175000: episode: 3212, duration: 0.804s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.405 [28.000, 211.000], mean observation: 0.250 [0.000, 84.000], loss: 0.090225, mean_absolute_error: 0.530581, mean_q: 5.693082, mean_eps: 0.100000\n",
      " 114998/175000: episode: 3213, duration: 0.795s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.048 [0.000, 211.000], mean observation: 0.395 [0.000, 84.000], loss: 0.217681, mean_absolute_error: 0.490940, mean_q: 5.463237, mean_eps: 0.100000\n",
      " 115041/175000: episode: 3214, duration: 0.832s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.977 [33.000, 218.000], mean observation: 0.570 [0.000, 86.000], loss: 0.367244, mean_absolute_error: 0.454369, mean_q: 5.517276, mean_eps: 0.100000\n",
      " 115090/175000: episode: 3215, duration: 0.873s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.612 [17.000, 207.000], mean observation: 0.407 [0.000, 98.000], loss: 26.503226, mean_absolute_error: 0.709191, mean_q: 6.603236, mean_eps: 0.100000\n",
      " 115115/175000: episode: 3216, duration: 0.464s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 158.040 [108.000, 211.000], mean observation: 0.184 [0.000, 50.000], loss: 31.419718, mean_absolute_error: 0.863561, mean_q: 7.686232, mean_eps: 0.100000\n",
      " 115141/175000: episode: 3217, duration: 0.495s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 163.692 [38.000, 211.000], mean observation: 0.221 [0.000, 52.000], loss: 0.303667, mean_absolute_error: 0.503391, mean_q: 5.710592, mean_eps: 0.100000\n",
      " 115177/175000: episode: 3218, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.472 [37.000, 211.000], mean observation: 0.288 [0.000, 72.000], loss: 137.703395, mean_absolute_error: 1.262475, mean_q: 7.269855, mean_eps: 0.100000\n",
      " 115215/175000: episode: 3219, duration: 0.706s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.579 [38.000, 222.000], mean observation: 0.592 [0.000, 76.000], loss: 22.830857, mean_absolute_error: 0.753241, mean_q: 7.009398, mean_eps: 0.100000\n",
      " 115261/175000: episode: 3220, duration: 0.846s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.804 [1.000, 211.000], mean observation: 0.233 [0.000, 92.000], loss: 0.517819, mean_absolute_error: 0.493042, mean_q: 5.275254, mean_eps: 0.100000\n",
      " 115317/175000: episode: 3221, duration: 1.010s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 113.482 [28.000, 207.000], mean observation: 0.504 [0.000, 112.000], loss: 98.210164, mean_absolute_error: 1.117040, mean_q: 7.434852, mean_eps: 0.100000\n",
      " 115359/175000: episode: 3222, duration: 0.750s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.262 [48.000, 207.000], mean observation: 0.527 [0.000, 84.000], loss: 36.315257, mean_absolute_error: 0.766165, mean_q: 6.759755, mean_eps: 0.100000\n",
      " 115391/175000: episode: 3223, duration: 0.643s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 164.656 [66.000, 215.000], mean observation: 0.304 [0.000, 64.000], loss: 0.214774, mean_absolute_error: 0.452843, mean_q: 5.510098, mean_eps: 0.100000\n",
      " 115429/175000: episode: 3224, duration: 0.723s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.842 [68.000, 215.000], mean observation: 0.241 [0.000, 76.000], loss: 172.440147, mean_absolute_error: 1.499797, mean_q: 8.218418, mean_eps: 0.100000\n",
      " 115462/175000: episode: 3225, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 95.485 [17.000, 221.000], mean observation: 0.325 [0.000, 66.000], loss: 0.095884, mean_absolute_error: 0.512726, mean_q: 5.873794, mean_eps: 0.100000\n",
      " 115492/175000: episode: 3226, duration: 0.563s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.867 [17.000, 203.000], mean observation: 0.243 [0.000, 60.000], loss: 0.123092, mean_absolute_error: 0.507869, mean_q: 6.011984, mean_eps: 0.100000\n",
      " 115535/175000: episode: 3227, duration: 0.809s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.791 [6.000, 202.000], mean observation: 0.623 [0.000, 86.000], loss: 0.138960, mean_absolute_error: 0.478928, mean_q: 5.690228, mean_eps: 0.100000\n",
      " 115593/175000: episode: 3228, duration: 1.083s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 93.776 [9.000, 216.000], mean observation: 0.773 [0.000, 116.000], loss: 0.098372, mean_absolute_error: 0.475647, mean_q: 5.733857, mean_eps: 0.100000\n",
      " 115648/175000: episode: 3229, duration: 0.992s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 71.127 [1.000, 209.000], mean observation: 0.754 [0.000, 110.000], loss: 0.096235, mean_absolute_error: 0.478875, mean_q: 5.828708, mean_eps: 0.100000\n",
      " 115686/175000: episode: 3230, duration: 0.718s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.105 [6.000, 207.000], mean observation: 0.498 [0.000, 76.000], loss: 0.121052, mean_absolute_error: 0.499402, mean_q: 5.923804, mean_eps: 0.100000\n",
      " 115723/175000: episode: 3231, duration: 0.671s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 86.649 [3.000, 193.000], mean observation: 0.324 [0.000, 74.000], loss: 182.332557, mean_absolute_error: 1.610165, mean_q: 8.758766, mean_eps: 0.100000\n",
      " 115767/175000: episode: 3232, duration: 0.786s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.136 [1.000, 215.000], mean observation: 0.690 [0.000, 88.000], loss: 0.149614, mean_absolute_error: 0.458760, mean_q: 5.948889, mean_eps: 0.100000\n",
      " 115813/175000: episode: 3233, duration: 0.847s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 63.413 [8.000, 208.000], mean observation: 0.459 [0.000, 92.000], loss: 0.155289, mean_absolute_error: 0.478155, mean_q: 6.083828, mean_eps: 0.100000\n",
      " 115848/175000: episode: 3234, duration: 0.690s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 103.057 [6.000, 222.000], mean observation: 0.253 [0.000, 70.000], loss: 0.230681, mean_absolute_error: 0.479589, mean_q: 5.994357, mean_eps: 0.100000\n",
      " 115906/175000: episode: 3235, duration: 1.054s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 89.138 [12.000, 201.000], mean observation: 0.903 [0.000, 116.000], loss: 36.009012, mean_absolute_error: 0.816735, mean_q: 7.442138, mean_eps: 0.100000\n",
      " 115939/175000: episode: 3236, duration: 0.598s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.000 [38.000, 224.000], mean observation: 0.169 [0.000, 66.000], loss: 0.105700, mean_absolute_error: 0.472318, mean_q: 5.772073, mean_eps: 0.100000\n",
      " 115963/175000: episode: 3237, duration: 0.440s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 36.375 [28.000, 179.000], mean observation: 0.115 [0.000, 48.000], loss: 0.106153, mean_absolute_error: 0.509659, mean_q: 5.978981, mean_eps: 0.100000\n",
      " 116003/175000: episode: 3238, duration: 0.759s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 92.025 [28.000, 171.000], mean observation: 0.428 [0.000, 80.000], loss: 58.942339, mean_absolute_error: 0.958593, mean_q: 7.572701, mean_eps: 0.100000\n",
      " 116043/175000: episode: 3239, duration: 0.720s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 90.075 [0.000, 178.000], mean observation: 0.326 [0.000, 80.000], loss: 229.046858, mean_absolute_error: 1.962939, mean_q: 9.743101, mean_eps: 0.100000\n",
      " 116079/175000: episode: 3240, duration: 0.682s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [18.000, 188.000], mean observation: 0.347 [0.000, 72.000], loss: 0.457816, mean_absolute_error: 0.566886, mean_q: 6.148844, mean_eps: 0.100000\n",
      " 116114/175000: episode: 3241, duration: 0.663s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.600 [23.000, 186.000], mean observation: 0.301 [0.000, 70.000], loss: 0.167167, mean_absolute_error: 0.540497, mean_q: 5.909683, mean_eps: 0.100000\n",
      " 116170/175000: episode: 3242, duration: 1.005s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 128.286 [14.000, 214.000], mean observation: 0.530 [0.000, 112.000], loss: 129.011281, mean_absolute_error: 1.418308, mean_q: 9.055720, mean_eps: 0.100000\n",
      " 116197/175000: episode: 3243, duration: 0.519s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 66.519 [0.000, 169.000], mean observation: 0.171 [0.000, 54.000], loss: 23.118690, mean_absolute_error: 0.596476, mean_q: 5.906392, mean_eps: 0.100000\n",
      " 116226/175000: episode: 3244, duration: 0.513s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 47.310 [18.000, 137.000], mean observation: 0.169 [0.000, 58.000], loss: 0.143400, mean_absolute_error: 0.493248, mean_q: 6.153224, mean_eps: 0.100000\n",
      " 116247/175000: episode: 3245, duration: 0.368s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 58.714 [38.000, 139.000], mean observation: 0.077 [0.000, 42.000], loss: 0.161739, mean_absolute_error: 0.470309, mean_q: 5.953909, mean_eps: 0.100000\n",
      " 116271/175000: episode: 3246, duration: 0.452s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 67.417 [0.000, 217.000], mean observation: 0.179 [0.000, 48.000], loss: 0.167603, mean_absolute_error: 0.482884, mean_q: 5.963179, mean_eps: 0.100000\n",
      " 116305/175000: episode: 3247, duration: 0.670s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 63.618 [6.000, 176.000], mean observation: 0.332 [0.000, 68.000], loss: 65.135093, mean_absolute_error: 0.914321, mean_q: 7.467612, mean_eps: 0.100000\n",
      " 116344/175000: episode: 3248, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 53.077 [6.000, 147.000], mean observation: 0.308 [0.000, 78.000], loss: 33.253471, mean_absolute_error: 0.763863, mean_q: 7.337153, mean_eps: 0.100000\n",
      " 116387/175000: episode: 3249, duration: 0.806s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.163 [4.000, 221.000], mean observation: 0.436 [0.000, 86.000], loss: 19.836638, mean_absolute_error: 0.712827, mean_q: 7.562065, mean_eps: 0.100000\n",
      " 116416/175000: episode: 3250, duration: 0.578s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 97.931 [6.000, 158.000], mean observation: 0.156 [0.000, 58.000], loss: 435.316556, mean_absolute_error: 3.073104, mean_q: 12.361218, mean_eps: 0.100000\n",
      " 116436/175000: episode: 3251, duration: 0.462s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 89.300 [38.000, 169.000], mean observation: 0.086 [0.000, 40.000], loss: 0.163590, mean_absolute_error: 0.494123, mean_q: 6.246423, mean_eps: 0.100000\n",
      " 116474/175000: episode: 3252, duration: 0.748s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.579 [2.000, 191.000], mean observation: 0.244 [0.000, 76.000], loss: 0.181625, mean_absolute_error: 0.512114, mean_q: 6.509327, mean_eps: 0.100000\n",
      " 116503/175000: episode: 3253, duration: 0.507s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.552 [38.000, 139.000], mean observation: 0.166 [0.000, 58.000], loss: 10.318951, mean_absolute_error: 0.985687, mean_q: 10.618787, mean_eps: 0.100000\n",
      " 116541/175000: episode: 3254, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.763 [12.000, 215.000], mean observation: 0.360 [0.000, 76.000], loss: 9.721661, mean_absolute_error: 0.739624, mean_q: 8.386594, mean_eps: 0.100000\n",
      " 116596/175000: episode: 3255, duration: 1.033s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.127 [1.000, 217.000], mean observation: 0.494 [0.000, 110.000], loss: 149.768785, mean_absolute_error: 1.291587, mean_q: 7.866825, mean_eps: 0.100000\n",
      " 116625/175000: episode: 3256, duration: 0.587s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.138 [21.000, 208.000], mean observation: 0.176 [0.000, 58.000], loss: 6.624415, mean_absolute_error: 0.970560, mean_q: 11.242095, mean_eps: 0.100000\n",
      " 116652/175000: episode: 3257, duration: 0.485s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 119.963 [38.000, 176.000], mean observation: 0.157 [0.000, 54.000], loss: 0.152503, mean_absolute_error: 0.453549, mean_q: 6.505231, mean_eps: 0.100000\n",
      " 116692/175000: episode: 3258, duration: 0.842s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 130.325 [38.000, 221.000], mean observation: 0.307 [0.000, 80.000], loss: 153.868190, mean_absolute_error: 1.276397, mean_q: 7.587237, mean_eps: 0.100000\n",
      " 116719/175000: episode: 3259, duration: 0.557s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 120.148 [44.000, 176.000], mean observation: 0.172 [0.000, 54.000], loss: 3.713821, mean_absolute_error: 0.683645, mean_q: 8.190705, mean_eps: 0.100000\n",
      " 116769/175000: episode: 3260, duration: 0.933s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 68.040 [28.000, 176.000], mean observation: 0.376 [0.000, 100.000], loss: 0.206350, mean_absolute_error: 0.481137, mean_q: 6.480811, mean_eps: 0.100000\n",
      " 116813/175000: episode: 3261, duration: 0.820s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.227 [28.000, 158.000], mean observation: 0.246 [0.000, 88.000], loss: 0.184860, mean_absolute_error: 0.457523, mean_q: 6.172488, mean_eps: 0.100000\n",
      " 116859/175000: episode: 3262, duration: 0.818s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.348 [38.000, 208.000], mean observation: 0.224 [0.000, 92.000], loss: 0.160373, mean_absolute_error: 0.488552, mean_q: 6.266604, mean_eps: 0.100000\n",
      " 116889/175000: episode: 3263, duration: 0.604s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 76.567 [28.000, 192.000], mean observation: 0.093 [0.000, 60.000], loss: 0.152067, mean_absolute_error: 0.488818, mean_q: 5.894333, mean_eps: 0.100000\n",
      " 116932/175000: episode: 3264, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 70.651 [25.000, 169.000], mean observation: 0.268 [0.000, 86.000], loss: 3.959421, mean_absolute_error: 0.685276, mean_q: 7.430901, mean_eps: 0.100000\n",
      " 116974/175000: episode: 3265, duration: 0.800s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.905 [1.000, 222.000], mean observation: 0.414 [0.000, 84.000], loss: 18.980164, mean_absolute_error: 0.786660, mean_q: 7.389423, mean_eps: 0.100000\n",
      " 117007/175000: episode: 3266, duration: 0.571s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 73.152 [37.000, 156.000], mean observation: 0.130 [0.000, 66.000], loss: 0.124547, mean_absolute_error: 0.613770, mean_q: 6.512831, mean_eps: 0.100000\n",
      " 117064/175000: episode: 3267, duration: 1.051s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 49.579 [1.000, 200.000], mean observation: 0.265 [0.000, 114.000], loss: 25.097615, mean_absolute_error: 0.794038, mean_q: 7.256647, mean_eps: 0.100000\n",
      " 117113/175000: episode: 3268, duration: 0.932s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.082 [1.000, 218.000], mean observation: 0.356 [0.000, 98.000], loss: 59.394876, mean_absolute_error: 0.948604, mean_q: 7.076615, mean_eps: 0.100000\n",
      " 117121/175000: episode: 3269, duration: 0.147s, episode steps: 8, steps per second: 54, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 38.000 [38.000, 38.000], mean observation: 0.021 [0.000, 16.000], loss: 0.116048, mean_absolute_error: 0.608407, mean_q: 6.209617, mean_eps: 0.100000\n",
      " 117161/175000: episode: 3270, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 91.150 [1.000, 221.000], mean observation: 0.247 [0.000, 80.000], loss: 47.480322, mean_absolute_error: 1.114661, mean_q: 9.090202, mean_eps: 0.100000\n",
      " 117198/175000: episode: 3271, duration: 0.673s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 100.514 [1.000, 215.000], mean observation: 0.368 [0.000, 74.000], loss: 0.124110, mean_absolute_error: 0.593990, mean_q: 5.933127, mean_eps: 0.100000\n",
      " 117234/175000: episode: 3272, duration: 0.681s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.000 [1.000, 218.000], mean observation: 0.352 [0.000, 72.000], loss: 0.158817, mean_absolute_error: 0.564964, mean_q: 5.746640, mean_eps: 0.100000\n",
      " 117253/175000: episode: 3273, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 67.158 [1.000, 221.000], mean observation: 0.146 [0.000, 38.000], loss: 6.247247, mean_absolute_error: 0.860880, mean_q: 8.614882, mean_eps: 0.100000\n",
      " 117289/175000: episode: 3274, duration: 0.674s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 62.667 [1.000, 203.000], mean observation: 0.385 [0.000, 72.000], loss: 0.183592, mean_absolute_error: 0.548573, mean_q: 5.956403, mean_eps: 0.100000\n",
      " 117336/175000: episode: 3275, duration: 0.840s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 70.447 [1.000, 210.000], mean observation: 0.455 [0.000, 94.000], loss: 0.148509, mean_absolute_error: 0.556621, mean_q: 5.960176, mean_eps: 0.100000\n",
      " 117376/175000: episode: 3276, duration: 0.806s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 85.925 [0.000, 203.000], mean observation: 0.326 [0.000, 80.000], loss: 0.185240, mean_absolute_error: 0.532915, mean_q: 6.034584, mean_eps: 0.100000\n",
      " 117406/175000: episode: 3277, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 87.833 [38.000, 203.000], mean observation: 0.313 [0.000, 60.000], loss: 84.941000, mean_absolute_error: 1.047536, mean_q: 7.525250, mean_eps: 0.100000\n",
      " 117447/175000: episode: 3278, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.024 [41.000, 216.000], mean observation: 0.328 [0.000, 82.000], loss: 0.137525, mean_absolute_error: 0.483292, mean_q: 5.837098, mean_eps: 0.100000\n",
      " 117480/175000: episode: 3279, duration: 0.643s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 141.788 [31.000, 203.000], mean observation: 0.305 [0.000, 66.000], loss: 0.438656, mean_absolute_error: 0.672185, mean_q: 7.745419, mean_eps: 0.100000\n",
      " 117513/175000: episode: 3280, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 147.970 [55.000, 203.000], mean observation: 0.219 [0.000, 66.000], loss: 11.650019, mean_absolute_error: 0.556774, mean_q: 5.949332, mean_eps: 0.100000\n",
      " 117558/175000: episode: 3281, duration: 0.825s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.356 [1.000, 211.000], mean observation: 0.580 [0.000, 90.000], loss: 1.800161, mean_absolute_error: 0.642291, mean_q: 7.390762, mean_eps: 0.100000\n",
      " 117599/175000: episode: 3282, duration: 0.722s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.244 [1.000, 203.000], mean observation: 0.445 [0.000, 82.000], loss: 0.478640, mean_absolute_error: 0.666420, mean_q: 7.432093, mean_eps: 0.100000\n",
      " 117657/175000: episode: 3283, duration: 1.067s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 102.224 [28.000, 203.000], mean observation: 0.805 [0.000, 116.000], loss: 11.946080, mean_absolute_error: 0.618706, mean_q: 6.031122, mean_eps: 0.100000\n",
      " 117688/175000: episode: 3284, duration: 0.592s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 66.839 [9.000, 170.000], mean observation: 0.325 [0.000, 62.000], loss: 172.425897, mean_absolute_error: 1.539059, mean_q: 8.259335, mean_eps: 0.100000\n",
      " 117721/175000: episode: 3285, duration: 0.653s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 103.455 [7.000, 222.000], mean observation: 0.213 [0.000, 66.000], loss: 0.154990, mean_absolute_error: 0.591803, mean_q: 6.428162, mean_eps: 0.100000\n",
      " 117763/175000: episode: 3286, duration: 0.785s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.857 [28.000, 222.000], mean observation: 0.203 [0.000, 84.000], loss: 3.002248, mean_absolute_error: 0.584146, mean_q: 6.268770, mean_eps: 0.100000\n",
      " 117800/175000: episode: 3287, duration: 0.735s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 58.946 [9.000, 158.000], mean observation: 0.207 [0.000, 74.000], loss: 0.180342, mean_absolute_error: 0.547890, mean_q: 5.948199, mean_eps: 0.100000\n",
      " 117833/175000: episode: 3288, duration: 0.621s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.273 [1.000, 158.000], mean observation: 0.283 [0.000, 66.000], loss: 0.180754, mean_absolute_error: 0.557039, mean_q: 5.945328, mean_eps: 0.100000\n",
      " 117880/175000: episode: 3289, duration: 0.906s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.681 [1.000, 221.000], mean observation: 0.461 [0.000, 94.000], loss: 0.154909, mean_absolute_error: 0.552238, mean_q: 6.033552, mean_eps: 0.100000\n",
      " 117907/175000: episode: 3290, duration: 0.525s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 7.852 [1.000, 149.000], mean observation: 0.104 [0.000, 54.000], loss: 52.932893, mean_absolute_error: 0.989749, mean_q: 8.444560, mean_eps: 0.100000\n",
      " 117940/175000: episode: 3291, duration: 0.637s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.879 [1.000, 212.000], mean observation: 0.154 [0.000, 66.000], loss: 29.468166, mean_absolute_error: 0.880923, mean_q: 8.246283, mean_eps: 0.100000\n",
      " 117967/175000: episode: 3292, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.778 [27.000, 208.000], mean observation: 0.230 [0.000, 54.000], loss: 3.485041, mean_absolute_error: 0.767560, mean_q: 8.285068, mean_eps: 0.100000\n",
      " 117991/175000: episode: 3293, duration: 0.433s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 100.625 [11.000, 208.000], mean observation: 0.178 [0.000, 48.000], loss: 0.219541, mean_absolute_error: 0.532346, mean_q: 6.321199, mean_eps: 0.100000\n",
      " 118030/175000: episode: 3294, duration: 0.758s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.897 [38.000, 208.000], mean observation: 0.118 [0.000, 78.000], loss: 2.129223, mean_absolute_error: 0.822221, mean_q: 9.197016, mean_eps: 0.100000\n",
      " 118065/175000: episode: 3295, duration: 0.642s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.743 [15.000, 210.000], mean observation: 0.124 [0.000, 70.000], loss: 1.987783, mean_absolute_error: 0.631946, mean_q: 7.174042, mean_eps: 0.100000\n",
      " 118112/175000: episode: 3296, duration: 0.946s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.957 [1.000, 216.000], mean observation: 0.304 [0.000, 94.000], loss: 0.150119, mean_absolute_error: 0.513202, mean_q: 6.444194, mean_eps: 0.100000\n",
      " 118153/175000: episode: 3297, duration: 0.752s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 27.439 [1.000, 207.000], mean observation: 0.118 [0.000, 82.000], loss: 0.115965, mean_absolute_error: 0.489628, mean_q: 6.408526, mean_eps: 0.100000\n",
      " 118190/175000: episode: 3298, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 62.919 [1.000, 210.000], mean observation: 0.275 [0.000, 74.000], loss: 0.124674, mean_absolute_error: 0.447480, mean_q: 6.164605, mean_eps: 0.100000\n",
      " 118240/175000: episode: 3299, duration: 0.924s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 56.420 [1.000, 224.000], mean observation: 0.456 [0.000, 100.000], loss: 258.970974, mean_absolute_error: 1.681504, mean_q: 7.053914, mean_eps: 0.100000\n",
      " 118278/175000: episode: 3300, duration: 0.718s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 130.921 [24.000, 214.000], mean observation: 0.536 [0.000, 76.000], loss: 1.082704, mean_absolute_error: 0.448212, mean_q: 6.349963, mean_eps: 0.100000\n",
      " 118305/175000: episode: 3301, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.064 [0.000, 54.000], loss: 1.131424, mean_absolute_error: 0.413571, mean_q: 5.890719, mean_eps: 0.100000\n",
      " 118343/175000: episode: 3302, duration: 0.667s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 28.211 [1.000, 201.000], mean observation: 0.144 [0.000, 76.000], loss: 318.810911, mean_absolute_error: 1.879234, mean_q: 6.384884, mean_eps: 0.100000\n",
      " 118381/175000: episode: 3303, duration: 0.743s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.763 [1.000, 195.000], mean observation: 0.287 [0.000, 76.000], loss: 6.619569, mean_absolute_error: 0.477303, mean_q: 6.397955, mean_eps: 0.100000\n",
      " 118405/175000: episode: 3304, duration: 0.452s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 75.375 [30.000, 113.000], mean observation: 0.083 [0.000, 48.000], loss: 21.179245, mean_absolute_error: 0.615877, mean_q: 6.707220, mean_eps: 0.100000\n",
      " 118439/175000: episode: 3305, duration: 0.617s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.000 [63.000, 224.000], mean observation: 0.148 [0.000, 68.000], loss: 8.495500, mean_absolute_error: 0.607012, mean_q: 6.442679, mean_eps: 0.100000\n",
      " 118468/175000: episode: 3306, duration: 0.591s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 106.552 [39.000, 191.000], mean observation: 0.196 [0.000, 58.000], loss: 2.204389, mean_absolute_error: 0.634708, mean_q: 6.306974, mean_eps: 0.100000\n",
      " 118506/175000: episode: 3307, duration: 0.731s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.474 [27.000, 191.000], mean observation: 0.348 [0.000, 76.000], loss: 668.859998, mean_absolute_error: 3.752916, mean_q: 7.154589, mean_eps: 0.100000\n",
      " 118538/175000: episode: 3308, duration: 0.574s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 90.281 [27.000, 211.000], mean observation: 0.350 [0.000, 64.000], loss: 5.985994, mean_absolute_error: 0.763126, mean_q: 6.224617, mean_eps: 0.100000\n",
      " 118572/175000: episode: 3309, duration: 0.674s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 81.588 [8.000, 188.000], mean observation: 0.246 [0.000, 68.000], loss: 10.627077, mean_absolute_error: 0.797640, mean_q: 6.153581, mean_eps: 0.100000\n",
      " 118605/175000: episode: 3310, duration: 0.668s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 86.030 [27.000, 188.000], mean observation: 0.249 [0.000, 66.000], loss: 3.570109, mean_absolute_error: 0.775424, mean_q: 5.816463, mean_eps: 0.100000\n",
      " 118638/175000: episode: 3311, duration: 0.567s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.576 [29.000, 216.000], mean observation: 0.185 [0.000, 66.000], loss: 1.265554, mean_absolute_error: 0.725357, mean_q: 5.394679, mean_eps: 0.100000\n",
      " 118663/175000: episode: 3312, duration: 0.460s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 109.000 [34.000, 215.000], mean observation: 0.200 [0.000, 50.000], loss: 1.221806, mean_absolute_error: 0.779014, mean_q: 5.691370, mean_eps: 0.100000\n",
      " 118683/175000: episode: 3313, duration: 0.369s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 71.850 [1.000, 166.000], mean observation: 0.147 [0.000, 40.000], loss: 3.006384, mean_absolute_error: 0.776822, mean_q: 5.549116, mean_eps: 0.100000\n",
      " 118726/175000: episode: 3314, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 50.023 [1.000, 166.000], mean observation: 0.370 [0.000, 86.000], loss: 2.006672, mean_absolute_error: 0.805528, mean_q: 5.482223, mean_eps: 0.100000\n",
      " 118755/175000: episode: 3315, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 63.138 [1.000, 222.000], mean observation: 0.188 [0.000, 58.000], loss: 1.428780, mean_absolute_error: 0.808000, mean_q: 5.420827, mean_eps: 0.100000\n",
      " 118777/175000: episode: 3316, duration: 0.458s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 75.773 [1.000, 181.000], mean observation: 0.142 [0.000, 44.000], loss: 1.057858, mean_absolute_error: 0.745064, mean_q: 5.125698, mean_eps: 0.100000\n",
      " 118817/175000: episode: 3317, duration: 0.792s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 92.200 [3.000, 218.000], mean observation: 0.469 [0.000, 80.000], loss: 0.940554, mean_absolute_error: 0.755544, mean_q: 5.187999, mean_eps: 0.100000\n",
      " 118857/175000: episode: 3318, duration: 0.748s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 72.400 [7.000, 200.000], mean observation: 0.297 [0.000, 80.000], loss: 67.840529, mean_absolute_error: 1.176318, mean_q: 6.414129, mean_eps: 0.100000\n",
      " 118895/175000: episode: 3319, duration: 0.650s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.395 [7.000, 170.000], mean observation: 0.255 [0.000, 76.000], loss: 0.644831, mean_absolute_error: 0.745032, mean_q: 4.940937, mean_eps: 0.100000\n",
      " 118932/175000: episode: 3320, duration: 0.713s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 73.541 [3.000, 170.000], mean observation: 0.254 [0.000, 74.000], loss: 0.417646, mean_absolute_error: 0.734489, mean_q: 4.696008, mean_eps: 0.100000\n",
      " 118977/175000: episode: 3321, duration: 0.867s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 79.200 [7.000, 183.000], mean observation: 0.324 [0.000, 90.000], loss: 0.665307, mean_absolute_error: 0.710229, mean_q: 4.646764, mean_eps: 0.100000\n",
      " 119011/175000: episode: 3322, duration: 0.620s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.676 [32.000, 220.000], mean observation: 0.202 [0.000, 68.000], loss: 22.134166, mean_absolute_error: 0.778510, mean_q: 4.375026, mean_eps: 0.100000\n",
      " 119050/175000: episode: 3323, duration: 0.738s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.385 [1.000, 191.000], mean observation: 0.487 [0.000, 78.000], loss: 68.140804, mean_absolute_error: 0.971000, mean_q: 4.428814, mean_eps: 0.100000\n",
      " 119094/175000: episode: 3324, duration: 0.821s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.545 [6.000, 189.000], mean observation: 0.226 [0.000, 88.000], loss: 0.554244, mean_absolute_error: 0.659920, mean_q: 4.386712, mean_eps: 0.100000\n",
      " 119121/175000: episode: 3325, duration: 0.526s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 100.741 [7.000, 209.000], mean observation: 0.173 [0.000, 54.000], loss: 0.564671, mean_absolute_error: 0.654452, mean_q: 4.312640, mean_eps: 0.100000\n",
      " 119148/175000: episode: 3326, duration: 0.591s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 104.481 [7.000, 209.000], mean observation: 0.248 [0.000, 54.000], loss: 26.509436, mean_absolute_error: 0.766703, mean_q: 4.657492, mean_eps: 0.100000\n",
      " 119173/175000: episode: 3327, duration: 0.519s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.080 [17.000, 223.000], mean observation: 0.145 [0.000, 50.000], loss: 0.921607, mean_absolute_error: 0.653938, mean_q: 4.565216, mean_eps: 0.100000\n",
      " 119219/175000: episode: 3328, duration: 0.773s, episode steps: 46, steps per second: 60, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.957 [0.000, 223.000], mean observation: 0.523 [0.000, 92.000], loss: 0.883459, mean_absolute_error: 0.649495, mean_q: 4.538515, mean_eps: 0.100000\n",
      " 119238/175000: episode: 3329, duration: 0.431s, episode steps: 19, steps per second: 44, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 111.053 [46.000, 170.000], mean observation: 0.074 [0.000, 38.000], loss: 1365.418111, mean_absolute_error: 7.013194, mean_q: 7.352099, mean_eps: 0.100000\n",
      " 119264/175000: episode: 3330, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.615 [7.000, 170.000], mean observation: 0.155 [0.000, 52.000], loss: 0.707438, mean_absolute_error: 0.657192, mean_q: 4.343320, mean_eps: 0.100000\n",
      " 119280/175000: episode: 3331, duration: 0.393s, episode steps: 16, steps per second: 41, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 112.438 [46.000, 208.000], mean observation: 0.090 [0.000, 32.000], loss: 0.496455, mean_absolute_error: 0.655836, mean_q: 4.390616, mean_eps: 0.100000\n",
      " 119313/175000: episode: 3332, duration: 0.657s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 108.242 [9.000, 191.000], mean observation: 0.183 [0.000, 66.000], loss: 248.924780, mean_absolute_error: 1.897555, mean_q: 5.788568, mean_eps: 0.100000\n",
      " 119344/175000: episode: 3333, duration: 0.574s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 107.581 [8.000, 218.000], mean observation: 0.219 [0.000, 62.000], loss: 0.524322, mean_absolute_error: 0.654648, mean_q: 4.282407, mean_eps: 0.100000\n",
      " 119403/175000: episode: 3334, duration: 1.148s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 84.542 [8.000, 223.000], mean observation: 0.738 [0.000, 118.000], loss: 14.646413, mean_absolute_error: 0.695605, mean_q: 4.229749, mean_eps: 0.100000\n",
      " 119438/175000: episode: 3335, duration: 0.662s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.829 [8.000, 213.000], mean observation: 0.271 [0.000, 70.000], loss: 0.187084, mean_absolute_error: 0.621680, mean_q: 3.929694, mean_eps: 0.100000\n",
      " 119477/175000: episode: 3336, duration: 0.718s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.846 [8.000, 191.000], mean observation: 0.312 [0.000, 78.000], loss: 3.691571, mean_absolute_error: 0.646002, mean_q: 3.842233, mean_eps: 0.100000\n",
      " 119513/175000: episode: 3337, duration: 0.625s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 102.583 [9.000, 222.000], mean observation: 0.215 [0.000, 72.000], loss: 1.289769, mean_absolute_error: 0.761848, mean_q: 5.513056, mean_eps: 0.100000\n",
      " 119520/175000: episode: 3338, duration: 0.137s, episode steps: 7, steps per second: 51, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 132.143 [49.000, 146.000], mean observation: 0.027 [0.000, 14.000], loss: 0.081239, mean_absolute_error: 0.587342, mean_q: 3.828480, mean_eps: 0.100000\n",
      " 119538/175000: episode: 3339, duration: 0.389s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 129.611 [38.000, 222.000], mean observation: 0.067 [0.000, 36.000], loss: 7.572870, mean_absolute_error: 0.624519, mean_q: 4.046635, mean_eps: 0.100000\n",
      " 119567/175000: episode: 3340, duration: 0.528s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.207 [38.000, 222.000], mean observation: 0.166 [0.000, 58.000], loss: 1.288277, mean_absolute_error: 0.592216, mean_q: 4.092980, mean_eps: 0.100000\n",
      " 119622/175000: episode: 3341, duration: 1.057s, episode steps: 55, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 101.836 [6.000, 222.000], mean observation: 0.546 [0.000, 110.000], loss: 0.416693, mean_absolute_error: 0.594023, mean_q: 4.051154, mean_eps: 0.100000\n",
      " 119652/175000: episode: 3342, duration: 0.569s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 119.000 [1.000, 222.000], mean observation: 0.326 [0.000, 60.000], loss: 0.172841, mean_absolute_error: 0.571312, mean_q: 3.953138, mean_eps: 0.100000\n",
      " 119686/175000: episode: 3343, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.471 [9.000, 184.000], mean observation: 0.214 [0.000, 68.000], loss: 0.297501, mean_absolute_error: 0.573151, mean_q: 4.085296, mean_eps: 0.100000\n",
      " 119723/175000: episode: 3344, duration: 0.669s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 101.892 [23.000, 220.000], mean observation: 0.256 [0.000, 74.000], loss: 1.045157, mean_absolute_error: 0.558823, mean_q: 4.186719, mean_eps: 0.100000\n",
      " 119764/175000: episode: 3345, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.122 [38.000, 183.000], mean observation: 0.224 [0.000, 82.000], loss: 0.182096, mean_absolute_error: 0.548789, mean_q: 4.189117, mean_eps: 0.100000\n",
      " 119816/175000: episode: 3346, duration: 1.024s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.000 [38.000, 183.000], mean observation: 0.492 [0.000, 104.000], loss: 0.792038, mean_absolute_error: 0.547980, mean_q: 4.212227, mean_eps: 0.100000\n",
      " 119832/175000: episode: 3347, duration: 0.377s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 132.875 [36.000, 186.000], mean observation: 0.108 [0.000, 32.000], loss: 1467.675163, mean_absolute_error: 7.302837, mean_q: 6.746137, mean_eps: 0.100000\n",
      " 119857/175000: episode: 3348, duration: 0.501s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 146.520 [67.000, 208.000], mean observation: 0.079 [0.000, 50.000], loss: 18.850626, mean_absolute_error: 0.618406, mean_q: 4.428603, mean_eps: 0.100000\n",
      " 119877/175000: episode: 3349, duration: 0.360s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 113.550 [17.000, 221.000], mean observation: 0.159 [0.000, 40.000], loss: 3.789884, mean_absolute_error: 0.563490, mean_q: 4.559057, mean_eps: 0.100000\n",
      " 119912/175000: episode: 3350, duration: 0.634s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 133.114 [59.000, 178.000], mean observation: 0.241 [0.000, 70.000], loss: 171.177145, mean_absolute_error: 1.325556, mean_q: 4.719632, mean_eps: 0.100000\n",
      " 119939/175000: episode: 3351, duration: 0.514s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 93.185 [59.000, 178.000], mean observation: 0.262 [0.000, 54.000], loss: 6.273500, mean_absolute_error: 0.606752, mean_q: 4.685559, mean_eps: 0.100000\n",
      " 119963/175000: episode: 3352, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 155.083 [43.000, 178.000], mean observation: 0.119 [0.000, 48.000], loss: 9.281392, mean_absolute_error: 0.623442, mean_q: 4.647483, mean_eps: 0.100000\n",
      " 120002/175000: episode: 3353, duration: 0.764s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 67.103 [28.000, 195.000], mean observation: 0.405 [0.000, 78.000], loss: 11.120242, mean_absolute_error: 0.652312, mean_q: 4.683188, mean_eps: 0.100000\n",
      " 120040/175000: episode: 3354, duration: 0.743s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.158 [28.000, 216.000], mean observation: 0.314 [0.000, 76.000], loss: 7.058369, mean_absolute_error: 0.660669, mean_q: 4.957942, mean_eps: 0.100000\n",
      " 120066/175000: episode: 3355, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 69.538 [28.000, 216.000], mean observation: 0.251 [0.000, 52.000], loss: 1217.298580, mean_absolute_error: 6.253443, mean_q: 7.046480, mean_eps: 0.100000\n",
      " 120077/175000: episode: 3356, duration: 0.216s, episode steps: 11, steps per second: 51, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 112.818 [10.000, 216.000], mean observation: 0.074 [0.000, 22.000], loss: 5.262609, mean_absolute_error: 0.669109, mean_q: 5.307583, mean_eps: 0.100000\n",
      " 120141/175000: episode: 3357, duration: 1.158s, episode steps: 64, steps per second: 55, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 112.562 [7.000, 216.000], mean observation: 0.856 [0.000, 128.000], loss: 1.666475, mean_absolute_error: 0.619762, mean_q: 4.869968, mean_eps: 0.100000\n",
      " 120193/175000: episode: 3358, duration: 0.994s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.038 [16.000, 216.000], mean observation: 0.289 [0.000, 104.000], loss: 1.223894, mean_absolute_error: 0.605994, mean_q: 4.716243, mean_eps: 0.100000\n",
      " 120230/175000: episode: 3359, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.757 [28.000, 216.000], mean observation: 0.253 [0.000, 74.000], loss: 1.239017, mean_absolute_error: 0.605775, mean_q: 4.641448, mean_eps: 0.100000\n",
      " 120275/175000: episode: 3360, duration: 0.804s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 127.933 [9.000, 216.000], mean observation: 0.329 [0.000, 90.000], loss: 0.872403, mean_absolute_error: 0.651369, mean_q: 4.925944, mean_eps: 0.100000\n",
      " 120302/175000: episode: 3361, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 199.407 [150.000, 223.000], mean observation: 0.145 [0.000, 54.000], loss: 0.814723, mean_absolute_error: 0.618589, mean_q: 4.878822, mean_eps: 0.100000\n",
      " 120331/175000: episode: 3362, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 191.448 [91.000, 223.000], mean observation: 0.165 [0.000, 58.000], loss: 0.538121, mean_absolute_error: 0.581327, mean_q: 4.915948, mean_eps: 0.100000\n",
      " 120366/175000: episode: 3363, duration: 0.673s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 162.371 [35.000, 221.000], mean observation: 0.258 [0.000, 70.000], loss: 1.579336, mean_absolute_error: 0.572063, mean_q: 4.892103, mean_eps: 0.100000\n",
      " 120419/175000: episode: 3364, duration: 1.002s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 92.283 [5.000, 204.000], mean observation: 0.572 [0.000, 106.000], loss: 1.326077, mean_absolute_error: 0.561832, mean_q: 4.876759, mean_eps: 0.100000\n",
      " 120469/175000: episode: 3365, duration: 0.925s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.960 [5.000, 207.000], mean observation: 0.550 [0.000, 100.000], loss: 281.539609, mean_absolute_error: 1.909384, mean_q: 5.838466, mean_eps: 0.100000\n",
      " 120511/175000: episode: 3366, duration: 0.724s, episode steps: 42, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.190 [7.000, 207.000], mean observation: 0.296 [0.000, 84.000], loss: 0.783449, mean_absolute_error: 0.545644, mean_q: 4.368340, mean_eps: 0.100000\n",
      " 120552/175000: episode: 3367, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 103.756 [9.000, 216.000], mean observation: 0.435 [0.000, 82.000], loss: 0.333644, mean_absolute_error: 0.531870, mean_q: 4.432915, mean_eps: 0.100000\n",
      " 120581/175000: episode: 3368, duration: 0.731s, episode steps: 29, steps per second: 40, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 159.621 [25.000, 216.000], mean observation: 0.159 [0.000, 58.000], loss: 0.255675, mean_absolute_error: 0.515088, mean_q: 4.337353, mean_eps: 0.100000\n",
      " 120608/175000: episode: 3369, duration: 0.675s, episode steps: 27, steps per second: 40, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 63.519 [9.000, 216.000], mean observation: 0.169 [0.000, 54.000], loss: 0.196620, mean_absolute_error: 0.514767, mean_q: 4.424800, mean_eps: 0.100000\n",
      " 120657/175000: episode: 3370, duration: 1.045s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.714 [9.000, 207.000], mean observation: 0.481 [0.000, 98.000], loss: 25.584452, mean_absolute_error: 0.630358, mean_q: 4.605425, mean_eps: 0.100000\n",
      " 120703/175000: episode: 3371, duration: 0.846s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 53.804 [16.000, 209.000], mean observation: 0.263 [0.000, 92.000], loss: 37.429541, mean_absolute_error: 0.815720, mean_q: 6.023541, mean_eps: 0.100000\n",
      " 120747/175000: episode: 3372, duration: 0.811s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 149.432 [16.000, 208.000], mean observation: 0.277 [0.000, 88.000], loss: 22.053519, mean_absolute_error: 0.758543, mean_q: 5.786524, mean_eps: 0.100000\n",
      " 120785/175000: episode: 3373, duration: 0.742s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 146.237 [43.000, 221.000], mean observation: 0.275 [0.000, 76.000], loss: 0.690902, mean_absolute_error: 0.555358, mean_q: 4.618719, mean_eps: 0.100000\n",
      " 120810/175000: episode: 3374, duration: 0.442s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 127.680 [1.000, 219.000], mean observation: 0.139 [0.000, 50.000], loss: 307.988682, mean_absolute_error: 2.164950, mean_q: 7.425714, mean_eps: 0.100000\n",
      " 120833/175000: episode: 3375, duration: 0.446s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.826 [1.000, 181.000], mean observation: 0.110 [0.000, 46.000], loss: 0.387820, mean_absolute_error: 0.552445, mean_q: 4.866091, mean_eps: 0.100000\n",
      " 120884/175000: episode: 3376, duration: 1.042s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 94.490 [1.000, 196.000], mean observation: 0.430 [0.000, 102.000], loss: 0.663830, mean_absolute_error: 0.554393, mean_q: 4.500808, mean_eps: 0.100000\n",
      " 120910/175000: episode: 3377, duration: 0.519s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.000 [57.000, 214.000], mean observation: 0.114 [0.000, 52.000], loss: 0.509841, mean_absolute_error: 0.552476, mean_q: 4.495105, mean_eps: 0.100000\n",
      " 120935/175000: episode: 3378, duration: 0.443s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 96.240 [72.000, 194.000], mean observation: 0.103 [0.000, 50.000], loss: 0.478061, mean_absolute_error: 0.534092, mean_q: 4.114309, mean_eps: 0.100000\n",
      " 120979/175000: episode: 3379, duration: 0.844s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.955 [10.000, 218.000], mean observation: 0.301 [0.000, 88.000], loss: 0.367633, mean_absolute_error: 0.533604, mean_q: 3.947500, mean_eps: 0.100000\n",
      " 121007/175000: episode: 3380, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 102.321 [20.000, 215.000], mean observation: 0.152 [0.000, 56.000], loss: 183.073738, mean_absolute_error: 1.586060, mean_q: 6.325318, mean_eps: 0.100000\n",
      " 121047/175000: episode: 3381, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 77.475 [15.000, 224.000], mean observation: 0.430 [0.000, 80.000], loss: 0.335016, mean_absolute_error: 0.557428, mean_q: 3.983498, mean_eps: 0.100000\n",
      " 121079/175000: episode: 3382, duration: 0.598s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 134.531 [26.000, 215.000], mean observation: 0.298 [0.000, 64.000], loss: 0.379691, mean_absolute_error: 0.550011, mean_q: 3.957925, mean_eps: 0.100000\n",
      " 121091/175000: episode: 3383, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 92.083 [1.000, 194.000], mean observation: 0.053 [0.000, 24.000], loss: 31.255966, mean_absolute_error: 1.184523, mean_q: 9.260473, mean_eps: 0.100000\n",
      " 121141/175000: episode: 3384, duration: 1.026s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.420 [1.000, 215.000], mean observation: 0.259 [0.000, 100.000], loss: 0.349151, mean_absolute_error: 0.547503, mean_q: 4.326438, mean_eps: 0.100000\n",
      " 121173/175000: episode: 3385, duration: 0.654s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 130.938 [14.000, 215.000], mean observation: 0.195 [0.000, 64.000], loss: 0.277659, mean_absolute_error: 0.522265, mean_q: 4.365517, mean_eps: 0.100000\n",
      " 121223/175000: episode: 3386, duration: 0.974s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.060 [7.000, 216.000], mean observation: 0.315 [0.000, 100.000], loss: 102.905209, mean_absolute_error: 1.088085, mean_q: 5.641270, mean_eps: 0.100000\n",
      " 121268/175000: episode: 3387, duration: 0.883s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.178 [6.000, 195.000], mean observation: 0.363 [0.000, 90.000], loss: 0.261367, mean_absolute_error: 0.498249, mean_q: 4.162871, mean_eps: 0.100000\n",
      " 121304/175000: episode: 3388, duration: 0.842s, episode steps: 36, steps per second: 43, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 129.833 [7.000, 216.000], mean observation: 0.219 [0.000, 72.000], loss: 0.281131, mean_absolute_error: 0.498024, mean_q: 4.031725, mean_eps: 0.100000\n",
      " 121342/175000: episode: 3389, duration: 0.816s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.211 [18.000, 191.000], mean observation: 0.318 [0.000, 76.000], loss: 26.063917, mean_absolute_error: 0.780854, mean_q: 6.041895, mean_eps: 0.100000\n",
      " 121366/175000: episode: 3390, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.625 [27.000, 218.000], mean observation: 0.239 [0.000, 48.000], loss: 0.309419, mean_absolute_error: 0.504633, mean_q: 4.396437, mean_eps: 0.100000\n",
      " 121393/175000: episode: 3391, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 98.185 [15.000, 211.000], mean observation: 0.274 [0.000, 54.000], loss: 0.554063, mean_absolute_error: 0.499499, mean_q: 4.136162, mean_eps: 0.100000\n",
      " 121437/175000: episode: 3392, duration: 0.807s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.932 [38.000, 218.000], mean observation: 0.396 [0.000, 88.000], loss: 0.769732, mean_absolute_error: 0.504076, mean_q: 4.064246, mean_eps: 0.100000\n",
      " 121458/175000: episode: 3393, duration: 0.380s, episode steps: 21, steps per second: 55, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.381 [38.000, 218.000], mean observation: 0.112 [0.000, 42.000], loss: 0.496736, mean_absolute_error: 0.502375, mean_q: 3.796411, mean_eps: 0.100000\n",
      " 121503/175000: episode: 3394, duration: 0.848s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.356 [1.000, 218.000], mean observation: 0.573 [0.000, 90.000], loss: 0.382724, mean_absolute_error: 0.516674, mean_q: 3.748082, mean_eps: 0.100000\n",
      " 121553/175000: episode: 3395, duration: 0.976s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.240 [1.000, 218.000], mean observation: 0.643 [0.000, 100.000], loss: 1.230132, mean_absolute_error: 0.532050, mean_q: 3.880037, mean_eps: 0.100000\n",
      " 121599/175000: episode: 3396, duration: 0.841s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 177.783 [40.000, 219.000], mean observation: 0.472 [0.000, 92.000], loss: 35.329889, mean_absolute_error: 0.834538, mean_q: 5.401915, mean_eps: 0.100000\n",
      " 121663/175000: episode: 3397, duration: 1.206s, episode steps: 64, steps per second: 53, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 158.328 [40.000, 218.000], mean observation: 0.505 [0.000, 128.000], loss: 0.586394, mean_absolute_error: 0.665508, mean_q: 5.159316, mean_eps: 0.100000\n",
      " 121689/175000: episode: 3398, duration: 0.498s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 162.769 [67.000, 215.000], mean observation: 0.102 [0.000, 52.000], loss: 0.703652, mean_absolute_error: 0.531384, mean_q: 3.598827, mean_eps: 0.100000\n",
      " 121730/175000: episode: 3399, duration: 0.772s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.146 [38.000, 220.000], mean observation: 0.323 [0.000, 82.000], loss: 1.743641, mean_absolute_error: 0.667134, mean_q: 4.982510, mean_eps: 0.100000\n",
      " 121776/175000: episode: 3400, duration: 0.907s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.000 [7.000, 176.000], mean observation: 0.666 [0.000, 92.000], loss: 351.889083, mean_absolute_error: 2.240697, mean_q: 5.010626, mean_eps: 0.100000\n",
      " 121816/175000: episode: 3401, duration: 0.804s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 81.325 [20.000, 202.000], mean observation: 0.351 [0.000, 80.000], loss: 0.501927, mean_absolute_error: 0.575844, mean_q: 3.627690, mean_eps: 0.100000\n",
      " 121858/175000: episode: 3402, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 86.452 [28.000, 191.000], mean observation: 0.428 [0.000, 84.000], loss: 0.355625, mean_absolute_error: 0.605393, mean_q: 3.689577, mean_eps: 0.100000\n",
      " 121887/175000: episode: 3403, duration: 0.524s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.931 [14.000, 209.000], mean observation: 0.122 [0.000, 58.000], loss: 0.276028, mean_absolute_error: 0.616214, mean_q: 3.643122, mean_eps: 0.100000\n",
      " 121897/175000: episode: 3404, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 121.500 [38.000, 158.000], mean observation: 0.052 [0.000, 20.000], loss: 0.297324, mean_absolute_error: 0.599257, mean_q: 3.438491, mean_eps: 0.100000\n",
      " 121939/175000: episode: 3405, duration: 0.814s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.571 [18.000, 202.000], mean observation: 0.306 [0.000, 84.000], loss: 302.863042, mean_absolute_error: 2.085366, mean_q: 5.076415, mean_eps: 0.100000\n",
      " 121969/175000: episode: 3406, duration: 0.633s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 113.533 [37.000, 202.000], mean observation: 0.210 [0.000, 60.000], loss: 23.853842, mean_absolute_error: 0.879965, mean_q: 5.533307, mean_eps: 0.100000\n",
      " 121994/175000: episode: 3407, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 99.760 [2.000, 148.000], mean observation: 0.172 [0.000, 50.000], loss: 0.349941, mean_absolute_error: 0.544550, mean_q: 3.475622, mean_eps: 0.100000\n",
      " 122015/175000: episode: 3408, duration: 0.384s, episode steps: 21, steps per second: 55, episode reward: 1.000, mean reward: 0.048 [0.000, 1.000], mean action: 90.238 [37.000, 148.000], mean observation: 0.168 [0.000, 41.000], loss: 0.287608, mean_absolute_error: 0.545773, mean_q: 3.631539, mean_eps: 0.100000\n",
      " 122069/175000: episode: 3409, duration: 1.089s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 111.037 [5.000, 222.000], mean observation: 0.552 [0.000, 108.000], loss: 473.639879, mean_absolute_error: 2.872057, mean_q: 6.167657, mean_eps: 0.100000\n",
      " 122082/175000: episode: 3410, duration: 0.246s, episode steps: 13, steps per second: 53, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 44.000 [44.000, 44.000], mean observation: 0.033 [0.000, 26.000], loss: 0.272011, mean_absolute_error: 0.541581, mean_q: 3.753347, mean_eps: 0.100000\n",
      " 122123/175000: episode: 3411, duration: 0.753s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.927 [4.000, 212.000], mean observation: 0.285 [0.000, 82.000], loss: 5.255069, mean_absolute_error: 0.553737, mean_q: 3.772392, mean_eps: 0.100000\n",
      " 122139/175000: episode: 3412, duration: 0.317s, episode steps: 16, steps per second: 50, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 75.500 [44.000, 202.000], mean observation: 0.070 [0.000, 32.000], loss: 0.244805, mean_absolute_error: 0.511313, mean_q: 3.440807, mean_eps: 0.100000\n",
      " 122191/175000: episode: 3413, duration: 0.946s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 46.192 [10.000, 102.000], mean observation: 0.210 [0.000, 104.000], loss: 0.310437, mean_absolute_error: 0.551229, mean_q: 4.021922, mean_eps: 0.100000\n",
      " 122231/175000: episode: 3414, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 51.475 [1.000, 183.000], mean observation: 0.231 [0.000, 80.000], loss: 178.552826, mean_absolute_error: 1.490854, mean_q: 5.616265, mean_eps: 0.100000\n",
      " 122268/175000: episode: 3415, duration: 0.848s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.405 [44.000, 209.000], mean observation: 0.242 [0.000, 74.000], loss: 0.451344, mean_absolute_error: 0.538416, mean_q: 3.863690, mean_eps: 0.100000\n",
      " 122314/175000: episode: 3416, duration: 0.862s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.152 [10.000, 207.000], mean observation: 0.567 [0.000, 92.000], loss: 0.464866, mean_absolute_error: 0.675502, mean_q: 5.246316, mean_eps: 0.100000\n",
      " 122354/175000: episode: 3417, duration: 0.731s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 68.950 [12.000, 199.000], mean observation: 0.435 [0.000, 80.000], loss: 0.254431, mean_absolute_error: 0.521964, mean_q: 3.381446, mean_eps: 0.100000\n",
      " 122381/175000: episode: 3418, duration: 0.545s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.815 [49.000, 167.000], mean observation: 0.164 [0.000, 54.000], loss: 0.226494, mean_absolute_error: 0.510536, mean_q: 3.351086, mean_eps: 0.100000\n",
      " 122408/175000: episode: 3419, duration: 0.500s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.556 [49.000, 219.000], mean observation: 0.191 [0.000, 54.000], loss: 4.542024, mean_absolute_error: 0.525942, mean_q: 3.372497, mean_eps: 0.100000\n",
      " 122426/175000: episode: 3420, duration: 0.359s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 140.778 [118.000, 201.000], mean observation: 0.089 [0.000, 36.000], loss: 1956.936299, mean_absolute_error: 9.531384, mean_q: 6.751471, mean_eps: 0.100000\n",
      " 122456/175000: episode: 3421, duration: 0.607s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.167 [1.000, 196.000], mean observation: 0.182 [0.000, 60.000], loss: 0.543831, mean_absolute_error: 0.501744, mean_q: 3.283683, mean_eps: 0.100000\n",
      " 122481/175000: episode: 3422, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 130.320 [16.000, 182.000], mean observation: 0.111 [0.000, 50.000], loss: 0.260932, mean_absolute_error: 0.506750, mean_q: 3.418292, mean_eps: 0.100000\n",
      " 122503/175000: episode: 3423, duration: 0.373s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 131.500 [1.000, 139.000], mean observation: 0.068 [0.000, 44.000], loss: 0.174882, mean_absolute_error: 0.519615, mean_q: 3.537548, mean_eps: 0.100000\n",
      " 122530/175000: episode: 3424, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 104.370 [71.000, 218.000], mean observation: 0.169 [0.000, 54.000], loss: 0.271575, mean_absolute_error: 0.515654, mean_q: 3.461151, mean_eps: 0.100000\n",
      " 122573/175000: episode: 3425, duration: 0.786s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 80.674 [14.000, 139.000], mean observation: 0.242 [0.000, 86.000], loss: 4.234349, mean_absolute_error: 0.543167, mean_q: 3.785309, mean_eps: 0.100000\n",
      " 122619/175000: episode: 3426, duration: 0.800s, episode steps: 46, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.500 [9.000, 214.000], mean observation: 0.362 [0.000, 92.000], loss: 0.614166, mean_absolute_error: 0.507745, mean_q: 3.556896, mean_eps: 0.100000\n",
      " 122671/175000: episode: 3427, duration: 0.942s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 81.442 [9.000, 223.000], mean observation: 0.454 [0.000, 104.000], loss: 0.607204, mean_absolute_error: 0.522005, mean_q: 3.668446, mean_eps: 0.100000\n",
      " 122684/175000: episode: 3428, duration: 0.287s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 109.385 [102.000, 150.000], mean observation: 0.034 [0.000, 26.000], loss: 0.656128, mean_absolute_error: 0.528587, mean_q: 3.666033, mean_eps: 0.100000\n",
      " 122702/175000: episode: 3429, duration: 0.353s, episode steps: 18, steps per second: 51, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 94.833 [15.000, 198.000], mean observation: 0.097 [0.000, 36.000], loss: 0.773628, mean_absolute_error: 0.535521, mean_q: 3.667515, mean_eps: 0.100000\n",
      " 122745/175000: episode: 3430, duration: 0.810s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.674 [15.000, 203.000], mean observation: 0.474 [0.000, 86.000], loss: 0.802035, mean_absolute_error: 0.516615, mean_q: 3.433314, mean_eps: 0.100000\n",
      " 122792/175000: episode: 3431, duration: 0.842s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 115.915 [15.000, 152.000], mean observation: 0.238 [0.000, 94.000], loss: 0.882471, mean_absolute_error: 0.544313, mean_q: 3.654680, mean_eps: 0.100000\n",
      " 122817/175000: episode: 3432, duration: 0.658s, episode steps: 25, steps per second: 38, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.480 [39.000, 132.000], mean observation: 0.102 [0.000, 50.000], loss: 0.806138, mean_absolute_error: 0.567162, mean_q: 3.815827, mean_eps: 0.100000\n",
      " 122848/175000: episode: 3433, duration: 0.584s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 90.710 [9.000, 193.000], mean observation: 0.215 [0.000, 62.000], loss: 0.874689, mean_absolute_error: 0.578293, mean_q: 3.836319, mean_eps: 0.100000\n",
      " 122880/175000: episode: 3434, duration: 0.630s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 88.062 [9.000, 186.000], mean observation: 0.243 [0.000, 64.000], loss: 0.567837, mean_absolute_error: 0.598772, mean_q: 3.725401, mean_eps: 0.100000\n",
      " 122917/175000: episode: 3435, duration: 0.718s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 143.757 [11.000, 224.000], mean observation: 0.405 [0.000, 74.000], loss: 1.000147, mean_absolute_error: 0.588948, mean_q: 4.096115, mean_eps: 0.100000\n",
      " 122960/175000: episode: 3436, duration: 0.791s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 137.977 [1.000, 224.000], mean observation: 0.529 [0.000, 86.000], loss: 2.566919, mean_absolute_error: 0.610327, mean_q: 4.814884, mean_eps: 0.100000\n",
      " 123015/175000: episode: 3437, duration: 1.037s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.909 [1.000, 221.000], mean observation: 0.458 [0.000, 110.000], loss: 0.344275, mean_absolute_error: 0.548613, mean_q: 4.286374, mean_eps: 0.100000\n",
      " 123048/175000: episode: 3438, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 138.000 [102.000, 150.000], mean observation: 0.114 [0.000, 66.000], loss: 0.808793, mean_absolute_error: 0.533508, mean_q: 4.239369, mean_eps: 0.100000\n",
      " 123063/175000: episode: 3439, duration: 0.307s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 124.467 [12.000, 163.000], mean observation: 0.057 [0.000, 30.000], loss: 512.786180, mean_absolute_error: 3.193041, mean_q: 8.230790, mean_eps: 0.100000\n",
      " 123084/175000: episode: 3440, duration: 0.418s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.524 [102.000, 193.000], mean observation: 0.073 [0.000, 42.000], loss: 0.764273, mean_absolute_error: 0.549772, mean_q: 4.358855, mean_eps: 0.100000\n",
      " 123112/175000: episode: 3441, duration: 0.590s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 100.679 [11.000, 220.000], mean observation: 0.238 [0.000, 56.000], loss: 0.959359, mean_absolute_error: 0.582995, mean_q: 4.561975, mean_eps: 0.100000\n",
      " 123141/175000: episode: 3442, duration: 0.585s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 137.069 [102.000, 163.000], mean observation: 0.100 [0.000, 58.000], loss: 0.781108, mean_absolute_error: 0.578412, mean_q: 4.589819, mean_eps: 0.100000\n",
      " 123159/175000: episode: 3443, duration: 0.313s, episode steps: 18, steps per second: 57, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 130.111 [71.000, 176.000], mean observation: 0.092 [0.000, 36.000], loss: 0.562436, mean_absolute_error: 0.570883, mean_q: 4.094010, mean_eps: 0.100000\n",
      " 123195/175000: episode: 3444, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [55.000, 176.000], mean observation: 0.172 [0.000, 72.000], loss: 0.508073, mean_absolute_error: 0.518154, mean_q: 3.630157, mean_eps: 0.100000\n",
      " 123233/175000: episode: 3445, duration: 0.740s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 109.868 [35.000, 176.000], mean observation: 0.238 [0.000, 76.000], loss: 259.565860, mean_absolute_error: 1.844827, mean_q: 5.694495, mean_eps: 0.100000\n",
      " 123267/175000: episode: 3446, duration: 0.595s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.500 [6.000, 219.000], mean observation: 0.198 [0.000, 68.000], loss: 340.503549, mean_absolute_error: 2.249907, mean_q: 6.088808, mean_eps: 0.100000\n",
      " 123300/175000: episode: 3447, duration: 0.663s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 108.636 [4.000, 193.000], mean observation: 0.258 [0.000, 66.000], loss: 0.942573, mean_absolute_error: 0.535203, mean_q: 3.965233, mean_eps: 0.100000\n",
      " 123324/175000: episode: 3448, duration: 0.557s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 97.875 [28.000, 176.000], mean observation: 0.122 [0.000, 48.000], loss: 0.842007, mean_absolute_error: 0.522353, mean_q: 3.929054, mean_eps: 0.100000\n",
      " 123356/175000: episode: 3449, duration: 0.626s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 97.344 [55.000, 176.000], mean observation: 0.272 [0.000, 64.000], loss: 0.156532, mean_absolute_error: 0.503876, mean_q: 3.841315, mean_eps: 0.100000\n",
      " 123365/175000: episode: 3450, duration: 0.201s, episode steps: 9, steps per second: 45, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 176.222 [125.000, 221.000], mean observation: 0.051 [0.000, 18.000], loss: 0.189286, mean_absolute_error: 0.496031, mean_q: 3.691969, mean_eps: 0.100000\n",
      " 123399/175000: episode: 3451, duration: 0.627s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.676 [11.000, 221.000], mean observation: 0.316 [0.000, 68.000], loss: 0.295536, mean_absolute_error: 0.515670, mean_q: 3.971659, mean_eps: 0.100000\n",
      " 123447/175000: episode: 3452, duration: 0.876s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 108.417 [55.000, 221.000], mean observation: 0.483 [0.000, 96.000], loss: 0.337824, mean_absolute_error: 0.512328, mean_q: 3.908031, mean_eps: 0.100000\n",
      " 123489/175000: episode: 3453, duration: 0.805s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 75.881 [38.000, 221.000], mean observation: 0.237 [0.000, 84.000], loss: 1.304110, mean_absolute_error: 0.520318, mean_q: 3.843514, mean_eps: 0.100000\n",
      " 123523/175000: episode: 3454, duration: 0.614s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.324 [44.000, 221.000], mean observation: 0.225 [0.000, 68.000], loss: 3.016282, mean_absolute_error: 0.521192, mean_q: 3.688642, mean_eps: 0.100000\n",
      " 123554/175000: episode: 3455, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.323 [41.000, 176.000], mean observation: 0.256 [0.000, 62.000], loss: 0.294524, mean_absolute_error: 0.522625, mean_q: 3.729949, mean_eps: 0.100000\n",
      " 123584/175000: episode: 3456, duration: 0.592s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 111.533 [55.000, 213.000], mean observation: 0.218 [0.000, 60.000], loss: 0.203975, mean_absolute_error: 0.522170, mean_q: 3.518092, mean_eps: 0.100000\n",
      " 123632/175000: episode: 3457, duration: 0.922s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.542 [33.000, 212.000], mean observation: 0.445 [0.000, 96.000], loss: 0.290365, mean_absolute_error: 0.527005, mean_q: 3.464804, mean_eps: 0.100000\n",
      " 123660/175000: episode: 3458, duration: 0.574s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.036 [55.000, 207.000], mean observation: 0.223 [0.000, 56.000], loss: 0.581194, mean_absolute_error: 0.520865, mean_q: 3.232617, mean_eps: 0.100000\n",
      " 123712/175000: episode: 3459, duration: 0.971s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.173 [43.000, 218.000], mean observation: 0.431 [0.000, 104.000], loss: 43.871644, mean_absolute_error: 0.839853, mean_q: 4.553685, mean_eps: 0.100000\n",
      " 123754/175000: episode: 3460, duration: 0.791s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.095 [44.000, 218.000], mean observation: 0.596 [0.000, 84.000], loss: 0.401482, mean_absolute_error: 0.667036, mean_q: 4.910468, mean_eps: 0.100000\n",
      " 123784/175000: episode: 3461, duration: 0.575s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 109.967 [60.000, 210.000], mean observation: 0.187 [0.000, 60.000], loss: 0.259096, mean_absolute_error: 0.513684, mean_q: 3.402478, mean_eps: 0.100000\n",
      " 123823/175000: episode: 3462, duration: 0.706s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.615 [19.000, 194.000], mean observation: 0.297 [0.000, 78.000], loss: 0.298919, mean_absolute_error: 0.520182, mean_q: 3.414245, mean_eps: 0.100000\n",
      " 123849/175000: episode: 3463, duration: 0.527s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 135.231 [43.000, 150.000], mean observation: 0.191 [0.000, 52.000], loss: 0.383243, mean_absolute_error: 0.513074, mean_q: 3.394184, mean_eps: 0.100000\n",
      " 123878/175000: episode: 3464, duration: 0.533s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 127.483 [15.000, 150.000], mean observation: 0.200 [0.000, 58.000], loss: 0.403783, mean_absolute_error: 0.509972, mean_q: 3.357273, mean_eps: 0.100000\n",
      " 123924/175000: episode: 3465, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 135.043 [4.000, 221.000], mean observation: 0.569 [0.000, 92.000], loss: 0.467334, mean_absolute_error: 0.510895, mean_q: 3.240386, mean_eps: 0.100000\n",
      " 123974/175000: episode: 3466, duration: 0.913s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.060 [17.000, 209.000], mean observation: 0.657 [0.000, 100.000], loss: 1.581067, mean_absolute_error: 0.510248, mean_q: 3.180776, mean_eps: 0.100000\n",
      " 124007/175000: episode: 3467, duration: 0.576s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 64.636 [28.000, 117.000], mean observation: 0.217 [0.000, 66.000], loss: 0.345675, mean_absolute_error: 0.517100, mean_q: 3.303645, mean_eps: 0.100000\n",
      " 124026/175000: episode: 3468, duration: 0.348s, episode steps: 19, steps per second: 55, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.263 [52.000, 192.000], mean observation: 0.160 [0.000, 38.000], loss: 0.241854, mean_absolute_error: 0.504516, mean_q: 3.355510, mean_eps: 0.100000\n",
      " 124073/175000: episode: 3469, duration: 0.879s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.979 [28.000, 191.000], mean observation: 0.221 [0.000, 94.000], loss: 5.018178, mean_absolute_error: 0.531086, mean_q: 3.423724, mean_eps: 0.100000\n",
      " 124121/175000: episode: 3470, duration: 0.849s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.208 [20.000, 222.000], mean observation: 0.339 [0.000, 96.000], loss: 0.793529, mean_absolute_error: 0.509378, mean_q: 3.404156, mean_eps: 0.100000\n",
      " 124153/175000: episode: 3471, duration: 0.583s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 85.000 [5.000, 102.000], mean observation: 0.192 [0.000, 64.000], loss: 340.890852, mean_absolute_error: 2.231280, mean_q: 5.511586, mean_eps: 0.100000\n",
      " 124189/175000: episode: 3472, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.861 [9.000, 209.000], mean observation: 0.280 [0.000, 72.000], loss: 0.476183, mean_absolute_error: 0.520922, mean_q: 3.448145, mean_eps: 0.100000\n",
      " 124212/175000: episode: 3473, duration: 0.432s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 145.957 [72.000, 203.000], mean observation: 0.137 [0.000, 46.000], loss: 0.634003, mean_absolute_error: 0.525503, mean_q: 3.379005, mean_eps: 0.100000\n",
      " 124249/175000: episode: 3474, duration: 0.746s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.243 [56.000, 223.000], mean observation: 0.344 [0.000, 74.000], loss: 108.287747, mean_absolute_error: 1.162360, mean_q: 5.027713, mean_eps: 0.100000\n",
      " 124284/175000: episode: 3475, duration: 0.719s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.714 [56.000, 203.000], mean observation: 0.250 [0.000, 70.000], loss: 0.336496, mean_absolute_error: 0.511529, mean_q: 3.328397, mean_eps: 0.100000\n",
      " 124333/175000: episode: 3476, duration: 1.053s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 120.000 [5.000, 209.000], mean observation: 0.329 [0.000, 98.000], loss: 103.569509, mean_absolute_error: 1.095771, mean_q: 4.674199, mean_eps: 0.100000\n",
      " 124386/175000: episode: 3477, duration: 0.916s, episode steps: 53, steps per second: 58, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.717 [7.000, 209.000], mean observation: 0.556 [0.000, 106.000], loss: 0.201893, mean_absolute_error: 0.514569, mean_q: 3.419150, mean_eps: 0.100000\n",
      " 124425/175000: episode: 3478, duration: 0.697s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.256 [3.000, 206.000], mean observation: 0.185 [0.000, 78.000], loss: 0.272628, mean_absolute_error: 0.515232, mean_q: 3.308407, mean_eps: 0.100000\n",
      " 124441/175000: episode: 3479, duration: 0.291s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 59.938 [5.000, 201.000], mean observation: 0.076 [0.000, 32.000], loss: 0.558480, mean_absolute_error: 0.897387, mean_q: 7.201980, mean_eps: 0.100000\n",
      " 124491/175000: episode: 3480, duration: 0.988s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.680 [44.000, 208.000], mean observation: 0.246 [0.000, 100.000], loss: 0.201773, mean_absolute_error: 0.516726, mean_q: 3.275982, mean_eps: 0.100000\n",
      " 124516/175000: episode: 3481, duration: 0.498s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 49.000 [44.000, 134.000], mean observation: 0.083 [0.000, 50.000], loss: 0.263112, mean_absolute_error: 0.525839, mean_q: 3.295943, mean_eps: 0.100000\n",
      " 124547/175000: episode: 3482, duration: 0.632s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.645 [35.000, 215.000], mean observation: 0.285 [0.000, 62.000], loss: 0.480741, mean_absolute_error: 0.530807, mean_q: 3.417156, mean_eps: 0.100000\n",
      " 124579/175000: episode: 3483, duration: 0.570s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.375 [29.000, 215.000], mean observation: 0.280 [0.000, 64.000], loss: 9.432618, mean_absolute_error: 0.767337, mean_q: 5.577066, mean_eps: 0.100000\n",
      " 124598/175000: episode: 3484, duration: 0.354s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.316 [38.000, 185.000], mean observation: 0.167 [0.000, 38.000], loss: 0.232838, mean_absolute_error: 0.501874, mean_q: 3.230895, mean_eps: 0.100000\n",
      " 124639/175000: episode: 3485, duration: 0.785s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 75.805 [16.000, 176.000], mean observation: 0.497 [0.000, 82.000], loss: 0.485107, mean_absolute_error: 0.506698, mean_q: 3.251881, mean_eps: 0.100000\n",
      " 124688/175000: episode: 3486, duration: 0.919s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.184 [2.000, 221.000], mean observation: 0.419 [0.000, 98.000], loss: 0.322705, mean_absolute_error: 0.513788, mean_q: 3.359440, mean_eps: 0.100000\n",
      " 124726/175000: episode: 3487, duration: 0.808s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.921 [38.000, 222.000], mean observation: 0.197 [0.000, 76.000], loss: 0.537327, mean_absolute_error: 0.507491, mean_q: 3.324096, mean_eps: 0.100000\n",
      " 124766/175000: episode: 3488, duration: 0.745s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 74.875 [33.000, 172.000], mean observation: 0.240 [0.000, 80.000], loss: 0.481970, mean_absolute_error: 0.504173, mean_q: 3.234693, mean_eps: 0.100000\n",
      " 124802/175000: episode: 3489, duration: 0.643s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 122.694 [15.000, 172.000], mean observation: 0.307 [0.000, 72.000], loss: 1.533693, mean_absolute_error: 0.690328, mean_q: 5.032971, mean_eps: 0.100000\n",
      " 124832/175000: episode: 3490, duration: 0.577s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 125.267 [11.000, 191.000], mean observation: 0.233 [0.000, 60.000], loss: 0.841512, mean_absolute_error: 0.510221, mean_q: 3.272190, mean_eps: 0.100000\n",
      " 124862/175000: episode: 3491, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 144.467 [139.000, 213.000], mean observation: 0.128 [0.000, 60.000], loss: 0.271802, mean_absolute_error: 0.512407, mean_q: 3.508172, mean_eps: 0.100000\n",
      " 124899/175000: episode: 3492, duration: 0.645s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.081 [6.000, 199.000], mean observation: 0.300 [0.000, 74.000], loss: 0.457762, mean_absolute_error: 0.519833, mean_q: 3.532192, mean_eps: 0.100000\n",
      " 124930/175000: episode: 3493, duration: 0.655s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 90.935 [42.000, 178.000], mean observation: 0.234 [0.000, 62.000], loss: 0.533638, mean_absolute_error: 0.505356, mean_q: 3.359013, mean_eps: 0.100000\n",
      " 124969/175000: episode: 3494, duration: 0.742s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.949 [18.000, 167.000], mean observation: 0.358 [0.000, 78.000], loss: 0.479689, mean_absolute_error: 0.510055, mean_q: 3.455650, mean_eps: 0.100000\n",
      " 124995/175000: episode: 3495, duration: 0.433s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 81.077 [16.000, 146.000], mean observation: 0.189 [0.000, 52.000], loss: 3.946534, mean_absolute_error: 0.823338, mean_q: 6.549776, mean_eps: 0.100000\n",
      " 125022/175000: episode: 3496, duration: 0.508s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.963 [24.000, 217.000], mean observation: 0.282 [0.000, 54.000], loss: 4.460188, mean_absolute_error: 0.792988, mean_q: 6.243047, mean_eps: 0.100000\n",
      " 125071/175000: episode: 3497, duration: 0.881s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.490 [0.000, 182.000], mean observation: 0.585 [0.000, 98.000], loss: 0.383385, mean_absolute_error: 0.547255, mean_q: 3.770320, mean_eps: 0.100000\n",
      " 125091/175000: episode: 3498, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 95.800 [18.000, 203.000], mean observation: 0.053 [0.000, 40.000], loss: 0.346982, mean_absolute_error: 0.524052, mean_q: 3.439687, mean_eps: 0.100000\n",
      " 125143/175000: episode: 3499, duration: 0.924s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 132.788 [14.000, 203.000], mean observation: 0.368 [0.000, 104.000], loss: 14.320304, mean_absolute_error: 0.706926, mean_q: 5.063376, mean_eps: 0.100000\n",
      " 125187/175000: episode: 3500, duration: 0.801s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.364 [3.000, 204.000], mean observation: 0.474 [0.000, 88.000], loss: 0.326332, mean_absolute_error: 0.548012, mean_q: 4.158083, mean_eps: 0.100000\n",
      " 125221/175000: episode: 3501, duration: 0.629s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 109.794 [14.000, 221.000], mean observation: 0.337 [0.000, 68.000], loss: 0.557241, mean_absolute_error: 0.516010, mean_q: 3.759441, mean_eps: 0.100000\n",
      " 125249/175000: episode: 3502, duration: 0.509s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 75.000 [4.000, 203.000], mean observation: 0.123 [0.000, 56.000], loss: 1.101046, mean_absolute_error: 0.527289, mean_q: 3.821419, mean_eps: 0.100000\n",
      " 125301/175000: episode: 3503, duration: 0.972s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 103.808 [8.000, 223.000], mean observation: 0.500 [0.000, 104.000], loss: 0.495249, mean_absolute_error: 0.509230, mean_q: 3.535183, mean_eps: 0.100000\n",
      " 125325/175000: episode: 3504, duration: 0.449s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 102.750 [14.000, 185.000], mean observation: 0.213 [0.000, 48.000], loss: 0.302249, mean_absolute_error: 0.507029, mean_q: 3.391144, mean_eps: 0.100000\n",
      " 125348/175000: episode: 3505, duration: 0.435s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 90.261 [12.000, 203.000], mean observation: 0.196 [0.000, 46.000], loss: 0.207702, mean_absolute_error: 0.518220, mean_q: 3.595043, mean_eps: 0.100000\n",
      " 125394/175000: episode: 3506, duration: 0.888s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.174 [18.000, 203.000], mean observation: 0.560 [0.000, 92.000], loss: 15.284140, mean_absolute_error: 0.851728, mean_q: 6.407502, mean_eps: 0.100000\n",
      " 125426/175000: episode: 3507, duration: 0.610s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.781 [13.000, 184.000], mean observation: 0.170 [0.000, 64.000], loss: 0.462806, mean_absolute_error: 0.526030, mean_q: 3.878094, mean_eps: 0.100000\n",
      " 125458/175000: episode: 3508, duration: 0.716s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 77.438 [31.000, 218.000], mean observation: 0.252 [0.000, 64.000], loss: 0.418136, mean_absolute_error: 0.515096, mean_q: 4.262031, mean_eps: 0.100000\n",
      " 125502/175000: episode: 3509, duration: 0.949s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.409 [1.000, 189.000], mean observation: 0.205 [0.000, 88.000], loss: 0.271105, mean_absolute_error: 0.522324, mean_q: 4.334863, mean_eps: 0.100000\n",
      " 125524/175000: episode: 3510, duration: 0.527s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 86.500 [46.000, 116.000], mean observation: 0.101 [0.000, 44.000], loss: 0.188480, mean_absolute_error: 0.494914, mean_q: 3.901399, mean_eps: 0.100000\n",
      " 125572/175000: episode: 3511, duration: 1.053s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 78.208 [60.000, 211.000], mean observation: 0.340 [0.000, 96.000], loss: 32.789698, mean_absolute_error: 0.752097, mean_q: 5.017333, mean_eps: 0.100000\n",
      " 125603/175000: episode: 3512, duration: 0.723s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 88.258 [60.000, 164.000], mean observation: 0.132 [0.000, 62.000], loss: 0.229062, mean_absolute_error: 0.472203, mean_q: 3.442027, mean_eps: 0.100000\n",
      " 125644/175000: episode: 3513, duration: 0.970s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.268 [60.000, 204.000], mean observation: 0.261 [0.000, 82.000], loss: 0.732906, mean_absolute_error: 0.490524, mean_q: 3.463175, mean_eps: 0.100000\n",
      " 125680/175000: episode: 3514, duration: 0.852s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 62.722 [29.000, 182.000], mean observation: 0.253 [0.000, 72.000], loss: 0.774706, mean_absolute_error: 0.507738, mean_q: 3.475286, mean_eps: 0.100000\n",
      " 125742/175000: episode: 3515, duration: 1.289s, episode steps: 62, steps per second: 48, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 81.645 [29.000, 190.000], mean observation: 0.851 [0.000, 124.000], loss: 0.480429, mean_absolute_error: 0.524336, mean_q: 3.758067, mean_eps: 0.100000\n",
      " 125782/175000: episode: 3516, duration: 0.846s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 126.300 [57.000, 213.000], mean observation: 0.271 [0.000, 80.000], loss: 302.960457, mean_absolute_error: 2.030077, mean_q: 5.108784, mean_eps: 0.100000\n",
      " 125821/175000: episode: 3517, duration: 0.750s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 134.282 [20.000, 191.000], mean observation: 0.361 [0.000, 78.000], loss: 0.224183, mean_absolute_error: 0.524629, mean_q: 3.377042, mean_eps: 0.100000\n",
      " 125866/175000: episode: 3518, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 66.600 [16.000, 186.000], mean observation: 0.456 [0.000, 90.000], loss: 0.222044, mean_absolute_error: 0.503716, mean_q: 3.212377, mean_eps: 0.100000\n",
      " 125899/175000: episode: 3519, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.424 [17.000, 213.000], mean observation: 0.286 [0.000, 66.000], loss: 126.344328, mean_absolute_error: 1.272914, mean_q: 5.406444, mean_eps: 0.100000\n",
      " 125925/175000: episode: 3520, duration: 0.602s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.385 [38.000, 197.000], mean observation: 0.155 [0.000, 52.000], loss: 0.242170, mean_absolute_error: 0.500058, mean_q: 3.307289, mean_eps: 0.100000\n",
      " 125962/175000: episode: 3521, duration: 0.654s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 80.541 [15.000, 139.000], mean observation: 0.178 [0.000, 74.000], loss: 0.299351, mean_absolute_error: 0.682829, mean_q: 5.333951, mean_eps: 0.100000\n",
      " 126000/175000: episode: 3522, duration: 0.822s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.632 [14.000, 222.000], mean observation: 0.279 [0.000, 76.000], loss: 40.437343, mean_absolute_error: 0.875876, mean_q: 5.307431, mean_eps: 0.100000\n",
      " 126039/175000: episode: 3523, duration: 0.705s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.615 [28.000, 203.000], mean observation: 0.229 [0.000, 78.000], loss: 365.761792, mean_absolute_error: 2.306234, mean_q: 5.212840, mean_eps: 0.100000\n",
      " 126081/175000: episode: 3524, duration: 0.817s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.357 [28.000, 139.000], mean observation: 0.269 [0.000, 84.000], loss: 0.210754, mean_absolute_error: 0.514121, mean_q: 3.400698, mean_eps: 0.100000\n",
      " 126109/175000: episode: 3525, duration: 0.640s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.107 [6.000, 170.000], mean observation: 0.161 [0.000, 56.000], loss: 0.235327, mean_absolute_error: 0.513990, mean_q: 3.507980, mean_eps: 0.100000\n",
      " 126147/175000: episode: 3526, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 109.105 [9.000, 215.000], mean observation: 0.283 [0.000, 76.000], loss: 0.330985, mean_absolute_error: 0.526564, mean_q: 3.620597, mean_eps: 0.100000\n",
      " 126175/175000: episode: 3527, duration: 0.558s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.214 [11.000, 215.000], mean observation: 0.126 [0.000, 56.000], loss: 0.901089, mean_absolute_error: 0.530663, mean_q: 3.990204, mean_eps: 0.100000\n",
      " 126220/175000: episode: 3528, duration: 1.008s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 164.444 [55.000, 216.000], mean observation: 0.275 [0.000, 90.000], loss: 0.253894, mean_absolute_error: 0.522228, mean_q: 3.823935, mean_eps: 0.100000\n",
      " 126243/175000: episode: 3529, duration: 0.538s, episode steps: 23, steps per second: 43, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 76.870 [12.000, 194.000], mean observation: 0.252 [0.000, 46.000], loss: 0.363432, mean_absolute_error: 0.507923, mean_q: 3.611731, mean_eps: 0.100000\n",
      " 126278/175000: episode: 3530, duration: 0.807s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.771 [50.000, 190.000], mean observation: 0.402 [0.000, 70.000], loss: 0.374956, mean_absolute_error: 0.502062, mean_q: 3.574550, mean_eps: 0.100000\n",
      " 126310/175000: episode: 3531, duration: 0.629s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 109.719 [50.000, 199.000], mean observation: 0.328 [0.000, 64.000], loss: 3.363633, mean_absolute_error: 0.730473, mean_q: 5.662558, mean_eps: 0.100000\n",
      " 126351/175000: episode: 3532, duration: 0.795s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.659 [17.000, 190.000], mean observation: 0.378 [0.000, 82.000], loss: 0.316473, mean_absolute_error: 0.527952, mean_q: 3.433277, mean_eps: 0.100000\n",
      " 126385/175000: episode: 3533, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.500 [8.000, 208.000], mean observation: 0.478 [0.000, 68.000], loss: 0.270450, mean_absolute_error: 0.517454, mean_q: 3.394039, mean_eps: 0.100000\n",
      " 126440/175000: episode: 3534, duration: 1.127s, episode steps: 55, steps per second: 49, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 91.818 [14.000, 190.000], mean observation: 0.529 [0.000, 110.000], loss: 0.291404, mean_absolute_error: 0.535965, mean_q: 3.430994, mean_eps: 0.100000\n",
      " 126461/175000: episode: 3535, duration: 0.485s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 109.952 [24.000, 215.000], mean observation: 0.138 [0.000, 42.000], loss: 0.430203, mean_absolute_error: 0.526425, mean_q: 3.442352, mean_eps: 0.100000\n",
      " 126504/175000: episode: 3536, duration: 0.808s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.070 [6.000, 222.000], mean observation: 0.505 [0.000, 86.000], loss: 0.402989, mean_absolute_error: 0.527584, mean_q: 3.770078, mean_eps: 0.100000\n",
      " 126548/175000: episode: 3537, duration: 0.861s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.682 [12.000, 220.000], mean observation: 0.507 [0.000, 88.000], loss: 0.418257, mean_absolute_error: 0.528890, mean_q: 3.657067, mean_eps: 0.100000\n",
      " 126585/175000: episode: 3538, duration: 0.706s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.730 [1.000, 222.000], mean observation: 0.429 [0.000, 74.000], loss: 0.481764, mean_absolute_error: 0.552068, mean_q: 3.857113, mean_eps: 0.100000\n",
      " 126625/175000: episode: 3539, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 132.925 [1.000, 203.000], mean observation: 0.426 [0.000, 80.000], loss: 0.466242, mean_absolute_error: 0.533735, mean_q: 3.796753, mean_eps: 0.100000\n",
      " 126675/175000: episode: 3540, duration: 0.926s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.920 [5.000, 222.000], mean observation: 0.647 [0.000, 100.000], loss: 309.487936, mean_absolute_error: 2.021218, mean_q: 5.078961, mean_eps: 0.100000\n",
      " 126705/175000: episode: 3541, duration: 0.685s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 94.333 [26.000, 219.000], mean observation: 0.272 [0.000, 60.000], loss: 2.237784, mean_absolute_error: 0.526752, mean_q: 3.518445, mean_eps: 0.100000\n",
      " 126750/175000: episode: 3542, duration: 0.851s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 91.556 [1.000, 223.000], mean observation: 0.583 [0.000, 90.000], loss: 0.676661, mean_absolute_error: 0.541337, mean_q: 3.570561, mean_eps: 0.100000\n",
      " 126800/175000: episode: 3543, duration: 0.928s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.760 [1.000, 223.000], mean observation: 0.566 [0.000, 100.000], loss: 2.595602, mean_absolute_error: 0.545434, mean_q: 3.593604, mean_eps: 0.100000\n",
      " 126825/175000: episode: 3544, duration: 0.508s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 87.360 [1.000, 212.000], mean observation: 0.235 [0.000, 50.000], loss: 1.162008, mean_absolute_error: 0.545634, mean_q: 3.478778, mean_eps: 0.100000\n",
      " 126848/175000: episode: 3545, duration: 0.438s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 131.696 [26.000, 223.000], mean observation: 0.202 [0.000, 46.000], loss: 0.717668, mean_absolute_error: 0.528612, mean_q: 3.400663, mean_eps: 0.100000\n",
      " 126880/175000: episode: 3546, duration: 0.638s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 171.938 [51.000, 223.000], mean observation: 0.085 [0.000, 64.000], loss: 0.572810, mean_absolute_error: 0.535164, mean_q: 3.565619, mean_eps: 0.100000\n",
      " 126927/175000: episode: 3547, duration: 0.868s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.149 [10.000, 212.000], mean observation: 0.582 [0.000, 94.000], loss: 0.411739, mean_absolute_error: 0.519783, mean_q: 3.256411, mean_eps: 0.100000\n",
      " 126974/175000: episode: 3548, duration: 0.843s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.660 [3.000, 214.000], mean observation: 0.589 [0.000, 94.000], loss: 0.422410, mean_absolute_error: 0.558797, mean_q: 3.606100, mean_eps: 0.100000\n",
      " 127005/175000: episode: 3549, duration: 0.580s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 85.677 [15.000, 175.000], mean observation: 0.262 [0.000, 62.000], loss: 0.925059, mean_absolute_error: 0.548233, mean_q: 3.509199, mean_eps: 0.100000\n",
      " 127037/175000: episode: 3550, duration: 0.573s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.500 [8.000, 170.000], mean observation: 0.384 [0.000, 64.000], loss: 1.404719, mean_absolute_error: 0.557435, mean_q: 3.643502, mean_eps: 0.100000\n",
      " 127067/175000: episode: 3551, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 101.500 [26.000, 212.000], mean observation: 0.206 [0.000, 60.000], loss: 1.644070, mean_absolute_error: 0.529714, mean_q: 3.427626, mean_eps: 0.100000\n",
      " 127085/175000: episode: 3552, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 102.611 [36.000, 198.000], mean observation: 0.107 [0.000, 36.000], loss: 0.542854, mean_absolute_error: 0.543130, mean_q: 3.332769, mean_eps: 0.100000\n",
      " 127103/175000: episode: 3553, duration: 0.313s, episode steps: 18, steps per second: 58, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 84.889 [26.000, 212.000], mean observation: 0.090 [0.000, 36.000], loss: 0.527044, mean_absolute_error: 0.531432, mean_q: 3.324576, mean_eps: 0.100000\n",
      " 127125/175000: episode: 3554, duration: 0.427s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 95.909 [14.000, 212.000], mean observation: 0.130 [0.000, 44.000], loss: 0.858811, mean_absolute_error: 0.539284, mean_q: 3.132129, mean_eps: 0.100000\n",
      " 127165/175000: episode: 3555, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.225 [14.000, 212.000], mean observation: 0.463 [0.000, 80.000], loss: 0.994897, mean_absolute_error: 0.541697, mean_q: 3.186725, mean_eps: 0.100000\n",
      " 127194/175000: episode: 3556, duration: 0.514s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 114.345 [14.000, 202.000], mean observation: 0.303 [0.000, 58.000], loss: 416.661026, mean_absolute_error: 2.620106, mean_q: 5.844128, mean_eps: 0.100000\n",
      " 127203/175000: episode: 3557, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 96.778 [14.000, 167.000], mean observation: 0.058 [0.000, 18.000], loss: 0.713383, mean_absolute_error: 0.558942, mean_q: 3.151331, mean_eps: 0.100000\n",
      " 127252/175000: episode: 3558, duration: 0.910s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.592 [14.000, 187.000], mean observation: 0.295 [0.000, 98.000], loss: 0.592284, mean_absolute_error: 0.573370, mean_q: 3.368042, mean_eps: 0.100000\n",
      " 127283/175000: episode: 3559, duration: 0.625s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.161 [28.000, 224.000], mean observation: 0.146 [0.000, 62.000], loss: 7.524185, mean_absolute_error: 0.602108, mean_q: 3.609215, mean_eps: 0.100000\n",
      " 127335/175000: episode: 3560, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.673 [11.000, 194.000], mean observation: 0.676 [0.000, 104.000], loss: 1.206801, mean_absolute_error: 0.706521, mean_q: 4.994285, mean_eps: 0.100000\n",
      " 127376/175000: episode: 3561, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.927 [1.000, 223.000], mean observation: 0.291 [0.000, 82.000], loss: 0.449848, mean_absolute_error: 0.595843, mean_q: 3.703047, mean_eps: 0.100000\n",
      " 127396/175000: episode: 3562, duration: 0.436s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 94.050 [14.000, 215.000], mean observation: 0.173 [0.000, 40.000], loss: 0.695302, mean_absolute_error: 0.571534, mean_q: 3.527244, mean_eps: 0.100000\n",
      " 127412/175000: episode: 3563, duration: 0.362s, episode steps: 16, steps per second: 44, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 87.062 [1.000, 167.000], mean observation: 0.081 [0.000, 32.000], loss: 1.194519, mean_absolute_error: 0.540110, mean_q: 3.600570, mean_eps: 0.100000\n",
      " 127441/175000: episode: 3564, duration: 0.555s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 76.897 [1.000, 185.000], mean observation: 0.186 [0.000, 58.000], loss: 0.597759, mean_absolute_error: 0.580890, mean_q: 3.638386, mean_eps: 0.100000\n",
      " 127462/175000: episode: 3565, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.000 [28.000, 202.000], mean observation: 0.064 [0.000, 42.000], loss: 0.719299, mean_absolute_error: 0.589932, mean_q: 3.747940, mean_eps: 0.100000\n",
      " 127479/175000: episode: 3566, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 73.235 [28.000, 167.000], mean observation: 0.073 [0.000, 34.000], loss: 0.990370, mean_absolute_error: 0.607386, mean_q: 3.986584, mean_eps: 0.100000\n",
      " 127526/175000: episode: 3567, duration: 0.866s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 66.979 [14.000, 202.000], mean observation: 0.354 [0.000, 94.000], loss: 1.019363, mean_absolute_error: 0.587404, mean_q: 3.741050, mean_eps: 0.100000\n",
      " 127559/175000: episode: 3568, duration: 0.608s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 112.576 [14.000, 208.000], mean observation: 0.272 [0.000, 66.000], loss: 0.370118, mean_absolute_error: 0.564400, mean_q: 3.577529, mean_eps: 0.100000\n",
      " 127589/175000: episode: 3569, duration: 0.561s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.800 [14.000, 213.000], mean observation: 0.273 [0.000, 60.000], loss: 0.479693, mean_absolute_error: 0.552785, mean_q: 3.598689, mean_eps: 0.100000\n",
      " 127640/175000: episode: 3570, duration: 0.924s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 164.784 [3.000, 206.000], mean observation: 0.523 [0.000, 102.000], loss: 0.445471, mean_absolute_error: 0.546544, mean_q: 3.560387, mean_eps: 0.100000\n",
      " 127659/175000: episode: 3571, duration: 0.373s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 205.684 [146.000, 213.000], mean observation: 0.050 [0.000, 38.000], loss: 0.403448, mean_absolute_error: 0.528481, mean_q: 3.169145, mean_eps: 0.100000\n",
      " 127683/175000: episode: 3572, duration: 0.433s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 192.542 [71.000, 213.000], mean observation: 0.131 [0.000, 48.000], loss: 0.403419, mean_absolute_error: 0.544484, mean_q: 3.323479, mean_eps: 0.100000\n",
      " 127723/175000: episode: 3573, duration: 0.726s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 172.475 [37.000, 213.000], mean observation: 0.190 [0.000, 80.000], loss: 2.036983, mean_absolute_error: 0.561217, mean_q: 3.244885, mean_eps: 0.100000\n",
      " 127763/175000: episode: 3574, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 154.450 [8.000, 219.000], mean observation: 0.251 [0.000, 80.000], loss: 496.738157, mean_absolute_error: 2.931091, mean_q: 5.047822, mean_eps: 0.100000\n",
      " 127798/175000: episode: 3575, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.371 [27.000, 215.000], mean observation: 0.407 [0.000, 70.000], loss: 0.546218, mean_absolute_error: 0.582765, mean_q: 3.214991, mean_eps: 0.100000\n",
      " 127829/175000: episode: 3576, duration: 0.583s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.903 [8.000, 215.000], mean observation: 0.273 [0.000, 62.000], loss: 0.414216, mean_absolute_error: 0.570934, mean_q: 3.156609, mean_eps: 0.100000\n",
      " 127859/175000: episode: 3577, duration: 0.524s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 130.000 [8.000, 187.000], mean observation: 0.322 [0.000, 60.000], loss: 0.394531, mean_absolute_error: 0.582463, mean_q: 3.144711, mean_eps: 0.100000\n",
      " 127877/175000: episode: 3578, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.000 [45.000, 209.000], mean observation: 0.097 [0.000, 36.000], loss: 0.364727, mean_absolute_error: 0.585183, mean_q: 3.113227, mean_eps: 0.100000\n",
      " 127915/175000: episode: 3579, duration: 0.663s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.500 [21.000, 209.000], mean observation: 0.353 [0.000, 76.000], loss: 0.687984, mean_absolute_error: 0.577347, mean_q: 3.173232, mean_eps: 0.100000\n",
      " 127951/175000: episode: 3580, duration: 0.645s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 126.056 [8.000, 209.000], mean observation: 0.359 [0.000, 72.000], loss: 0.315776, mean_absolute_error: 0.573340, mean_q: 3.093900, mean_eps: 0.100000\n",
      " 127988/175000: episode: 3581, duration: 0.731s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.784 [8.000, 209.000], mean observation: 0.446 [0.000, 74.000], loss: 1.406724, mean_absolute_error: 0.573826, mean_q: 3.314662, mean_eps: 0.100000\n",
      " 128029/175000: episode: 3582, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.732 [27.000, 210.000], mean observation: 0.518 [0.000, 82.000], loss: 590.714331, mean_absolute_error: 3.406276, mean_q: 5.908381, mean_eps: 0.100000\n",
      " 128065/175000: episode: 3583, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.889 [12.000, 210.000], mean observation: 0.437 [0.000, 72.000], loss: 0.482896, mean_absolute_error: 0.555784, mean_q: 3.259116, mean_eps: 0.100000\n",
      " 128078/175000: episode: 3584, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 104.615 [33.000, 220.000], mean observation: 0.084 [0.000, 26.000], loss: 0.389400, mean_absolute_error: 0.556254, mean_q: 3.190699, mean_eps: 0.100000\n",
      " 128104/175000: episode: 3585, duration: 0.496s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 126.115 [38.000, 222.000], mean observation: 0.214 [0.000, 52.000], loss: 0.479402, mean_absolute_error: 0.567136, mean_q: 3.425162, mean_eps: 0.100000\n",
      " 128157/175000: episode: 3586, duration: 0.983s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.019 [4.000, 210.000], mean observation: 0.705 [0.000, 106.000], loss: 0.422398, mean_absolute_error: 0.573332, mean_q: 3.335007, mean_eps: 0.100000\n",
      " 128179/175000: episode: 3587, duration: 0.374s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 108.182 [38.000, 200.000], mean observation: 0.098 [0.000, 44.000], loss: 0.570116, mean_absolute_error: 0.579771, mean_q: 3.366786, mean_eps: 0.100000\n",
      " 128205/175000: episode: 3588, duration: 0.500s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 94.500 [40.000, 188.000], mean observation: 0.204 [0.000, 52.000], loss: 0.577544, mean_absolute_error: 0.578763, mean_q: 3.454317, mean_eps: 0.100000\n",
      " 128261/175000: episode: 3589, duration: 1.048s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 87.125 [39.000, 194.000], mean observation: 0.666 [0.000, 112.000], loss: 0.441167, mean_absolute_error: 0.596777, mean_q: 3.570285, mean_eps: 0.100000\n",
      " 128314/175000: episode: 3590, duration: 0.926s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.528 [59.000, 217.000], mean observation: 0.356 [0.000, 106.000], loss: 202.277115, mean_absolute_error: 1.637287, mean_q: 5.011849, mean_eps: 0.100000\n",
      " 128345/175000: episode: 3591, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.000 [1.000, 221.000], mean observation: 0.208 [0.000, 62.000], loss: 0.372905, mean_absolute_error: 0.616205, mean_q: 3.590327, mean_eps: 0.100000\n",
      " 128371/175000: episode: 3592, duration: 0.453s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 111.731 [58.000, 169.000], mean observation: 0.220 [0.000, 52.000], loss: 0.379711, mean_absolute_error: 0.620730, mean_q: 3.581445, mean_eps: 0.100000\n",
      " 128393/175000: episode: 3593, duration: 0.436s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 135.818 [11.000, 170.000], mean observation: 0.081 [0.000, 44.000], loss: 0.273045, mean_absolute_error: 0.604519, mean_q: 3.513789, mean_eps: 0.100000\n",
      " 128422/175000: episode: 3594, duration: 0.498s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 136.345 [1.000, 177.000], mean observation: 0.135 [0.000, 58.000], loss: 0.197773, mean_absolute_error: 0.606501, mean_q: 3.534604, mean_eps: 0.100000\n",
      " 128437/175000: episode: 3595, duration: 0.295s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 130.067 [39.000, 220.000], mean observation: 0.102 [0.000, 30.000], loss: 0.237699, mean_absolute_error: 0.592864, mean_q: 3.378774, mean_eps: 0.100000\n",
      " 128500/175000: episode: 3596, duration: 1.144s, episode steps: 63, steps per second: 55, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 100.175 [37.000, 190.000], mean observation: 0.785 [0.000, 126.000], loss: 5.494102, mean_absolute_error: 0.647790, mean_q: 4.033875, mean_eps: 0.100000\n",
      " 128541/175000: episode: 3597, duration: 0.776s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 44.415 [8.000, 199.000], mean observation: 0.129 [0.000, 82.000], loss: 1.118752, mean_absolute_error: 0.574831, mean_q: 3.486389, mean_eps: 0.100000\n",
      " 128556/175000: episode: 3598, duration: 0.302s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 87.800 [8.000, 178.000], mean observation: 0.109 [0.000, 30.000], loss: 5.003156, mean_absolute_error: 0.577846, mean_q: 3.384846, mean_eps: 0.100000\n",
      " 128604/175000: episode: 3599, duration: 0.903s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.167 [20.000, 127.000], mean observation: 0.276 [0.000, 96.000], loss: 479.768698, mean_absolute_error: 2.844369, mean_q: 5.153778, mean_eps: 0.100000\n",
      " 128636/175000: episode: 3600, duration: 0.629s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 123.781 [8.000, 216.000], mean observation: 0.348 [0.000, 64.000], loss: 0.974103, mean_absolute_error: 0.619413, mean_q: 3.839998, mean_eps: 0.100000\n",
      " 128674/175000: episode: 3601, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.105 [0.000, 209.000], mean observation: 0.309 [0.000, 76.000], loss: 0.708273, mean_absolute_error: 0.627786, mean_q: 3.704149, mean_eps: 0.100000\n",
      " 128699/175000: episode: 3602, duration: 0.453s, episode steps: 25, steps per second: 55, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 135.440 [84.000, 150.000], mean observation: 0.065 [0.000, 50.000], loss: 0.569192, mean_absolute_error: 0.620319, mean_q: 3.557416, mean_eps: 0.100000\n",
      " 128723/175000: episode: 3603, duration: 0.443s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.917 [37.000, 210.000], mean observation: 0.115 [0.000, 48.000], loss: 0.608405, mean_absolute_error: 0.620195, mean_q: 3.532404, mean_eps: 0.100000\n",
      " 128738/175000: episode: 3604, duration: 0.298s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 150.000 [150.000, 150.000], mean observation: 0.037 [0.000, 30.000], loss: 0.544433, mean_absolute_error: 0.625166, mean_q: 3.569087, mean_eps: 0.100000\n",
      " 128778/175000: episode: 3605, duration: 0.708s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.250 [24.000, 201.000], mean observation: 0.493 [0.000, 80.000], loss: 0.660495, mean_absolute_error: 0.635023, mean_q: 3.684779, mean_eps: 0.100000\n",
      " 128817/175000: episode: 3606, duration: 0.704s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.308 [12.000, 220.000], mean observation: 0.288 [0.000, 78.000], loss: 0.442746, mean_absolute_error: 0.629968, mean_q: 3.508737, mean_eps: 0.100000\n",
      " 128859/175000: episode: 3607, duration: 0.739s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.595 [12.000, 197.000], mean observation: 0.411 [0.000, 84.000], loss: 0.255886, mean_absolute_error: 0.611753, mean_q: 3.472210, mean_eps: 0.100000\n",
      " 128892/175000: episode: 3608, duration: 0.630s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.212 [12.000, 201.000], mean observation: 0.359 [0.000, 66.000], loss: 0.426614, mean_absolute_error: 0.618672, mean_q: 3.591818, mean_eps: 0.100000\n",
      " 128942/175000: episode: 3609, duration: 0.928s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.980 [12.000, 201.000], mean observation: 0.590 [0.000, 100.000], loss: 0.463767, mean_absolute_error: 0.603585, mean_q: 3.514729, mean_eps: 0.100000\n",
      " 128989/175000: episode: 3610, duration: 0.857s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 95.085 [13.000, 201.000], mean observation: 0.357 [0.000, 94.000], loss: 42.459951, mean_absolute_error: 0.752182, mean_q: 3.171797, mean_eps: 0.100000\n",
      " 129018/175000: episode: 3611, duration: 0.582s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.931 [38.000, 174.000], mean observation: 0.218 [0.000, 58.000], loss: 0.307896, mean_absolute_error: 0.556989, mean_q: 3.111554, mean_eps: 0.100000\n",
      " 129042/175000: episode: 3612, duration: 0.533s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 76.708 [38.000, 163.000], mean observation: 0.070 [0.000, 48.000], loss: 0.417401, mean_absolute_error: 0.548384, mean_q: 3.129805, mean_eps: 0.100000\n",
      " 129084/175000: episode: 3613, duration: 0.799s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.881 [2.000, 187.000], mean observation: 0.348 [0.000, 84.000], loss: 0.331742, mean_absolute_error: 0.549525, mean_q: 3.050434, mean_eps: 0.100000\n",
      " 129103/175000: episode: 3614, duration: 0.356s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 130.263 [27.000, 157.000], mean observation: 0.139 [0.000, 38.000], loss: 0.204994, mean_absolute_error: 0.556903, mean_q: 3.007799, mean_eps: 0.100000\n",
      " 129130/175000: episode: 3615, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.222 [13.000, 163.000], mean observation: 0.243 [0.000, 54.000], loss: 1.955586, mean_absolute_error: 0.572038, mean_q: 3.088417, mean_eps: 0.100000\n",
      " 129159/175000: episode: 3616, duration: 0.545s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 146.655 [27.000, 223.000], mean observation: 0.283 [0.000, 58.000], loss: 1.076074, mean_absolute_error: 0.589818, mean_q: 3.213732, mean_eps: 0.100000\n",
      " 129204/175000: episode: 3617, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 128.956 [1.000, 179.000], mean observation: 0.604 [0.000, 90.000], loss: 1.845944, mean_absolute_error: 0.636551, mean_q: 3.305037, mean_eps: 0.100000\n",
      " 129240/175000: episode: 3618, duration: 0.707s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.611 [7.000, 218.000], mean observation: 0.397 [0.000, 72.000], loss: 141.854478, mean_absolute_error: 1.287973, mean_q: 3.319499, mean_eps: 0.100000\n",
      " 129289/175000: episode: 3619, duration: 0.887s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.837 [27.000, 208.000], mean observation: 0.651 [0.000, 98.000], loss: 1.054753, mean_absolute_error: 0.689588, mean_q: 3.539051, mean_eps: 0.100000\n",
      " 129321/175000: episode: 3620, duration: 0.563s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 120.156 [35.000, 183.000], mean observation: 0.199 [0.000, 64.000], loss: 0.553516, mean_absolute_error: 0.742596, mean_q: 3.514395, mean_eps: 0.100000\n",
      " 129338/175000: episode: 3621, duration: 0.296s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 126.824 [34.000, 204.000], mean observation: 0.080 [0.000, 34.000], loss: 3.237536, mean_absolute_error: 0.757686, mean_q: 3.753367, mean_eps: 0.100000\n",
      " 129356/175000: episode: 3622, duration: 0.384s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.611 [63.000, 180.000], mean observation: 0.050 [0.000, 36.000], loss: 1.229889, mean_absolute_error: 0.737996, mean_q: 3.556135, mean_eps: 0.100000\n",
      " 129397/175000: episode: 3623, duration: 0.741s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.244 [14.000, 194.000], mean observation: 0.547 [0.000, 82.000], loss: 1.118538, mean_absolute_error: 0.809428, mean_q: 3.825257, mean_eps: 0.100000\n",
      " 129435/175000: episode: 3624, duration: 0.659s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.526 [23.000, 222.000], mean observation: 0.334 [0.000, 76.000], loss: 4.456684, mean_absolute_error: 0.821689, mean_q: 4.041973, mean_eps: 0.100000\n",
      " 129470/175000: episode: 3625, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 103.971 [8.000, 163.000], mean observation: 0.365 [0.000, 70.000], loss: 1401.643558, mean_absolute_error: 7.224309, mean_q: 5.934914, mean_eps: 0.100000\n",
      " 129512/175000: episode: 3626, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.548 [10.000, 222.000], mean observation: 0.187 [0.000, 84.000], loss: 0.692809, mean_absolute_error: 0.834530, mean_q: 4.119097, mean_eps: 0.100000\n",
      " 129560/175000: episode: 3627, duration: 0.911s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.979 [24.000, 223.000], mean observation: 0.233 [0.000, 96.000], loss: 3.809686, mean_absolute_error: 0.889313, mean_q: 4.221591, mean_eps: 0.100000\n",
      " 129591/175000: episode: 3628, duration: 0.583s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.355 [27.000, 186.000], mean observation: 0.097 [0.000, 62.000], loss: 0.472132, mean_absolute_error: 0.856962, mean_q: 4.266146, mean_eps: 0.100000\n",
      " 129606/175000: episode: 3629, duration: 0.324s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 131.133 [28.000, 150.000], mean observation: 0.044 [0.000, 30.000], loss: 0.698738, mean_absolute_error: 0.839205, mean_q: 4.294201, mean_eps: 0.100000\n",
      " 129652/175000: episode: 3630, duration: 0.848s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 126.087 [5.000, 222.000], mean observation: 0.311 [0.000, 92.000], loss: 0.864957, mean_absolute_error: 0.864146, mean_q: 4.216767, mean_eps: 0.100000\n",
      " 129680/175000: episode: 3631, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.357 [27.000, 186.000], mean observation: 0.125 [0.000, 56.000], loss: 0.935377, mean_absolute_error: 0.849338, mean_q: 4.171364, mean_eps: 0.100000\n",
      " 129710/175000: episode: 3632, duration: 0.598s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 116.933 [27.000, 186.000], mean observation: 0.179 [0.000, 60.000], loss: 601.958022, mean_absolute_error: 3.765912, mean_q: 6.365867, mean_eps: 0.100000\n",
      " 129727/175000: episode: 3633, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 142.353 [27.000, 199.000], mean observation: 0.105 [0.000, 34.000], loss: 24.592506, mean_absolute_error: 1.004973, mean_q: 4.419399, mean_eps: 0.100000\n",
      " 129762/175000: episode: 3634, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.914 [51.000, 186.000], mean observation: 0.296 [0.000, 70.000], loss: 2.390234, mean_absolute_error: 0.935577, mean_q: 4.190409, mean_eps: 0.100000\n",
      " 129800/175000: episode: 3635, duration: 0.713s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 117.868 [1.000, 223.000], mean observation: 0.354 [0.000, 76.000], loss: 3.625397, mean_absolute_error: 0.971548, mean_q: 4.153429, mean_eps: 0.100000\n",
      " 129832/175000: episode: 3636, duration: 0.610s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.094 [27.000, 186.000], mean observation: 0.186 [0.000, 64.000], loss: 2.856937, mean_absolute_error: 0.992120, mean_q: 4.212979, mean_eps: 0.100000\n",
      " 129867/175000: episode: 3637, duration: 0.665s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.686 [25.000, 187.000], mean observation: 0.273 [0.000, 70.000], loss: 2.587954, mean_absolute_error: 1.054453, mean_q: 4.476918, mean_eps: 0.100000\n",
      " 129888/175000: episode: 3638, duration: 0.429s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 93.619 [27.000, 186.000], mean observation: 0.123 [0.000, 42.000], loss: 3.313933, mean_absolute_error: 1.000352, mean_q: 4.014992, mean_eps: 0.100000\n",
      " 129934/175000: episode: 3639, duration: 0.853s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.261 [4.000, 186.000], mean observation: 0.443 [0.000, 92.000], loss: 2.593043, mean_absolute_error: 1.009899, mean_q: 4.228441, mean_eps: 0.100000\n",
      " 129951/175000: episode: 3640, duration: 0.314s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 38.294 [27.000, 183.000], mean observation: 0.064 [0.000, 34.000], loss: 1.944304, mean_absolute_error: 0.930238, mean_q: 3.910518, mean_eps: 0.100000\n",
      " 129976/175000: episode: 3641, duration: 0.495s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 27.080 [27.000, 29.000], mean observation: 0.088 [0.000, 50.000], loss: 24.087564, mean_absolute_error: 0.928309, mean_q: 4.145514, mean_eps: 0.100000\n",
      " 129993/175000: episode: 3642, duration: 0.350s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 110.765 [27.000, 209.000], mean observation: 0.122 [0.000, 34.000], loss: 6.210322, mean_absolute_error: 0.786657, mean_q: 4.407597, mean_eps: 0.100000\n",
      " 130028/175000: episode: 3643, duration: 0.701s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.514 [17.000, 223.000], mean observation: 0.160 [0.000, 70.000], loss: 0.852398, mean_absolute_error: 0.782716, mean_q: 4.175989, mean_eps: 0.100000\n",
      " 130074/175000: episode: 3644, duration: 0.852s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.652 [16.000, 223.000], mean observation: 0.346 [0.000, 92.000], loss: 0.605259, mean_absolute_error: 0.720491, mean_q: 4.005926, mean_eps: 0.100000\n",
      " 130126/175000: episode: 3645, duration: 0.924s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 52.923 [20.000, 189.000], mean observation: 0.395 [0.000, 104.000], loss: 0.369989, mean_absolute_error: 0.654057, mean_q: 3.960711, mean_eps: 0.100000\n",
      " 130168/175000: episode: 3646, duration: 0.816s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.643 [22.000, 200.000], mean observation: 0.362 [0.000, 84.000], loss: 0.914162, mean_absolute_error: 0.599531, mean_q: 3.742400, mean_eps: 0.100000\n",
      " 130185/175000: episode: 3647, duration: 0.361s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 102.588 [28.000, 189.000], mean observation: 0.066 [0.000, 34.000], loss: 1.374615, mean_absolute_error: 0.602174, mean_q: 3.805383, mean_eps: 0.100000\n",
      " 130214/175000: episode: 3648, duration: 0.510s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 59.345 [28.000, 173.000], mean observation: 0.219 [0.000, 58.000], loss: 5.613004, mean_absolute_error: 0.630889, mean_q: 3.828072, mean_eps: 0.100000\n",
      " 130240/175000: episode: 3649, duration: 0.532s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 63.808 [6.000, 157.000], mean observation: 0.155 [0.000, 52.000], loss: 1.491495, mean_absolute_error: 0.618890, mean_q: 3.736296, mean_eps: 0.100000\n",
      " 130274/175000: episode: 3650, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.088 [24.000, 207.000], mean observation: 0.373 [0.000, 68.000], loss: 1.078933, mean_absolute_error: 0.629184, mean_q: 3.816081, mean_eps: 0.100000\n",
      " 130294/175000: episode: 3651, duration: 0.371s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 78.550 [1.000, 139.000], mean observation: 0.133 [0.000, 40.000], loss: 0.655840, mean_absolute_error: 0.648246, mean_q: 3.854421, mean_eps: 0.100000\n",
      " 130337/175000: episode: 3652, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.651 [32.000, 200.000], mean observation: 0.306 [0.000, 86.000], loss: 0.428331, mean_absolute_error: 0.632226, mean_q: 3.507676, mean_eps: 0.100000\n",
      " 130356/175000: episode: 3653, duration: 0.351s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 50.368 [41.000, 219.000], mean observation: 0.050 [0.000, 38.000], loss: 0.440125, mean_absolute_error: 0.635835, mean_q: 3.402803, mean_eps: 0.100000\n",
      " 130373/175000: episode: 3654, duration: 0.356s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 56.118 [23.000, 206.000], mean observation: 0.051 [0.000, 34.000], loss: 0.350216, mean_absolute_error: 0.625151, mean_q: 3.471101, mean_eps: 0.100000\n",
      " 130410/175000: episode: 3655, duration: 0.674s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 115.919 [23.000, 196.000], mean observation: 0.225 [0.000, 74.000], loss: 0.279376, mean_absolute_error: 0.613194, mean_q: 3.360482, mean_eps: 0.100000\n",
      " 130462/175000: episode: 3656, duration: 0.931s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 110.596 [23.000, 210.000], mean observation: 0.498 [0.000, 104.000], loss: 0.283660, mean_absolute_error: 0.599034, mean_q: 3.424862, mean_eps: 0.100000\n",
      " 130499/175000: episode: 3657, duration: 0.642s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 47.486 [23.000, 173.000], mean observation: 0.111 [0.000, 74.000], loss: 0.371749, mean_absolute_error: 0.605651, mean_q: 3.540690, mean_eps: 0.100000\n",
      " 130528/175000: episode: 3658, duration: 0.650s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 113.138 [35.000, 223.000], mean observation: 0.195 [0.000, 58.000], loss: 0.301701, mean_absolute_error: 0.605117, mean_q: 3.475440, mean_eps: 0.100000\n",
      " 130557/175000: episode: 3659, duration: 0.614s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 60.793 [35.000, 187.000], mean observation: 0.100 [0.000, 58.000], loss: 0.537731, mean_absolute_error: 0.596647, mean_q: 3.506650, mean_eps: 0.100000\n",
      " 130597/175000: episode: 3660, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 163.350 [17.000, 223.000], mean observation: 0.511 [0.000, 80.000], loss: 1.578488, mean_absolute_error: 0.617673, mean_q: 3.544824, mean_eps: 0.100000\n",
      " 130654/175000: episode: 3661, duration: 1.025s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 71.719 [6.000, 191.000], mean observation: 0.583 [0.000, 114.000], loss: 2.021233, mean_absolute_error: 0.608339, mean_q: 3.395658, mean_eps: 0.100000\n",
      " 130694/175000: episode: 3662, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 73.625 [6.000, 196.000], mean observation: 0.361 [0.000, 80.000], loss: 0.435388, mean_absolute_error: 0.586241, mean_q: 3.334767, mean_eps: 0.100000\n",
      " 130730/175000: episode: 3663, duration: 0.659s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 65.861 [1.000, 178.000], mean observation: 0.361 [0.000, 72.000], loss: 0.279337, mean_absolute_error: 0.581880, mean_q: 3.225936, mean_eps: 0.100000\n",
      " 130769/175000: episode: 3664, duration: 0.724s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 77.026 [29.000, 213.000], mean observation: 0.436 [0.000, 78.000], loss: 0.301146, mean_absolute_error: 0.576343, mean_q: 3.140556, mean_eps: 0.100000\n",
      " 130810/175000: episode: 3665, duration: 0.724s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.561 [29.000, 223.000], mean observation: 0.503 [0.000, 82.000], loss: 0.281140, mean_absolute_error: 0.586675, mean_q: 3.054858, mean_eps: 0.100000\n",
      " 130826/175000: episode: 3666, duration: 0.317s, episode steps: 16, steps per second: 50, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 98.312 [29.000, 179.000], mean observation: 0.104 [0.000, 32.000], loss: 5.900777, mean_absolute_error: 0.610094, mean_q: 3.209400, mean_eps: 0.100000\n",
      " 130862/175000: episode: 3667, duration: 0.653s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 98.083 [29.000, 218.000], mean observation: 0.253 [0.000, 72.000], loss: 0.412902, mean_absolute_error: 0.582978, mean_q: 3.164816, mean_eps: 0.100000\n",
      " 130904/175000: episode: 3668, duration: 0.762s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 121.095 [29.000, 197.000], mean observation: 0.478 [0.000, 84.000], loss: 0.419984, mean_absolute_error: 0.580662, mean_q: 3.092430, mean_eps: 0.100000\n",
      " 130941/175000: episode: 3669, duration: 0.730s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 129.514 [15.000, 209.000], mean observation: 0.425 [0.000, 74.000], loss: 0.659588, mean_absolute_error: 0.571697, mean_q: 3.135048, mean_eps: 0.100000\n",
      " 130985/175000: episode: 3670, duration: 0.798s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 126.500 [30.000, 213.000], mean observation: 0.518 [0.000, 88.000], loss: 1.265486, mean_absolute_error: 0.581791, mean_q: 3.066949, mean_eps: 0.100000\n",
      " 131014/175000: episode: 3671, duration: 0.554s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 72.172 [0.000, 217.000], mean observation: 0.188 [0.000, 58.000], loss: 0.686622, mean_absolute_error: 0.601636, mean_q: 3.126481, mean_eps: 0.100000\n",
      " 131057/175000: episode: 3672, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 61.628 [5.000, 198.000], mean observation: 0.282 [0.000, 86.000], loss: 1.163984, mean_absolute_error: 0.642769, mean_q: 3.222549, mean_eps: 0.100000\n",
      " 131093/175000: episode: 3673, duration: 0.641s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.250 [4.000, 219.000], mean observation: 0.258 [0.000, 72.000], loss: 1.486869, mean_absolute_error: 0.628902, mean_q: 3.009526, mean_eps: 0.100000\n",
      " 131123/175000: episode: 3674, duration: 0.528s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 65.300 [5.000, 178.000], mean observation: 0.236 [0.000, 60.000], loss: 1.124218, mean_absolute_error: 0.631772, mean_q: 3.145489, mean_eps: 0.100000\n",
      " 131156/175000: episode: 3675, duration: 0.628s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 61.788 [28.000, 218.000], mean observation: 0.226 [0.000, 66.000], loss: 1.087428, mean_absolute_error: 0.645517, mean_q: 3.223879, mean_eps: 0.100000\n",
      " 131204/175000: episode: 3676, duration: 1.003s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.438 [20.000, 212.000], mean observation: 0.648 [0.000, 96.000], loss: 62.311923, mean_absolute_error: 0.996708, mean_q: 4.142325, mean_eps: 0.100000\n",
      " 131242/175000: episode: 3677, duration: 0.735s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.816 [20.000, 208.000], mean observation: 0.366 [0.000, 76.000], loss: 0.875640, mean_absolute_error: 0.671686, mean_q: 3.608519, mean_eps: 0.100000\n",
      " 131274/175000: episode: 3678, duration: 0.599s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 99.750 [20.000, 208.000], mean observation: 0.209 [0.000, 64.000], loss: 0.798355, mean_absolute_error: 0.676104, mean_q: 3.612171, mean_eps: 0.100000\n",
      " 131305/175000: episode: 3679, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 83.839 [20.000, 208.000], mean observation: 0.174 [0.000, 62.000], loss: 0.586707, mean_absolute_error: 0.632964, mean_q: 3.521489, mean_eps: 0.100000\n",
      " 131345/175000: episode: 3680, duration: 0.706s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 105.000 [20.000, 208.000], mean observation: 0.441 [0.000, 80.000], loss: 0.747886, mean_absolute_error: 0.607989, mean_q: 3.384921, mean_eps: 0.100000\n",
      " 131379/175000: episode: 3681, duration: 0.709s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.676 [11.000, 208.000], mean observation: 0.215 [0.000, 68.000], loss: 0.918016, mean_absolute_error: 0.616677, mean_q: 3.641045, mean_eps: 0.100000\n",
      " 131414/175000: episode: 3682, duration: 0.645s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 135.457 [14.000, 212.000], mean observation: 0.442 [0.000, 70.000], loss: 0.737029, mean_absolute_error: 0.584749, mean_q: 3.255798, mean_eps: 0.100000\n",
      " 131454/175000: episode: 3683, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.025 [14.000, 208.000], mean observation: 0.349 [0.000, 80.000], loss: 107.006154, mean_absolute_error: 1.368869, mean_q: 6.709452, mean_eps: 0.100000\n",
      " 131503/175000: episode: 3684, duration: 0.892s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 83.633 [8.000, 218.000], mean observation: 0.338 [0.000, 98.000], loss: 0.935174, mean_absolute_error: 0.697243, mean_q: 4.527341, mean_eps: 0.100000\n",
      " 131538/175000: episode: 3685, duration: 0.687s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 106.114 [14.000, 196.000], mean observation: 0.141 [0.000, 70.000], loss: 0.284735, mean_absolute_error: 0.567580, mean_q: 3.131251, mean_eps: 0.100000\n",
      " 131588/175000: episode: 3686, duration: 0.908s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.240 [14.000, 219.000], mean observation: 0.765 [0.000, 100.000], loss: 0.379515, mean_absolute_error: 0.548053, mean_q: 2.776371, mean_eps: 0.100000\n",
      " 131629/175000: episode: 3687, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.878 [14.000, 215.000], mean observation: 0.476 [0.000, 82.000], loss: 1.236627, mean_absolute_error: 0.577746, mean_q: 2.936871, mean_eps: 0.100000\n",
      " 131666/175000: episode: 3688, duration: 0.750s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.189 [14.000, 215.000], mean observation: 0.573 [0.000, 74.000], loss: 0.451824, mean_absolute_error: 0.566133, mean_q: 2.672189, mean_eps: 0.100000\n",
      " 131687/175000: episode: 3689, duration: 0.375s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 73.714 [27.000, 180.000], mean observation: 0.054 [0.000, 42.000], loss: 0.433836, mean_absolute_error: 0.543810, mean_q: 2.367139, mean_eps: 0.100000\n",
      " 131732/175000: episode: 3690, duration: 0.904s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.000 [27.000, 215.000], mean observation: 0.320 [0.000, 90.000], loss: 0.369604, mean_absolute_error: 0.558267, mean_q: 2.461267, mean_eps: 0.100000\n",
      " 131784/175000: episode: 3691, duration: 0.997s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.442 [44.000, 206.000], mean observation: 0.415 [0.000, 104.000], loss: 0.622983, mean_absolute_error: 0.585803, mean_q: 2.483027, mean_eps: 0.100000\n",
      " 131825/175000: episode: 3692, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.902 [13.000, 206.000], mean observation: 0.305 [0.000, 82.000], loss: 1.181101, mean_absolute_error: 0.626391, mean_q: 2.642010, mean_eps: 0.100000\n",
      " 131844/175000: episode: 3693, duration: 0.366s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 104.000 [13.000, 217.000], mean observation: 0.186 [0.000, 38.000], loss: 0.589623, mean_absolute_error: 0.611120, mean_q: 2.368542, mean_eps: 0.100000\n",
      " 131879/175000: episode: 3694, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.229 [13.000, 206.000], mean observation: 0.452 [0.000, 70.000], loss: 1.134844, mean_absolute_error: 0.629402, mean_q: 2.501056, mean_eps: 0.100000\n",
      " 131932/175000: episode: 3695, duration: 1.005s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.113 [5.000, 193.000], mean observation: 0.530 [0.000, 106.000], loss: 847.445151, mean_absolute_error: 4.505709, mean_q: 3.689996, mean_eps: 0.100000\n",
      " 131952/175000: episode: 3696, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 92.000 [44.000, 220.000], mean observation: 0.115 [0.000, 40.000], loss: 0.226587, mean_absolute_error: 0.633416, mean_q: 2.595882, mean_eps: 0.100000\n",
      " 131970/175000: episode: 3697, duration: 0.371s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 92.778 [5.000, 192.000], mean observation: 0.123 [0.000, 36.000], loss: 0.218522, mean_absolute_error: 0.622028, mean_q: 2.507516, mean_eps: 0.100000\n",
      " 132010/175000: episode: 3698, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.375 [0.000, 208.000], mean observation: 0.468 [0.000, 80.000], loss: 0.257637, mean_absolute_error: 0.598014, mean_q: 2.587761, mean_eps: 0.100000\n",
      " 132042/175000: episode: 3699, duration: 0.557s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 97.031 [24.000, 208.000], mean observation: 0.228 [0.000, 64.000], loss: 0.218922, mean_absolute_error: 0.576668, mean_q: 2.427312, mean_eps: 0.100000\n",
      " 132078/175000: episode: 3700, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 81.528 [24.000, 150.000], mean observation: 0.248 [0.000, 72.000], loss: 0.193336, mean_absolute_error: 0.574746, mean_q: 2.355269, mean_eps: 0.100000\n",
      " 132117/175000: episode: 3701, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.051 [1.000, 174.000], mean observation: 0.331 [0.000, 78.000], loss: 0.150329, mean_absolute_error: 0.545586, mean_q: 2.127898, mean_eps: 0.100000\n",
      " 132162/175000: episode: 3702, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 100.400 [1.000, 194.000], mean observation: 0.519 [0.000, 90.000], loss: 1.496205, mean_absolute_error: 0.559247, mean_q: 2.484883, mean_eps: 0.100000\n",
      " 132214/175000: episode: 3703, duration: 0.958s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.981 [1.000, 197.000], mean observation: 0.525 [0.000, 104.000], loss: 0.261174, mean_absolute_error: 0.549997, mean_q: 2.412943, mean_eps: 0.100000\n",
      " 132252/175000: episode: 3704, duration: 0.702s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 79.789 [1.000, 214.000], mean observation: 0.244 [0.000, 76.000], loss: 0.159099, mean_absolute_error: 0.538367, mean_q: 2.428031, mean_eps: 0.100000\n",
      " 132286/175000: episode: 3705, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.029 [1.000, 199.000], mean observation: 0.309 [0.000, 68.000], loss: 0.223668, mean_absolute_error: 0.533532, mean_q: 2.218878, mean_eps: 0.100000\n",
      " 132343/175000: episode: 3706, duration: 1.040s, episode steps: 57, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 83.246 [1.000, 202.000], mean observation: 0.702 [0.000, 114.000], loss: 0.448779, mean_absolute_error: 0.545307, mean_q: 2.159959, mean_eps: 0.100000\n",
      " 132379/175000: episode: 3707, duration: 0.647s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 67.611 [1.000, 219.000], mean observation: 0.352 [0.000, 72.000], loss: 2.832831, mean_absolute_error: 0.557208, mean_q: 2.143560, mean_eps: 0.100000\n",
      " 132407/175000: episode: 3708, duration: 0.500s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 104.786 [1.000, 206.000], mean observation: 0.265 [0.000, 56.000], loss: 0.254257, mean_absolute_error: 0.538661, mean_q: 2.010134, mean_eps: 0.100000\n",
      " 132457/175000: episode: 3709, duration: 0.897s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.180 [1.000, 199.000], mean observation: 0.386 [0.000, 100.000], loss: 3.776992, mean_absolute_error: 0.584512, mean_q: 2.227363, mean_eps: 0.100000\n",
      " 132495/175000: episode: 3710, duration: 0.665s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.184 [1.000, 219.000], mean observation: 0.479 [0.000, 76.000], loss: 0.967349, mean_absolute_error: 0.591555, mean_q: 2.285349, mean_eps: 0.100000\n",
      " 132535/175000: episode: 3711, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 96.600 [1.000, 219.000], mean observation: 0.436 [0.000, 80.000], loss: 0.164898, mean_absolute_error: 0.580506, mean_q: 2.043131, mean_eps: 0.100000\n",
      " 132547/175000: episode: 3712, duration: 0.218s, episode steps: 12, steps per second: 55, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 99.000 [59.000, 195.000], mean observation: 0.066 [0.000, 24.000], loss: 0.212237, mean_absolute_error: 0.599266, mean_q: 2.117752, mean_eps: 0.100000\n",
      " 132570/175000: episode: 3713, duration: 0.458s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 160.217 [59.000, 195.000], mean observation: 0.179 [0.000, 46.000], loss: 0.355786, mean_absolute_error: 0.574315, mean_q: 1.912826, mean_eps: 0.100000\n",
      " 132609/175000: episode: 3714, duration: 0.719s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 151.231 [37.000, 195.000], mean observation: 0.200 [0.000, 78.000], loss: 0.339586, mean_absolute_error: 0.599739, mean_q: 2.343626, mean_eps: 0.100000\n",
      " 132649/175000: episode: 3715, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 105.450 [20.000, 184.000], mean observation: 0.257 [0.000, 80.000], loss: 0.768724, mean_absolute_error: 0.582560, mean_q: 2.085097, mean_eps: 0.100000\n",
      " 132689/175000: episode: 3716, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 127.000 [33.000, 163.000], mean observation: 0.313 [0.000, 80.000], loss: 0.327896, mean_absolute_error: 0.581308, mean_q: 1.894551, mean_eps: 0.100000\n",
      " 132718/175000: episode: 3717, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.690 [59.000, 222.000], mean observation: 0.183 [0.000, 58.000], loss: 0.288913, mean_absolute_error: 0.598415, mean_q: 2.113366, mean_eps: 0.100000\n",
      " 132749/175000: episode: 3718, duration: 0.645s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 115.677 [39.000, 194.000], mean observation: 0.287 [0.000, 62.000], loss: 2.406123, mean_absolute_error: 0.615672, mean_q: 2.358591, mean_eps: 0.100000\n",
      " 132786/175000: episode: 3719, duration: 0.662s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.054 [58.000, 210.000], mean observation: 0.511 [0.000, 74.000], loss: 0.284343, mean_absolute_error: 0.592474, mean_q: 2.113033, mean_eps: 0.100000\n",
      " 132828/175000: episode: 3720, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.238 [2.000, 223.000], mean observation: 0.619 [0.000, 84.000], loss: 0.364915, mean_absolute_error: 0.597440, mean_q: 2.100705, mean_eps: 0.100000\n",
      " 132866/175000: episode: 3721, duration: 0.732s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.316 [17.000, 224.000], mean observation: 0.319 [0.000, 76.000], loss: 0.277463, mean_absolute_error: 0.604630, mean_q: 2.143502, mean_eps: 0.100000\n",
      " 132893/175000: episode: 3722, duration: 0.501s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 93.667 [0.000, 193.000], mean observation: 0.135 [0.000, 54.000], loss: 0.224007, mean_absolute_error: 0.594047, mean_q: 2.122752, mean_eps: 0.100000\n",
      " 132927/175000: episode: 3723, duration: 0.594s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.647 [6.000, 193.000], mean observation: 0.274 [0.000, 68.000], loss: 0.184677, mean_absolute_error: 0.576000, mean_q: 1.947337, mean_eps: 0.100000\n",
      " 132944/175000: episode: 3724, duration: 0.371s, episode steps: 17, steps per second: 46, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 64.941 [17.000, 173.000], mean observation: 0.115 [0.000, 34.000], loss: 0.522398, mean_absolute_error: 0.582885, mean_q: 2.176656, mean_eps: 0.100000\n",
      " 132970/175000: episode: 3725, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.308 [63.000, 152.000], mean observation: 0.084 [0.000, 52.000], loss: 43.897601, mean_absolute_error: 0.773136, mean_q: 1.922375, mean_eps: 0.100000\n",
      " 132989/175000: episode: 3726, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 95.947 [91.000, 185.000], mean observation: 0.047 [0.000, 38.000], loss: 1.545707, mean_absolute_error: 0.595389, mean_q: 1.940484, mean_eps: 0.100000\n",
      " 133025/175000: episode: 3727, duration: 0.706s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.000 [3.000, 221.000], mean observation: 0.116 [0.000, 72.000], loss: 5.834896, mean_absolute_error: 0.636477, mean_q: 2.014998, mean_eps: 0.100000\n",
      " 133055/175000: episode: 3728, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 73.800 [1.000, 176.000], mean observation: 0.336 [0.000, 60.000], loss: 0.742578, mean_absolute_error: 0.606983, mean_q: 1.713984, mean_eps: 0.100000\n",
      " 133095/175000: episode: 3729, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 70.200 [0.000, 218.000], mean observation: 0.440 [0.000, 80.000], loss: 0.777250, mean_absolute_error: 0.641929, mean_q: 1.882025, mean_eps: 0.100000\n",
      " 133119/175000: episode: 3730, duration: 0.448s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 80.500 [2.000, 223.000], mean observation: 0.215 [0.000, 48.000], loss: 1.231791, mean_absolute_error: 0.627555, mean_q: 1.540307, mean_eps: 0.100000\n",
      " 133133/175000: episode: 3731, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 78.929 [50.000, 209.000], mean observation: 0.053 [0.000, 28.000], loss: 1.220127, mean_absolute_error: 0.637713, mean_q: 1.421175, mean_eps: 0.100000\n",
      " 133176/175000: episode: 3732, duration: 0.796s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 94.419 [17.000, 210.000], mean observation: 0.255 [0.000, 86.000], loss: 4.108211, mean_absolute_error: 0.665366, mean_q: 1.740311, mean_eps: 0.100000\n",
      " 133231/175000: episode: 3733, duration: 1.022s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.345 [17.000, 211.000], mean observation: 0.826 [0.000, 110.000], loss: 0.548676, mean_absolute_error: 0.639662, mean_q: 1.530007, mean_eps: 0.100000\n",
      " 133260/175000: episode: 3734, duration: 0.599s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 82.724 [50.000, 214.000], mean observation: 0.213 [0.000, 58.000], loss: 0.679558, mean_absolute_error: 0.654628, mean_q: 1.605863, mean_eps: 0.100000\n",
      " 133298/175000: episode: 3735, duration: 0.693s, episode steps: 38, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.132 [12.000, 218.000], mean observation: 0.479 [0.000, 76.000], loss: 0.608557, mean_absolute_error: 0.661840, mean_q: 1.676779, mean_eps: 0.100000\n",
      " 133339/175000: episode: 3736, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.829 [32.000, 222.000], mean observation: 0.254 [0.000, 82.000], loss: 1.878552, mean_absolute_error: 0.663429, mean_q: 1.687801, mean_eps: 0.100000\n",
      " 133387/175000: episode: 3737, duration: 0.886s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 83.854 [10.000, 167.000], mean observation: 0.236 [0.000, 96.000], loss: 1.696935, mean_absolute_error: 0.652430, mean_q: 1.424403, mean_eps: 0.100000\n",
      " 133431/175000: episode: 3738, duration: 0.812s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.568 [4.000, 222.000], mean observation: 0.352 [0.000, 88.000], loss: 0.800445, mean_absolute_error: 0.674116, mean_q: 1.482439, mean_eps: 0.100000\n",
      " 133440/175000: episode: 3739, duration: 0.215s, episode steps: 9, steps per second: 42, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 75.111 [33.000, 108.000], mean observation: 0.049 [0.000, 18.000], loss: 0.509157, mean_absolute_error: 0.680839, mean_q: 1.521871, mean_eps: 0.100000\n",
      " 133479/175000: episode: 3740, duration: 0.761s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.000 [10.000, 202.000], mean observation: 0.417 [0.000, 78.000], loss: 1.167907, mean_absolute_error: 0.694395, mean_q: 1.617161, mean_eps: 0.100000\n",
      " 133518/175000: episode: 3741, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.103 [27.000, 191.000], mean observation: 0.276 [0.000, 78.000], loss: 1.580846, mean_absolute_error: 0.713575, mean_q: 1.948324, mean_eps: 0.100000\n",
      " 133557/175000: episode: 3742, duration: 0.737s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.923 [12.000, 163.000], mean observation: 0.346 [0.000, 78.000], loss: 1.170370, mean_absolute_error: 0.694548, mean_q: 1.544208, mean_eps: 0.100000\n",
      " 133580/175000: episode: 3743, duration: 0.489s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.174 [10.000, 189.000], mean observation: 0.217 [0.000, 46.000], loss: 0.938035, mean_absolute_error: 0.720124, mean_q: 2.026390, mean_eps: 0.100000\n",
      " 133624/175000: episode: 3744, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 92.705 [0.000, 221.000], mean observation: 0.306 [0.000, 88.000], loss: 0.997739, mean_absolute_error: 0.865018, mean_q: 3.645490, mean_eps: 0.100000\n",
      " 133657/175000: episode: 3745, duration: 0.670s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.545 [12.000, 217.000], mean observation: 0.235 [0.000, 66.000], loss: 0.378645, mean_absolute_error: 0.708820, mean_q: 1.821981, mean_eps: 0.100000\n",
      " 133702/175000: episode: 3746, duration: 0.797s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.822 [12.000, 220.000], mean observation: 0.467 [0.000, 90.000], loss: 0.345858, mean_absolute_error: 0.728680, mean_q: 2.259645, mean_eps: 0.100000\n",
      " 133736/175000: episode: 3747, duration: 0.640s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.324 [12.000, 165.000], mean observation: 0.309 [0.000, 68.000], loss: 0.356642, mean_absolute_error: 0.712177, mean_q: 2.050875, mean_eps: 0.100000\n",
      " 133763/175000: episode: 3748, duration: 0.523s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.926 [12.000, 190.000], mean observation: 0.254 [0.000, 54.000], loss: 6.555716, mean_absolute_error: 0.743474, mean_q: 2.131310, mean_eps: 0.100000\n",
      " 133779/175000: episode: 3749, duration: 0.305s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 116.312 [12.000, 190.000], mean observation: 0.123 [0.000, 32.000], loss: 0.352033, mean_absolute_error: 0.702583, mean_q: 2.030176, mean_eps: 0.100000\n",
      " 133811/175000: episode: 3750, duration: 0.594s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 112.719 [12.000, 190.000], mean observation: 0.332 [0.000, 64.000], loss: 0.426881, mean_absolute_error: 0.710867, mean_q: 2.223257, mean_eps: 0.100000\n",
      " 133837/175000: episode: 3751, duration: 0.540s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 113.231 [12.000, 190.000], mean observation: 0.215 [0.000, 52.000], loss: 0.328654, mean_absolute_error: 0.705885, mean_q: 2.113338, mean_eps: 0.100000\n",
      " 133883/175000: episode: 3752, duration: 0.832s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.000 [31.000, 190.000], mean observation: 0.342 [0.000, 92.000], loss: 0.320065, mean_absolute_error: 0.689165, mean_q: 2.038761, mean_eps: 0.100000\n",
      " 133926/175000: episode: 3753, duration: 0.784s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.395 [30.000, 214.000], mean observation: 0.500 [0.000, 86.000], loss: 0.539910, mean_absolute_error: 0.702117, mean_q: 2.261067, mean_eps: 0.100000\n",
      " 133965/175000: episode: 3754, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.436 [22.000, 223.000], mean observation: 0.508 [0.000, 78.000], loss: 7.446080, mean_absolute_error: 0.719317, mean_q: 1.946641, mean_eps: 0.100000\n",
      " 134006/175000: episode: 3755, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.780 [22.000, 163.000], mean observation: 0.490 [0.000, 82.000], loss: 0.692102, mean_absolute_error: 0.679500, mean_q: 1.920932, mean_eps: 0.100000\n",
      " 134039/175000: episode: 3756, duration: 0.572s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 98.000 [0.000, 216.000], mean observation: 0.477 [0.000, 66.000], loss: 0.591741, mean_absolute_error: 0.689390, mean_q: 1.955482, mean_eps: 0.100000\n",
      " 134082/175000: episode: 3757, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 124.140 [0.000, 197.000], mean observation: 0.522 [0.000, 86.000], loss: 2.910571, mean_absolute_error: 0.698558, mean_q: 1.802913, mean_eps: 0.100000\n",
      " 134095/175000: episode: 3758, duration: 0.231s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 67.692 [43.000, 197.000], mean observation: 0.068 [0.000, 26.000], loss: 0.633282, mean_absolute_error: 0.679299, mean_q: 1.669717, mean_eps: 0.100000\n",
      " 134152/175000: episode: 3759, duration: 1.089s, episode steps: 57, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 115.035 [7.000, 204.000], mean observation: 0.700 [0.000, 114.000], loss: 0.738437, mean_absolute_error: 0.688852, mean_q: 1.802013, mean_eps: 0.100000\n",
      " 134199/175000: episode: 3760, duration: 0.852s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 128.234 [0.000, 197.000], mean observation: 0.450 [0.000, 94.000], loss: 0.725171, mean_absolute_error: 0.697227, mean_q: 1.886327, mean_eps: 0.100000\n",
      " 134247/175000: episode: 3761, duration: 0.862s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 104.396 [0.000, 171.000], mean observation: 0.395 [0.000, 96.000], loss: 6.652617, mean_absolute_error: 0.735183, mean_q: 2.020563, mean_eps: 0.100000\n",
      " 134305/175000: episode: 3762, duration: 1.071s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 135.897 [13.000, 217.000], mean observation: 0.805 [0.000, 116.000], loss: 3.437141, mean_absolute_error: 0.711965, mean_q: 1.969393, mean_eps: 0.100000\n",
      " 134334/175000: episode: 3763, duration: 0.519s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 121.379 [0.000, 197.000], mean observation: 0.221 [0.000, 58.000], loss: 0.400251, mean_absolute_error: 0.690029, mean_q: 1.792187, mean_eps: 0.100000\n",
      " 134371/175000: episode: 3764, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.297 [0.000, 216.000], mean observation: 0.247 [0.000, 74.000], loss: 0.381192, mean_absolute_error: 0.680137, mean_q: 1.949058, mean_eps: 0.100000\n",
      " 134402/175000: episode: 3765, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.581 [0.000, 197.000], mean observation: 0.196 [0.000, 62.000], loss: 0.591890, mean_absolute_error: 0.673786, mean_q: 1.877829, mean_eps: 0.100000\n",
      " 134449/175000: episode: 3766, duration: 0.839s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.787 [22.000, 148.000], mean observation: 0.388 [0.000, 94.000], loss: 0.649264, mean_absolute_error: 0.678785, mean_q: 1.769759, mean_eps: 0.100000\n",
      " 134509/175000: episode: 3767, duration: 1.081s, episode steps: 60, steps per second: 56, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 107.283 [24.000, 217.000], mean observation: 0.443 [0.000, 120.000], loss: 365.993470, mean_absolute_error: 2.385799, mean_q: 2.889530, mean_eps: 0.100000\n",
      " 134555/175000: episode: 3768, duration: 0.805s, episode steps: 46, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 51.696 [5.000, 148.000], mean observation: 0.390 [0.000, 92.000], loss: 0.584130, mean_absolute_error: 0.710292, mean_q: 2.045505, mean_eps: 0.100000\n",
      " 134593/175000: episode: 3769, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.921 [5.000, 223.000], mean observation: 0.369 [0.000, 76.000], loss: 2.402189, mean_absolute_error: 0.723585, mean_q: 1.904268, mean_eps: 0.100000\n",
      " 134631/175000: episode: 3770, duration: 0.649s, episode steps: 38, steps per second: 59, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.316 [0.000, 210.000], mean observation: 0.313 [0.000, 76.000], loss: 0.369667, mean_absolute_error: 0.721340, mean_q: 1.842670, mean_eps: 0.100000\n",
      " 134664/175000: episode: 3771, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 75.455 [3.000, 197.000], mean observation: 0.184 [0.000, 66.000], loss: 12.737385, mean_absolute_error: 0.799874, mean_q: 2.234440, mean_eps: 0.100000\n",
      " 134709/175000: episode: 3772, duration: 0.879s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.622 [13.000, 197.000], mean observation: 0.511 [0.000, 90.000], loss: 1.604388, mean_absolute_error: 0.723261, mean_q: 1.897792, mean_eps: 0.100000\n",
      " 134723/175000: episode: 3773, duration: 0.260s, episode steps: 14, steps per second: 54, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 199.857 [197.000, 207.000], mean observation: 0.048 [0.000, 28.000], loss: 0.228804, mean_absolute_error: 0.706031, mean_q: 1.801645, mean_eps: 0.100000\n",
      " 134768/175000: episode: 3774, duration: 0.858s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.578 [37.000, 223.000], mean observation: 0.430 [0.000, 90.000], loss: 0.340792, mean_absolute_error: 0.695149, mean_q: 1.759529, mean_eps: 0.100000\n",
      " 134801/175000: episode: 3775, duration: 0.658s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 96.455 [2.000, 191.000], mean observation: 0.316 [0.000, 66.000], loss: 0.260298, mean_absolute_error: 0.690834, mean_q: 1.776124, mean_eps: 0.100000\n",
      " 134850/175000: episode: 3776, duration: 0.842s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 104.041 [6.000, 191.000], mean observation: 0.346 [0.000, 98.000], loss: 27.197885, mean_absolute_error: 0.931729, mean_q: 3.203474, mean_eps: 0.100000\n",
      " 134863/175000: episode: 3777, duration: 0.203s, episode steps: 13, steps per second: 64, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 88.385 [59.000, 137.000], mean observation: 0.054 [0.000, 26.000], loss: 0.263875, mean_absolute_error: 0.666675, mean_q: 1.539549, mean_eps: 0.100000\n",
      " 134903/175000: episode: 3778, duration: 0.800s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 55.975 [16.000, 137.000], mean observation: 0.250 [0.000, 80.000], loss: 0.149721, mean_absolute_error: 0.681995, mean_q: 1.811052, mean_eps: 0.100000\n",
      " 134942/175000: episode: 3779, duration: 0.787s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.821 [12.000, 222.000], mean observation: 0.305 [0.000, 78.000], loss: 0.589734, mean_absolute_error: 0.686180, mean_q: 1.940940, mean_eps: 0.100000\n",
      " 134995/175000: episode: 3780, duration: 1.009s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 78.698 [5.000, 207.000], mean observation: 0.436 [0.000, 106.000], loss: 24.489792, mean_absolute_error: 0.849712, mean_q: 2.651399, mean_eps: 0.100000\n",
      " 135036/175000: episode: 3781, duration: 0.886s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.976 [6.000, 207.000], mean observation: 0.368 [0.000, 82.000], loss: 0.238574, mean_absolute_error: 0.664222, mean_q: 1.725931, mean_eps: 0.100000\n",
      " 135068/175000: episode: 3782, duration: 0.691s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 78.250 [22.000, 207.000], mean observation: 0.301 [0.000, 64.000], loss: 0.293621, mean_absolute_error: 0.676472, mean_q: 1.827198, mean_eps: 0.100000\n",
      " 135106/175000: episode: 3783, duration: 0.903s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.868 [27.000, 208.000], mean observation: 0.194 [0.000, 76.000], loss: 0.148628, mean_absolute_error: 0.676227, mean_q: 1.877291, mean_eps: 0.100000\n",
      " 135154/175000: episode: 3784, duration: 0.903s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 180.208 [27.000, 222.000], mean observation: 0.193 [0.000, 96.000], loss: 0.243587, mean_absolute_error: 0.656528, mean_q: 1.763883, mean_eps: 0.100000\n",
      " 135180/175000: episode: 3785, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 171.885 [4.000, 222.000], mean observation: 0.083 [0.000, 52.000], loss: 0.182582, mean_absolute_error: 0.656356, mean_q: 1.886933, mean_eps: 0.100000\n",
      " 135217/175000: episode: 3786, duration: 0.715s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.432 [6.000, 222.000], mean observation: 0.248 [0.000, 74.000], loss: 0.254817, mean_absolute_error: 0.643602, mean_q: 1.784762, mean_eps: 0.100000\n",
      " 135251/175000: episode: 3787, duration: 0.612s, episode steps: 34, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.765 [34.000, 191.000], mean observation: 0.263 [0.000, 68.000], loss: 0.251198, mean_absolute_error: 0.635298, mean_q: 1.642757, mean_eps: 0.100000\n",
      " 135301/175000: episode: 3788, duration: 0.972s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.680 [1.000, 207.000], mean observation: 0.408 [0.000, 100.000], loss: 0.404421, mean_absolute_error: 0.661156, mean_q: 1.930922, mean_eps: 0.100000\n",
      " 135332/175000: episode: 3789, duration: 0.582s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 148.419 [1.000, 222.000], mean observation: 0.114 [0.000, 62.000], loss: 1.220613, mean_absolute_error: 0.666674, mean_q: 1.841701, mean_eps: 0.100000\n",
      " 135364/175000: episode: 3790, duration: 0.664s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 148.469 [16.000, 222.000], mean observation: 0.147 [0.000, 64.000], loss: 0.143095, mean_absolute_error: 0.655356, mean_q: 1.588555, mean_eps: 0.100000\n",
      " 135392/175000: episode: 3791, duration: 0.588s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 79.500 [1.000, 195.000], mean observation: 0.166 [0.000, 56.000], loss: 0.294844, mean_absolute_error: 0.654628, mean_q: 1.591682, mean_eps: 0.100000\n",
      " 135417/175000: episode: 3792, duration: 0.491s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.920 [1.000, 153.000], mean observation: 0.207 [0.000, 50.000], loss: 559.417766, mean_absolute_error: 3.373379, mean_q: 4.453767, mean_eps: 0.100000\n",
      " 135473/175000: episode: 3793, duration: 1.008s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 113.339 [1.000, 221.000], mean observation: 0.912 [0.000, 112.000], loss: 0.252655, mean_absolute_error: 0.662918, mean_q: 2.111843, mean_eps: 0.100000\n",
      " 135516/175000: episode: 3794, duration: 0.787s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.791 [1.000, 216.000], mean observation: 0.641 [0.000, 86.000], loss: 325.345023, mean_absolute_error: 2.260189, mean_q: 3.625146, mean_eps: 0.100000\n",
      " 135538/175000: episode: 3795, duration: 0.419s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 127.227 [39.000, 216.000], mean observation: 0.162 [0.000, 44.000], loss: 1.215053, mean_absolute_error: 0.823837, mean_q: 3.725122, mean_eps: 0.100000\n",
      " 135580/175000: episode: 3796, duration: 0.817s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.310 [38.000, 216.000], mean observation: 0.367 [0.000, 84.000], loss: 0.234470, mean_absolute_error: 0.655891, mean_q: 1.934980, mean_eps: 0.100000\n",
      " 135597/175000: episode: 3797, duration: 0.353s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 138.176 [50.000, 217.000], mean observation: 0.082 [0.000, 34.000], loss: 0.172995, mean_absolute_error: 0.643877, mean_q: 1.915245, mean_eps: 0.100000\n",
      " 135641/175000: episode: 3798, duration: 0.764s, episode steps: 44, steps per second: 58, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 147.455 [42.000, 184.000], mean observation: 0.374 [0.000, 88.000], loss: 0.547582, mean_absolute_error: 0.652330, mean_q: 2.077958, mean_eps: 0.100000\n",
      " 135690/175000: episode: 3799, duration: 0.874s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.796 [7.000, 180.000], mean observation: 0.395 [0.000, 98.000], loss: 0.295886, mean_absolute_error: 0.641326, mean_q: 2.066190, mean_eps: 0.100000\n",
      " 135714/175000: episode: 3800, duration: 0.437s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.375 [0.000, 211.000], mean observation: 0.250 [0.000, 48.000], loss: 0.155115, mean_absolute_error: 0.637879, mean_q: 1.961490, mean_eps: 0.100000\n",
      " 135748/175000: episode: 3801, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.706 [12.000, 217.000], mean observation: 0.415 [0.000, 68.000], loss: 0.303882, mean_absolute_error: 0.648519, mean_q: 2.030952, mean_eps: 0.100000\n",
      " 135775/175000: episode: 3802, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 106.407 [38.000, 165.000], mean observation: 0.175 [0.000, 54.000], loss: 6.312944, mean_absolute_error: 0.655649, mean_q: 1.682698, mean_eps: 0.100000\n",
      " 135810/175000: episode: 3803, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.457 [9.000, 186.000], mean observation: 0.355 [0.000, 70.000], loss: 0.522535, mean_absolute_error: 0.636995, mean_q: 1.829273, mean_eps: 0.100000\n",
      " 135833/175000: episode: 3804, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 91.826 [0.000, 171.000], mean observation: 0.131 [0.000, 46.000], loss: 0.239965, mean_absolute_error: 0.633361, mean_q: 1.834290, mean_eps: 0.100000\n",
      " 135870/175000: episode: 3805, duration: 0.675s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 163.811 [76.000, 215.000], mean observation: 0.216 [0.000, 74.000], loss: 0.255383, mean_absolute_error: 0.631181, mean_q: 1.508950, mean_eps: 0.100000\n",
      " 135898/175000: episode: 3806, duration: 0.511s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.750 [9.000, 191.000], mean observation: 0.153 [0.000, 56.000], loss: 0.282706, mean_absolute_error: 0.642851, mean_q: 1.493522, mean_eps: 0.100000\n",
      " 135944/175000: episode: 3807, duration: 0.850s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.717 [1.000, 222.000], mean observation: 0.326 [0.000, 92.000], loss: 0.280272, mean_absolute_error: 0.656851, mean_q: 1.505647, mean_eps: 0.100000\n",
      " 135994/175000: episode: 3808, duration: 0.945s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.100 [1.000, 211.000], mean observation: 0.683 [0.000, 100.000], loss: 0.728508, mean_absolute_error: 0.782709, mean_q: 2.925727, mean_eps: 0.100000\n",
      " 136016/175000: episode: 3809, duration: 0.427s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 157.227 [82.000, 214.000], mean observation: 0.130 [0.000, 44.000], loss: 0.245124, mean_absolute_error: 0.666346, mean_q: 1.707981, mean_eps: 0.100000\n",
      " 136057/175000: episode: 3810, duration: 0.879s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.512 [29.000, 211.000], mean observation: 0.277 [0.000, 82.000], loss: 0.374696, mean_absolute_error: 0.652875, mean_q: 1.569782, mean_eps: 0.100000\n",
      " 136088/175000: episode: 3811, duration: 0.595s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 150.290 [27.000, 211.000], mean observation: 0.334 [0.000, 62.000], loss: 0.294179, mean_absolute_error: 0.645508, mean_q: 1.553539, mean_eps: 0.100000\n",
      " 136120/175000: episode: 3812, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.844 [5.000, 197.000], mean observation: 0.227 [0.000, 64.000], loss: 0.319156, mean_absolute_error: 0.636016, mean_q: 1.504578, mean_eps: 0.100000\n",
      " 136145/175000: episode: 3813, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 54.000 [27.000, 150.000], mean observation: 0.113 [0.000, 50.000], loss: 0.244753, mean_absolute_error: 0.644425, mean_q: 1.545103, mean_eps: 0.100000\n",
      " 136185/175000: episode: 3814, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 115.275 [28.000, 211.000], mean observation: 0.367 [0.000, 80.000], loss: 0.190155, mean_absolute_error: 0.638719, mean_q: 1.558710, mean_eps: 0.100000\n",
      " 136208/175000: episode: 3815, duration: 0.501s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 104.652 [6.000, 171.000], mean observation: 0.198 [0.000, 46.000], loss: 0.330065, mean_absolute_error: 0.635562, mean_q: 1.664374, mean_eps: 0.100000\n",
      " 136261/175000: episode: 3816, duration: 0.999s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 136.925 [6.000, 212.000], mean observation: 0.552 [0.000, 106.000], loss: 1.427604, mean_absolute_error: 0.636445, mean_q: 1.631233, mean_eps: 0.100000\n",
      " 136292/175000: episode: 3817, duration: 0.605s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.387 [6.000, 216.000], mean observation: 0.258 [0.000, 62.000], loss: 0.201471, mean_absolute_error: 0.622502, mean_q: 1.445160, mean_eps: 0.100000\n",
      " 136341/175000: episode: 3818, duration: 0.969s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 93.408 [39.000, 209.000], mean observation: 0.554 [0.000, 98.000], loss: 0.279197, mean_absolute_error: 0.631595, mean_q: 1.649738, mean_eps: 0.100000\n",
      " 136383/175000: episode: 3819, duration: 0.804s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.619 [0.000, 171.000], mean observation: 0.319 [0.000, 84.000], loss: 0.368433, mean_absolute_error: 0.634486, mean_q: 1.566964, mean_eps: 0.100000\n",
      " 136443/175000: episode: 3820, duration: 1.096s, episode steps: 60, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 107.400 [0.000, 211.000], mean observation: 0.689 [0.000, 120.000], loss: 1.489700, mean_absolute_error: 0.758223, mean_q: 2.980415, mean_eps: 0.100000\n",
      " 136477/175000: episode: 3821, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.206 [0.000, 211.000], mean observation: 0.351 [0.000, 68.000], loss: 0.305507, mean_absolute_error: 0.640423, mean_q: 1.721179, mean_eps: 0.100000\n",
      " 136504/175000: episode: 3822, duration: 0.486s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 147.630 [0.000, 193.000], mean observation: 0.195 [0.000, 54.000], loss: 0.241805, mean_absolute_error: 0.643089, mean_q: 1.716161, mean_eps: 0.100000\n",
      " 136541/175000: episode: 3823, duration: 0.712s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 143.946 [0.000, 190.000], mean observation: 0.311 [0.000, 74.000], loss: 79.595697, mean_absolute_error: 1.267256, mean_q: 5.051995, mean_eps: 0.100000\n",
      " 136586/175000: episode: 3824, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 124.444 [0.000, 224.000], mean observation: 0.521 [0.000, 90.000], loss: 0.415629, mean_absolute_error: 0.621149, mean_q: 1.559428, mean_eps: 0.100000\n",
      " 136631/175000: episode: 3825, duration: 0.811s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.889 [7.000, 175.000], mean observation: 0.437 [0.000, 90.000], loss: 0.366691, mean_absolute_error: 0.644458, mean_q: 1.722704, mean_eps: 0.100000\n",
      " 136667/175000: episode: 3826, duration: 0.660s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.000 [10.000, 177.000], mean observation: 0.308 [0.000, 72.000], loss: 4.930415, mean_absolute_error: 0.655978, mean_q: 1.644081, mean_eps: 0.100000\n",
      " 136693/175000: episode: 3827, duration: 0.497s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 107.962 [10.000, 219.000], mean observation: 0.198 [0.000, 52.000], loss: 0.518813, mean_absolute_error: 0.654927, mean_q: 1.834488, mean_eps: 0.100000\n",
      " 136723/175000: episode: 3828, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 102.433 [10.000, 214.000], mean observation: 0.235 [0.000, 60.000], loss: 0.259787, mean_absolute_error: 0.654189, mean_q: 1.753247, mean_eps: 0.100000\n",
      " 136780/175000: episode: 3829, duration: 1.057s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 66.877 [13.000, 219.000], mean observation: 0.719 [0.000, 114.000], loss: 0.204823, mean_absolute_error: 0.648150, mean_q: 1.605890, mean_eps: 0.100000\n",
      " 136821/175000: episode: 3830, duration: 0.807s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.122 [24.000, 219.000], mean observation: 0.409 [0.000, 82.000], loss: 0.143664, mean_absolute_error: 0.652716, mean_q: 1.633553, mean_eps: 0.100000\n",
      " 136843/175000: episode: 3831, duration: 0.381s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 135.045 [24.000, 211.000], mean observation: 0.121 [0.000, 44.000], loss: 0.225894, mean_absolute_error: 0.642352, mean_q: 1.487339, mean_eps: 0.100000\n",
      " 136876/175000: episode: 3832, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 70.212 [24.000, 179.000], mean observation: 0.313 [0.000, 66.000], loss: 0.179841, mean_absolute_error: 0.646844, mean_q: 1.576528, mean_eps: 0.100000\n",
      " 136924/175000: episode: 3833, duration: 0.965s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 122.396 [43.000, 219.000], mean observation: 0.499 [0.000, 96.000], loss: 18.548258, mean_absolute_error: 0.746036, mean_q: 1.753920, mean_eps: 0.100000\n",
      " 136945/175000: episode: 3834, duration: 0.430s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 150.000 [43.000, 217.000], mean observation: 0.182 [0.000, 42.000], loss: 0.274560, mean_absolute_error: 0.630690, mean_q: 1.361190, mean_eps: 0.100000\n",
      " 136973/175000: episode: 3835, duration: 0.509s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 144.643 [43.000, 222.000], mean observation: 0.158 [0.000, 56.000], loss: 103.605595, mean_absolute_error: 1.292720, mean_q: 3.901068, mean_eps: 0.100000\n",
      " 137029/175000: episode: 3836, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 164.625 [43.000, 211.000], mean observation: 0.773 [0.000, 112.000], loss: 40.002589, mean_absolute_error: 0.915258, mean_q: 2.749201, mean_eps: 0.100000\n",
      " 137051/175000: episode: 3837, duration: 0.369s, episode steps: 22, steps per second: 60, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 138.045 [46.000, 211.000], mean observation: 0.200 [0.000, 44.000], loss: 1.054851, mean_absolute_error: 0.662634, mean_q: 1.709627, mean_eps: 0.100000\n",
      " 137092/175000: episode: 3838, duration: 0.777s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.829 [96.000, 211.000], mean observation: 0.331 [0.000, 82.000], loss: 0.134523, mean_absolute_error: 0.630532, mean_q: 1.392143, mean_eps: 0.100000\n",
      " 137137/175000: episode: 3839, duration: 0.860s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.222 [1.000, 211.000], mean observation: 0.639 [0.000, 90.000], loss: 46.693919, mean_absolute_error: 0.958212, mean_q: 2.882610, mean_eps: 0.100000\n",
      " 137168/175000: episode: 3840, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.452 [31.000, 211.000], mean observation: 0.299 [0.000, 62.000], loss: 0.145698, mean_absolute_error: 0.625785, mean_q: 1.457106, mean_eps: 0.100000\n",
      " 137218/175000: episode: 3841, duration: 0.961s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.540 [7.000, 210.000], mean observation: 0.615 [0.000, 100.000], loss: 94.342699, mean_absolute_error: 1.148210, mean_q: 2.705589, mean_eps: 0.100000\n",
      " 137256/175000: episode: 3842, duration: 0.759s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.158 [28.000, 211.000], mean observation: 0.308 [0.000, 76.000], loss: 0.325130, mean_absolute_error: 0.645976, mean_q: 1.843278, mean_eps: 0.100000\n",
      " 137299/175000: episode: 3843, duration: 0.869s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.279 [1.000, 221.000], mean observation: 0.296 [0.000, 86.000], loss: 0.218952, mean_absolute_error: 0.631998, mean_q: 1.821954, mean_eps: 0.100000\n",
      " 137340/175000: episode: 3844, duration: 0.781s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.049 [11.000, 221.000], mean observation: 0.301 [0.000, 82.000], loss: 0.180871, mean_absolute_error: 0.622261, mean_q: 1.913335, mean_eps: 0.100000\n",
      " 137360/175000: episode: 3845, duration: 0.431s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 106.350 [41.000, 195.000], mean observation: 0.142 [0.000, 40.000], loss: 0.955242, mean_absolute_error: 0.618348, mean_q: 1.830582, mean_eps: 0.100000\n",
      " 137390/175000: episode: 3846, duration: 0.590s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 134.967 [43.000, 221.000], mean observation: 0.163 [0.000, 60.000], loss: 0.215224, mean_absolute_error: 0.613971, mean_q: 1.750408, mean_eps: 0.100000\n",
      " 137435/175000: episode: 3847, duration: 0.807s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 172.556 [33.000, 221.000], mean observation: 0.499 [0.000, 90.000], loss: 0.458920, mean_absolute_error: 0.621023, mean_q: 1.630527, mean_eps: 0.100000\n",
      " 137490/175000: episode: 3848, duration: 0.993s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 114.855 [27.000, 222.000], mean observation: 0.362 [0.000, 110.000], loss: 0.209371, mean_absolute_error: 0.606039, mean_q: 1.452222, mean_eps: 0.100000\n",
      " 137539/175000: episode: 3849, duration: 0.879s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 168.347 [42.000, 221.000], mean observation: 0.624 [0.000, 98.000], loss: 0.214124, mean_absolute_error: 0.612455, mean_q: 1.552286, mean_eps: 0.100000\n",
      " 137576/175000: episode: 3850, duration: 0.785s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 97.135 [38.000, 187.000], mean observation: 0.286 [0.000, 74.000], loss: 0.168558, mean_absolute_error: 0.613347, mean_q: 1.613095, mean_eps: 0.100000\n",
      " 137608/175000: episode: 3851, duration: 0.700s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.125 [38.000, 208.000], mean observation: 0.104 [0.000, 64.000], loss: 0.167610, mean_absolute_error: 0.596860, mean_q: 1.534140, mean_eps: 0.100000\n",
      " 137643/175000: episode: 3852, duration: 0.712s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.314 [38.000, 208.000], mean observation: 0.318 [0.000, 70.000], loss: 0.610891, mean_absolute_error: 0.590189, mean_q: 1.387547, mean_eps: 0.100000\n",
      " 137668/175000: episode: 3853, duration: 0.514s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 112.200 [38.000, 208.000], mean observation: 0.154 [0.000, 50.000], loss: 0.155989, mean_absolute_error: 0.573466, mean_q: 1.296103, mean_eps: 0.100000\n",
      " 137699/175000: episode: 3854, duration: 0.651s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 134.871 [38.000, 208.000], mean observation: 0.285 [0.000, 62.000], loss: 0.196934, mean_absolute_error: 0.584987, mean_q: 1.539079, mean_eps: 0.100000\n",
      " 137744/175000: episode: 3855, duration: 0.905s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.289 [11.000, 208.000], mean observation: 0.367 [0.000, 90.000], loss: 0.754382, mean_absolute_error: 0.588599, mean_q: 1.451277, mean_eps: 0.100000\n",
      " 137771/175000: episode: 3856, duration: 0.547s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 178.333 [40.000, 218.000], mean observation: 0.182 [0.000, 54.000], loss: 0.303192, mean_absolute_error: 0.594516, mean_q: 1.424875, mean_eps: 0.100000\n",
      " 137825/175000: episode: 3857, duration: 1.034s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 102.926 [22.000, 208.000], mean observation: 0.440 [0.000, 108.000], loss: 0.201891, mean_absolute_error: 0.584289, mean_q: 1.353402, mean_eps: 0.100000\n",
      " 137859/175000: episode: 3858, duration: 0.572s, episode steps: 34, steps per second: 59, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 115.235 [4.000, 208.000], mean observation: 0.372 [0.000, 68.000], loss: 0.153053, mean_absolute_error: 0.571803, mean_q: 1.256937, mean_eps: 0.100000\n",
      " 137891/175000: episode: 3859, duration: 0.589s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 141.781 [4.000, 224.000], mean observation: 0.346 [0.000, 64.000], loss: 0.144297, mean_absolute_error: 0.583052, mean_q: 1.285403, mean_eps: 0.100000\n",
      " 137927/175000: episode: 3860, duration: 0.658s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.056 [9.000, 208.000], mean observation: 0.353 [0.000, 72.000], loss: 0.256052, mean_absolute_error: 0.583230, mean_q: 1.174982, mean_eps: 0.100000\n",
      " 137960/175000: episode: 3861, duration: 0.645s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 119.758 [7.000, 211.000], mean observation: 0.416 [0.000, 66.000], loss: 0.136400, mean_absolute_error: 0.587905, mean_q: 1.299464, mean_eps: 0.100000\n",
      " 137994/175000: episode: 3862, duration: 0.673s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.853 [12.000, 89.000], mean observation: 0.119 [0.000, 68.000], loss: 0.836925, mean_absolute_error: 0.603906, mean_q: 1.350849, mean_eps: 0.100000\n",
      " 138042/175000: episode: 3863, duration: 0.846s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.208 [35.000, 208.000], mean observation: 0.259 [0.000, 96.000], loss: 1.370634, mean_absolute_error: 0.615075, mean_q: 1.411194, mean_eps: 0.100000\n",
      " 138085/175000: episode: 3864, duration: 0.779s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.674 [1.000, 208.000], mean observation: 0.397 [0.000, 86.000], loss: 0.350588, mean_absolute_error: 0.591269, mean_q: 1.301167, mean_eps: 0.100000\n",
      " 138139/175000: episode: 3865, duration: 1.011s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 82.611 [1.000, 223.000], mean observation: 0.388 [0.000, 108.000], loss: 92.462596, mean_absolute_error: 1.108674, mean_q: 2.637831, mean_eps: 0.100000\n",
      " 138198/175000: episode: 3866, duration: 1.074s, episode steps: 59, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 124.881 [23.000, 223.000], mean observation: 0.577 [0.000, 118.000], loss: 0.191025, mean_absolute_error: 0.588304, mean_q: 1.384303, mean_eps: 0.100000\n",
      " 138239/175000: episode: 3867, duration: 0.711s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 169.683 [91.000, 207.000], mean observation: 0.342 [0.000, 82.000], loss: 0.179820, mean_absolute_error: 0.589977, mean_q: 1.307222, mean_eps: 0.100000\n",
      " 138279/175000: episode: 3868, duration: 0.751s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 111.125 [4.000, 207.000], mean observation: 0.269 [0.000, 80.000], loss: 0.176808, mean_absolute_error: 0.595927, mean_q: 1.345108, mean_eps: 0.100000\n",
      " 138312/175000: episode: 3869, duration: 0.644s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.273 [4.000, 195.000], mean observation: 0.253 [0.000, 66.000], loss: 0.122663, mean_absolute_error: 0.590739, mean_q: 1.308750, mean_eps: 0.100000\n",
      " 138338/175000: episode: 3870, duration: 0.520s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 156.769 [4.000, 195.000], mean observation: 0.085 [0.000, 52.000], loss: 0.134095, mean_absolute_error: 0.566545, mean_q: 1.174903, mean_eps: 0.100000\n",
      " 138382/175000: episode: 3871, duration: 0.839s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.773 [4.000, 217.000], mean observation: 0.552 [0.000, 88.000], loss: 7.422230, mean_absolute_error: 0.609099, mean_q: 1.352642, mean_eps: 0.100000\n",
      " 138411/175000: episode: 3872, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 107.931 [39.000, 195.000], mean observation: 0.292 [0.000, 58.000], loss: 0.146353, mean_absolute_error: 0.580382, mean_q: 1.329701, mean_eps: 0.100000\n",
      " 138444/175000: episode: 3873, duration: 0.631s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.424 [4.000, 197.000], mean observation: 0.366 [0.000, 66.000], loss: 0.167219, mean_absolute_error: 0.575278, mean_q: 1.328243, mean_eps: 0.100000\n",
      " 138480/175000: episode: 3874, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 96.222 [27.000, 208.000], mean observation: 0.447 [0.000, 72.000], loss: 0.865452, mean_absolute_error: 0.581554, mean_q: 1.415773, mean_eps: 0.100000\n",
      " 138506/175000: episode: 3875, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 95.077 [27.000, 208.000], mean observation: 0.156 [0.000, 52.000], loss: 1.910477, mean_absolute_error: 0.584872, mean_q: 1.327834, mean_eps: 0.100000\n",
      " 138541/175000: episode: 3876, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.914 [6.000, 217.000], mean observation: 0.401 [0.000, 70.000], loss: 0.255531, mean_absolute_error: 0.582844, mean_q: 1.418498, mean_eps: 0.100000\n",
      " 138575/175000: episode: 3877, duration: 0.599s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 31.824 [27.000, 176.000], mean observation: 0.114 [0.000, 68.000], loss: 0.217612, mean_absolute_error: 0.608062, mean_q: 1.651174, mean_eps: 0.100000\n",
      " 138602/175000: episode: 3878, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 59.111 [27.000, 221.000], mean observation: 0.121 [0.000, 54.000], loss: 477.073761, mean_absolute_error: 2.927080, mean_q: 3.930896, mean_eps: 0.100000\n",
      " 138639/175000: episode: 3879, duration: 0.679s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 69.892 [27.000, 224.000], mean observation: 0.189 [0.000, 74.000], loss: 52.245083, mean_absolute_error: 1.013245, mean_q: 3.628356, mean_eps: 0.100000\n",
      " 138675/175000: episode: 3880, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 100.139 [27.000, 148.000], mean observation: 0.164 [0.000, 72.000], loss: 11.094770, mean_absolute_error: 0.671236, mean_q: 1.693483, mean_eps: 0.100000\n",
      " 138694/175000: episode: 3881, duration: 0.381s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.046 [0.000, 38.000], loss: 0.246204, mean_absolute_error: 0.621442, mean_q: 1.656425, mean_eps: 0.100000\n",
      " 138711/175000: episode: 3882, duration: 0.301s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.041 [0.000, 34.000], loss: 0.236945, mean_absolute_error: 0.601698, mean_q: 1.646163, mean_eps: 0.100000\n",
      " 138736/175000: episode: 3883, duration: 0.537s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.059 [0.000, 50.000], loss: 0.249648, mean_absolute_error: 0.611278, mean_q: 1.648682, mean_eps: 0.100000\n",
      " 138785/175000: episode: 3884, duration: 0.941s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 122.796 [3.000, 213.000], mean observation: 0.543 [0.000, 98.000], loss: 0.212357, mean_absolute_error: 0.611315, mean_q: 1.530847, mean_eps: 0.100000\n",
      " 138823/175000: episode: 3885, duration: 0.665s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.500 [19.000, 222.000], mean observation: 0.178 [0.000, 76.000], loss: 0.256816, mean_absolute_error: 0.619317, mean_q: 1.829315, mean_eps: 0.100000\n",
      " 138843/175000: episode: 3886, duration: 0.397s, episode steps: 20, steps per second: 50, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 83.050 [20.000, 190.000], mean observation: 0.093 [0.000, 40.000], loss: 0.162074, mean_absolute_error: 0.605527, mean_q: 1.758949, mean_eps: 0.100000\n",
      " 138894/175000: episode: 3887, duration: 0.924s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 62.471 [27.000, 166.000], mean observation: 0.193 [0.000, 102.000], loss: 0.222180, mean_absolute_error: 0.595612, mean_q: 1.692150, mean_eps: 0.100000\n",
      " 138931/175000: episode: 3888, duration: 0.688s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 49.865 [24.000, 139.000], mean observation: 0.213 [0.000, 74.000], loss: 0.208253, mean_absolute_error: 0.586559, mean_q: 1.751700, mean_eps: 0.100000\n",
      " 138965/175000: episode: 3889, duration: 0.650s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 36.529 [23.000, 207.000], mean observation: 0.111 [0.000, 68.000], loss: 0.453079, mean_absolute_error: 0.602513, mean_q: 1.588042, mean_eps: 0.100000\n",
      " 139008/175000: episode: 3890, duration: 0.837s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.767 [24.000, 222.000], mean observation: 0.293 [0.000, 86.000], loss: 0.578423, mean_absolute_error: 0.589962, mean_q: 1.556711, mean_eps: 0.100000\n",
      " 139063/175000: episode: 3891, duration: 1.002s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 140.364 [2.000, 222.000], mean observation: 0.578 [0.000, 110.000], loss: 0.211817, mean_absolute_error: 0.595333, mean_q: 1.815074, mean_eps: 0.100000\n",
      " 139082/175000: episode: 3892, duration: 0.386s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.684 [30.000, 222.000], mean observation: 0.083 [0.000, 38.000], loss: 0.172316, mean_absolute_error: 0.578719, mean_q: 1.855969, mean_eps: 0.100000\n",
      " 139113/175000: episode: 3893, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 170.710 [165.000, 222.000], mean observation: 0.076 [0.000, 62.000], loss: 0.181042, mean_absolute_error: 0.574969, mean_q: 1.720375, mean_eps: 0.100000\n",
      " 139149/175000: episode: 3894, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 154.278 [29.000, 222.000], mean observation: 0.387 [0.000, 72.000], loss: 0.137960, mean_absolute_error: 0.578446, mean_q: 1.454111, mean_eps: 0.100000\n",
      " 139169/175000: episode: 3895, duration: 0.405s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 123.950 [1.000, 222.000], mean observation: 0.131 [0.000, 40.000], loss: 0.197428, mean_absolute_error: 0.588530, mean_q: 1.535812, mean_eps: 0.100000\n",
      " 139191/175000: episode: 3896, duration: 0.394s, episode steps: 22, steps per second: 56, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 165.000 [165.000, 165.000], mean observation: 0.053 [0.000, 44.000], loss: 0.204434, mean_absolute_error: 0.582040, mean_q: 1.536315, mean_eps: 0.100000\n",
      " 139222/175000: episode: 3897, duration: 0.579s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 156.000 [85.000, 195.000], mean observation: 0.091 [0.000, 62.000], loss: 90.578115, mean_absolute_error: 1.198567, mean_q: 3.958969, mean_eps: 0.100000\n",
      " 139259/175000: episode: 3898, duration: 0.674s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 154.784 [61.000, 220.000], mean observation: 0.224 [0.000, 74.000], loss: 0.652197, mean_absolute_error: 0.759449, mean_q: 3.536944, mean_eps: 0.100000\n",
      " 139299/175000: episode: 3899, duration: 0.734s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 156.550 [17.000, 214.000], mean observation: 0.391 [0.000, 80.000], loss: 1.614958, mean_absolute_error: 0.603513, mean_q: 1.710616, mean_eps: 0.100000\n",
      " 139324/175000: episode: 3900, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 118.560 [43.000, 195.000], mean observation: 0.160 [0.000, 50.000], loss: 0.158423, mean_absolute_error: 0.589956, mean_q: 1.605429, mean_eps: 0.100000\n",
      " 139347/175000: episode: 3901, duration: 0.450s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 84.000 [43.000, 189.000], mean observation: 0.163 [0.000, 46.000], loss: 0.114460, mean_absolute_error: 0.580181, mean_q: 1.697928, mean_eps: 0.100000\n",
      " 139384/175000: episode: 3902, duration: 0.718s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 75.784 [7.000, 217.000], mean observation: 0.448 [0.000, 74.000], loss: 0.198584, mean_absolute_error: 0.562528, mean_q: 1.605618, mean_eps: 0.100000\n",
      " 139417/175000: episode: 3903, duration: 0.679s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.212 [13.000, 189.000], mean observation: 0.372 [0.000, 66.000], loss: 0.213776, mean_absolute_error: 0.545003, mean_q: 1.603011, mean_eps: 0.100000\n",
      " 139450/175000: episode: 3904, duration: 0.600s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.030 [4.000, 223.000], mean observation: 0.374 [0.000, 66.000], loss: 0.160768, mean_absolute_error: 0.573578, mean_q: 1.935186, mean_eps: 0.100000\n",
      " 139489/175000: episode: 3905, duration: 0.720s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.205 [7.000, 202.000], mean observation: 0.440 [0.000, 78.000], loss: 0.160649, mean_absolute_error: 0.575479, mean_q: 1.866565, mean_eps: 0.100000\n",
      " 139525/175000: episode: 3906, duration: 0.647s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.278 [7.000, 207.000], mean observation: 0.405 [0.000, 72.000], loss: 0.160776, mean_absolute_error: 0.578238, mean_q: 1.768710, mean_eps: 0.100000\n",
      " 139563/175000: episode: 3907, duration: 0.712s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 63.316 [5.000, 185.000], mean observation: 0.411 [0.000, 76.000], loss: 0.240369, mean_absolute_error: 0.575807, mean_q: 1.701322, mean_eps: 0.100000\n",
      " 139596/175000: episode: 3908, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.485 [2.000, 185.000], mean observation: 0.440 [0.000, 66.000], loss: 0.242548, mean_absolute_error: 0.572841, mean_q: 1.640331, mean_eps: 0.100000\n",
      " 139630/175000: episode: 3909, duration: 0.722s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.294 [4.000, 187.000], mean observation: 0.348 [0.000, 68.000], loss: 0.218324, mean_absolute_error: 0.559006, mean_q: 1.520971, mean_eps: 0.100000\n",
      " 139677/175000: episode: 3910, duration: 0.883s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 90.702 [4.000, 204.000], mean observation: 0.518 [0.000, 94.000], loss: 0.193812, mean_absolute_error: 0.539134, mean_q: 1.269006, mean_eps: 0.100000\n",
      " 139719/175000: episode: 3911, duration: 0.742s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.000 [14.000, 221.000], mean observation: 0.358 [0.000, 84.000], loss: 0.499223, mean_absolute_error: 0.558684, mean_q: 1.334623, mean_eps: 0.100000\n",
      " 139741/175000: episode: 3912, duration: 0.441s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 88.136 [14.000, 167.000], mean observation: 0.128 [0.000, 44.000], loss: 0.223200, mean_absolute_error: 0.574227, mean_q: 1.329504, mean_eps: 0.100000\n",
      " 139762/175000: episode: 3913, duration: 0.429s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 127.571 [49.000, 220.000], mean observation: 0.098 [0.000, 42.000], loss: 0.372637, mean_absolute_error: 0.550882, mean_q: 1.124279, mean_eps: 0.100000\n",
      " 139808/175000: episode: 3914, duration: 0.867s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 162.043 [14.000, 220.000], mean observation: 0.477 [0.000, 92.000], loss: 0.236748, mean_absolute_error: 0.564575, mean_q: 1.186024, mean_eps: 0.100000\n",
      " 139853/175000: episode: 3915, duration: 0.897s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.400 [13.000, 220.000], mean observation: 0.453 [0.000, 90.000], loss: 0.272766, mean_absolute_error: 0.568684, mean_q: 1.172704, mean_eps: 0.100000\n",
      " 139897/175000: episode: 3916, duration: 0.834s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 85.455 [28.000, 223.000], mean observation: 0.446 [0.000, 88.000], loss: 0.203230, mean_absolute_error: 0.572482, mean_q: 1.325011, mean_eps: 0.100000\n",
      " 139937/175000: episode: 3917, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.900 [7.000, 220.000], mean observation: 0.460 [0.000, 80.000], loss: 0.243718, mean_absolute_error: 0.562725, mean_q: 1.259263, mean_eps: 0.100000\n",
      " 139976/175000: episode: 3918, duration: 0.709s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 134.872 [33.000, 220.000], mean observation: 0.418 [0.000, 78.000], loss: 0.208449, mean_absolute_error: 0.564567, mean_q: 1.276894, mean_eps: 0.100000\n",
      " 140005/175000: episode: 3919, duration: 0.624s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 160.759 [3.000, 220.000], mean observation: 0.299 [0.000, 58.000], loss: 0.428741, mean_absolute_error: 0.564009, mean_q: 1.259741, mean_eps: 0.100000\n",
      " 140035/175000: episode: 3920, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.100 [1.000, 220.000], mean observation: 0.227 [0.000, 60.000], loss: 0.392730, mean_absolute_error: 0.559652, mean_q: 1.198535, mean_eps: 0.100000\n",
      " 140092/175000: episode: 3921, duration: 1.070s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 139.842 [1.000, 223.000], mean observation: 0.459 [0.000, 114.000], loss: 0.264959, mean_absolute_error: 0.562694, mean_q: 1.187116, mean_eps: 0.100000\n",
      " 140132/175000: episode: 3922, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 147.225 [1.000, 220.000], mean observation: 0.489 [0.000, 80.000], loss: 0.520871, mean_absolute_error: 0.545939, mean_q: 1.231174, mean_eps: 0.100000\n",
      " 140170/175000: episode: 3923, duration: 0.736s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.974 [61.000, 221.000], mean observation: 0.449 [0.000, 76.000], loss: 0.371969, mean_absolute_error: 0.551984, mean_q: 1.477900, mean_eps: 0.100000\n",
      " 140199/175000: episode: 3924, duration: 0.508s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 107.103 [10.000, 200.000], mean observation: 0.209 [0.000, 58.000], loss: 14.576858, mean_absolute_error: 0.622996, mean_q: 1.515582, mean_eps: 0.100000\n",
      " 140228/175000: episode: 3925, duration: 0.542s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 114.655 [24.000, 200.000], mean observation: 0.121 [0.000, 58.000], loss: 0.269289, mean_absolute_error: 0.550089, mean_q: 1.362919, mean_eps: 0.100000\n",
      " 140269/175000: episode: 3926, duration: 0.772s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.707 [4.000, 222.000], mean observation: 0.332 [0.000, 82.000], loss: 1.186801, mean_absolute_error: 0.564499, mean_q: 1.462777, mean_eps: 0.100000\n",
      " 140281/175000: episode: 3927, duration: 0.214s, episode steps: 12, steps per second: 56, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 119.500 [59.000, 205.000], mean observation: 0.073 [0.000, 24.000], loss: 1.647897, mean_absolute_error: 0.555590, mean_q: 1.398005, mean_eps: 0.100000\n",
      " 140337/175000: episode: 3928, duration: 1.020s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 99.661 [4.000, 205.000], mean observation: 0.547 [0.000, 112.000], loss: 0.656371, mean_absolute_error: 0.655473, mean_q: 2.675812, mean_eps: 0.100000\n",
      " 140370/175000: episode: 3929, duration: 0.588s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.485 [35.000, 220.000], mean observation: 0.193 [0.000, 66.000], loss: 0.260780, mean_absolute_error: 0.545053, mean_q: 1.498005, mean_eps: 0.100000\n",
      " 140421/175000: episode: 3930, duration: 0.954s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 140.235 [4.000, 220.000], mean observation: 0.242 [0.000, 102.000], loss: 0.453544, mean_absolute_error: 0.545008, mean_q: 1.492179, mean_eps: 0.100000\n",
      " 140451/175000: episode: 3931, duration: 0.573s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.933 [89.000, 215.000], mean observation: 0.073 [0.000, 60.000], loss: 2289.558239, mean_absolute_error: 10.948433, mean_q: 4.032811, mean_eps: 0.100000\n",
      " 140485/175000: episode: 3932, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.824 [4.000, 222.000], mean observation: 0.403 [0.000, 68.000], loss: 0.221309, mean_absolute_error: 0.558319, mean_q: 1.654886, mean_eps: 0.100000\n",
      " 140527/175000: episode: 3933, duration: 0.736s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 144.738 [4.000, 220.000], mean observation: 0.478 [0.000, 84.000], loss: 0.306535, mean_absolute_error: 0.546331, mean_q: 1.582573, mean_eps: 0.100000\n",
      " 140564/175000: episode: 3934, duration: 0.714s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 76.054 [9.000, 195.000], mean observation: 0.427 [0.000, 74.000], loss: 0.145805, mean_absolute_error: 0.533869, mean_q: 1.428890, mean_eps: 0.100000\n",
      " 140611/175000: episode: 3935, duration: 0.849s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 111.787 [3.000, 222.000], mean observation: 0.534 [0.000, 94.000], loss: 2879.278646, mean_absolute_error: 13.565975, mean_q: 4.193565, mean_eps: 0.100000\n",
      " 140639/175000: episode: 3936, duration: 0.523s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 128.143 [4.000, 222.000], mean observation: 0.241 [0.000, 56.000], loss: 0.181688, mean_absolute_error: 0.768186, mean_q: 4.038550, mean_eps: 0.100000\n",
      " 140653/175000: episode: 3937, duration: 0.312s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 107.357 [67.000, 148.000], mean observation: 0.051 [0.000, 28.000], loss: 0.148960, mean_absolute_error: 0.546775, mean_q: 1.534121, mean_eps: 0.100000\n",
      " 140699/175000: episode: 3938, duration: 0.826s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.783 [9.000, 220.000], mean observation: 0.457 [0.000, 92.000], loss: 0.420402, mean_absolute_error: 0.534931, mean_q: 1.590352, mean_eps: 0.100000\n",
      " 140739/175000: episode: 3939, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 131.100 [1.000, 220.000], mean observation: 0.418 [0.000, 80.000], loss: 0.179667, mean_absolute_error: 0.538302, mean_q: 1.565987, mean_eps: 0.100000\n",
      " 140763/175000: episode: 3940, duration: 0.450s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.792 [20.000, 220.000], mean observation: 0.141 [0.000, 48.000], loss: 0.141220, mean_absolute_error: 0.637286, mean_q: 2.782218, mean_eps: 0.100000\n",
      " 140793/175000: episode: 3941, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 122.800 [1.000, 212.000], mean observation: 0.170 [0.000, 60.000], loss: 0.146506, mean_absolute_error: 0.530309, mean_q: 1.585197, mean_eps: 0.100000\n",
      " 140825/175000: episode: 3942, duration: 0.572s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.219 [31.000, 222.000], mean observation: 0.223 [0.000, 64.000], loss: 0.198619, mean_absolute_error: 0.516977, mean_q: 1.561338, mean_eps: 0.100000\n",
      " 140859/175000: episode: 3943, duration: 0.589s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 153.118 [24.000, 220.000], mean observation: 0.325 [0.000, 68.000], loss: 0.321377, mean_absolute_error: 0.528177, mean_q: 1.629776, mean_eps: 0.100000\n",
      " 140901/175000: episode: 3944, duration: 0.799s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 157.738 [6.000, 220.000], mean observation: 0.384 [0.000, 84.000], loss: 0.168035, mean_absolute_error: 0.514412, mean_q: 1.483347, mean_eps: 0.100000\n",
      " 140932/175000: episode: 3945, duration: 0.578s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 156.516 [37.000, 220.000], mean observation: 0.222 [0.000, 62.000], loss: 2116.801785, mean_absolute_error: 10.129425, mean_q: 3.888890, mean_eps: 0.100000\n",
      " 140954/175000: episode: 3946, duration: 0.472s, episode steps: 22, steps per second: 47, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 119.091 [24.000, 220.000], mean observation: 0.091 [0.000, 44.000], loss: 0.088201, mean_absolute_error: 0.508587, mean_q: 1.508790, mean_eps: 0.100000\n",
      " 140971/175000: episode: 3947, duration: 0.301s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 80.000 [24.000, 220.000], mean observation: 0.049 [0.000, 34.000], loss: 0.127334, mean_absolute_error: 0.502414, mean_q: 1.368551, mean_eps: 0.100000\n",
      " 141014/175000: episode: 3948, duration: 0.760s, episode steps: 43, steps per second: 57, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.744 [24.000, 220.000], mean observation: 0.381 [0.000, 86.000], loss: 0.362869, mean_absolute_error: 0.502281, mean_q: 1.260458, mean_eps: 0.100000\n",
      " 141059/175000: episode: 3949, duration: 0.821s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.778 [24.000, 207.000], mean observation: 0.460 [0.000, 90.000], loss: 1.143965, mean_absolute_error: 0.523888, mean_q: 1.460932, mean_eps: 0.100000\n",
      " 141084/175000: episode: 3950, duration: 0.480s, episode steps: 25, steps per second: 52, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 107.080 [37.000, 195.000], mean observation: 0.215 [0.000, 50.000], loss: 0.160626, mean_absolute_error: 0.527009, mean_q: 1.601441, mean_eps: 0.100000\n",
      " 141124/175000: episode: 3951, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 173.225 [37.000, 204.000], mean observation: 0.380 [0.000, 80.000], loss: 0.133616, mean_absolute_error: 0.516041, mean_q: 1.477730, mean_eps: 0.100000\n",
      " 141165/175000: episode: 3952, duration: 0.809s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.024 [16.000, 204.000], mean observation: 0.475 [0.000, 82.000], loss: 1.183739, mean_absolute_error: 0.511047, mean_q: 1.338305, mean_eps: 0.100000\n",
      " 141205/175000: episode: 3953, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 99.875 [18.000, 222.000], mean observation: 0.282 [0.000, 80.000], loss: 0.815046, mean_absolute_error: 0.512993, mean_q: 1.459620, mean_eps: 0.100000\n",
      " 141247/175000: episode: 3954, duration: 0.727s, episode steps: 42, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.071 [4.000, 222.000], mean observation: 0.506 [0.000, 84.000], loss: 0.487210, mean_absolute_error: 0.506434, mean_q: 1.518403, mean_eps: 0.100000\n",
      " 141277/175000: episode: 3955, duration: 0.580s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 130.933 [61.000, 222.000], mean observation: 0.185 [0.000, 60.000], loss: 0.152439, mean_absolute_error: 0.508826, mean_q: 1.417747, mean_eps: 0.100000\n",
      " 141317/175000: episode: 3956, duration: 0.778s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.600 [12.000, 222.000], mean observation: 0.299 [0.000, 80.000], loss: 510.912857, mean_absolute_error: 2.945116, mean_q: 3.240228, mean_eps: 0.100000\n",
      " 141365/175000: episode: 3957, duration: 0.926s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 146.229 [7.000, 215.000], mean observation: 0.680 [0.000, 96.000], loss: 0.150007, mean_absolute_error: 0.536258, mean_q: 1.549514, mean_eps: 0.100000\n",
      " 141419/175000: episode: 3958, duration: 0.953s, episode steps: 54, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 127.630 [19.000, 201.000], mean observation: 0.543 [0.000, 108.000], loss: 6.264453, mean_absolute_error: 0.683977, mean_q: 2.819124, mean_eps: 0.100000\n",
      " 141465/175000: episode: 3959, duration: 0.841s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 124.457 [14.000, 191.000], mean observation: 0.584 [0.000, 92.000], loss: 0.162301, mean_absolute_error: 0.540830, mean_q: 1.549853, mean_eps: 0.100000\n",
      " 141502/175000: episode: 3960, duration: 0.712s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 126.757 [14.000, 222.000], mean observation: 0.377 [0.000, 74.000], loss: 0.193379, mean_absolute_error: 0.531443, mean_q: 1.467613, mean_eps: 0.100000\n",
      " 141528/175000: episode: 3961, duration: 0.580s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 108.077 [8.000, 222.000], mean observation: 0.235 [0.000, 52.000], loss: 0.094763, mean_absolute_error: 0.533081, mean_q: 1.495136, mean_eps: 0.100000\n",
      " 141579/175000: episode: 3962, duration: 0.937s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.510 [8.000, 222.000], mean observation: 0.468 [0.000, 102.000], loss: 0.112999, mean_absolute_error: 0.538015, mean_q: 1.413553, mean_eps: 0.100000\n",
      " 141619/175000: episode: 3963, duration: 0.863s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 66.500 [8.000, 222.000], mean observation: 0.302 [0.000, 80.000], loss: 0.105926, mean_absolute_error: 0.535905, mean_q: 1.186683, mean_eps: 0.100000\n",
      " 141636/175000: episode: 3964, duration: 0.384s, episode steps: 17, steps per second: 44, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 149.118 [92.000, 224.000], mean observation: 0.045 [0.000, 34.000], loss: 0.179320, mean_absolute_error: 0.537822, mean_q: 1.097270, mean_eps: 0.100000\n",
      " 141685/175000: episode: 3965, duration: 0.931s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.653 [33.000, 215.000], mean observation: 0.458 [0.000, 98.000], loss: 0.467793, mean_absolute_error: 0.542058, mean_q: 1.106273, mean_eps: 0.100000\n",
      " 141714/175000: episode: 3966, duration: 0.504s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 42.655 [14.000, 103.000], mean observation: 0.134 [0.000, 58.000], loss: 0.505576, mean_absolute_error: 0.542400, mean_q: 1.112208, mean_eps: 0.100000\n",
      " 141735/175000: episode: 3967, duration: 0.404s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 39.571 [33.000, 56.000], mean observation: 0.054 [0.000, 42.000], loss: 0.166334, mean_absolute_error: 0.538338, mean_q: 1.113261, mean_eps: 0.100000\n",
      " 141768/175000: episode: 3968, duration: 0.646s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 44.848 [33.000, 56.000], mean observation: 0.092 [0.000, 66.000], loss: 0.641732, mean_absolute_error: 0.733125, mean_q: 3.291359, mean_eps: 0.100000\n",
      " 141791/175000: episode: 3969, duration: 0.460s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 103.565 [24.000, 208.000], mean observation: 0.147 [0.000, 46.000], loss: 0.246527, mean_absolute_error: 0.530808, mean_q: 1.095159, mean_eps: 0.100000\n",
      " 141843/175000: episode: 3970, duration: 0.949s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 119.442 [24.000, 208.000], mean observation: 0.841 [0.000, 104.000], loss: 49.109581, mean_absolute_error: 0.988400, mean_q: 3.634993, mean_eps: 0.100000\n",
      " 141874/175000: episode: 3971, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 69.581 [8.000, 210.000], mean observation: 0.215 [0.000, 62.000], loss: 83.932510, mean_absolute_error: 0.905736, mean_q: 0.963928, mean_eps: 0.100000\n",
      " 141926/175000: episode: 3972, duration: 0.935s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.942 [8.000, 210.000], mean observation: 0.525 [0.000, 104.000], loss: 3754.959798, mean_absolute_error: 17.440036, mean_q: 3.578379, mean_eps: 0.100000\n",
      " 141959/175000: episode: 3973, duration: 0.581s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 46.636 [8.000, 209.000], mean observation: 0.088 [0.000, 66.000], loss: 0.318476, mean_absolute_error: 0.702475, mean_q: 2.979700, mean_eps: 0.100000\n",
      " 142013/175000: episode: 3974, duration: 1.021s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.124 [0.000, 108.000], loss: 0.121185, mean_absolute_error: 0.528584, mean_q: 1.107325, mean_eps: 0.100000\n",
      " 142063/175000: episode: 3975, duration: 0.881s, episode steps: 50, steps per second: 57, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.060 [5.000, 187.000], mean observation: 0.239 [0.000, 100.000], loss: 0.137814, mean_absolute_error: 0.527598, mean_q: 1.168397, mean_eps: 0.100000\n",
      " 142114/175000: episode: 3976, duration: 1.096s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.157 [8.000, 208.000], mean observation: 0.522 [0.000, 102.000], loss: 0.324150, mean_absolute_error: 0.524871, mean_q: 1.028265, mean_eps: 0.100000\n",
      " 142130/175000: episode: 3977, duration: 0.296s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 183.250 [177.000, 187.000], mean observation: 0.044 [0.000, 32.000], loss: 0.117606, mean_absolute_error: 0.535820, mean_q: 1.053711, mean_eps: 0.100000\n",
      " 142152/175000: episode: 3978, duration: 0.457s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 109.045 [8.000, 187.000], mean observation: 0.095 [0.000, 44.000], loss: 0.089954, mean_absolute_error: 0.527116, mean_q: 0.922870, mean_eps: 0.100000\n",
      " 142185/175000: episode: 3979, duration: 0.687s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.424 [2.000, 221.000], mean observation: 0.292 [0.000, 66.000], loss: 0.228636, mean_absolute_error: 0.545127, mean_q: 1.024906, mean_eps: 0.100000\n",
      " 142228/175000: episode: 3980, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 148.884 [14.000, 187.000], mean observation: 0.278 [0.000, 86.000], loss: 0.128655, mean_absolute_error: 0.552134, mean_q: 1.108089, mean_eps: 0.100000\n",
      " 142249/175000: episode: 3981, duration: 0.437s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.952 [37.000, 193.000], mean observation: 0.124 [0.000, 42.000], loss: 0.114378, mean_absolute_error: 0.550733, mean_q: 1.041149, mean_eps: 0.100000\n",
      " 142280/175000: episode: 3982, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 122.419 [5.000, 197.000], mean observation: 0.132 [0.000, 62.000], loss: 0.185135, mean_absolute_error: 0.547094, mean_q: 1.090072, mean_eps: 0.100000\n",
      " 142324/175000: episode: 3983, duration: 0.848s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.523 [4.000, 200.000], mean observation: 0.434 [0.000, 88.000], loss: 0.138809, mean_absolute_error: 0.551424, mean_q: 1.017455, mean_eps: 0.100000\n",
      " 142363/175000: episode: 3984, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 44.179 [5.000, 196.000], mean observation: 0.203 [0.000, 78.000], loss: 0.376027, mean_absolute_error: 0.550268, mean_q: 1.097086, mean_eps: 0.100000\n",
      " 142384/175000: episode: 3985, duration: 0.440s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 113.476 [6.000, 215.000], mean observation: 0.152 [0.000, 42.000], loss: 0.550265, mean_absolute_error: 0.525656, mean_q: 1.071920, mean_eps: 0.100000\n",
      " 142419/175000: episode: 3986, duration: 0.715s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 22.543 [4.000, 175.000], mean observation: 0.203 [0.000, 70.000], loss: 0.475456, mean_absolute_error: 0.530577, mean_q: 1.117152, mean_eps: 0.100000\n",
      " 142455/175000: episode: 3987, duration: 0.673s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 13.750 [7.000, 97.000], mean observation: 0.159 [0.000, 72.000], loss: 0.121765, mean_absolute_error: 0.517891, mean_q: 1.124832, mean_eps: 0.100000\n",
      " 142494/175000: episode: 3988, duration: 0.714s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 46.513 [7.000, 211.000], mean observation: 0.183 [0.000, 78.000], loss: 1.792099, mean_absolute_error: 0.522314, mean_q: 1.055189, mean_eps: 0.100000\n",
      " 142531/175000: episode: 3989, duration: 0.656s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 7.865 [7.000, 39.000], mean observation: 0.088 [0.000, 74.000], loss: 0.174854, mean_absolute_error: 0.522617, mean_q: 1.052570, mean_eps: 0.100000\n",
      " 142567/175000: episode: 3990, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 102.694 [7.000, 206.000], mean observation: 0.373 [0.000, 72.000], loss: 0.089188, mean_absolute_error: 0.527518, mean_q: 1.074849, mean_eps: 0.100000\n",
      " 142607/175000: episode: 3991, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 13.625 [7.000, 145.000], mean observation: 0.164 [0.000, 80.000], loss: 0.126614, mean_absolute_error: 0.542462, mean_q: 1.136509, mean_eps: 0.100000\n",
      " 142634/175000: episode: 3992, duration: 0.500s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.667 [1.000, 187.000], mean observation: 0.191 [0.000, 54.000], loss: 0.080492, mean_absolute_error: 0.538368, mean_q: 1.156251, mean_eps: 0.100000\n",
      " 142673/175000: episode: 3993, duration: 0.703s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.410 [7.000, 196.000], mean observation: 0.374 [0.000, 78.000], loss: 0.150002, mean_absolute_error: 0.556429, mean_q: 1.429647, mean_eps: 0.100000\n",
      " 142710/175000: episode: 3994, duration: 0.668s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.378 [8.000, 223.000], mean observation: 0.391 [0.000, 74.000], loss: 1.146314, mean_absolute_error: 0.566426, mean_q: 1.319845, mean_eps: 0.100000\n",
      " 142759/175000: episode: 3995, duration: 0.890s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.224 [43.000, 211.000], mean observation: 0.552 [0.000, 98.000], loss: 5185.757220, mean_absolute_error: 23.732254, mean_q: 2.762558, mean_eps: 0.100000\n",
      " 142786/175000: episode: 3996, duration: 0.494s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 103.074 [49.000, 196.000], mean observation: 0.120 [0.000, 54.000], loss: 0.117219, mean_absolute_error: 0.563722, mean_q: 1.304754, mean_eps: 0.100000\n",
      " 142828/175000: episode: 3997, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.429 [13.000, 137.000], mean observation: 0.173 [0.000, 84.000], loss: 1103.201534, mean_absolute_error: 5.614434, mean_q: 3.047626, mean_eps: 0.100000\n",
      " 142852/175000: episode: 3998, duration: 0.511s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 81.292 [49.000, 137.000], mean observation: 0.084 [0.000, 48.000], loss: 0.192465, mean_absolute_error: 0.555087, mean_q: 1.359892, mean_eps: 0.100000\n",
      " 142878/175000: episode: 3999, duration: 0.492s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.615 [49.000, 187.000], mean observation: 0.177 [0.000, 52.000], loss: 0.204351, mean_absolute_error: 0.543258, mean_q: 1.081954, mean_eps: 0.100000\n",
      " 142905/175000: episode: 4000, duration: 0.533s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 151.000 [49.000, 213.000], mean observation: 0.201 [0.000, 54.000], loss: 0.137936, mean_absolute_error: 0.552723, mean_q: 1.141036, mean_eps: 0.100000\n",
      " 142943/175000: episode: 4001, duration: 0.664s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.553 [0.000, 182.000], mean observation: 0.225 [0.000, 76.000], loss: 0.208239, mean_absolute_error: 0.544844, mean_q: 1.018621, mean_eps: 0.100000\n",
      " 142971/175000: episode: 4002, duration: 0.551s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.071 [11.000, 182.000], mean observation: 0.110 [0.000, 56.000], loss: 0.087313, mean_absolute_error: 0.557005, mean_q: 1.155514, mean_eps: 0.100000\n",
      " 143015/175000: episode: 4003, duration: 0.781s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 77.545 [6.000, 141.000], mean observation: 0.287 [0.000, 88.000], loss: 0.172219, mean_absolute_error: 0.544982, mean_q: 1.127832, mean_eps: 0.100000\n",
      " 143040/175000: episode: 4004, duration: 0.515s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 94.280 [6.000, 171.000], mean observation: 0.177 [0.000, 50.000], loss: 0.075356, mean_absolute_error: 0.537440, mean_q: 0.991296, mean_eps: 0.100000\n",
      " 143061/175000: episode: 4005, duration: 0.424s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 135.381 [6.000, 200.000], mean observation: 0.142 [0.000, 42.000], loss: 0.102263, mean_absolute_error: 0.529388, mean_q: 1.001341, mean_eps: 0.100000\n",
      " 143089/175000: episode: 4006, duration: 0.530s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 164.429 [86.000, 206.000], mean observation: 0.107 [0.000, 56.000], loss: 0.123307, mean_absolute_error: 0.544151, mean_q: 1.152573, mean_eps: 0.100000\n",
      " 143132/175000: episode: 4007, duration: 0.809s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 129.233 [27.000, 206.000], mean observation: 0.288 [0.000, 86.000], loss: 0.152659, mean_absolute_error: 0.543003, mean_q: 0.993069, mean_eps: 0.100000\n",
      " 143152/175000: episode: 4008, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 161.850 [38.000, 215.000], mean observation: 0.133 [0.000, 40.000], loss: 0.126053, mean_absolute_error: 0.534413, mean_q: 0.647858, mean_eps: 0.100000\n",
      " 143177/175000: episode: 4009, duration: 0.536s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 152.280 [38.000, 215.000], mean observation: 0.182 [0.000, 50.000], loss: 0.130730, mean_absolute_error: 0.557679, mean_q: 1.007892, mean_eps: 0.100000\n",
      " 143209/175000: episode: 4010, duration: 0.577s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 149.844 [73.000, 215.000], mean observation: 0.308 [0.000, 64.000], loss: 0.098536, mean_absolute_error: 0.545776, mean_q: 0.785621, mean_eps: 0.100000\n",
      " 143268/175000: episode: 4011, duration: 1.116s, episode steps: 59, steps per second: 53, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 129.508 [10.000, 224.000], mean observation: 0.763 [0.000, 118.000], loss: 0.275438, mean_absolute_error: 0.663219, mean_q: 2.095268, mean_eps: 0.100000\n",
      " 143327/175000: episode: 4012, duration: 1.080s, episode steps: 59, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 141.305 [48.000, 198.000], mean observation: 0.759 [0.000, 118.000], loss: 0.109993, mean_absolute_error: 0.598645, mean_q: 1.320157, mean_eps: 0.100000\n",
      " 143357/175000: episode: 4013, duration: 0.562s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.000 [23.000, 191.000], mean observation: 0.279 [0.000, 60.000], loss: 0.201215, mean_absolute_error: 0.562297, mean_q: 0.850997, mean_eps: 0.100000\n",
      " 143389/175000: episode: 4014, duration: 0.584s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 135.031 [2.000, 222.000], mean observation: 0.249 [0.000, 64.000], loss: 0.142178, mean_absolute_error: 0.566902, mean_q: 0.959000, mean_eps: 0.100000\n",
      " 143441/175000: episode: 4015, duration: 0.969s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 151.173 [48.000, 221.000], mean observation: 0.551 [0.000, 104.000], loss: 1.121604, mean_absolute_error: 0.566682, mean_q: 0.968361, mean_eps: 0.100000\n",
      " 143504/175000: episode: 4016, duration: 1.134s, episode steps: 63, steps per second: 56, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 156.762 [61.000, 199.000], mean observation: 0.943 [0.000, 126.000], loss: 0.212298, mean_absolute_error: 0.554314, mean_q: 0.827938, mean_eps: 0.100000\n",
      " 143547/175000: episode: 4017, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 166.372 [6.000, 213.000], mean observation: 0.530 [0.000, 86.000], loss: 0.998690, mean_absolute_error: 0.562566, mean_q: 0.920988, mean_eps: 0.100000\n",
      " 143587/175000: episode: 4018, duration: 0.737s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 177.225 [32.000, 215.000], mean observation: 0.441 [0.000, 80.000], loss: 0.147163, mean_absolute_error: 0.554813, mean_q: 1.054495, mean_eps: 0.100000\n",
      " 143648/175000: episode: 4019, duration: 1.229s, episode steps: 61, steps per second: 50, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 165.525 [4.000, 215.000], mean observation: 0.737 [0.000, 122.000], loss: 0.263847, mean_absolute_error: 0.546199, mean_q: 0.932493, mean_eps: 0.100000\n",
      " 143692/175000: episode: 4020, duration: 0.841s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 156.159 [0.000, 220.000], mean observation: 0.522 [0.000, 88.000], loss: 558.887601, mean_absolute_error: 3.183392, mean_q: 2.561247, mean_eps: 0.100000\n",
      " 143728/175000: episode: 4021, duration: 0.707s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.306 [87.000, 215.000], mean observation: 0.193 [0.000, 72.000], loss: 0.105291, mean_absolute_error: 0.559870, mean_q: 0.916959, mean_eps: 0.100000\n",
      " 143755/175000: episode: 4022, duration: 0.564s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 158.556 [109.000, 215.000], mean observation: 0.189 [0.000, 54.000], loss: 0.209979, mean_absolute_error: 0.552517, mean_q: 1.057799, mean_eps: 0.100000\n",
      " 143778/175000: episode: 4023, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 164.609 [52.000, 221.000], mean observation: 0.249 [0.000, 46.000], loss: 0.139719, mean_absolute_error: 0.547516, mean_q: 1.108073, mean_eps: 0.100000\n",
      " 143827/175000: episode: 4024, duration: 0.877s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 125.041 [4.000, 221.000], mean observation: 0.625 [0.000, 98.000], loss: 1.398701, mean_absolute_error: 0.555027, mean_q: 1.069478, mean_eps: 0.100000\n",
      " 143872/175000: episode: 4025, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 146.489 [43.000, 222.000], mean observation: 0.180 [0.000, 90.000], loss: 0.200696, mean_absolute_error: 0.561985, mean_q: 1.200983, mean_eps: 0.100000\n",
      " 143898/175000: episode: 4026, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 165.808 [44.000, 221.000], mean observation: 0.131 [0.000, 52.000], loss: 0.305423, mean_absolute_error: 0.551686, mean_q: 0.967882, mean_eps: 0.100000\n",
      " 143928/175000: episode: 4027, duration: 0.583s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 112.733 [44.000, 193.000], mean observation: 0.326 [0.000, 60.000], loss: 0.129636, mean_absolute_error: 0.551916, mean_q: 0.968169, mean_eps: 0.100000\n",
      " 143958/175000: episode: 4028, duration: 0.568s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 135.500 [0.000, 193.000], mean observation: 0.319 [0.000, 60.000], loss: 0.132553, mean_absolute_error: 0.549308, mean_q: 0.833951, mean_eps: 0.100000\n",
      " 143987/175000: episode: 4029, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 110.310 [9.000, 193.000], mean observation: 0.191 [0.000, 58.000], loss: 0.118813, mean_absolute_error: 0.566822, mean_q: 1.010568, mean_eps: 0.100000\n",
      " 144012/175000: episode: 4030, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 135.520 [33.000, 200.000], mean observation: 0.158 [0.000, 50.000], loss: 2006.812506, mean_absolute_error: 9.747885, mean_q: 4.095227, mean_eps: 0.100000\n",
      " 144038/175000: episode: 4031, duration: 0.506s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 133.423 [11.000, 211.000], mean observation: 0.225 [0.000, 52.000], loss: 0.585298, mean_absolute_error: 0.551079, mean_q: 0.854780, mean_eps: 0.100000\n",
      " 144085/175000: episode: 4032, duration: 0.843s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 129.872 [6.000, 211.000], mean observation: 0.523 [0.000, 94.000], loss: 1367.356344, mean_absolute_error: 6.751324, mean_q: 2.438937, mean_eps: 0.100000\n",
      " 144095/175000: episode: 4033, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 70.400 [37.000, 105.000], mean observation: 0.041 [0.000, 20.000], loss: 0.107904, mean_absolute_error: 0.542710, mean_q: 0.790725, mean_eps: 0.100000\n",
      " 144105/175000: episode: 4034, duration: 0.231s, episode steps: 10, steps per second: 43, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 130.100 [37.000, 211.000], mean observation: 0.057 [0.000, 20.000], loss: 193.679864, mean_absolute_error: 1.471331, mean_q: 1.761314, mean_eps: 0.100000\n",
      " 144153/175000: episode: 4035, duration: 0.851s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.083 [3.000, 180.000], mean observation: 0.434 [0.000, 96.000], loss: 0.332722, mean_absolute_error: 0.541358, mean_q: 0.988188, mean_eps: 0.100000\n",
      " 144169/175000: episode: 4036, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 149.312 [58.000, 180.000], mean observation: 0.114 [0.000, 32.000], loss: 0.276233, mean_absolute_error: 0.538910, mean_q: 1.081470, mean_eps: 0.100000\n",
      " 144207/175000: episode: 4037, duration: 0.654s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.579 [3.000, 220.000], mean observation: 0.394 [0.000, 76.000], loss: 0.107439, mean_absolute_error: 0.541976, mean_q: 1.033462, mean_eps: 0.100000\n",
      " 144246/175000: episode: 4038, duration: 0.722s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 138.744 [76.000, 180.000], mean observation: 0.283 [0.000, 78.000], loss: 4.201980, mean_absolute_error: 0.560562, mean_q: 0.945123, mean_eps: 0.100000\n",
      " 144281/175000: episode: 4039, duration: 0.733s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.343 [1.000, 222.000], mean observation: 0.262 [0.000, 70.000], loss: 0.318634, mean_absolute_error: 0.555331, mean_q: 1.137123, mean_eps: 0.100000\n",
      " 144320/175000: episode: 4040, duration: 0.753s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.641 [23.000, 222.000], mean observation: 0.396 [0.000, 78.000], loss: 0.227332, mean_absolute_error: 0.539149, mean_q: 0.945706, mean_eps: 0.100000\n",
      " 144351/175000: episode: 4041, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 127.419 [15.000, 215.000], mean observation: 0.376 [0.000, 62.000], loss: 0.113905, mean_absolute_error: 0.535944, mean_q: 1.024499, mean_eps: 0.100000\n",
      " 144387/175000: episode: 4042, duration: 0.652s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 155.639 [15.000, 219.000], mean observation: 0.466 [0.000, 72.000], loss: 0.420258, mean_absolute_error: 0.536720, mean_q: 0.888937, mean_eps: 0.100000\n",
      " 144440/175000: episode: 4043, duration: 1.008s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 59.849 [0.000, 199.000], mean observation: 0.880 [0.000, 106.000], loss: 0.132657, mean_absolute_error: 0.544673, mean_q: 0.924816, mean_eps: 0.100000\n",
      " 144473/175000: episode: 4044, duration: 0.796s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.303 [0.000, 168.000], mean observation: 0.204 [0.000, 66.000], loss: 2.163287, mean_absolute_error: 0.559263, mean_q: 0.909877, mean_eps: 0.100000\n",
      " 144521/175000: episode: 4045, duration: 0.997s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 65.167 [0.000, 188.000], mean observation: 0.435 [0.000, 96.000], loss: 0.733935, mean_absolute_error: 0.574160, mean_q: 1.009333, mean_eps: 0.100000\n",
      " 144559/175000: episode: 4046, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.421 [0.000, 200.000], mean observation: 0.229 [0.000, 76.000], loss: 0.136111, mean_absolute_error: 0.560064, mean_q: 1.099488, mean_eps: 0.100000\n",
      " 144610/175000: episode: 4047, duration: 1.076s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.824 [36.000, 203.000], mean observation: 0.548 [0.000, 102.000], loss: 0.851209, mean_absolute_error: 0.567529, mean_q: 1.309761, mean_eps: 0.100000\n",
      " 144646/175000: episode: 4048, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.917 [20.000, 204.000], mean observation: 0.306 [0.000, 72.000], loss: 0.619010, mean_absolute_error: 0.567069, mean_q: 1.167463, mean_eps: 0.100000\n",
      " 144683/175000: episode: 4049, duration: 0.708s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 104.568 [71.000, 207.000], mean observation: 0.255 [0.000, 74.000], loss: 0.346745, mean_absolute_error: 0.558908, mean_q: 1.104161, mean_eps: 0.100000\n",
      " 144729/175000: episode: 4050, duration: 0.894s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.304 [25.000, 201.000], mean observation: 0.269 [0.000, 92.000], loss: 0.180927, mean_absolute_error: 0.568302, mean_q: 1.092291, mean_eps: 0.100000\n",
      " 144769/175000: episode: 4051, duration: 0.701s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 75.125 [19.000, 173.000], mean observation: 0.273 [0.000, 80.000], loss: 0.405124, mean_absolute_error: 0.563866, mean_q: 0.967999, mean_eps: 0.100000\n",
      " 144807/175000: episode: 4052, duration: 0.682s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.184 [25.000, 170.000], mean observation: 0.209 [0.000, 76.000], loss: 0.122961, mean_absolute_error: 0.574986, mean_q: 0.969102, mean_eps: 0.100000\n",
      " 144846/175000: episode: 4053, duration: 0.736s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 76.359 [5.000, 177.000], mean observation: 0.214 [0.000, 78.000], loss: 0.302718, mean_absolute_error: 0.581787, mean_q: 1.084818, mean_eps: 0.100000\n",
      " 144878/175000: episode: 4054, duration: 0.739s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 143.906 [55.000, 216.000], mean observation: 0.179 [0.000, 64.000], loss: 0.321158, mean_absolute_error: 0.576404, mean_q: 0.776534, mean_eps: 0.100000\n",
      " 144918/175000: episode: 4055, duration: 0.755s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.750 [55.000, 217.000], mean observation: 0.338 [0.000, 80.000], loss: 1.105594, mean_absolute_error: 0.571267, mean_q: 0.715099, mean_eps: 0.100000\n",
      " 144953/175000: episode: 4056, duration: 0.682s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.914 [5.000, 216.000], mean observation: 0.310 [0.000, 70.000], loss: 0.113841, mean_absolute_error: 0.561892, mean_q: 0.757164, mean_eps: 0.100000\n",
      " 145003/175000: episode: 4057, duration: 0.972s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 70.420 [28.000, 218.000], mean observation: 0.338 [0.000, 100.000], loss: 0.268939, mean_absolute_error: 0.569436, mean_q: 0.768155, mean_eps: 0.100000\n",
      " 145022/175000: episode: 4058, duration: 0.429s, episode steps: 19, steps per second: 44, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 81.421 [14.000, 192.000], mean observation: 0.131 [0.000, 38.000], loss: 0.129507, mean_absolute_error: 0.575197, mean_q: 0.733341, mean_eps: 0.100000\n",
      " 145046/175000: episode: 4059, duration: 0.469s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 26.000 [26.000, 26.000], mean observation: 0.057 [0.000, 48.000], loss: 0.259555, mean_absolute_error: 0.574804, mean_q: 0.699971, mean_eps: 0.100000\n",
      " 145105/175000: episode: 4060, duration: 1.213s, episode steps: 59, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 84.475 [26.000, 214.000], mean observation: 0.336 [0.000, 118.000], loss: 0.237074, mean_absolute_error: 0.579415, mean_q: 0.811929, mean_eps: 0.100000\n",
      " 145135/175000: episode: 4061, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 64.533 [26.000, 192.000], mean observation: 0.149 [0.000, 60.000], loss: 0.620735, mean_absolute_error: 0.587115, mean_q: 0.713546, mean_eps: 0.100000\n",
      " 145163/175000: episode: 4062, duration: 0.530s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.893 [26.000, 206.000], mean observation: 0.176 [0.000, 56.000], loss: 0.155712, mean_absolute_error: 0.593651, mean_q: 0.772826, mean_eps: 0.100000\n",
      " 145192/175000: episode: 4063, duration: 0.595s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.138 [9.000, 206.000], mean observation: 0.230 [0.000, 58.000], loss: 0.311243, mean_absolute_error: 0.586570, mean_q: 0.763813, mean_eps: 0.100000\n",
      " 145238/175000: episode: 4064, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.522 [26.000, 181.000], mean observation: 0.482 [0.000, 92.000], loss: 0.466572, mean_absolute_error: 0.585138, mean_q: 0.803608, mean_eps: 0.100000\n",
      " 145263/175000: episode: 4065, duration: 0.467s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 149.840 [18.000, 211.000], mean observation: 0.126 [0.000, 50.000], loss: 0.136841, mean_absolute_error: 0.583355, mean_q: 0.838639, mean_eps: 0.100000\n",
      " 145297/175000: episode: 4066, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 41.588 [6.000, 213.000], mean observation: 0.140 [0.000, 68.000], loss: 0.297592, mean_absolute_error: 0.586958, mean_q: 0.905423, mean_eps: 0.100000\n",
      " 145345/175000: episode: 4067, duration: 0.880s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.083 [6.000, 201.000], mean observation: 0.237 [0.000, 96.000], loss: 0.616613, mean_absolute_error: 0.593289, mean_q: 0.903110, mean_eps: 0.100000\n",
      " 145384/175000: episode: 4068, duration: 0.733s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 84.051 [1.000, 218.000], mean observation: 0.172 [0.000, 78.000], loss: 0.147787, mean_absolute_error: 0.591382, mean_q: 0.872019, mean_eps: 0.100000\n",
      " 145412/175000: episode: 4069, duration: 0.630s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 95.321 [30.000, 211.000], mean observation: 0.164 [0.000, 56.000], loss: 0.147100, mean_absolute_error: 0.593435, mean_q: 0.753519, mean_eps: 0.100000\n",
      " 145454/175000: episode: 4070, duration: 0.784s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.881 [14.000, 217.000], mean observation: 0.334 [0.000, 84.000], loss: 0.639363, mean_absolute_error: 0.607738, mean_q: 0.798088, mean_eps: 0.100000\n",
      " 145498/175000: episode: 4071, duration: 0.830s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 64.205 [1.000, 129.000], mean observation: 0.340 [0.000, 88.000], loss: 0.497786, mean_absolute_error: 0.613593, mean_q: 0.836408, mean_eps: 0.100000\n",
      " 145542/175000: episode: 4072, duration: 0.792s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.727 [1.000, 215.000], mean observation: 0.574 [0.000, 88.000], loss: 0.165140, mean_absolute_error: 0.625804, mean_q: 0.931249, mean_eps: 0.100000\n",
      " 145587/175000: episode: 4073, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 80.511 [5.000, 201.000], mean observation: 0.430 [0.000, 90.000], loss: 0.196383, mean_absolute_error: 0.636663, mean_q: 0.843938, mean_eps: 0.100000\n",
      " 145619/175000: episode: 4074, duration: 0.597s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 85.406 [6.000, 211.000], mean observation: 0.297 [0.000, 64.000], loss: 78.285240, mean_absolute_error: 0.989428, mean_q: 0.834227, mean_eps: 0.100000\n",
      " 145655/175000: episode: 4075, duration: 0.649s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 173.583 [84.000, 224.000], mean observation: 0.357 [0.000, 72.000], loss: 2.150138, mean_absolute_error: 0.643112, mean_q: 0.722509, mean_eps: 0.100000\n",
      " 145701/175000: episode: 4076, duration: 0.884s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.674 [37.000, 212.000], mean observation: 0.415 [0.000, 92.000], loss: 6119.248976, mean_absolute_error: 28.086010, mean_q: 3.846588, mean_eps: 0.100000\n",
      " 145747/175000: episode: 4077, duration: 0.828s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 155.565 [19.000, 212.000], mean observation: 0.315 [0.000, 92.000], loss: 0.243707, mean_absolute_error: 0.642443, mean_q: 0.983021, mean_eps: 0.100000\n",
      " 145798/175000: episode: 4078, duration: 0.916s, episode steps: 51, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.922 [25.000, 212.000], mean observation: 0.385 [0.000, 102.000], loss: 0.270993, mean_absolute_error: 0.633070, mean_q: 0.883826, mean_eps: 0.100000\n",
      " 145836/175000: episode: 4079, duration: 0.753s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 171.474 [13.000, 212.000], mean observation: 0.318 [0.000, 76.000], loss: 0.282735, mean_absolute_error: 0.629305, mean_q: 0.930410, mean_eps: 0.100000\n",
      " 145871/175000: episode: 4080, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 175.486 [58.000, 212.000], mean observation: 0.257 [0.000, 70.000], loss: 0.291709, mean_absolute_error: 0.619364, mean_q: 1.009250, mean_eps: 0.100000\n",
      " 145912/175000: episode: 4081, duration: 0.859s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 79.268 [3.000, 212.000], mean observation: 0.371 [0.000, 82.000], loss: 1.159675, mean_absolute_error: 0.755200, mean_q: 2.592840, mean_eps: 0.100000\n",
      " 145955/175000: episode: 4082, duration: 0.832s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.465 [42.000, 169.000], mean observation: 0.285 [0.000, 86.000], loss: 2053.373986, mean_absolute_error: 9.862684, mean_q: 2.654721, mean_eps: 0.100000\n",
      " 146000/175000: episode: 4083, duration: 0.901s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 52.378 [42.000, 196.000], mean observation: 0.240 [0.000, 90.000], loss: 0.260463, mean_absolute_error: 0.586556, mean_q: 0.960420, mean_eps: 0.100000\n",
      " 146049/175000: episode: 4084, duration: 0.953s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 92.469 [10.000, 152.000], mean observation: 0.406 [0.000, 98.000], loss: 0.125118, mean_absolute_error: 0.616711, mean_q: 1.364103, mean_eps: 0.100000\n",
      " 146090/175000: episode: 4085, duration: 0.735s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.220 [3.000, 218.000], mean observation: 0.249 [0.000, 82.000], loss: 0.270769, mean_absolute_error: 0.599426, mean_q: 1.335335, mean_eps: 0.100000\n",
      " 146135/175000: episode: 4086, duration: 0.816s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 138.511 [18.000, 195.000], mean observation: 0.336 [0.000, 90.000], loss: 0.271884, mean_absolute_error: 0.607669, mean_q: 1.291801, mean_eps: 0.100000\n",
      " 146170/175000: episode: 4087, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.257 [125.000, 195.000], mean observation: 0.174 [0.000, 70.000], loss: 0.251129, mean_absolute_error: 0.593927, mean_q: 1.167755, mean_eps: 0.100000\n",
      " 146214/175000: episode: 4088, duration: 0.816s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 138.023 [125.000, 195.000], mean observation: 0.266 [0.000, 88.000], loss: 0.142455, mean_absolute_error: 0.615681, mean_q: 1.463954, mean_eps: 0.100000\n",
      " 146265/175000: episode: 4089, duration: 0.942s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 148.392 [15.000, 222.000], mean observation: 0.566 [0.000, 102.000], loss: 0.187644, mean_absolute_error: 0.603091, mean_q: 1.437669, mean_eps: 0.100000\n",
      " 146303/175000: episode: 4090, duration: 0.650s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 179.026 [41.000, 222.000], mean observation: 0.301 [0.000, 76.000], loss: 0.197457, mean_absolute_error: 0.596815, mean_q: 1.409307, mean_eps: 0.100000\n",
      " 146339/175000: episode: 4091, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 188.500 [15.000, 222.000], mean observation: 0.339 [0.000, 72.000], loss: 0.147646, mean_absolute_error: 0.561513, mean_q: 1.046958, mean_eps: 0.100000\n",
      " 146376/175000: episode: 4092, duration: 0.708s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 182.243 [68.000, 222.000], mean observation: 0.444 [0.000, 74.000], loss: 0.199246, mean_absolute_error: 0.584685, mean_q: 1.412385, mean_eps: 0.100000\n",
      " 146405/175000: episode: 4093, duration: 0.567s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 99.034 [17.000, 192.000], mean observation: 0.143 [0.000, 58.000], loss: 0.113763, mean_absolute_error: 0.568890, mean_q: 1.209364, mean_eps: 0.100000\n",
      " 146432/175000: episode: 4094, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.259 [34.000, 195.000], mean observation: 0.196 [0.000, 54.000], loss: 0.114047, mean_absolute_error: 0.580089, mean_q: 1.351993, mean_eps: 0.100000\n",
      " 146459/175000: episode: 4095, duration: 0.499s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 102.407 [5.000, 195.000], mean observation: 0.318 [0.000, 54.000], loss: 0.439772, mean_absolute_error: 0.573686, mean_q: 1.176823, mean_eps: 0.100000\n",
      " 146501/175000: episode: 4096, duration: 0.779s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.714 [34.000, 218.000], mean observation: 0.577 [0.000, 84.000], loss: 0.233672, mean_absolute_error: 0.577162, mean_q: 1.175892, mean_eps: 0.100000\n",
      " 146554/175000: episode: 4097, duration: 0.950s, episode steps: 53, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 69.057 [5.000, 195.000], mean observation: 0.830 [0.000, 106.000], loss: 0.118122, mean_absolute_error: 0.587015, mean_q: 1.209500, mean_eps: 0.100000\n",
      " 146582/175000: episode: 4098, duration: 0.474s, episode steps: 28, steps per second: 59, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 151.679 [7.000, 195.000], mean observation: 0.296 [0.000, 56.000], loss: 0.286787, mean_absolute_error: 0.580343, mean_q: 1.171526, mean_eps: 0.100000\n",
      " 146614/175000: episode: 4099, duration: 0.558s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.594 [13.000, 187.000], mean observation: 0.218 [0.000, 64.000], loss: 0.507097, mean_absolute_error: 0.592934, mean_q: 1.473397, mean_eps: 0.100000\n",
      " 146640/175000: episode: 4100, duration: 0.532s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.038 [14.000, 145.000], mean observation: 0.193 [0.000, 52.000], loss: 1225.094952, mean_absolute_error: 6.304001, mean_q: 4.325211, mean_eps: 0.100000\n",
      " 146663/175000: episode: 4101, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 145.000 [60.000, 184.000], mean observation: 0.111 [0.000, 46.000], loss: 0.153251, mean_absolute_error: 0.612273, mean_q: 1.460182, mean_eps: 0.100000\n",
      " 146696/175000: episode: 4102, duration: 0.633s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 160.303 [57.000, 167.000], mean observation: 0.104 [0.000, 66.000], loss: 1.153347, mean_absolute_error: 0.621051, mean_q: 1.478138, mean_eps: 0.100000\n",
      " 146739/175000: episode: 4103, duration: 0.825s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 112.860 [9.000, 186.000], mean observation: 0.265 [0.000, 86.000], loss: 0.177086, mean_absolute_error: 0.595352, mean_q: 1.149393, mean_eps: 0.100000\n",
      " 146756/175000: episode: 4104, duration: 0.355s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.588 [62.000, 177.000], mean observation: 0.068 [0.000, 34.000], loss: 0.143325, mean_absolute_error: 0.583610, mean_q: 1.206289, mean_eps: 0.100000\n",
      " 146799/175000: episode: 4105, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.372 [0.000, 135.000], mean observation: 0.223 [0.000, 86.000], loss: 7.188903, mean_absolute_error: 0.769248, mean_q: 2.881171, mean_eps: 0.100000\n",
      " 146836/175000: episode: 4106, duration: 0.729s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.351 [6.000, 208.000], mean observation: 0.228 [0.000, 74.000], loss: 1094.841522, mean_absolute_error: 5.631429, mean_q: 3.113339, mean_eps: 0.100000\n",
      " 146867/175000: episode: 4107, duration: 0.611s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 164.129 [68.000, 208.000], mean observation: 0.189 [0.000, 62.000], loss: 2825.750249, mean_absolute_error: 13.337822, mean_q: 3.286199, mean_eps: 0.100000\n",
      " 146903/175000: episode: 4108, duration: 0.683s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 156.250 [24.000, 208.000], mean observation: 0.242 [0.000, 72.000], loss: 0.147426, mean_absolute_error: 0.591358, mean_q: 1.148884, mean_eps: 0.100000\n",
      " 146945/175000: episode: 4109, duration: 0.784s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.143 [24.000, 216.000], mean observation: 0.384 [0.000, 84.000], loss: 0.132188, mean_absolute_error: 0.601790, mean_q: 1.359067, mean_eps: 0.100000\n",
      " 146995/175000: episode: 4110, duration: 0.838s, episode steps: 50, steps per second: 60, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 140.400 [24.000, 224.000], mean observation: 0.626 [0.000, 100.000], loss: 0.527177, mean_absolute_error: 0.622623, mean_q: 1.559269, mean_eps: 0.100000\n",
      " 147025/175000: episode: 4111, duration: 0.573s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 152.600 [31.000, 208.000], mean observation: 0.165 [0.000, 60.000], loss: 0.130769, mean_absolute_error: 0.601458, mean_q: 1.119835, mean_eps: 0.100000\n",
      " 147064/175000: episode: 4112, duration: 0.731s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.205 [34.000, 207.000], mean observation: 0.268 [0.000, 78.000], loss: 0.335536, mean_absolute_error: 0.615751, mean_q: 1.343724, mean_eps: 0.100000\n",
      " 147095/175000: episode: 4113, duration: 0.580s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.484 [14.000, 207.000], mean observation: 0.231 [0.000, 62.000], loss: 0.238487, mean_absolute_error: 0.617844, mean_q: 1.407549, mean_eps: 0.100000\n",
      " 147125/175000: episode: 4114, duration: 0.603s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 150.633 [28.000, 215.000], mean observation: 0.324 [0.000, 60.000], loss: 0.274815, mean_absolute_error: 0.597933, mean_q: 1.198180, mean_eps: 0.100000\n",
      " 147170/175000: episode: 4115, duration: 0.813s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.644 [9.000, 205.000], mean observation: 0.479 [0.000, 90.000], loss: 45.710178, mean_absolute_error: 0.802256, mean_q: 1.277437, mean_eps: 0.100000\n",
      " 147223/175000: episode: 4116, duration: 0.930s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.075 [24.000, 221.000], mean observation: 0.631 [0.000, 106.000], loss: 0.280279, mean_absolute_error: 0.606130, mean_q: 1.318205, mean_eps: 0.100000\n",
      " 147242/175000: episode: 4117, duration: 0.369s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.474 [64.000, 194.000], mean observation: 0.085 [0.000, 38.000], loss: 0.190300, mean_absolute_error: 0.617308, mean_q: 1.239704, mean_eps: 0.100000\n",
      " 147292/175000: episode: 4118, duration: 0.915s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 138.940 [10.000, 201.000], mean observation: 0.689 [0.000, 100.000], loss: 0.099064, mean_absolute_error: 0.616749, mean_q: 1.276469, mean_eps: 0.100000\n",
      " 147333/175000: episode: 4119, duration: 0.817s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 82.659 [14.000, 221.000], mean observation: 0.615 [0.000, 82.000], loss: 0.177466, mean_absolute_error: 0.613589, mean_q: 1.291800, mean_eps: 0.100000\n",
      " 147364/175000: episode: 4120, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 87.581 [27.000, 207.000], mean observation: 0.224 [0.000, 62.000], loss: 0.102084, mean_absolute_error: 0.616911, mean_q: 1.408865, mean_eps: 0.100000\n",
      " 147383/175000: episode: 4121, duration: 0.371s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.053 [27.000, 171.000], mean observation: 0.091 [0.000, 38.000], loss: 0.124602, mean_absolute_error: 0.613721, mean_q: 1.397188, mean_eps: 0.100000\n",
      " 147410/175000: episode: 4122, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 128.444 [9.000, 200.000], mean observation: 0.263 [0.000, 54.000], loss: 0.346755, mean_absolute_error: 0.606482, mean_q: 1.347789, mean_eps: 0.100000\n",
      " 147440/175000: episode: 4123, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.467 [27.000, 200.000], mean observation: 0.302 [0.000, 60.000], loss: 0.148529, mean_absolute_error: 0.585248, mean_q: 1.269019, mean_eps: 0.100000\n",
      " 147488/175000: episode: 4124, duration: 0.960s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 150.854 [24.000, 215.000], mean observation: 0.665 [0.000, 96.000], loss: 0.129060, mean_absolute_error: 0.586272, mean_q: 1.489902, mean_eps: 0.100000\n",
      " 147539/175000: episode: 4125, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.294 [63.000, 215.000], mean observation: 0.594 [0.000, 102.000], loss: 0.354348, mean_absolute_error: 0.594648, mean_q: 1.601697, mean_eps: 0.100000\n",
      " 147557/175000: episode: 4126, duration: 0.372s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 177.444 [50.000, 215.000], mean observation: 0.089 [0.000, 36.000], loss: 0.207285, mean_absolute_error: 0.590461, mean_q: 1.583157, mean_eps: 0.100000\n",
      " 147592/175000: episode: 4127, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 176.486 [0.000, 215.000], mean observation: 0.230 [0.000, 70.000], loss: 0.146607, mean_absolute_error: 0.598664, mean_q: 1.792722, mean_eps: 0.100000\n",
      " 147621/175000: episode: 4128, duration: 0.562s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 153.103 [19.000, 215.000], mean observation: 0.264 [0.000, 58.000], loss: 0.166980, mean_absolute_error: 0.600091, mean_q: 1.772414, mean_eps: 0.100000\n",
      " 147667/175000: episode: 4129, duration: 0.834s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 209.522 [62.000, 215.000], mean observation: 0.106 [0.000, 92.000], loss: 0.159621, mean_absolute_error: 0.605357, mean_q: 1.551799, mean_eps: 0.100000\n",
      " 147703/175000: episode: 4130, duration: 0.639s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 170.583 [42.000, 215.000], mean observation: 0.277 [0.000, 72.000], loss: 0.328454, mean_absolute_error: 0.619147, mean_q: 1.474900, mean_eps: 0.100000\n",
      " 147733/175000: episode: 4131, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 215.000 [215.000, 215.000], mean observation: 0.070 [0.000, 60.000], loss: 0.155152, mean_absolute_error: 0.623226, mean_q: 1.573005, mean_eps: 0.100000\n",
      " 147769/175000: episode: 4132, duration: 0.648s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 203.250 [48.000, 215.000], mean observation: 0.177 [0.000, 72.000], loss: 0.255452, mean_absolute_error: 0.618310, mean_q: 1.544835, mean_eps: 0.100000\n",
      " 147807/175000: episode: 4133, duration: 0.659s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 162.553 [15.000, 218.000], mean observation: 0.205 [0.000, 76.000], loss: 462.543505, mean_absolute_error: 2.674963, mean_q: 1.605528, mean_eps: 0.100000\n",
      " 147842/175000: episode: 4134, duration: 0.696s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 136.543 [36.000, 215.000], mean observation: 0.165 [0.000, 70.000], loss: 0.280516, mean_absolute_error: 0.610759, mean_q: 1.566836, mean_eps: 0.100000\n",
      " 147897/175000: episode: 4135, duration: 0.983s, episode steps: 55, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 93.018 [8.000, 216.000], mean observation: 0.613 [0.000, 110.000], loss: 0.444738, mean_absolute_error: 0.608472, mean_q: 1.402804, mean_eps: 0.100000\n",
      " 147923/175000: episode: 4136, duration: 0.444s, episode steps: 26, steps per second: 59, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 137.500 [37.000, 209.000], mean observation: 0.210 [0.000, 52.000], loss: 0.324565, mean_absolute_error: 0.619682, mean_q: 1.425178, mean_eps: 0.100000\n",
      " 147959/175000: episode: 4137, duration: 0.633s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.528 [30.000, 209.000], mean observation: 0.191 [0.000, 72.000], loss: 0.483094, mean_absolute_error: 0.619543, mean_q: 1.370459, mean_eps: 0.100000\n",
      " 148021/175000: episode: 4138, duration: 1.199s, episode steps: 62, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 66.968 [5.000, 215.000], mean observation: 0.536 [0.000, 124.000], loss: 0.660705, mean_absolute_error: 0.631036, mean_q: 1.426376, mean_eps: 0.100000\n",
      " 148076/175000: episode: 4139, duration: 1.004s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 52.818 [4.000, 211.000], mean observation: 0.173 [0.000, 110.000], loss: 0.499060, mean_absolute_error: 0.632388, mean_q: 1.445378, mean_eps: 0.100000\n",
      " 148114/175000: episode: 4140, duration: 0.726s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 63.974 [4.000, 224.000], mean observation: 0.294 [0.000, 76.000], loss: 0.458274, mean_absolute_error: 0.618455, mean_q: 1.141865, mean_eps: 0.100000\n",
      " 148157/175000: episode: 4141, duration: 0.829s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.023 [4.000, 154.000], mean observation: 0.412 [0.000, 86.000], loss: 532.228182, mean_absolute_error: 3.130700, mean_q: 2.694750, mean_eps: 0.100000\n",
      " 148195/175000: episode: 4142, duration: 0.682s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 47.368 [25.000, 202.000], mean observation: 0.094 [0.000, 76.000], loss: 0.165381, mean_absolute_error: 0.625172, mean_q: 1.152614, mean_eps: 0.100000\n",
      " 148242/175000: episode: 4143, duration: 0.849s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 45.745 [6.000, 175.000], mean observation: 0.192 [0.000, 94.000], loss: 0.223390, mean_absolute_error: 0.620261, mean_q: 1.044323, mean_eps: 0.100000\n",
      " 148298/175000: episode: 4144, duration: 1.042s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 53.768 [41.000, 212.000], mean observation: 0.176 [0.000, 112.000], loss: 2303.142497, mean_absolute_error: 11.072464, mean_q: 3.448914, mean_eps: 0.100000\n",
      " 148349/175000: episode: 4145, duration: 0.917s, episode steps: 51, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 47.667 [14.000, 192.000], mean observation: 0.556 [0.000, 102.000], loss: 1.067616, mean_absolute_error: 0.642324, mean_q: 1.296425, mean_eps: 0.100000\n",
      " 148369/175000: episode: 4146, duration: 0.352s, episode steps: 20, steps per second: 57, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 76.150 [14.000, 148.000], mean observation: 0.129 [0.000, 40.000], loss: 0.187633, mean_absolute_error: 0.638352, mean_q: 1.285352, mean_eps: 0.100000\n",
      " 148404/175000: episode: 4147, duration: 0.641s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 59.571 [16.000, 199.000], mean observation: 0.412 [0.000, 70.000], loss: 0.191236, mean_absolute_error: 0.626749, mean_q: 1.260687, mean_eps: 0.100000\n",
      " 148428/175000: episode: 4148, duration: 0.503s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 106.792 [60.000, 137.000], mean observation: 0.124 [0.000, 48.000], loss: 0.144411, mean_absolute_error: 0.611982, mean_q: 1.055680, mean_eps: 0.100000\n",
      " 148473/175000: episode: 4149, duration: 0.851s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.956 [34.000, 221.000], mean observation: 0.420 [0.000, 90.000], loss: 0.126099, mean_absolute_error: 0.608434, mean_q: 1.166224, mean_eps: 0.100000\n",
      " 148521/175000: episode: 4150, duration: 0.885s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.812 [82.000, 180.000], mean observation: 0.408 [0.000, 96.000], loss: 721.778178, mean_absolute_error: 3.959201, mean_q: 2.711209, mean_eps: 0.100000\n",
      " 148560/175000: episode: 4151, duration: 0.754s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 124.436 [1.000, 195.000], mean observation: 0.356 [0.000, 78.000], loss: 0.137697, mean_absolute_error: 0.625216, mean_q: 1.349752, mean_eps: 0.100000\n",
      " 148606/175000: episode: 4152, duration: 0.833s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 132.565 [4.000, 195.000], mean observation: 0.533 [0.000, 92.000], loss: 0.219671, mean_absolute_error: 0.636004, mean_q: 1.561061, mean_eps: 0.100000\n",
      " 148635/175000: episode: 4153, duration: 0.503s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.103 [83.000, 195.000], mean observation: 0.240 [0.000, 58.000], loss: 0.192957, mean_absolute_error: 0.625100, mean_q: 1.370937, mean_eps: 0.100000\n",
      " 148672/175000: episode: 4154, duration: 0.725s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 79.378 [1.000, 165.000], mean observation: 0.316 [0.000, 74.000], loss: 0.214702, mean_absolute_error: 0.625087, mean_q: 1.344722, mean_eps: 0.100000\n",
      " 148695/175000: episode: 4155, duration: 0.441s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 139.435 [96.000, 215.000], mean observation: 0.158 [0.000, 46.000], loss: 0.463579, mean_absolute_error: 0.619558, mean_q: 1.274923, mean_eps: 0.100000\n",
      " 148728/175000: episode: 4156, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 206.121 [149.000, 209.000], mean observation: 0.138 [0.000, 66.000], loss: 2.051758, mean_absolute_error: 0.626232, mean_q: 1.307208, mean_eps: 0.100000\n",
      " 148773/175000: episode: 4157, duration: 0.876s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.378 [16.000, 217.000], mean observation: 0.334 [0.000, 90.000], loss: 821.189790, mean_absolute_error: 4.395176, mean_q: 2.721210, mean_eps: 0.100000\n",
      " 148808/175000: episode: 4158, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 160.371 [61.000, 220.000], mean observation: 0.258 [0.000, 70.000], loss: 0.265187, mean_absolute_error: 0.633197, mean_q: 1.365691, mean_eps: 0.100000\n",
      " 148849/175000: episode: 4159, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.683 [1.000, 217.000], mean observation: 0.309 [0.000, 82.000], loss: 0.223403, mean_absolute_error: 0.628302, mean_q: 1.391088, mean_eps: 0.100000\n",
      " 148884/175000: episode: 4160, duration: 0.640s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.743 [7.000, 221.000], mean observation: 0.255 [0.000, 70.000], loss: 0.364216, mean_absolute_error: 0.616148, mean_q: 1.451462, mean_eps: 0.100000\n",
      " 148928/175000: episode: 4161, duration: 0.884s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 167.000 [83.000, 221.000], mean observation: 0.317 [0.000, 88.000], loss: 0.198541, mean_absolute_error: 0.608137, mean_q: 1.360820, mean_eps: 0.100000\n",
      " 148963/175000: episode: 4162, duration: 0.729s, episode steps: 35, steps per second: 48, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 154.829 [23.000, 217.000], mean observation: 0.369 [0.000, 69.000], loss: 0.185701, mean_absolute_error: 0.629883, mean_q: 1.699022, mean_eps: 0.100000\n",
      " 148981/175000: episode: 4163, duration: 0.348s, episode steps: 18, steps per second: 52, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 142.778 [83.000, 221.000], mean observation: 0.083 [0.000, 36.000], loss: 99.954525, mean_absolute_error: 1.069960, mean_q: 1.717636, mean_eps: 0.100000\n",
      " 149038/175000: episode: 4164, duration: 1.011s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 176.825 [24.000, 221.000], mean observation: 0.320 [0.000, 114.000], loss: 155.241896, mean_absolute_error: 1.298417, mean_q: 1.532675, mean_eps: 0.100000\n",
      " 149092/175000: episode: 4165, duration: 1.112s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 117.056 [2.000, 215.000], mean observation: 0.290 [0.000, 108.000], loss: 171.265351, mean_absolute_error: 1.476059, mean_q: 2.667271, mean_eps: 0.100000\n",
      " 149128/175000: episode: 4166, duration: 0.741s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 84.444 [24.000, 205.000], mean observation: 0.203 [0.000, 72.000], loss: 0.332411, mean_absolute_error: 0.594728, mean_q: 1.139201, mean_eps: 0.100000\n",
      " 149171/175000: episode: 4167, duration: 0.836s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.837 [24.000, 207.000], mean observation: 0.286 [0.000, 86.000], loss: 0.190175, mean_absolute_error: 0.601948, mean_q: 1.162104, mean_eps: 0.100000\n",
      " 149207/175000: episode: 4168, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 57.194 [12.000, 144.000], mean observation: 0.254 [0.000, 72.000], loss: 0.171596, mean_absolute_error: 0.602176, mean_q: 1.079029, mean_eps: 0.100000\n",
      " 149252/175000: episode: 4169, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 66.222 [16.000, 195.000], mean observation: 0.343 [0.000, 90.000], loss: 5.202627, mean_absolute_error: 0.623986, mean_q: 1.154453, mean_eps: 0.100000\n",
      " 149272/175000: episode: 4170, duration: 0.426s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 78.100 [15.000, 195.000], mean observation: 0.172 [0.000, 40.000], loss: 0.597802, mean_absolute_error: 0.597678, mean_q: 1.221012, mean_eps: 0.100000\n",
      " 149322/175000: episode: 4171, duration: 0.982s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.720 [15.000, 217.000], mean observation: 0.563 [0.000, 100.000], loss: 49.380482, mean_absolute_error: 0.801251, mean_q: 1.310944, mean_eps: 0.100000\n",
      " 149362/175000: episode: 4172, duration: 0.713s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 123.675 [10.000, 199.000], mean observation: 0.453 [0.000, 80.000], loss: 64.279992, mean_absolute_error: 0.865408, mean_q: 1.160220, mean_eps: 0.100000\n",
      " 149395/175000: episode: 4173, duration: 0.588s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.000 [1.000, 210.000], mean observation: 0.424 [0.000, 66.000], loss: 0.347099, mean_absolute_error: 0.607774, mean_q: 1.067350, mean_eps: 0.100000\n",
      " 149436/175000: episode: 4174, duration: 0.791s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 146.902 [42.000, 204.000], mean observation: 0.420 [0.000, 82.000], loss: 0.175205, mean_absolute_error: 0.585185, mean_q: 0.961223, mean_eps: 0.100000\n",
      " 149494/175000: episode: 4175, duration: 1.069s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 104.241 [1.000, 214.000], mean observation: 0.637 [0.000, 116.000], loss: 0.120938, mean_absolute_error: 0.576489, mean_q: 0.885666, mean_eps: 0.100000\n",
      " 149546/175000: episode: 4176, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.519 [40.000, 224.000], mean observation: 0.300 [0.000, 104.000], loss: 0.372586, mean_absolute_error: 0.582242, mean_q: 0.907746, mean_eps: 0.100000\n",
      " 149576/175000: episode: 4177, duration: 0.612s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 76.000 [6.000, 207.000], mean observation: 0.333 [0.000, 60.000], loss: 0.190161, mean_absolute_error: 0.595266, mean_q: 1.197628, mean_eps: 0.100000\n",
      " 149619/175000: episode: 4178, duration: 0.790s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.698 [6.000, 216.000], mean observation: 0.454 [0.000, 86.000], loss: 0.140347, mean_absolute_error: 0.596061, mean_q: 0.934447, mean_eps: 0.100000\n",
      " 149653/175000: episode: 4179, duration: 0.645s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.794 [7.000, 195.000], mean observation: 0.254 [0.000, 68.000], loss: 2161.980505, mean_absolute_error: 10.387258, mean_q: 2.788932, mean_eps: 0.100000\n",
      " 149701/175000: episode: 4180, duration: 0.870s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 166.625 [44.000, 195.000], mean observation: 0.514 [0.000, 96.000], loss: 0.171739, mean_absolute_error: 0.588827, mean_q: 0.752223, mean_eps: 0.100000\n",
      " 149736/175000: episode: 4181, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.800 [24.000, 199.000], mean observation: 0.385 [0.000, 70.000], loss: 0.184489, mean_absolute_error: 0.585889, mean_q: 0.707686, mean_eps: 0.100000\n",
      " 149781/175000: episode: 4182, duration: 0.871s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 159.600 [2.000, 215.000], mean observation: 0.648 [0.000, 90.000], loss: 1.082807, mean_absolute_error: 0.584039, mean_q: 0.663828, mean_eps: 0.100000\n",
      " 149809/175000: episode: 4183, duration: 0.547s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 143.143 [14.000, 215.000], mean observation: 0.303 [0.000, 56.000], loss: 0.219265, mean_absolute_error: 0.586694, mean_q: 0.710942, mean_eps: 0.100000\n",
      " 149837/175000: episode: 4184, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 76.250 [1.000, 215.000], mean observation: 0.262 [0.000, 56.000], loss: 0.281440, mean_absolute_error: 0.583234, mean_q: 0.707536, mean_eps: 0.100000\n",
      " 149872/175000: episode: 4185, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.229 [13.000, 215.000], mean observation: 0.427 [0.000, 70.000], loss: 2168.624249, mean_absolute_error: 10.420104, mean_q: 3.105643, mean_eps: 0.100000\n",
      " 149892/175000: episode: 4186, duration: 0.416s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 62.550 [58.000, 149.000], mean observation: 0.051 [0.000, 40.000], loss: 0.131852, mean_absolute_error: 0.571046, mean_q: 0.809624, mean_eps: 0.100000\n",
      " 149923/175000: episode: 4187, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 84.290 [14.000, 188.000], mean observation: 0.339 [0.000, 62.000], loss: 0.356079, mean_absolute_error: 0.566959, mean_q: 0.882364, mean_eps: 0.100000\n",
      " 149957/175000: episode: 4188, duration: 0.643s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.824 [24.000, 222.000], mean observation: 0.545 [0.000, 68.000], loss: 0.191615, mean_absolute_error: 0.572470, mean_q: 0.806354, mean_eps: 0.100000\n",
      " 149997/175000: episode: 4189, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.025 [17.000, 187.000], mean observation: 0.533 [0.000, 80.000], loss: 0.168859, mean_absolute_error: 0.583726, mean_q: 0.903773, mean_eps: 0.100000\n",
      " 150037/175000: episode: 4190, duration: 0.804s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 119.275 [9.000, 224.000], mean observation: 0.448 [0.000, 80.000], loss: 1.649204, mean_absolute_error: 0.606333, mean_q: 1.029633, mean_eps: 0.100000\n",
      " 150071/175000: episode: 4191, duration: 0.593s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.088 [14.000, 187.000], mean observation: 0.382 [0.000, 68.000], loss: 0.357850, mean_absolute_error: 0.596867, mean_q: 0.927064, mean_eps: 0.100000\n",
      " 150127/175000: episode: 4192, duration: 1.021s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 98.768 [0.000, 187.000], mean observation: 0.603 [0.000, 112.000], loss: 0.197161, mean_absolute_error: 0.573491, mean_q: 0.970422, mean_eps: 0.100000\n",
      " 150162/175000: episode: 4193, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.171 [2.000, 224.000], mean observation: 0.294 [0.000, 70.000], loss: 0.177633, mean_absolute_error: 0.562548, mean_q: 1.077116, mean_eps: 0.100000\n",
      " 150210/175000: episode: 4194, duration: 0.859s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.104 [30.000, 209.000], mean observation: 0.467 [0.000, 96.000], loss: 3.640364, mean_absolute_error: 0.570188, mean_q: 1.072328, mean_eps: 0.100000\n",
      " 150255/175000: episode: 4195, duration: 0.826s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.067 [27.000, 202.000], mean observation: 0.699 [0.000, 90.000], loss: 0.187775, mean_absolute_error: 0.546538, mean_q: 1.224842, mean_eps: 0.100000\n",
      " 150305/175000: episode: 4196, duration: 0.906s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.200 [101.000, 195.000], mean observation: 0.472 [0.000, 100.000], loss: 0.114838, mean_absolute_error: 0.539881, mean_q: 1.080189, mean_eps: 0.100000\n",
      " 150341/175000: episode: 4197, duration: 0.643s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.528 [21.000, 218.000], mean observation: 0.490 [0.000, 72.000], loss: 0.158287, mean_absolute_error: 0.558351, mean_q: 1.062462, mean_eps: 0.100000\n",
      " 150397/175000: episode: 4198, duration: 1.034s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 77.161 [9.000, 217.000], mean observation: 0.637 [0.000, 112.000], loss: 0.167557, mean_absolute_error: 0.556125, mean_q: 1.058801, mean_eps: 0.100000\n",
      " 150427/175000: episode: 4199, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 80.567 [11.000, 174.000], mean observation: 0.171 [0.000, 60.000], loss: 0.233523, mean_absolute_error: 0.563475, mean_q: 1.080642, mean_eps: 0.100000\n",
      " 150449/175000: episode: 4200, duration: 0.442s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 89.318 [11.000, 209.000], mean observation: 0.125 [0.000, 44.000], loss: 0.298009, mean_absolute_error: 0.573591, mean_q: 1.132542, mean_eps: 0.100000\n",
      " 150490/175000: episode: 4201, duration: 0.736s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 103.146 [5.000, 195.000], mean observation: 0.345 [0.000, 82.000], loss: 1008.395476, mean_absolute_error: 5.224938, mean_q: 2.733295, mean_eps: 0.100000\n",
      " 150506/175000: episode: 4202, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 115.875 [93.000, 179.000], mean observation: 0.087 [0.000, 32.000], loss: 0.098027, mean_absolute_error: 0.593935, mean_q: 0.737472, mean_eps: 0.100000\n",
      " 150531/175000: episode: 4203, duration: 0.443s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 79.640 [2.000, 190.000], mean observation: 0.198 [0.000, 50.000], loss: 0.182746, mean_absolute_error: 0.596139, mean_q: 0.717125, mean_eps: 0.100000\n",
      " 150554/175000: episode: 4204, duration: 0.443s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 100.217 [14.000, 216.000], mean observation: 0.127 [0.000, 46.000], loss: 0.094425, mean_absolute_error: 0.585376, mean_q: 0.713451, mean_eps: 0.100000\n",
      " 150595/175000: episode: 4205, duration: 0.723s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.537 [18.000, 208.000], mean observation: 0.521 [0.000, 82.000], loss: 1458.277414, mean_absolute_error: 7.228713, mean_q: 2.586161, mean_eps: 0.100000\n",
      " 150635/175000: episode: 4206, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.975 [13.000, 215.000], mean observation: 0.387 [0.000, 80.000], loss: 4.405986, mean_absolute_error: 0.591127, mean_q: 0.742325, mean_eps: 0.100000\n",
      " 150683/175000: episode: 4207, duration: 0.871s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 88.667 [8.000, 211.000], mean observation: 0.358 [0.000, 96.000], loss: 0.112207, mean_absolute_error: 0.565996, mean_q: 0.745149, mean_eps: 0.100000\n",
      " 150721/175000: episode: 4208, duration: 0.722s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 102.684 [24.000, 195.000], mean observation: 0.267 [0.000, 76.000], loss: 0.084263, mean_absolute_error: 0.570285, mean_q: 0.675228, mean_eps: 0.100000\n",
      " 150743/175000: episode: 4209, duration: 0.388s, episode steps: 22, steps per second: 57, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.909 [18.000, 203.000], mean observation: 0.152 [0.000, 44.000], loss: 0.152106, mean_absolute_error: 0.565564, mean_q: 0.737298, mean_eps: 0.100000\n",
      " 150770/175000: episode: 4210, duration: 0.535s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 87.222 [24.000, 208.000], mean observation: 0.184 [0.000, 54.000], loss: 12.774336, mean_absolute_error: 0.613936, mean_q: 0.759450, mean_eps: 0.100000\n",
      " 150800/175000: episode: 4211, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.167 [7.000, 212.000], mean observation: 0.355 [0.000, 60.000], loss: 0.294618, mean_absolute_error: 0.547932, mean_q: 0.829800, mean_eps: 0.100000\n",
      " 150816/175000: episode: 4212, duration: 0.386s, episode steps: 16, steps per second: 41, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 83.500 [59.000, 208.000], mean observation: 0.041 [0.000, 32.000], loss: 0.336549, mean_absolute_error: 0.543753, mean_q: 0.923120, mean_eps: 0.100000\n",
      " 150860/175000: episode: 4213, duration: 0.889s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 165.295 [7.000, 208.000], mean observation: 0.403 [0.000, 88.000], loss: 7.138238, mean_absolute_error: 0.569568, mean_q: 0.896409, mean_eps: 0.100000\n",
      " 150911/175000: episode: 4214, duration: 0.941s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 164.824 [39.000, 208.000], mean observation: 0.485 [0.000, 102.000], loss: 0.155331, mean_absolute_error: 0.530037, mean_q: 0.920363, mean_eps: 0.100000\n",
      " 150957/175000: episode: 4215, duration: 0.858s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 161.065 [12.000, 208.000], mean observation: 0.353 [0.000, 92.000], loss: 1963.978252, mean_absolute_error: 9.387741, mean_q: 2.381502, mean_eps: 0.100000\n",
      " 151004/175000: episode: 4216, duration: 0.886s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.000 [12.000, 208.000], mean observation: 0.444 [0.000, 94.000], loss: 6.603464, mean_absolute_error: 0.558198, mean_q: 0.865212, mean_eps: 0.100000\n",
      " 151047/175000: episode: 4217, duration: 0.826s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.884 [12.000, 208.000], mean observation: 0.437 [0.000, 86.000], loss: 0.154603, mean_absolute_error: 0.528405, mean_q: 0.886618, mean_eps: 0.100000\n",
      " 151092/175000: episode: 4218, duration: 0.850s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.267 [12.000, 220.000], mean observation: 0.589 [0.000, 90.000], loss: 1.142736, mean_absolute_error: 0.537290, mean_q: 0.820429, mean_eps: 0.100000\n",
      " 151124/175000: episode: 4219, duration: 0.685s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 113.375 [1.000, 205.000], mean observation: 0.346 [0.000, 64.000], loss: 0.100400, mean_absolute_error: 0.534690, mean_q: 0.815732, mean_eps: 0.100000\n",
      " 151152/175000: episode: 4220, duration: 0.555s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 108.964 [27.000, 190.000], mean observation: 0.229 [0.000, 56.000], loss: 0.114605, mean_absolute_error: 0.543545, mean_q: 0.928226, mean_eps: 0.100000\n",
      " 151194/175000: episode: 4221, duration: 0.785s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.167 [3.000, 208.000], mean observation: 0.531 [0.000, 84.000], loss: 0.118102, mean_absolute_error: 0.550381, mean_q: 0.961681, mean_eps: 0.100000\n",
      " 151223/175000: episode: 4222, duration: 0.560s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 117.759 [27.000, 171.000], mean observation: 0.168 [0.000, 58.000], loss: 0.165946, mean_absolute_error: 0.556390, mean_q: 0.848529, mean_eps: 0.100000\n",
      " 151260/175000: episode: 4223, duration: 0.717s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 98.459 [1.000, 210.000], mean observation: 0.326 [0.000, 74.000], loss: 0.102666, mean_absolute_error: 0.556131, mean_q: 0.785526, mean_eps: 0.100000\n",
      " 151282/175000: episode: 4224, duration: 0.461s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 148.091 [7.000, 219.000], mean observation: 0.087 [0.000, 44.000], loss: 0.091763, mean_absolute_error: 0.553776, mean_q: 0.751403, mean_eps: 0.100000\n",
      " 151311/175000: episode: 4225, duration: 0.531s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 166.793 [165.000, 217.000], mean observation: 0.071 [0.000, 58.000], loss: 0.138032, mean_absolute_error: 0.551787, mean_q: 0.800995, mean_eps: 0.100000\n",
      " 151342/175000: episode: 4226, duration: 0.585s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.677 [8.000, 215.000], mean observation: 0.224 [0.000, 62.000], loss: 3266.528348, mean_absolute_error: 15.481023, mean_q: 5.490138, mean_eps: 0.100000\n",
      " 151382/175000: episode: 4227, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.000 [12.000, 215.000], mean observation: 0.382 [0.000, 80.000], loss: 0.093641, mean_absolute_error: 0.561390, mean_q: 0.740128, mean_eps: 0.100000\n",
      " 151416/175000: episode: 4228, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 169.824 [57.000, 215.000], mean observation: 0.179 [0.000, 68.000], loss: 2595.241186, mean_absolute_error: 12.301873, mean_q: 3.053256, mean_eps: 0.100000\n",
      " 151446/175000: episode: 4229, duration: 0.560s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 169.067 [16.000, 215.000], mean observation: 0.168 [0.000, 60.000], loss: 0.426996, mean_absolute_error: 0.549884, mean_q: 0.778284, mean_eps: 0.100000\n",
      " 151492/175000: episode: 4230, duration: 0.882s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.848 [38.000, 215.000], mean observation: 0.540 [0.000, 92.000], loss: 0.326814, mean_absolute_error: 0.554297, mean_q: 0.887431, mean_eps: 0.100000\n",
      " 151546/175000: episode: 4231, duration: 1.035s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 153.389 [23.000, 215.000], mean observation: 0.555 [0.000, 108.000], loss: 0.099772, mean_absolute_error: 0.557104, mean_q: 1.049825, mean_eps: 0.100000\n",
      " 151583/175000: episode: 4232, duration: 0.679s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 112.081 [26.000, 188.000], mean observation: 0.329 [0.000, 74.000], loss: 18.243728, mean_absolute_error: 0.618066, mean_q: 0.978470, mean_eps: 0.100000\n",
      " 151632/175000: episode: 4233, duration: 0.910s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.551 [1.000, 197.000], mean observation: 0.482 [0.000, 98.000], loss: 935.148257, mean_absolute_error: 4.833339, mean_q: 2.461447, mean_eps: 0.100000\n",
      " 151657/175000: episode: 4234, duration: 0.533s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 145.680 [38.000, 201.000], mean observation: 0.153 [0.000, 50.000], loss: 1159.040643, mean_absolute_error: 5.923987, mean_q: 3.569337, mean_eps: 0.100000\n",
      " 151702/175000: episode: 4235, duration: 0.845s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.267 [23.000, 195.000], mean observation: 0.308 [0.000, 90.000], loss: 5.243941, mean_absolute_error: 0.560797, mean_q: 0.957154, mean_eps: 0.100000\n",
      " 151729/175000: episode: 4236, duration: 0.521s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 114.667 [38.000, 195.000], mean observation: 0.231 [0.000, 54.000], loss: 2562.573754, mean_absolute_error: 12.151658, mean_q: 3.574658, mean_eps: 0.100000\n",
      " 151773/175000: episode: 4237, duration: 0.811s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.364 [29.000, 207.000], mean observation: 0.219 [0.000, 88.000], loss: 12.518240, mean_absolute_error: 0.589378, mean_q: 1.070775, mean_eps: 0.100000\n",
      " 151801/175000: episode: 4238, duration: 0.536s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 94.250 [2.000, 211.000], mean observation: 0.201 [0.000, 56.000], loss: 1387.734512, mean_absolute_error: 6.933624, mean_q: 3.556767, mean_eps: 0.100000\n",
      " 151816/175000: episode: 4239, duration: 0.293s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 133.533 [54.000, 215.000], mean observation: 0.088 [0.000, 30.000], loss: 0.179186, mean_absolute_error: 0.534644, mean_q: 0.991612, mean_eps: 0.100000\n",
      " 151863/175000: episode: 4240, duration: 0.869s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.043 [38.000, 139.000], mean observation: 0.385 [0.000, 94.000], loss: 10.387693, mean_absolute_error: 0.577286, mean_q: 0.817775, mean_eps: 0.100000\n",
      " 151913/175000: episode: 4241, duration: 0.925s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.280 [17.000, 172.000], mean observation: 0.365 [0.000, 100.000], loss: 0.405084, mean_absolute_error: 0.529323, mean_q: 0.793167, mean_eps: 0.100000\n",
      " 151939/175000: episode: 4242, duration: 0.451s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.923 [36.000, 139.000], mean observation: 0.073 [0.000, 52.000], loss: 0.074148, mean_absolute_error: 0.524519, mean_q: 0.882659, mean_eps: 0.100000\n",
      " 151973/175000: episode: 4243, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.353 [99.000, 193.000], mean observation: 0.104 [0.000, 68.000], loss: 30.841001, mean_absolute_error: 0.668908, mean_q: 0.879364, mean_eps: 0.100000\n",
      " 151988/175000: episode: 4244, duration: 0.281s, episode steps: 15, steps per second: 53, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 117.067 [38.000, 208.000], mean observation: 0.090 [0.000, 30.000], loss: 0.104748, mean_absolute_error: 0.545864, mean_q: 0.863287, mean_eps: 0.100000\n",
      " 152019/175000: episode: 4245, duration: 0.598s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.194 [38.000, 208.000], mean observation: 0.192 [0.000, 62.000], loss: 0.100486, mean_absolute_error: 0.530543, mean_q: 0.855320, mean_eps: 0.100000\n",
      " 152047/175000: episode: 4246, duration: 0.493s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 113.214 [38.000, 214.000], mean observation: 0.119 [0.000, 56.000], loss: 0.166313, mean_absolute_error: 0.537956, mean_q: 0.869922, mean_eps: 0.100000\n",
      " 152087/175000: episode: 4247, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 106.275 [8.000, 208.000], mean observation: 0.336 [0.000, 80.000], loss: 0.227987, mean_absolute_error: 0.537924, mean_q: 0.853042, mean_eps: 0.100000\n",
      " 152136/175000: episode: 4248, duration: 0.917s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 117.102 [27.000, 208.000], mean observation: 0.525 [0.000, 98.000], loss: 6.052278, mean_absolute_error: 0.551174, mean_q: 0.817225, mean_eps: 0.100000\n",
      " 152169/175000: episode: 4249, duration: 0.654s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 114.303 [50.000, 175.000], mean observation: 0.224 [0.000, 66.000], loss: 0.134819, mean_absolute_error: 0.526120, mean_q: 0.838528, mean_eps: 0.100000\n",
      " 152204/175000: episode: 4250, duration: 0.637s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 88.143 [6.000, 224.000], mean observation: 0.264 [0.000, 70.000], loss: 1.573298, mean_absolute_error: 0.548684, mean_q: 0.918443, mean_eps: 0.100000\n",
      " 152242/175000: episode: 4251, duration: 0.716s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 86.789 [38.000, 207.000], mean observation: 0.186 [0.000, 76.000], loss: 0.122234, mean_absolute_error: 0.530216, mean_q: 0.815672, mean_eps: 0.100000\n",
      " 152268/175000: episode: 4252, duration: 0.494s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 60.385 [37.000, 127.000], mean observation: 0.092 [0.000, 52.000], loss: 0.107350, mean_absolute_error: 0.549592, mean_q: 0.831861, mean_eps: 0.100000\n",
      " 152297/175000: episode: 4253, duration: 0.568s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 77.103 [38.000, 161.000], mean observation: 0.122 [0.000, 58.000], loss: 0.357468, mean_absolute_error: 0.540606, mean_q: 0.812610, mean_eps: 0.100000\n",
      " 152330/175000: episode: 4254, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 71.030 [31.000, 184.000], mean observation: 0.234 [0.000, 66.000], loss: 10.471707, mean_absolute_error: 0.577556, mean_q: 0.844215, mean_eps: 0.100000\n",
      " 152353/175000: episode: 4255, duration: 0.450s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 68.870 [16.000, 152.000], mean observation: 0.162 [0.000, 46.000], loss: 0.870949, mean_absolute_error: 0.535763, mean_q: 0.867337, mean_eps: 0.100000\n",
      " 152367/175000: episode: 4256, duration: 0.240s, episode steps: 14, steps per second: 58, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 104.643 [38.000, 152.000], mean observation: 0.074 [0.000, 28.000], loss: 0.084743, mean_absolute_error: 0.531794, mean_q: 0.753774, mean_eps: 0.100000\n",
      " 152419/175000: episode: 4257, duration: 0.920s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 101.135 [9.000, 215.000], mean observation: 0.582 [0.000, 104.000], loss: 1.487185, mean_absolute_error: 0.533037, mean_q: 0.774998, mean_eps: 0.100000\n",
      " 152473/175000: episode: 4258, duration: 1.019s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 73.056 [1.000, 176.000], mean observation: 0.428 [0.000, 108.000], loss: 0.078168, mean_absolute_error: 0.526589, mean_q: 0.808656, mean_eps: 0.100000\n",
      " 152508/175000: episode: 4259, duration: 0.658s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 25.314 [12.000, 72.000], mean observation: 0.128 [0.000, 70.000], loss: 0.100043, mean_absolute_error: 0.528668, mean_q: 0.791082, mean_eps: 0.100000\n",
      " 152524/175000: episode: 4260, duration: 0.370s, episode steps: 16, steps per second: 43, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 82.062 [12.000, 187.000], mean observation: 0.113 [0.000, 32.000], loss: 0.093930, mean_absolute_error: 0.532404, mean_q: 0.796067, mean_eps: 0.100000\n",
      " 152566/175000: episode: 4261, duration: 0.762s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 70.310 [8.000, 220.000], mean observation: 0.484 [0.000, 84.000], loss: 0.164295, mean_absolute_error: 0.530456, mean_q: 0.748120, mean_eps: 0.100000\n",
      " 152603/175000: episode: 4262, duration: 0.652s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.946 [12.000, 222.000], mean observation: 0.344 [0.000, 74.000], loss: 0.131900, mean_absolute_error: 0.542790, mean_q: 0.709908, mean_eps: 0.100000\n",
      " 152627/175000: episode: 4263, duration: 0.443s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 55.500 [12.000, 67.000], mean observation: 0.080 [0.000, 48.000], loss: 0.128319, mean_absolute_error: 0.558155, mean_q: 0.776090, mean_eps: 0.100000\n",
      " 152658/175000: episode: 4264, duration: 0.581s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.161 [12.000, 222.000], mean observation: 0.215 [0.000, 62.000], loss: 0.581557, mean_absolute_error: 0.568577, mean_q: 0.686311, mean_eps: 0.100000\n",
      " 152706/175000: episode: 4265, duration: 0.870s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 88.354 [12.000, 215.000], mean observation: 0.480 [0.000, 96.000], loss: 11.486358, mean_absolute_error: 0.631001, mean_q: 0.607424, mean_eps: 0.100000\n",
      " 152735/175000: episode: 4266, duration: 0.510s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 152.793 [48.000, 224.000], mean observation: 0.317 [0.000, 58.000], loss: 11.483722, mean_absolute_error: 0.630848, mean_q: 0.641971, mean_eps: 0.100000\n",
      " 152772/175000: episode: 4267, duration: 0.724s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 86.243 [12.000, 169.000], mean observation: 0.295 [0.000, 74.000], loss: 3134.748970, mean_absolute_error: 14.680477, mean_q: 2.820527, mean_eps: 0.100000\n",
      " 152805/175000: episode: 4268, duration: 0.682s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.182 [8.000, 183.000], mean observation: 0.204 [0.000, 66.000], loss: 0.201945, mean_absolute_error: 0.558278, mean_q: 0.725856, mean_eps: 0.100000\n",
      " 152840/175000: episode: 4269, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.343 [30.000, 208.000], mean observation: 0.266 [0.000, 70.000], loss: 0.088284, mean_absolute_error: 0.558159, mean_q: 0.720266, mean_eps: 0.100000\n",
      " 152866/175000: episode: 4270, duration: 0.491s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 119.077 [42.000, 173.000], mean observation: 0.180 [0.000, 52.000], loss: 0.124477, mean_absolute_error: 0.560145, mean_q: 0.730616, mean_eps: 0.100000\n",
      " 152914/175000: episode: 4271, duration: 0.882s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.625 [18.000, 213.000], mean observation: 0.576 [0.000, 96.000], loss: 8.533800, mean_absolute_error: 0.597422, mean_q: 0.752497, mean_eps: 0.100000\n",
      " 152949/175000: episode: 4272, duration: 0.674s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 81.400 [3.000, 169.000], mean observation: 0.360 [0.000, 70.000], loss: 2116.330644, mean_absolute_error: 10.143373, mean_q: 2.991743, mean_eps: 0.100000\n",
      " 152986/175000: episode: 4273, duration: 0.667s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.405 [14.000, 188.000], mean observation: 0.318 [0.000, 74.000], loss: 0.112007, mean_absolute_error: 0.553756, mean_q: 0.768089, mean_eps: 0.100000\n",
      " 153006/175000: episode: 4274, duration: 0.360s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 100.700 [42.000, 169.000], mean observation: 0.142 [0.000, 40.000], loss: 0.133586, mean_absolute_error: 0.557298, mean_q: 0.731701, mean_eps: 0.100000\n",
      " 153053/175000: episode: 4275, duration: 0.880s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 73.149 [18.000, 195.000], mean observation: 0.378 [0.000, 94.000], loss: 14.120002, mean_absolute_error: 0.620002, mean_q: 0.695239, mean_eps: 0.100000\n",
      " 153071/175000: episode: 4276, duration: 0.314s, episode steps: 18, steps per second: 57, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 119.222 [38.000, 195.000], mean observation: 0.155 [0.000, 36.000], loss: 0.115567, mean_absolute_error: 0.562146, mean_q: 0.872842, mean_eps: 0.100000\n",
      " 153103/175000: episode: 4277, duration: 0.606s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 126.719 [42.000, 207.000], mean observation: 0.341 [0.000, 64.000], loss: 0.191121, mean_absolute_error: 0.554047, mean_q: 0.791530, mean_eps: 0.100000\n",
      " 153148/175000: episode: 4278, duration: 0.842s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.200 [24.000, 208.000], mean observation: 0.541 [0.000, 90.000], loss: 0.143878, mean_absolute_error: 0.559856, mean_q: 0.795731, mean_eps: 0.100000\n",
      " 153186/175000: episode: 4279, duration: 0.731s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.842 [17.000, 218.000], mean observation: 0.377 [0.000, 76.000], loss: 0.136735, mean_absolute_error: 0.580950, mean_q: 0.839929, mean_eps: 0.100000\n",
      " 153207/175000: episode: 4280, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 126.190 [25.000, 169.000], mean observation: 0.194 [0.000, 42.000], loss: 3.141812, mean_absolute_error: 0.610685, mean_q: 0.823194, mean_eps: 0.100000\n",
      " 153242/175000: episode: 4281, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.057 [61.000, 169.000], mean observation: 0.301 [0.000, 70.000], loss: 0.111625, mean_absolute_error: 0.592832, mean_q: 0.783162, mean_eps: 0.100000\n",
      " 153263/175000: episode: 4282, duration: 0.385s, episode steps: 21, steps per second: 55, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.333 [26.000, 199.000], mean observation: 0.163 [0.000, 42.000], loss: 3676.127850, mean_absolute_error: 17.251726, mean_q: 4.570558, mean_eps: 0.100000\n",
      " 153283/175000: episode: 4283, duration: 0.377s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 128.400 [2.000, 173.000], mean observation: 0.138 [0.000, 40.000], loss: 0.208824, mean_absolute_error: 0.570331, mean_q: 0.582190, mean_eps: 0.100000\n",
      " 153330/175000: episode: 4284, duration: 0.898s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.957 [48.000, 216.000], mean observation: 0.489 [0.000, 94.000], loss: 0.107705, mean_absolute_error: 0.551268, mean_q: 0.568005, mean_eps: 0.100000\n",
      " 153353/175000: episode: 4285, duration: 0.438s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 111.391 [21.000, 179.000], mean observation: 0.220 [0.000, 46.000], loss: 0.202926, mean_absolute_error: 0.548830, mean_q: 0.719773, mean_eps: 0.100000\n",
      " 153396/175000: episode: 4286, duration: 0.825s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.186 [26.000, 216.000], mean observation: 0.505 [0.000, 86.000], loss: 971.544998, mean_absolute_error: 5.049860, mean_q: 2.626567, mean_eps: 0.100000\n",
      " 153434/175000: episode: 4287, duration: 0.747s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.658 [10.000, 202.000], mean observation: 0.424 [0.000, 76.000], loss: 0.125977, mean_absolute_error: 0.588373, mean_q: 0.907341, mean_eps: 0.100000\n",
      " 153470/175000: episode: 4288, duration: 0.662s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.222 [50.000, 202.000], mean observation: 0.417 [0.000, 72.000], loss: 0.229029, mean_absolute_error: 0.592157, mean_q: 0.804263, mean_eps: 0.100000\n",
      " 153499/175000: episode: 4289, duration: 0.521s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.517 [45.000, 193.000], mean observation: 0.272 [0.000, 58.000], loss: 19.545294, mean_absolute_error: 0.692105, mean_q: 0.774138, mean_eps: 0.100000\n",
      " 153550/175000: episode: 4290, duration: 0.939s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 149.824 [55.000, 202.000], mean observation: 0.588 [0.000, 102.000], loss: 4.518972, mean_absolute_error: 0.627847, mean_q: 0.904958, mean_eps: 0.100000\n",
      " 153565/175000: episode: 4291, duration: 0.283s, episode steps: 15, steps per second: 53, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 102.133 [50.000, 148.000], mean observation: 0.115 [0.000, 30.000], loss: 2876.115394, mean_absolute_error: 13.797283, mean_q: 5.691931, mean_eps: 0.100000\n",
      " 153602/175000: episode: 4292, duration: 0.638s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 138.108 [50.000, 210.000], mean observation: 0.432 [0.000, 74.000], loss: 0.090805, mean_absolute_error: 0.590310, mean_q: 0.815044, mean_eps: 0.100000\n",
      " 153622/175000: episode: 4293, duration: 0.388s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 93.400 [4.000, 148.000], mean observation: 0.169 [0.000, 40.000], loss: 0.321290, mean_absolute_error: 0.571336, mean_q: 0.795802, mean_eps: 0.100000\n",
      " 153674/175000: episode: 4294, duration: 0.920s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 146.462 [50.000, 224.000], mean observation: 0.612 [0.000, 104.000], loss: 0.170160, mean_absolute_error: 0.570382, mean_q: 0.827424, mean_eps: 0.100000\n",
      " 153705/175000: episode: 4295, duration: 0.559s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.774 [21.000, 203.000], mean observation: 0.327 [0.000, 62.000], loss: 2094.166176, mean_absolute_error: 10.093600, mean_q: 3.260026, mean_eps: 0.100000\n",
      " 153727/175000: episode: 4296, duration: 0.397s, episode steps: 22, steps per second: 55, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 131.545 [37.000, 203.000], mean observation: 0.238 [0.000, 44.000], loss: 1204.132380, mean_absolute_error: 6.266129, mean_q: 4.763862, mean_eps: 0.100000\n",
      " 153792/175000: episode: 4297, duration: 1.212s, episode steps: 65, steps per second: 54, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 119.662 [15.000, 193.000], mean observation: 0.838 [0.000, 130.000], loss: 0.280232, mean_absolute_error: 0.573208, mean_q: 0.859702, mean_eps: 0.100000\n",
      " 153816/175000: episode: 4298, duration: 0.493s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 134.250 [44.000, 186.000], mean observation: 0.261 [0.000, 48.000], loss: 0.160987, mean_absolute_error: 0.562169, mean_q: 0.745035, mean_eps: 0.100000\n",
      " 153850/175000: episode: 4299, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.353 [44.000, 191.000], mean observation: 0.309 [0.000, 68.000], loss: 0.358501, mean_absolute_error: 0.568953, mean_q: 0.782855, mean_eps: 0.100000\n",
      " 153904/175000: episode: 4300, duration: 0.984s, episode steps: 54, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 102.852 [44.000, 191.000], mean observation: 0.696 [0.000, 108.000], loss: 13.378310, mean_absolute_error: 0.635624, mean_q: 0.786782, mean_eps: 0.100000\n",
      " 153927/175000: episode: 4301, duration: 0.469s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 124.783 [42.000, 203.000], mean observation: 0.127 [0.000, 46.000], loss: 0.071826, mean_absolute_error: 0.561122, mean_q: 0.714645, mean_eps: 0.100000\n",
      " 153957/175000: episode: 4302, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 95.467 [42.000, 203.000], mean observation: 0.132 [0.000, 60.000], loss: 0.113989, mean_absolute_error: 0.566084, mean_q: 0.807969, mean_eps: 0.100000\n",
      " 153985/175000: episode: 4303, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.679 [28.000, 197.000], mean observation: 0.165 [0.000, 56.000], loss: 0.106988, mean_absolute_error: 0.556515, mean_q: 0.750537, mean_eps: 0.100000\n",
      " 154016/175000: episode: 4304, duration: 0.590s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 130.065 [16.000, 213.000], mean observation: 0.360 [0.000, 62.000], loss: 0.151456, mean_absolute_error: 0.553938, mean_q: 0.821128, mean_eps: 0.100000\n",
      " 154036/175000: episode: 4305, duration: 0.428s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 112.900 [37.000, 191.000], mean observation: 0.153 [0.000, 40.000], loss: 44.712144, mean_absolute_error: 1.093898, mean_q: 4.760600, mean_eps: 0.100000\n",
      " 154064/175000: episode: 4306, duration: 0.587s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 131.714 [18.000, 201.000], mean observation: 0.241 [0.000, 56.000], loss: 0.359965, mean_absolute_error: 0.573941, mean_q: 0.791250, mean_eps: 0.100000\n",
      " 154085/175000: episode: 4307, duration: 0.419s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 124.714 [37.000, 177.000], mean observation: 0.171 [0.000, 42.000], loss: 0.315349, mean_absolute_error: 0.608775, mean_q: 0.806348, mean_eps: 0.100000\n",
      " 154141/175000: episode: 4308, duration: 1.031s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 154.304 [13.000, 187.000], mean observation: 0.743 [0.000, 112.000], loss: 0.730753, mean_absolute_error: 0.623175, mean_q: 1.028423, mean_eps: 0.100000\n",
      " 154160/175000: episode: 4309, duration: 0.359s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 121.158 [37.000, 202.000], mean observation: 0.096 [0.000, 38.000], loss: 0.093251, mean_absolute_error: 0.589802, mean_q: 0.823866, mean_eps: 0.100000\n",
      " 154196/175000: episode: 4310, duration: 0.754s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 121.278 [37.000, 187.000], mean observation: 0.159 [0.000, 72.000], loss: 0.298252, mean_absolute_error: 0.581194, mean_q: 0.917941, mean_eps: 0.100000\n",
      " 154247/175000: episode: 4311, duration: 1.167s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 143.157 [20.000, 192.000], mean observation: 0.346 [0.000, 102.000], loss: 0.193585, mean_absolute_error: 0.570342, mean_q: 0.855314, mean_eps: 0.100000\n",
      " 154284/175000: episode: 4312, duration: 0.826s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.919 [37.000, 193.000], mean observation: 0.252 [0.000, 74.000], loss: 901.568696, mean_absolute_error: 4.745199, mean_q: 3.095476, mean_eps: 0.100000\n",
      " 154327/175000: episode: 4313, duration: 0.898s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 151.535 [14.000, 209.000], mean observation: 0.394 [0.000, 86.000], loss: 2347.166410, mean_absolute_error: 11.139500, mean_q: 2.730059, mean_eps: 0.100000\n",
      " 154370/175000: episode: 4314, duration: 0.994s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.070 [25.000, 211.000], mean observation: 0.497 [0.000, 86.000], loss: 0.169676, mean_absolute_error: 0.575561, mean_q: 1.215563, mean_eps: 0.100000\n",
      " 154428/175000: episode: 4315, duration: 1.271s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 110.759 [21.000, 204.000], mean observation: 0.707 [0.000, 116.000], loss: 0.130268, mean_absolute_error: 0.549558, mean_q: 1.116629, mean_eps: 0.100000\n",
      " 154444/175000: episode: 4316, duration: 0.417s, episode steps: 16, steps per second: 38, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 76.250 [38.000, 135.000], mean observation: 0.072 [0.000, 32.000], loss: 0.077963, mean_absolute_error: 0.551137, mean_q: 1.161177, mean_eps: 0.100000\n",
      " 154475/175000: episode: 4317, duration: 0.695s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 88.387 [37.000, 158.000], mean observation: 0.172 [0.000, 62.000], loss: 42.585383, mean_absolute_error: 0.867087, mean_q: 2.771695, mean_eps: 0.100000\n",
      " 154497/175000: episode: 4318, duration: 0.527s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 136.545 [87.000, 222.000], mean observation: 0.161 [0.000, 44.000], loss: 0.256367, mean_absolute_error: 0.557811, mean_q: 1.316478, mean_eps: 0.100000\n",
      " 154540/175000: episode: 4319, duration: 0.913s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 91.116 [4.000, 200.000], mean observation: 0.276 [0.000, 86.000], loss: 781.267379, mean_absolute_error: 4.103008, mean_q: 2.274152, mean_eps: 0.100000\n",
      " 154573/175000: episode: 4320, duration: 0.768s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 111.788 [14.000, 207.000], mean observation: 0.170 [0.000, 66.000], loss: 0.084181, mean_absolute_error: 0.566565, mean_q: 1.382863, mean_eps: 0.100000\n",
      " 154594/175000: episode: 4321, duration: 0.425s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 130.619 [87.000, 222.000], mean observation: 0.073 [0.000, 42.000], loss: 5.062766, mean_absolute_error: 0.591143, mean_q: 1.533766, mean_eps: 0.100000\n",
      " 154624/175000: episode: 4322, duration: 0.715s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.400 [41.000, 210.000], mean observation: 0.174 [0.000, 60.000], loss: 2564.303901, mean_absolute_error: 12.192336, mean_q: 4.323019, mean_eps: 0.100000\n",
      " 154671/175000: episode: 4323, duration: 0.966s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.426 [5.000, 213.000], mean observation: 0.295 [0.000, 94.000], loss: 17.704691, mean_absolute_error: 0.767799, mean_q: 2.799014, mean_eps: 0.100000\n",
      " 154699/175000: episode: 4324, duration: 0.565s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 122.714 [5.000, 210.000], mean observation: 0.179 [0.000, 56.000], loss: 0.401633, mean_absolute_error: 0.563471, mean_q: 1.258281, mean_eps: 0.100000\n",
      " 154706/175000: episode: 4325, duration: 0.141s, episode steps: 7, steps per second: 50, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 158.000 [158.000, 158.000], mean observation: 0.019 [0.000, 14.000], loss: 0.100393, mean_absolute_error: 0.566529, mean_q: 1.407212, mean_eps: 0.100000\n",
      " 154741/175000: episode: 4326, duration: 0.676s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.200 [5.000, 210.000], mean observation: 0.238 [0.000, 70.000], loss: 0.126184, mean_absolute_error: 0.556939, mean_q: 1.273143, mean_eps: 0.100000\n",
      " 154793/175000: episode: 4327, duration: 0.957s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 173.942 [42.000, 223.000], mean observation: 0.388 [0.000, 104.000], loss: 10.530185, mean_absolute_error: 0.627300, mean_q: 1.701543, mean_eps: 0.100000\n",
      " 154818/175000: episode: 4328, duration: 0.467s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 136.240 [87.000, 186.000], mean observation: 0.116 [0.000, 50.000], loss: 0.178423, mean_absolute_error: 0.553057, mean_q: 1.449156, mean_eps: 0.100000\n",
      " 154852/175000: episode: 4329, duration: 0.631s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.912 [38.000, 202.000], mean observation: 0.184 [0.000, 68.000], loss: 0.153127, mean_absolute_error: 0.550806, mean_q: 1.388328, mean_eps: 0.100000\n",
      " 154872/175000: episode: 4330, duration: 0.453s, episode steps: 20, steps per second: 44, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 110.550 [16.000, 169.000], mean observation: 0.072 [0.000, 40.000], loss: 1.453971, mean_absolute_error: 0.605054, mean_q: 1.739645, mean_eps: 0.100000\n",
      " 154897/175000: episode: 4331, duration: 0.508s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 78.120 [1.000, 219.000], mean observation: 0.149 [0.000, 50.000], loss: 13.305101, mean_absolute_error: 0.644583, mean_q: 1.330933, mean_eps: 0.100000\n",
      " 154926/175000: episode: 4332, duration: 0.521s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.966 [1.000, 221.000], mean observation: 0.187 [0.000, 58.000], loss: 0.433180, mean_absolute_error: 0.579761, mean_q: 1.316273, mean_eps: 0.100000\n",
      " 154973/175000: episode: 4333, duration: 0.862s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 60.851 [1.000, 193.000], mean observation: 0.557 [0.000, 94.000], loss: 26.781904, mean_absolute_error: 0.696323, mean_q: 1.326857, mean_eps: 0.100000\n",
      " 155000/175000: episode: 4334, duration: 0.498s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 111.889 [24.000, 200.000], mean observation: 0.235 [0.000, 54.000], loss: 0.256130, mean_absolute_error: 0.588332, mean_q: 1.606936, mean_eps: 0.100000\n",
      " 155027/175000: episode: 4335, duration: 0.507s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.519 [24.000, 196.000], mean observation: 0.160 [0.000, 54.000], loss: 0.557340, mean_absolute_error: 0.577017, mean_q: 1.640764, mean_eps: 0.100000\n",
      " 155048/175000: episode: 4336, duration: 0.434s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.095 [24.000, 139.000], mean observation: 0.053 [0.000, 42.000], loss: 0.697582, mean_absolute_error: 0.566545, mean_q: 1.141662, mean_eps: 0.100000\n",
      " 155094/175000: episode: 4337, duration: 0.849s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 69.587 [1.000, 206.000], mean observation: 0.218 [0.000, 92.000], loss: 0.193128, mean_absolute_error: 0.586598, mean_q: 1.111370, mean_eps: 0.100000\n",
      " 155128/175000: episode: 4338, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.294 [1.000, 216.000], mean observation: 0.347 [0.000, 68.000], loss: 0.253511, mean_absolute_error: 0.616569, mean_q: 1.089128, mean_eps: 0.100000\n",
      " 155171/175000: episode: 4339, duration: 0.786s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.628 [1.000, 210.000], mean observation: 0.454 [0.000, 86.000], loss: 5.437944, mean_absolute_error: 0.633296, mean_q: 1.003558, mean_eps: 0.100000\n",
      " 155201/175000: episode: 4340, duration: 0.621s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 82.267 [1.000, 222.000], mean observation: 0.272 [0.000, 60.000], loss: 9.275669, mean_absolute_error: 0.781144, mean_q: 2.775552, mean_eps: 0.100000\n",
      " 155257/175000: episode: 4341, duration: 1.025s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 67.339 [1.000, 209.000], mean observation: 0.777 [0.000, 112.000], loss: 24.384772, mean_absolute_error: 0.697066, mean_q: 1.035435, mean_eps: 0.100000\n",
      " 155275/175000: episode: 4342, duration: 0.322s, episode steps: 18, steps per second: 56, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 78.000 [14.000, 223.000], mean observation: 0.114 [0.000, 36.000], loss: 99.385092, mean_absolute_error: 1.023586, mean_q: 0.921351, mean_eps: 0.100000\n",
      " 155315/175000: episode: 4343, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.750 [9.000, 223.000], mean observation: 0.508 [0.000, 80.000], loss: 0.453141, mean_absolute_error: 0.582607, mean_q: 1.055200, mean_eps: 0.100000\n",
      " 155334/175000: episode: 4344, duration: 0.360s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 170.842 [14.000, 223.000], mean observation: 0.112 [0.000, 38.000], loss: 0.238563, mean_absolute_error: 0.601021, mean_q: 1.159903, mean_eps: 0.100000\n",
      " 155349/175000: episode: 4345, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 146.067 [1.000, 193.000], mean observation: 0.064 [0.000, 30.000], loss: 2840.568939, mean_absolute_error: 13.640264, mean_q: 5.993608, mean_eps: 0.100000\n",
      " 155381/175000: episode: 4346, duration: 0.579s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 93.594 [1.000, 193.000], mean observation: 0.204 [0.000, 64.000], loss: 0.161122, mean_absolute_error: 0.628486, mean_q: 1.231518, mean_eps: 0.100000\n",
      " 155430/175000: episode: 4347, duration: 0.875s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.755 [9.000, 206.000], mean observation: 0.277 [0.000, 98.000], loss: 5.224817, mean_absolute_error: 0.629387, mean_q: 1.061990, mean_eps: 0.100000\n",
      " 155462/175000: episode: 4348, duration: 0.582s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.375 [7.000, 202.000], mean observation: 0.204 [0.000, 64.000], loss: 0.516280, mean_absolute_error: 0.624532, mean_q: 1.103113, mean_eps: 0.100000\n",
      " 155507/175000: episode: 4349, duration: 0.797s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.778 [37.000, 218.000], mean observation: 0.234 [0.000, 90.000], loss: 0.245815, mean_absolute_error: 0.614395, mean_q: 1.012808, mean_eps: 0.100000\n",
      " 155539/175000: episode: 4350, duration: 0.567s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 154.500 [62.000, 219.000], mean observation: 0.103 [0.000, 64.000], loss: 0.200225, mean_absolute_error: 0.810021, mean_q: 3.421810, mean_eps: 0.100000\n",
      " 155573/175000: episode: 4351, duration: 0.673s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.676 [1.000, 213.000], mean observation: 0.197 [0.000, 68.000], loss: 9.753408, mean_absolute_error: 0.820835, mean_q: 3.127733, mean_eps: 0.100000\n",
      " 155609/175000: episode: 4352, duration: 0.648s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.583 [1.000, 154.000], mean observation: 0.235 [0.000, 72.000], loss: 1462.780643, mean_absolute_error: 7.263404, mean_q: 3.152992, mean_eps: 0.100000\n",
      " 155653/175000: episode: 4353, duration: 0.786s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.886 [43.000, 208.000], mean observation: 0.236 [0.000, 88.000], loss: 1.086017, mean_absolute_error: 0.585851, mean_q: 1.004193, mean_eps: 0.100000\n",
      " 155678/175000: episode: 4354, duration: 0.476s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 85.040 [6.000, 222.000], mean observation: 0.139 [0.000, 50.000], loss: 0.090076, mean_absolute_error: 0.590859, mean_q: 1.018053, mean_eps: 0.100000\n",
      " 155712/175000: episode: 4355, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.206 [6.000, 161.000], mean observation: 0.189 [0.000, 68.000], loss: 0.104624, mean_absolute_error: 0.596534, mean_q: 1.070782, mean_eps: 0.100000\n",
      " 155727/175000: episode: 4356, duration: 0.295s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 150.667 [6.000, 200.000], mean observation: 0.077 [0.000, 30.000], loss: 0.437115, mean_absolute_error: 0.592264, mean_q: 0.868425, mean_eps: 0.100000\n",
      " 155756/175000: episode: 4357, duration: 0.611s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 185.241 [5.000, 195.000], mean observation: 0.081 [0.000, 58.000], loss: 0.093558, mean_absolute_error: 0.611368, mean_q: 0.878577, mean_eps: 0.100000\n",
      " 155790/175000: episode: 4358, duration: 0.634s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.559 [7.000, 207.000], mean observation: 0.303 [0.000, 68.000], loss: 1.055024, mean_absolute_error: 0.636000, mean_q: 0.890368, mean_eps: 0.100000\n",
      " 155823/175000: episode: 4359, duration: 0.575s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.667 [11.000, 198.000], mean observation: 0.353 [0.000, 66.000], loss: 0.092469, mean_absolute_error: 0.632874, mean_q: 0.827534, mean_eps: 0.100000\n",
      " 155840/175000: episode: 4360, duration: 0.443s, episode steps: 17, steps per second: 38, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 75.294 [1.000, 188.000], mean observation: 0.079 [0.000, 34.000], loss: 0.164389, mean_absolute_error: 0.656697, mean_q: 0.918784, mean_eps: 0.100000\n",
      " 155870/175000: episode: 4361, duration: 0.592s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.500 [29.000, 210.000], mean observation: 0.294 [0.000, 60.000], loss: 0.070727, mean_absolute_error: 0.616793, mean_q: 0.761317, mean_eps: 0.100000\n",
      " 155894/175000: episode: 4362, duration: 0.464s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 145.042 [31.000, 222.000], mean observation: 0.146 [0.000, 48.000], loss: 0.345832, mean_absolute_error: 0.623169, mean_q: 0.806207, mean_eps: 0.100000\n",
      " 155908/175000: episode: 4363, duration: 0.296s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 106.071 [67.000, 200.000], mean observation: 0.042 [0.000, 28.000], loss: 0.165526, mean_absolute_error: 0.613909, mean_q: 0.851158, mean_eps: 0.100000\n",
      " 155947/175000: episode: 4364, duration: 0.713s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.795 [27.000, 208.000], mean observation: 0.280 [0.000, 78.000], loss: 0.426373, mean_absolute_error: 0.629024, mean_q: 0.904260, mean_eps: 0.100000\n",
      " 155959/175000: episode: 4365, duration: 0.245s, episode steps: 12, steps per second: 49, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 141.500 [3.000, 223.000], mean observation: 0.059 [0.000, 24.000], loss: 0.492964, mean_absolute_error: 0.625854, mean_q: 0.912895, mean_eps: 0.100000\n",
      " 156010/175000: episode: 4366, duration: 0.938s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.118 [6.000, 222.000], mean observation: 0.645 [0.000, 102.000], loss: 0.262592, mean_absolute_error: 0.645241, mean_q: 1.070142, mean_eps: 0.100000\n",
      " 156046/175000: episode: 4367, duration: 0.650s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.222 [44.000, 216.000], mean observation: 0.280 [0.000, 72.000], loss: 0.138832, mean_absolute_error: 0.656197, mean_q: 0.968292, mean_eps: 0.100000\n",
      " 156081/175000: episode: 4368, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.000 [46.000, 223.000], mean observation: 0.350 [0.000, 70.000], loss: 6.912218, mean_absolute_error: 0.675537, mean_q: 0.909697, mean_eps: 0.100000\n",
      " 156101/175000: episode: 4369, duration: 0.364s, episode steps: 20, steps per second: 55, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 80.650 [1.000, 222.000], mean observation: 0.098 [0.000, 40.000], loss: 6.908650, mean_absolute_error: 0.675727, mean_q: 0.842205, mean_eps: 0.100000\n",
      " 156149/175000: episode: 4370, duration: 0.834s, episode steps: 48, steps per second: 58, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 39.354 [1.000, 181.000], mean observation: 0.251 [0.000, 96.000], loss: 105.367829, mean_absolute_error: 1.258705, mean_q: 2.385348, mean_eps: 0.100000\n",
      " 156197/175000: episode: 4371, duration: 0.879s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.625 [1.000, 223.000], mean observation: 0.574 [0.000, 96.000], loss: 0.139789, mean_absolute_error: 0.645168, mean_q: 0.782831, mean_eps: 0.100000\n",
      " 156255/175000: episode: 4372, duration: 1.006s, episode steps: 58, steps per second: 58, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 121.931 [7.000, 223.000], mean observation: 0.801 [0.000, 116.000], loss: 0.429425, mean_absolute_error: 0.645998, mean_q: 0.686021, mean_eps: 0.100000\n",
      " 156312/175000: episode: 4373, duration: 1.056s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 83.649 [5.000, 223.000], mean observation: 0.717 [0.000, 114.000], loss: 0.121595, mean_absolute_error: 0.626265, mean_q: 0.689094, mean_eps: 0.100000\n",
      " 156349/175000: episode: 4374, duration: 0.696s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 132.135 [12.000, 213.000], mean observation: 0.216 [0.000, 74.000], loss: 0.269377, mean_absolute_error: 0.626892, mean_q: 0.995369, mean_eps: 0.100000\n",
      " 156380/175000: episode: 4375, duration: 0.561s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.355 [27.000, 209.000], mean observation: 0.119 [0.000, 62.000], loss: 0.193931, mean_absolute_error: 0.601518, mean_q: 0.865022, mean_eps: 0.100000\n",
      " 156420/175000: episode: 4376, duration: 0.787s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 100.250 [27.000, 209.000], mean observation: 0.404 [0.000, 80.000], loss: 0.464213, mean_absolute_error: 0.599414, mean_q: 0.820106, mean_eps: 0.100000\n",
      " 156459/175000: episode: 4377, duration: 0.766s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.154 [1.000, 221.000], mean observation: 0.311 [0.000, 78.000], loss: 0.220471, mean_absolute_error: 0.623701, mean_q: 0.649107, mean_eps: 0.100000\n",
      " 156493/175000: episode: 4378, duration: 0.634s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.559 [51.000, 200.000], mean observation: 0.321 [0.000, 68.000], loss: 0.163685, mean_absolute_error: 0.634927, mean_q: 0.782045, mean_eps: 0.100000\n",
      " 156524/175000: episode: 4379, duration: 0.573s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 104.097 [61.000, 221.000], mean observation: 0.237 [0.000, 62.000], loss: 0.180254, mean_absolute_error: 0.656893, mean_q: 0.719090, mean_eps: 0.100000\n",
      " 156564/175000: episode: 4380, duration: 0.779s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 139.100 [27.000, 208.000], mean observation: 0.597 [0.000, 80.000], loss: 0.298004, mean_absolute_error: 0.655406, mean_q: 0.710746, mean_eps: 0.100000\n",
      " 156589/175000: episode: 4381, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 106.880 [14.000, 189.000], mean observation: 0.179 [0.000, 50.000], loss: 0.164156, mean_absolute_error: 0.656345, mean_q: 0.602730, mean_eps: 0.100000\n",
      " 156629/175000: episode: 4382, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 128.525 [25.000, 224.000], mean observation: 0.410 [0.000, 80.000], loss: 0.640119, mean_absolute_error: 0.662593, mean_q: 0.669173, mean_eps: 0.100000\n",
      " 156665/175000: episode: 4383, duration: 0.651s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 27.361 [25.000, 42.000], mean observation: 0.085 [0.000, 72.000], loss: 0.160019, mean_absolute_error: 0.663406, mean_q: 0.971753, mean_eps: 0.100000\n",
      " 156688/175000: episode: 4384, duration: 0.421s, episode steps: 23, steps per second: 55, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 36.087 [25.000, 42.000], mean observation: 0.086 [0.000, 46.000], loss: 2.558718, mean_absolute_error: 0.641398, mean_q: 0.654382, mean_eps: 0.100000\n",
      " 156741/175000: episode: 4385, duration: 1.010s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.887 [25.000, 200.000], mean observation: 0.571 [0.000, 106.000], loss: 0.780702, mean_absolute_error: 0.629485, mean_q: 0.750183, mean_eps: 0.100000\n",
      " 156769/175000: episode: 4386, duration: 0.500s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 153.429 [12.000, 200.000], mean observation: 0.261 [0.000, 56.000], loss: 0.495307, mean_absolute_error: 0.628632, mean_q: 0.663777, mean_eps: 0.100000\n",
      " 156808/175000: episode: 4387, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.359 [29.000, 200.000], mean observation: 0.326 [0.000, 78.000], loss: 1002.654089, mean_absolute_error: 5.269924, mean_q: 2.856691, mean_eps: 0.100000\n",
      " 156845/175000: episode: 4388, duration: 0.720s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.324 [25.000, 200.000], mean observation: 0.314 [0.000, 74.000], loss: 0.541142, mean_absolute_error: 0.647981, mean_q: 0.826716, mean_eps: 0.100000\n",
      " 156881/175000: episode: 4389, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 132.333 [6.000, 214.000], mean observation: 0.311 [0.000, 72.000], loss: 0.142892, mean_absolute_error: 0.656738, mean_q: 0.948150, mean_eps: 0.100000\n",
      " 156923/175000: episode: 4390, duration: 0.746s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.571 [18.000, 207.000], mean observation: 0.347 [0.000, 84.000], loss: 196.001879, mean_absolute_error: 1.551746, mean_q: 1.102271, mean_eps: 0.100000\n",
      " 156974/175000: episode: 4391, duration: 1.007s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.725 [29.000, 200.000], mean observation: 0.341 [0.000, 102.000], loss: 1.568842, mean_absolute_error: 0.671421, mean_q: 0.682392, mean_eps: 0.100000\n",
      " 157019/175000: episode: 4392, duration: 0.787s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.556 [15.000, 213.000], mean observation: 0.499 [0.000, 90.000], loss: 0.468573, mean_absolute_error: 0.688533, mean_q: 0.637426, mean_eps: 0.100000\n",
      " 157065/175000: episode: 4393, duration: 0.871s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 86.348 [18.000, 198.000], mean observation: 0.521 [0.000, 92.000], loss: 0.621402, mean_absolute_error: 0.703338, mean_q: 0.542550, mean_eps: 0.100000\n",
      " 157098/175000: episode: 4394, duration: 0.601s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.333 [29.000, 200.000], mean observation: 0.306 [0.000, 66.000], loss: 47.949467, mean_absolute_error: 0.921383, mean_q: 0.543523, mean_eps: 0.100000\n",
      " 157147/175000: episode: 4395, duration: 0.905s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 110.082 [16.000, 200.000], mean observation: 0.510 [0.000, 98.000], loss: 0.319563, mean_absolute_error: 0.715672, mean_q: 0.490758, mean_eps: 0.100000\n",
      " 157179/175000: episode: 4396, duration: 0.579s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.469 [29.000, 200.000], mean observation: 0.295 [0.000, 64.000], loss: 0.733787, mean_absolute_error: 0.723286, mean_q: 0.587572, mean_eps: 0.100000\n",
      " 157205/175000: episode: 4397, duration: 0.528s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.115 [1.000, 189.000], mean observation: 0.304 [0.000, 52.000], loss: 1.516451, mean_absolute_error: 0.741569, mean_q: 0.500514, mean_eps: 0.100000\n",
      " 157241/175000: episode: 4398, duration: 0.654s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.889 [7.000, 189.000], mean observation: 0.328 [0.000, 72.000], loss: 0.863463, mean_absolute_error: 0.748935, mean_q: 0.582735, mean_eps: 0.100000\n",
      " 157279/175000: episode: 4399, duration: 0.660s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.842 [29.000, 202.000], mean observation: 0.344 [0.000, 76.000], loss: 22.504651, mean_absolute_error: 0.838298, mean_q: 0.653361, mean_eps: 0.100000\n",
      " 157330/175000: episode: 4400, duration: 0.929s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.196 [20.000, 214.000], mean observation: 0.323 [0.000, 102.000], loss: 2.736011, mean_absolute_error: 0.760514, mean_q: 0.847110, mean_eps: 0.100000\n",
      " 157365/175000: episode: 4401, duration: 0.665s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 68.343 [11.000, 172.000], mean observation: 0.147 [0.000, 70.000], loss: 1.135798, mean_absolute_error: 0.799182, mean_q: 0.762167, mean_eps: 0.100000\n",
      " 157406/175000: episode: 4402, duration: 0.738s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 69.341 [0.000, 207.000], mean observation: 0.263 [0.000, 82.000], loss: 9.935911, mean_absolute_error: 0.861406, mean_q: 0.679979, mean_eps: 0.100000\n",
      " 157449/175000: episode: 4403, duration: 0.783s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 69.047 [14.000, 213.000], mean observation: 0.476 [0.000, 86.000], loss: 0.806589, mean_absolute_error: 0.830994, mean_q: 0.649895, mean_eps: 0.100000\n",
      " 157482/175000: episode: 4404, duration: 0.617s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 106.909 [18.000, 195.000], mean observation: 0.232 [0.000, 66.000], loss: 0.349387, mean_absolute_error: 0.827279, mean_q: 0.507500, mean_eps: 0.100000\n",
      " 157530/175000: episode: 4405, duration: 0.869s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 129.917 [14.000, 218.000], mean observation: 0.665 [0.000, 96.000], loss: 4.151830, mean_absolute_error: 0.837302, mean_q: 0.516305, mean_eps: 0.100000\n",
      " 157574/175000: episode: 4406, duration: 0.808s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.068 [5.000, 206.000], mean observation: 0.450 [0.000, 88.000], loss: 1.227119, mean_absolute_error: 0.809382, mean_q: 0.558104, mean_eps: 0.100000\n",
      " 157606/175000: episode: 4407, duration: 0.577s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 95.719 [18.000, 206.000], mean observation: 0.342 [0.000, 64.000], loss: 22.006585, mean_absolute_error: 0.901729, mean_q: 0.642377, mean_eps: 0.100000\n",
      " 157639/175000: episode: 4408, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.606 [2.000, 206.000], mean observation: 0.312 [0.000, 66.000], loss: 1.979207, mean_absolute_error: 0.820907, mean_q: 0.588048, mean_eps: 0.100000\n",
      " 157671/175000: episode: 4409, duration: 0.574s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 84.625 [63.000, 172.000], mean observation: 0.199 [0.000, 64.000], loss: 2.995156, mean_absolute_error: 0.860134, mean_q: 0.562258, mean_eps: 0.100000\n",
      " 157703/175000: episode: 4410, duration: 0.628s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 77.219 [7.000, 215.000], mean observation: 0.311 [0.000, 64.000], loss: 1.430994, mean_absolute_error: 0.888099, mean_q: 0.550396, mean_eps: 0.100000\n",
      " 157739/175000: episode: 4411, duration: 0.652s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 91.806 [7.000, 223.000], mean observation: 0.264 [0.000, 72.000], loss: 0.447837, mean_absolute_error: 0.914694, mean_q: 0.617817, mean_eps: 0.100000\n",
      " 157771/175000: episode: 4412, duration: 0.602s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.188 [23.000, 205.000], mean observation: 0.192 [0.000, 64.000], loss: 0.651506, mean_absolute_error: 1.112690, mean_q: 2.957036, mean_eps: 0.100000\n",
      " 157790/175000: episode: 4413, duration: 0.361s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 84.316 [23.000, 221.000], mean observation: 0.145 [0.000, 38.000], loss: 34.875162, mean_absolute_error: 1.074995, mean_q: 0.760631, mean_eps: 0.100000\n",
      " 157811/175000: episode: 4414, duration: 0.374s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 108.857 [11.000, 205.000], mean observation: 0.106 [0.000, 42.000], loss: 45.046331, mean_absolute_error: 1.104734, mean_q: 0.670894, mean_eps: 0.100000\n",
      " 157847/175000: episode: 4415, duration: 0.675s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 82.028 [5.000, 217.000], mean observation: 0.334 [0.000, 72.000], loss: 68.033697, mean_absolute_error: 1.204796, mean_q: 0.635753, mean_eps: 0.100000\n",
      " 157882/175000: episode: 4416, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.457 [7.000, 217.000], mean observation: 0.353 [0.000, 70.000], loss: 0.507001, mean_absolute_error: 0.906288, mean_q: 0.634173, mean_eps: 0.100000\n",
      " 157922/175000: episode: 4417, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 117.900 [7.000, 223.000], mean observation: 0.347 [0.000, 80.000], loss: 0.889897, mean_absolute_error: 0.907651, mean_q: 0.615394, mean_eps: 0.100000\n",
      " 157968/175000: episode: 4418, duration: 0.874s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.196 [2.000, 178.000], mean observation: 0.484 [0.000, 92.000], loss: 1.408788, mean_absolute_error: 0.901400, mean_q: 0.648481, mean_eps: 0.100000\n",
      " 158022/175000: episode: 4419, duration: 1.038s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 98.074 [23.000, 187.000], mean observation: 0.553 [0.000, 108.000], loss: 2.544821, mean_absolute_error: 0.927722, mean_q: 0.596407, mean_eps: 0.100000\n",
      " 158077/175000: episode: 4420, duration: 1.037s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 116.055 [23.000, 203.000], mean observation: 0.564 [0.000, 110.000], loss: 2.832008, mean_absolute_error: 0.975133, mean_q: 0.759692, mean_eps: 0.100000\n",
      " 158122/175000: episode: 4421, duration: 0.823s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 70.511 [10.000, 205.000], mean observation: 0.212 [0.000, 90.000], loss: 2.079955, mean_absolute_error: 0.976084, mean_q: 0.751498, mean_eps: 0.100000\n",
      " 158169/175000: episode: 4422, duration: 0.867s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 57.745 [10.000, 221.000], mean observation: 0.428 [0.000, 94.000], loss: 12.050139, mean_absolute_error: 1.044487, mean_q: 1.034868, mean_eps: 0.100000\n",
      " 158207/175000: episode: 4423, duration: 0.663s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 102.947 [10.000, 215.000], mean observation: 0.312 [0.000, 76.000], loss: 2.113171, mean_absolute_error: 1.036151, mean_q: 0.921292, mean_eps: 0.100000\n",
      " 158226/175000: episode: 4424, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 10.000 [10.000, 10.000], mean observation: 0.046 [0.000, 38.000], loss: 1.845258, mean_absolute_error: 1.059037, mean_q: 0.822325, mean_eps: 0.100000\n",
      " 158267/175000: episode: 4425, duration: 0.742s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.439 [10.000, 213.000], mean observation: 0.368 [0.000, 82.000], loss: 19510.846845, mean_absolute_error: 87.787757, mean_q: 0.883841, mean_eps: 0.100000\n",
      " 158306/175000: episode: 4426, duration: 0.752s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.436 [1.000, 197.000], mean observation: 0.332 [0.000, 78.000], loss: 1.037337, mean_absolute_error: 1.066388, mean_q: 0.689354, mean_eps: 0.100000\n",
      " 158333/175000: episode: 4427, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 90.370 [1.000, 205.000], mean observation: 0.235 [0.000, 54.000], loss: 0.758020, mean_absolute_error: 1.043465, mean_q: 0.641262, mean_eps: 0.100000\n",
      " 158369/175000: episode: 4428, duration: 0.699s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 122.806 [1.000, 216.000], mean observation: 0.350 [0.000, 72.000], loss: 14.011765, mean_absolute_error: 1.082312, mean_q: 0.622679, mean_eps: 0.100000\n",
      " 158413/175000: episode: 4429, duration: 0.807s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 120.023 [1.000, 218.000], mean observation: 0.451 [0.000, 88.000], loss: 0.986947, mean_absolute_error: 1.012795, mean_q: 0.500367, mean_eps: 0.100000\n",
      " 158456/175000: episode: 4430, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 154.000 [1.000, 215.000], mean observation: 0.501 [0.000, 86.000], loss: 0.321673, mean_absolute_error: 1.000821, mean_q: 0.448721, mean_eps: 0.100000\n",
      " 158509/175000: episode: 4431, duration: 0.993s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 151.245 [25.000, 215.000], mean observation: 0.535 [0.000, 106.000], loss: 7.293633, mean_absolute_error: 1.009927, mean_q: 0.488162, mean_eps: 0.100000\n",
      " 158540/175000: episode: 4432, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 159.774 [25.000, 224.000], mean observation: 0.260 [0.000, 62.000], loss: 0.227709, mean_absolute_error: 0.968682, mean_q: 0.511454, mean_eps: 0.100000\n",
      " 158580/175000: episode: 4433, duration: 0.786s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 147.475 [25.000, 224.000], mean observation: 0.398 [0.000, 80.000], loss: 1813.969302, mean_absolute_error: 9.178086, mean_q: 2.363940, mean_eps: 0.100000\n",
      " 158601/175000: episode: 4434, duration: 0.438s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 134.333 [27.000, 204.000], mean observation: 0.156 [0.000, 42.000], loss: 0.225739, mean_absolute_error: 0.952047, mean_q: 0.448854, mean_eps: 0.100000\n",
      " 158637/175000: episode: 4435, duration: 0.651s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 68.222 [28.000, 156.000], mean observation: 0.207 [0.000, 72.000], loss: 0.282348, mean_absolute_error: 0.937586, mean_q: 0.500818, mean_eps: 0.100000\n",
      " 158688/175000: episode: 4436, duration: 0.983s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 67.824 [28.000, 202.000], mean observation: 0.261 [0.000, 102.000], loss: 0.431164, mean_absolute_error: 0.914273, mean_q: 0.531495, mean_eps: 0.100000\n",
      " 158723/175000: episode: 4437, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 39.371 [18.000, 135.000], mean observation: 0.125 [0.000, 70.000], loss: 20.856078, mean_absolute_error: 0.985547, mean_q: 0.796466, mean_eps: 0.100000\n",
      " 158755/175000: episode: 4438, duration: 0.597s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 65.438 [28.000, 156.000], mean observation: 0.133 [0.000, 64.000], loss: 0.238850, mean_absolute_error: 0.877421, mean_q: 0.790429, mean_eps: 0.100000\n",
      " 158781/175000: episode: 4439, duration: 0.551s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 46.346 [28.000, 171.000], mean observation: 0.070 [0.000, 52.000], loss: 0.278097, mean_absolute_error: 0.872905, mean_q: 0.800898, mean_eps: 0.100000\n",
      " 158798/175000: episode: 4440, duration: 0.294s, episode steps: 17, steps per second: 58, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 123.059 [28.000, 181.000], mean observation: 0.098 [0.000, 34.000], loss: 0.264641, mean_absolute_error: 0.860105, mean_q: 0.872072, mean_eps: 0.100000\n",
      " 158832/175000: episode: 4441, duration: 0.659s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 156.147 [8.000, 222.000], mean observation: 0.261 [0.000, 68.000], loss: 2.607796, mean_absolute_error: 0.854890, mean_q: 0.750169, mean_eps: 0.100000\n",
      " 158861/175000: episode: 4442, duration: 0.631s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 163.448 [72.000, 217.000], mean observation: 0.205 [0.000, 58.000], loss: 0.265230, mean_absolute_error: 0.832402, mean_q: 0.738560, mean_eps: 0.100000\n",
      " 158898/175000: episode: 4443, duration: 0.730s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 121.757 [0.000, 210.000], mean observation: 0.403 [0.000, 74.000], loss: 0.365699, mean_absolute_error: 0.825517, mean_q: 0.750492, mean_eps: 0.100000\n",
      " 158930/175000: episode: 4444, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 137.625 [1.000, 222.000], mean observation: 0.307 [0.000, 64.000], loss: 0.549979, mean_absolute_error: 0.817252, mean_q: 0.798589, mean_eps: 0.100000\n",
      " 158963/175000: episode: 4445, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 112.909 [15.000, 198.000], mean observation: 0.201 [0.000, 66.000], loss: 0.809122, mean_absolute_error: 0.823161, mean_q: 0.914367, mean_eps: 0.100000\n",
      " 158977/175000: episode: 4446, duration: 0.302s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 81.357 [18.000, 175.000], mean observation: 0.092 [0.000, 28.000], loss: 0.559364, mean_absolute_error: 0.824523, mean_q: 0.988542, mean_eps: 0.100000\n",
      " 159004/175000: episode: 4447, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.148 [47.000, 175.000], mean observation: 0.175 [0.000, 54.000], loss: 0.680695, mean_absolute_error: 0.835237, mean_q: 1.094034, mean_eps: 0.100000\n",
      " 159026/175000: episode: 4448, duration: 0.460s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 162.455 [49.000, 220.000], mean observation: 0.146 [0.000, 44.000], loss: 0.248036, mean_absolute_error: 0.832962, mean_q: 0.745200, mean_eps: 0.100000\n",
      " 159048/175000: episode: 4449, duration: 0.429s, episode steps: 22, steps per second: 51, episode reward: 1.000, mean reward: 0.045 [0.000, 1.000], mean action: 139.227 [18.000, 221.000], mean observation: 0.199 [0.000, 43.000], loss: 0.173050, mean_absolute_error: 0.842640, mean_q: 0.636932, mean_eps: 0.100000\n",
      " 159085/175000: episode: 4450, duration: 0.783s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 132.270 [18.000, 220.000], mean observation: 0.294 [0.000, 74.000], loss: 0.289291, mean_absolute_error: 0.840861, mean_q: 0.575392, mean_eps: 0.100000\n",
      " 159120/175000: episode: 4451, duration: 0.736s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.457 [8.000, 173.000], mean observation: 0.336 [0.000, 70.000], loss: 6.154920, mean_absolute_error: 0.851014, mean_q: 0.632351, mean_eps: 0.100000\n",
      " 159177/175000: episode: 4452, duration: 1.118s, episode steps: 57, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 125.842 [8.000, 224.000], mean observation: 0.590 [0.000, 114.000], loss: 2.200739, mean_absolute_error: 0.824098, mean_q: 0.675818, mean_eps: 0.100000\n",
      " 159195/175000: episode: 4453, duration: 0.309s, episode steps: 18, steps per second: 58, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.222 [18.000, 187.000], mean observation: 0.131 [0.000, 36.000], loss: 0.252139, mean_absolute_error: 0.811304, mean_q: 0.791193, mean_eps: 0.100000\n",
      " 159249/175000: episode: 4454, duration: 1.037s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 115.815 [8.000, 173.000], mean observation: 0.471 [0.000, 108.000], loss: 0.617143, mean_absolute_error: 0.803406, mean_q: 0.729030, mean_eps: 0.100000\n",
      " 159282/175000: episode: 4455, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.121 [8.000, 221.000], mean observation: 0.348 [0.000, 66.000], loss: 1.231675, mean_absolute_error: 0.799285, mean_q: 0.516191, mean_eps: 0.100000\n",
      " 159321/175000: episode: 4456, duration: 0.749s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 117.513 [8.000, 202.000], mean observation: 0.408 [0.000, 78.000], loss: 0.725797, mean_absolute_error: 0.796183, mean_q: 0.541936, mean_eps: 0.100000\n",
      " 159360/175000: episode: 4457, duration: 0.695s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.923 [3.000, 201.000], mean observation: 0.505 [0.000, 78.000], loss: 0.666199, mean_absolute_error: 0.796390, mean_q: 0.444429, mean_eps: 0.100000\n",
      " 159397/175000: episode: 4458, duration: 0.729s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 131.405 [8.000, 221.000], mean observation: 0.537 [0.000, 74.000], loss: 19.593638, mean_absolute_error: 0.882329, mean_q: 0.579000, mean_eps: 0.100000\n",
      " 159438/175000: episode: 4459, duration: 0.741s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.366 [12.000, 193.000], mean observation: 0.474 [0.000, 82.000], loss: 0.597091, mean_absolute_error: 0.799160, mean_q: 0.587809, mean_eps: 0.100000\n",
      " 159485/175000: episode: 4460, duration: 0.904s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 71.915 [16.000, 193.000], mean observation: 0.529 [0.000, 94.000], loss: 0.523569, mean_absolute_error: 0.783199, mean_q: 0.552140, mean_eps: 0.100000\n",
      " 159515/175000: episode: 4461, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.467 [33.000, 194.000], mean observation: 0.284 [0.000, 60.000], loss: 1.482324, mean_absolute_error: 0.790877, mean_q: 0.743382, mean_eps: 0.100000\n",
      " 159539/175000: episode: 4462, duration: 0.472s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.750 [16.000, 223.000], mean observation: 0.133 [0.000, 48.000], loss: 1.147396, mean_absolute_error: 0.775711, mean_q: 0.684436, mean_eps: 0.100000\n",
      " 159575/175000: episode: 4463, duration: 0.661s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.139 [23.000, 198.000], mean observation: 0.223 [0.000, 72.000], loss: 4.597632, mean_absolute_error: 0.810063, mean_q: 0.789627, mean_eps: 0.100000\n",
      " 159607/175000: episode: 4464, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 122.031 [18.000, 221.000], mean observation: 0.306 [0.000, 64.000], loss: 0.458634, mean_absolute_error: 0.812792, mean_q: 0.796931, mean_eps: 0.100000\n",
      " 159649/175000: episode: 4465, duration: 0.786s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.476 [8.000, 221.000], mean observation: 0.509 [0.000, 84.000], loss: 1362.324781, mean_absolute_error: 7.019283, mean_q: 2.379839, mean_eps: 0.100000\n",
      " 159703/175000: episode: 4466, duration: 0.950s, episode steps: 54, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 78.278 [17.000, 221.000], mean observation: 0.717 [0.000, 108.000], loss: 0.267660, mean_absolute_error: 0.831983, mean_q: 0.657748, mean_eps: 0.100000\n",
      " 159738/175000: episode: 4467, duration: 0.669s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.086 [16.000, 202.000], mean observation: 0.315 [0.000, 70.000], loss: 0.191303, mean_absolute_error: 0.818176, mean_q: 0.572911, mean_eps: 0.100000\n",
      " 159781/175000: episode: 4468, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 66.140 [18.000, 188.000], mean observation: 0.438 [0.000, 86.000], loss: 0.204815, mean_absolute_error: 0.818495, mean_q: 0.629743, mean_eps: 0.100000\n",
      " 159811/175000: episode: 4469, duration: 0.513s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 90.333 [18.000, 202.000], mean observation: 0.263 [0.000, 60.000], loss: 2723.421541, mean_absolute_error: 12.917370, mean_q: 0.632701, mean_eps: 0.100000\n",
      " 159847/175000: episode: 4470, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.167 [16.000, 222.000], mean observation: 0.354 [0.000, 72.000], loss: 0.185287, mean_absolute_error: 0.806788, mean_q: 0.584612, mean_eps: 0.100000\n",
      " 159895/175000: episode: 4471, duration: 0.888s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 75.375 [16.000, 202.000], mean observation: 0.525 [0.000, 96.000], loss: 0.213234, mean_absolute_error: 0.793878, mean_q: 0.598230, mean_eps: 0.100000\n",
      " 159933/175000: episode: 4472, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.632 [16.000, 197.000], mean observation: 0.380 [0.000, 76.000], loss: 0.207123, mean_absolute_error: 0.783487, mean_q: 0.641227, mean_eps: 0.100000\n",
      " 159967/175000: episode: 4473, duration: 0.586s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.618 [16.000, 207.000], mean observation: 0.230 [0.000, 68.000], loss: 0.195442, mean_absolute_error: 0.780811, mean_q: 0.592509, mean_eps: 0.100000\n",
      " 160011/175000: episode: 4474, duration: 0.872s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 76.114 [16.000, 204.000], mean observation: 0.459 [0.000, 88.000], loss: 0.305562, mean_absolute_error: 0.778540, mean_q: 0.566426, mean_eps: 0.100000\n",
      " 160040/175000: episode: 4475, duration: 0.576s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 95.862 [7.000, 185.000], mean observation: 0.222 [0.000, 58.000], loss: 0.941289, mean_absolute_error: 0.765279, mean_q: 0.610215, mean_eps: 0.100000\n",
      " 160079/175000: episode: 4476, duration: 0.731s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.795 [7.000, 176.000], mean observation: 0.318 [0.000, 78.000], loss: 57.734358, mean_absolute_error: 1.000526, mean_q: 0.664251, mean_eps: 0.100000\n",
      " 160125/175000: episode: 4477, duration: 1.018s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 90.391 [7.000, 176.000], mean observation: 0.304 [0.000, 92.000], loss: 24.312209, mean_absolute_error: 0.835884, mean_q: 0.820063, mean_eps: 0.100000\n",
      " 160158/175000: episode: 4478, duration: 0.750s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 95.818 [7.000, 185.000], mean observation: 0.194 [0.000, 66.000], loss: 0.226063, mean_absolute_error: 0.717352, mean_q: 0.804376, mean_eps: 0.100000\n",
      " 160189/175000: episode: 4479, duration: 0.740s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.710 [7.000, 207.000], mean observation: 0.129 [0.000, 62.000], loss: 0.199464, mean_absolute_error: 0.708751, mean_q: 0.784325, mean_eps: 0.100000\n",
      " 160228/175000: episode: 4480, duration: 0.834s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.974 [27.000, 197.000], mean observation: 0.215 [0.000, 78.000], loss: 3310.084886, mean_absolute_error: 15.592733, mean_q: 2.946403, mean_eps: 0.100000\n",
      " 160250/175000: episode: 4481, duration: 0.530s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 126.773 [35.000, 183.000], mean observation: 0.147 [0.000, 44.000], loss: 0.271138, mean_absolute_error: 0.704507, mean_q: 0.989054, mean_eps: 0.100000\n",
      " 160289/175000: episode: 4482, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.692 [17.000, 182.000], mean observation: 0.361 [0.000, 78.000], loss: 1.564236, mean_absolute_error: 0.709384, mean_q: 0.864165, mean_eps: 0.100000\n",
      " 160320/175000: episode: 4483, duration: 0.646s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 118.935 [12.000, 194.000], mean observation: 0.329 [0.000, 62.000], loss: 0.215369, mean_absolute_error: 0.707913, mean_q: 0.853614, mean_eps: 0.100000\n",
      " 160356/175000: episode: 4484, duration: 0.740s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 79.944 [4.000, 207.000], mean observation: 0.291 [0.000, 72.000], loss: 3.874550, mean_absolute_error: 0.714588, mean_q: 0.763599, mean_eps: 0.100000\n",
      " 160403/175000: episode: 4485, duration: 0.948s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.660 [112.000, 176.000], mean observation: 0.194 [0.000, 94.000], loss: 53.533041, mean_absolute_error: 0.950338, mean_q: 0.764013, mean_eps: 0.100000\n",
      " 160439/175000: episode: 4486, duration: 0.719s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 117.972 [35.000, 208.000], mean observation: 0.294 [0.000, 72.000], loss: 0.742795, mean_absolute_error: 0.692841, mean_q: 0.688364, mean_eps: 0.100000\n",
      " 160474/175000: episode: 4487, duration: 0.745s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 112.314 [1.000, 193.000], mean observation: 0.209 [0.000, 70.000], loss: 2.009996, mean_absolute_error: 0.688591, mean_q: 0.866036, mean_eps: 0.100000\n",
      " 160537/175000: episode: 4488, duration: 1.411s, episode steps: 63, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 147.810 [10.000, 204.000], mean observation: 0.696 [0.000, 126.000], loss: 72.430798, mean_absolute_error: 1.001232, mean_q: 0.774140, mean_eps: 0.100000\n",
      " 160572/175000: episode: 4489, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.229 [72.000, 221.000], mean observation: 0.440 [0.000, 70.000], loss: 1.681873, mean_absolute_error: 0.687471, mean_q: 0.584937, mean_eps: 0.100000\n",
      " 160619/175000: episode: 4490, duration: 1.023s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.340 [55.000, 212.000], mean observation: 0.583 [0.000, 94.000], loss: 3.498872, mean_absolute_error: 0.697913, mean_q: 0.600061, mean_eps: 0.100000\n",
      " 160645/175000: episode: 4491, duration: 0.547s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 176.846 [71.000, 211.000], mean observation: 0.135 [0.000, 52.000], loss: 10.525895, mean_absolute_error: 0.729803, mean_q: 0.669775, mean_eps: 0.100000\n",
      " 160678/175000: episode: 4492, duration: 0.626s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 153.727 [72.000, 211.000], mean observation: 0.268 [0.000, 66.000], loss: 0.350284, mean_absolute_error: 0.681711, mean_q: 0.637188, mean_eps: 0.100000\n",
      " 160716/175000: episode: 4493, duration: 0.772s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.605 [18.000, 222.000], mean observation: 0.386 [0.000, 76.000], loss: 2.964484, mean_absolute_error: 0.704555, mean_q: 0.772191, mean_eps: 0.100000\n",
      " 160740/175000: episode: 4494, duration: 0.518s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 106.917 [16.000, 211.000], mean observation: 0.180 [0.000, 48.000], loss: 1.688892, mean_absolute_error: 0.701649, mean_q: 0.781506, mean_eps: 0.100000\n",
      " 160763/175000: episode: 4495, duration: 0.464s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 180.957 [15.000, 211.000], mean observation: 0.141 [0.000, 46.000], loss: 9.085377, mean_absolute_error: 0.731773, mean_q: 0.751468, mean_eps: 0.100000\n",
      " 160793/175000: episode: 4496, duration: 0.676s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 175.200 [72.000, 211.000], mean observation: 0.147 [0.000, 60.000], loss: 14.274119, mean_absolute_error: 0.755396, mean_q: 0.721502, mean_eps: 0.100000\n",
      " 160847/175000: episode: 4497, duration: 1.086s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 148.537 [15.000, 211.000], mean observation: 0.670 [0.000, 108.000], loss: 0.574130, mean_absolute_error: 0.682387, mean_q: 0.966742, mean_eps: 0.100000\n",
      " 160900/175000: episode: 4498, duration: 1.009s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 135.698 [46.000, 192.000], mean observation: 0.579 [0.000, 106.000], loss: 0.814683, mean_absolute_error: 0.694590, mean_q: 0.941383, mean_eps: 0.100000\n",
      " 160936/175000: episode: 4499, duration: 0.727s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.861 [31.000, 183.000], mean observation: 0.312 [0.000, 72.000], loss: 4.957342, mean_absolute_error: 0.731761, mean_q: 0.671126, mean_eps: 0.100000\n",
      " 160976/175000: episode: 4500, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 102.750 [16.000, 192.000], mean observation: 0.386 [0.000, 80.000], loss: 0.531388, mean_absolute_error: 0.712007, mean_q: 0.661978, mean_eps: 0.100000\n",
      " 161013/175000: episode: 4501, duration: 0.820s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.730 [1.000, 183.000], mean observation: 0.349 [0.000, 74.000], loss: 3.192657, mean_absolute_error: 0.738922, mean_q: 0.585500, mean_eps: 0.100000\n",
      " 161060/175000: episode: 4502, duration: 1.021s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.447 [1.000, 186.000], mean observation: 0.414 [0.000, 94.000], loss: 0.178245, mean_absolute_error: 0.744024, mean_q: 0.607278, mean_eps: 0.100000\n",
      " 161097/175000: episode: 4503, duration: 0.802s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 62.703 [22.000, 212.000], mean observation: 0.370 [0.000, 74.000], loss: 0.564416, mean_absolute_error: 0.746981, mean_q: 0.533592, mean_eps: 0.100000\n",
      " 161123/175000: episode: 4504, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 71.846 [5.000, 193.000], mean observation: 0.226 [0.000, 52.000], loss: 212.370876, mean_absolute_error: 1.927954, mean_q: 3.548816, mean_eps: 0.100000\n",
      " 161151/175000: episode: 4505, duration: 0.542s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.714 [22.000, 183.000], mean observation: 0.228 [0.000, 56.000], loss: 0.140227, mean_absolute_error: 0.742464, mean_q: 0.664459, mean_eps: 0.100000\n",
      " 161202/175000: episode: 4506, duration: 1.089s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 66.098 [22.000, 183.000], mean observation: 0.587 [0.000, 102.000], loss: 0.784768, mean_absolute_error: 0.735966, mean_q: 0.835290, mean_eps: 0.100000\n",
      " 161240/175000: episode: 4507, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.711 [16.000, 198.000], mean observation: 0.283 [0.000, 76.000], loss: 0.124316, mean_absolute_error: 0.724664, mean_q: 0.661413, mean_eps: 0.100000\n",
      " 161256/175000: episode: 4508, duration: 0.410s, episode steps: 16, steps per second: 39, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 90.750 [44.000, 182.000], mean observation: 0.100 [0.000, 32.000], loss: 7447.171977, mean_absolute_error: 34.217673, mean_q: 5.435029, mean_eps: 0.100000\n",
      " 161297/175000: episode: 4509, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.537 [16.000, 211.000], mean observation: 0.302 [0.000, 82.000], loss: 2221.802800, mean_absolute_error: 10.734113, mean_q: 2.356271, mean_eps: 0.100000\n",
      " 161359/175000: episode: 4510, duration: 1.200s, episode steps: 62, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 111.161 [2.000, 222.000], mean observation: 0.816 [0.000, 124.000], loss: 1.870825, mean_absolute_error: 0.721473, mean_q: 0.798911, mean_eps: 0.100000\n",
      " 161403/175000: episode: 4511, duration: 0.861s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.977 [42.000, 222.000], mean observation: 0.452 [0.000, 88.000], loss: 8.520330, mean_absolute_error: 0.853900, mean_q: 2.279390, mean_eps: 0.100000\n",
      " 161436/175000: episode: 4512, duration: 0.659s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.818 [42.000, 222.000], mean observation: 0.283 [0.000, 66.000], loss: 0.624391, mean_absolute_error: 0.710772, mean_q: 0.914364, mean_eps: 0.100000\n",
      " 161463/175000: episode: 4513, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 97.889 [1.000, 222.000], mean observation: 0.343 [0.000, 54.000], loss: 22.674808, mean_absolute_error: 0.811363, mean_q: 0.847032, mean_eps: 0.100000\n",
      " 161507/175000: episode: 4514, duration: 0.905s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.205 [1.000, 222.000], mean observation: 0.568 [0.000, 88.000], loss: 0.497175, mean_absolute_error: 0.718587, mean_q: 0.872304, mean_eps: 0.100000\n",
      " 161536/175000: episode: 4515, duration: 0.605s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 33.759 [1.000, 187.000], mean observation: 0.097 [0.000, 58.000], loss: 1.873512, mean_absolute_error: 0.729625, mean_q: 0.800202, mean_eps: 0.100000\n",
      " 161572/175000: episode: 4516, duration: 0.817s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 78.750 [1.000, 209.000], mean observation: 0.208 [0.000, 72.000], loss: 0.693714, mean_absolute_error: 0.730449, mean_q: 0.735184, mean_eps: 0.100000\n",
      " 161602/175000: episode: 4517, duration: 0.570s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.467 [1.000, 215.000], mean observation: 0.223 [0.000, 60.000], loss: 0.258821, mean_absolute_error: 0.727869, mean_q: 0.737756, mean_eps: 0.100000\n",
      " 161635/175000: episode: 4518, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.030 [45.000, 209.000], mean observation: 0.239 [0.000, 66.000], loss: 0.171085, mean_absolute_error: 0.715868, mean_q: 0.560123, mean_eps: 0.100000\n",
      " 161675/175000: episode: 4519, duration: 0.791s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 134.675 [24.000, 209.000], mean observation: 0.449 [0.000, 80.000], loss: 479.676388, mean_absolute_error: 2.862580, mean_q: 0.815560, mean_eps: 0.100000\n",
      " 161719/175000: episode: 4520, duration: 0.913s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.364 [10.000, 200.000], mean observation: 0.503 [0.000, 88.000], loss: 0.748494, mean_absolute_error: 0.715148, mean_q: 0.604034, mean_eps: 0.100000\n",
      " 161759/175000: episode: 4521, duration: 0.783s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.450 [11.000, 201.000], mean observation: 0.456 [0.000, 80.000], loss: 41.189001, mean_absolute_error: 0.892764, mean_q: 0.593899, mean_eps: 0.100000\n",
      " 161782/175000: episode: 4522, duration: 0.479s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 109.261 [23.000, 223.000], mean observation: 0.164 [0.000, 46.000], loss: 0.175672, mean_absolute_error: 0.709238, mean_q: 0.644703, mean_eps: 0.100000\n",
      " 161814/175000: episode: 4523, duration: 0.609s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.781 [19.000, 218.000], mean observation: 0.279 [0.000, 64.000], loss: 1260.476745, mean_absolute_error: 6.501779, mean_q: 2.884694, mean_eps: 0.100000\n",
      " 161862/175000: episode: 4524, duration: 0.877s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.083 [44.000, 194.000], mean observation: 0.257 [0.000, 96.000], loss: 0.875092, mean_absolute_error: 0.720504, mean_q: 0.650686, mean_eps: 0.100000\n",
      " 161907/175000: episode: 4525, duration: 0.892s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.311 [20.000, 224.000], mean observation: 0.568 [0.000, 90.000], loss: 0.140285, mean_absolute_error: 0.704438, mean_q: 0.648415, mean_eps: 0.100000\n",
      " 161951/175000: episode: 4526, duration: 0.903s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.977 [58.000, 157.000], mean observation: 0.295 [0.000, 88.000], loss: 69.151225, mean_absolute_error: 1.013653, mean_q: 0.612680, mean_eps: 0.100000\n",
      " 161993/175000: episode: 4527, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.024 [5.000, 144.000], mean observation: 0.275 [0.000, 84.000], loss: 4.694566, mean_absolute_error: 0.735813, mean_q: 0.740412, mean_eps: 0.100000\n",
      " 162042/175000: episode: 4528, duration: 0.927s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.020 [35.000, 205.000], mean observation: 0.337 [0.000, 98.000], loss: 12.205746, mean_absolute_error: 0.758073, mean_q: 0.698933, mean_eps: 0.100000\n",
      " 162067/175000: episode: 4529, duration: 0.522s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 103.760 [12.000, 223.000], mean observation: 0.179 [0.000, 50.000], loss: 0.062974, mean_absolute_error: 0.714733, mean_q: 0.826717, mean_eps: 0.100000\n",
      " 162092/175000: episode: 4530, duration: 0.543s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 84.600 [7.000, 186.000], mean observation: 0.125 [0.000, 50.000], loss: 6.056135, mean_absolute_error: 0.740395, mean_q: 0.772046, mean_eps: 0.100000\n",
      " 162134/175000: episode: 4531, duration: 0.823s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.595 [23.000, 200.000], mean observation: 0.358 [0.000, 84.000], loss: 0.210405, mean_absolute_error: 0.713867, mean_q: 0.877789, mean_eps: 0.100000\n",
      " 162175/175000: episode: 4532, duration: 0.793s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 94.561 [31.000, 189.000], mean observation: 0.299 [0.000, 82.000], loss: 4.134443, mean_absolute_error: 0.722649, mean_q: 0.911547, mean_eps: 0.100000\n",
      " 162220/175000: episode: 4533, duration: 0.865s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.511 [0.000, 202.000], mean observation: 0.563 [0.000, 90.000], loss: 2083.475996, mean_absolute_error: 10.090522, mean_q: 2.541528, mean_eps: 0.100000\n",
      " 162247/175000: episode: 4534, duration: 0.501s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 155.556 [59.000, 221.000], mean observation: 0.147 [0.000, 54.000], loss: 0.219129, mean_absolute_error: 0.676410, mean_q: 0.895127, mean_eps: 0.100000\n",
      " 162288/175000: episode: 4535, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 56.537 [3.000, 163.000], mean observation: 0.234 [0.000, 82.000], loss: 1.112450, mean_absolute_error: 0.675003, mean_q: 0.870970, mean_eps: 0.100000\n",
      " 162339/175000: episode: 4536, duration: 0.995s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.941 [45.000, 218.000], mean observation: 0.475 [0.000, 102.000], loss: 234.318013, mean_absolute_error: 1.756567, mean_q: 1.132124, mean_eps: 0.100000\n",
      " 162376/175000: episode: 4537, duration: 0.762s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 61.649 [45.000, 219.000], mean observation: 0.144 [0.000, 74.000], loss: 7.288433, mean_absolute_error: 0.729742, mean_q: 0.959198, mean_eps: 0.100000\n",
      " 162429/175000: episode: 4538, duration: 1.097s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.887 [45.000, 218.000], mean observation: 0.523 [0.000, 106.000], loss: 49.110196, mean_absolute_error: 0.921347, mean_q: 0.958288, mean_eps: 0.100000\n",
      " 162482/175000: episode: 4539, duration: 0.966s, episode steps: 53, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 36.113 [1.000, 174.000], mean observation: 0.573 [0.000, 106.000], loss: 775.802322, mean_absolute_error: 4.272430, mean_q: 2.420151, mean_eps: 0.100000\n",
      " 162541/175000: episode: 4540, duration: 1.189s, episode steps: 59, steps per second: 50, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.508 [10.000, 191.000], mean observation: 0.601 [0.000, 118.000], loss: 0.129224, mean_absolute_error: 0.707270, mean_q: 0.714865, mean_eps: 0.100000\n",
      " 162586/175000: episode: 4541, duration: 0.917s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 81.356 [1.000, 199.000], mean observation: 0.419 [0.000, 90.000], loss: 5.179356, mean_absolute_error: 0.861574, mean_q: 2.470367, mean_eps: 0.100000\n",
      " 162641/175000: episode: 4542, duration: 1.007s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 101.327 [8.000, 222.000], mean observation: 0.518 [0.000, 110.000], loss: 0.284529, mean_absolute_error: 0.720459, mean_q: 0.861890, mean_eps: 0.100000\n",
      " 162678/175000: episode: 4543, duration: 0.745s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.108 [16.000, 120.000], mean observation: 0.248 [0.000, 74.000], loss: 6.097447, mean_absolute_error: 0.738716, mean_q: 0.775912, mean_eps: 0.100000\n",
      " 162712/175000: episode: 4544, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.647 [1.000, 203.000], mean observation: 0.256 [0.000, 68.000], loss: 4.327763, mean_absolute_error: 0.729762, mean_q: 0.762168, mean_eps: 0.100000\n",
      " 162756/175000: episode: 4545, duration: 1.152s, episode steps: 44, steps per second: 38, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.682 [1.000, 207.000], mean observation: 0.251 [0.000, 88.000], loss: 0.175067, mean_absolute_error: 0.708621, mean_q: 0.709183, mean_eps: 0.100000\n",
      " 162800/175000: episode: 4546, duration: 1.094s, episode steps: 44, steps per second: 40, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 35.182 [1.000, 206.000], mean observation: 0.224 [0.000, 88.000], loss: 0.121713, mean_absolute_error: 0.722270, mean_q: 0.664048, mean_eps: 0.100000\n",
      " 162832/175000: episode: 4547, duration: 0.783s, episode steps: 32, steps per second: 41, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 54.281 [1.000, 174.000], mean observation: 0.209 [0.000, 64.000], loss: 0.616922, mean_absolute_error: 0.736407, mean_q: 0.774341, mean_eps: 0.100000\n",
      " 162859/175000: episode: 4548, duration: 0.688s, episode steps: 27, steps per second: 39, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 8.370 [1.000, 195.000], mean observation: 0.087 [0.000, 54.000], loss: 4.612042, mean_absolute_error: 0.754927, mean_q: 0.927414, mean_eps: 0.100000\n",
      " 162891/175000: episode: 4549, duration: 0.684s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 8.875 [1.000, 193.000], mean observation: 0.119 [0.000, 64.000], loss: 2.041390, mean_absolute_error: 0.739243, mean_q: 0.639106, mean_eps: 0.100000\n",
      " 162946/175000: episode: 4550, duration: 1.094s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 66.418 [1.000, 193.000], mean observation: 0.292 [0.000, 110.000], loss: 1894.799719, mean_absolute_error: 9.266745, mean_q: 2.163937, mean_eps: 0.100000\n",
      " 162988/175000: episode: 4551, duration: 0.829s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 7.000 [1.000, 155.000], mean observation: 0.126 [0.000, 84.000], loss: 1.583206, mean_absolute_error: 0.716261, mean_q: 0.745658, mean_eps: 0.100000\n",
      " 163032/175000: episode: 4552, duration: 0.955s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 36.682 [1.000, 204.000], mean observation: 0.308 [0.000, 88.000], loss: 3.670871, mean_absolute_error: 0.742392, mean_q: 1.020449, mean_eps: 0.100000\n",
      " 163069/175000: episode: 4553, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 102.270 [1.000, 204.000], mean observation: 0.322 [0.000, 74.000], loss: 0.473005, mean_absolute_error: 0.732236, mean_q: 0.911011, mean_eps: 0.100000\n",
      " 163092/175000: episode: 4554, duration: 0.485s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 24.391 [1.000, 135.000], mean observation: 0.094 [0.000, 46.000], loss: 0.185952, mean_absolute_error: 0.741232, mean_q: 0.800780, mean_eps: 0.100000\n",
      " 163122/175000: episode: 4555, duration: 0.594s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 64.767 [1.000, 205.000], mean observation: 0.288 [0.000, 60.000], loss: 39.344238, mean_absolute_error: 0.923730, mean_q: 0.680450, mean_eps: 0.100000\n",
      " 163176/175000: episode: 4556, duration: 1.118s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.500 [1.000, 224.000], mean observation: 0.552 [0.000, 108.000], loss: 2291.569764, mean_absolute_error: 11.069019, mean_q: 2.323100, mean_eps: 0.100000\n",
      " 163203/175000: episode: 4557, duration: 0.549s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.815 [1.000, 186.000], mean observation: 0.243 [0.000, 54.000], loss: 2.024362, mean_absolute_error: 0.745155, mean_q: 0.865639, mean_eps: 0.100000\n",
      " 163245/175000: episode: 4558, duration: 0.879s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 63.286 [1.000, 221.000], mean observation: 0.217 [0.000, 84.000], loss: 0.700293, mean_absolute_error: 0.894581, mean_q: 2.502132, mean_eps: 0.100000\n",
      " 163283/175000: episode: 4559, duration: 0.730s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.711 [1.000, 212.000], mean observation: 0.236 [0.000, 76.000], loss: 5.125277, mean_absolute_error: 0.771256, mean_q: 0.651325, mean_eps: 0.100000\n",
      " 163311/175000: episode: 4560, duration: 0.593s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 14.964 [1.000, 199.000], mean observation: 0.068 [0.000, 56.000], loss: 3.132455, mean_absolute_error: 0.772962, mean_q: 0.689140, mean_eps: 0.100000\n",
      " 163349/175000: episode: 4561, duration: 0.822s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.526 [1.000, 180.000], mean observation: 0.194 [0.000, 76.000], loss: 3535.547487, mean_absolute_error: 16.629386, mean_q: 2.841755, mean_eps: 0.100000\n",
      " 163380/175000: episode: 4562, duration: 0.728s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 48.548 [1.000, 218.000], mean observation: 0.181 [0.000, 62.000], loss: 0.135630, mean_absolute_error: 0.758619, mean_q: 1.031708, mean_eps: 0.100000\n",
      " 163413/175000: episode: 4563, duration: 0.784s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 73.061 [1.000, 218.000], mean observation: 0.197 [0.000, 66.000], loss: 0.334461, mean_absolute_error: 0.756194, mean_q: 0.973222, mean_eps: 0.100000\n",
      " 163462/175000: episode: 4564, duration: 1.067s, episode steps: 49, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.163 [1.000, 218.000], mean observation: 0.532 [0.000, 98.000], loss: 0.502109, mean_absolute_error: 0.763518, mean_q: 1.034160, mean_eps: 0.100000\n",
      " 163499/175000: episode: 4565, duration: 0.800s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 34.432 [1.000, 152.000], mean observation: 0.133 [0.000, 74.000], loss: 45.415568, mean_absolute_error: 0.954370, mean_q: 0.918725, mean_eps: 0.100000\n",
      " 163551/175000: episode: 4566, duration: 1.064s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.500 [1.000, 195.000], mean observation: 0.580 [0.000, 104.000], loss: 0.133216, mean_absolute_error: 0.738330, mean_q: 0.862893, mean_eps: 0.100000\n",
      " 163578/175000: episode: 4567, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.259 [1.000, 222.000], mean observation: 0.104 [0.000, 54.000], loss: 0.421629, mean_absolute_error: 0.748662, mean_q: 0.828954, mean_eps: 0.100000\n",
      " 163587/175000: episode: 4568, duration: 0.162s, episode steps: 9, steps per second: 55, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.024 [0.000, 18.000], loss: 0.083645, mean_absolute_error: 0.747145, mean_q: 0.828689, mean_eps: 0.100000\n",
      " 163617/175000: episode: 4569, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 52.500 [1.000, 182.000], mean observation: 0.209 [0.000, 60.000], loss: 46.928030, mean_absolute_error: 1.154382, mean_q: 3.171126, mean_eps: 0.100000\n",
      " 163649/175000: episode: 4570, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 65.281 [1.000, 175.000], mean observation: 0.323 [0.000, 64.000], loss: 0.291638, mean_absolute_error: 0.744891, mean_q: 0.882766, mean_eps: 0.100000\n",
      " 163695/175000: episode: 4571, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.022 [1.000, 216.000], mean observation: 0.478 [0.000, 92.000], loss: 4.307445, mean_absolute_error: 0.750846, mean_q: 0.719523, mean_eps: 0.100000\n",
      " 163732/175000: episode: 4572, duration: 0.809s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 51.514 [1.000, 208.000], mean observation: 0.416 [0.000, 74.000], loss: 13.028034, mean_absolute_error: 0.960264, mean_q: 2.822817, mean_eps: 0.100000\n",
      " 163761/175000: episode: 4573, duration: 0.609s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.414 [1.000, 224.000], mean observation: 0.287 [0.000, 58.000], loss: 104.064462, mean_absolute_error: 1.202943, mean_q: 0.987113, mean_eps: 0.100000\n",
      " 163809/175000: episode: 4574, duration: 0.924s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 28.167 [1.000, 150.000], mean observation: 0.229 [0.000, 96.000], loss: 12.243655, mean_absolute_error: 0.773660, mean_q: 0.784213, mean_eps: 0.100000\n",
      " 163849/175000: episode: 4575, duration: 0.823s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 59.250 [1.000, 212.000], mean observation: 0.314 [0.000, 80.000], loss: 11.611203, mean_absolute_error: 0.805666, mean_q: 1.032455, mean_eps: 0.100000\n",
      " 163882/175000: episode: 4576, duration: 0.657s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 35.576 [1.000, 183.000], mean observation: 0.276 [0.000, 66.000], loss: 6.426734, mean_absolute_error: 0.783114, mean_q: 1.098162, mean_eps: 0.100000\n",
      " 163915/175000: episode: 4577, duration: 0.627s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.576 [1.000, 219.000], mean observation: 0.253 [0.000, 66.000], loss: 18.488790, mean_absolute_error: 0.814184, mean_q: 0.976273, mean_eps: 0.100000\n",
      " 163969/175000: episode: 4578, duration: 1.123s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 123.056 [1.000, 213.000], mean observation: 0.330 [0.000, 108.000], loss: 0.240136, mean_absolute_error: 0.740195, mean_q: 0.976940, mean_eps: 0.100000\n",
      " 164002/175000: episode: 4579, duration: 0.609s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.788 [59.000, 188.000], mean observation: 0.202 [0.000, 66.000], loss: 2.205070, mean_absolute_error: 0.727621, mean_q: 0.768197, mean_eps: 0.100000\n",
      " 164031/175000: episode: 4580, duration: 0.545s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 111.759 [59.000, 215.000], mean observation: 0.244 [0.000, 58.000], loss: 3.581305, mean_absolute_error: 0.771831, mean_q: 1.217014, mean_eps: 0.100000\n",
      " 164062/175000: episode: 4581, duration: 0.647s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 118.935 [5.000, 176.000], mean observation: 0.217 [0.000, 62.000], loss: 21.721376, mean_absolute_error: 0.826280, mean_q: 0.973588, mean_eps: 0.100000\n",
      " 164089/175000: episode: 4582, duration: 0.534s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.185 [42.000, 176.000], mean observation: 0.184 [0.000, 54.000], loss: 16.224229, mean_absolute_error: 0.798885, mean_q: 0.975111, mean_eps: 0.100000\n",
      " 164100/175000: episode: 4583, duration: 0.294s, episode steps: 11, steps per second: 37, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 111.455 [32.000, 176.000], mean observation: 0.075 [0.000, 22.000], loss: 0.243795, mean_absolute_error: 0.712106, mean_q: 0.823007, mean_eps: 0.100000\n",
      " 164117/175000: episode: 4584, duration: 0.382s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 24.824 [1.000, 126.000], mean observation: 0.048 [0.000, 34.000], loss: 0.152240, mean_absolute_error: 0.750894, mean_q: 1.327036, mean_eps: 0.100000\n",
      " 164153/175000: episode: 4585, duration: 0.709s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.250 [29.000, 185.000], mean observation: 0.453 [0.000, 72.000], loss: 0.469400, mean_absolute_error: 0.724236, mean_q: 0.789051, mean_eps: 0.100000\n",
      " 164204/175000: episode: 4586, duration: 0.983s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 145.725 [32.000, 219.000], mean observation: 0.347 [0.000, 102.000], loss: 6.617007, mean_absolute_error: 0.770329, mean_q: 0.899133, mean_eps: 0.100000\n",
      " 164251/175000: episode: 4587, duration: 0.912s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.106 [0.000, 223.000], mean observation: 0.313 [0.000, 94.000], loss: 35.579554, mean_absolute_error: 0.927466, mean_q: 1.192471, mean_eps: 0.100000\n",
      " 164283/175000: episode: 4588, duration: 0.598s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.844 [33.000, 223.000], mean observation: 0.268 [0.000, 64.000], loss: 22.734195, mean_absolute_error: 0.882403, mean_q: 1.408286, mean_eps: 0.100000\n",
      " 164321/175000: episode: 4589, duration: 0.812s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.947 [37.000, 208.000], mean observation: 0.135 [0.000, 76.000], loss: 7.338173, mean_absolute_error: 0.807782, mean_q: 1.285800, mean_eps: 0.100000\n",
      " 164345/175000: episode: 4590, duration: 0.471s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.542 [17.000, 208.000], mean observation: 0.150 [0.000, 48.000], loss: 76.900938, mean_absolute_error: 1.132023, mean_q: 1.675063, mean_eps: 0.100000\n",
      " 164364/175000: episode: 4591, duration: 0.374s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 174.053 [56.000, 208.000], mean observation: 0.096 [0.000, 38.000], loss: 31.046349, mean_absolute_error: 0.934506, mean_q: 1.517755, mean_eps: 0.100000\n",
      " 164398/175000: episode: 4592, duration: 0.691s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.971 [23.000, 208.000], mean observation: 0.147 [0.000, 68.000], loss: 0.724537, mean_absolute_error: 0.779455, mean_q: 1.457372, mean_eps: 0.100000\n",
      " 164436/175000: episode: 4593, duration: 0.746s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 165.737 [4.000, 208.000], mean observation: 0.294 [0.000, 76.000], loss: 2.394389, mean_absolute_error: 0.779990, mean_q: 1.225460, mean_eps: 0.100000\n",
      " 164461/175000: episode: 4594, duration: 0.571s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 116.240 [4.000, 208.000], mean observation: 0.214 [0.000, 50.000], loss: 0.085546, mean_absolute_error: 0.770346, mean_q: 0.918193, mean_eps: 0.100000\n",
      " 164483/175000: episode: 4595, duration: 0.391s, episode steps: 22, steps per second: 56, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 149.136 [4.000, 208.000], mean observation: 0.199 [0.000, 44.000], loss: 0.542970, mean_absolute_error: 0.774087, mean_q: 1.003067, mean_eps: 0.100000\n",
      " 164519/175000: episode: 4596, duration: 0.633s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 149.194 [4.000, 208.000], mean observation: 0.314 [0.000, 72.000], loss: 0.191243, mean_absolute_error: 0.790204, mean_q: 1.080497, mean_eps: 0.100000\n",
      " 164556/175000: episode: 4597, duration: 0.737s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.892 [4.000, 209.000], mean observation: 0.177 [0.000, 74.000], loss: 0.197253, mean_absolute_error: 0.799153, mean_q: 1.150889, mean_eps: 0.100000\n",
      " 164594/175000: episode: 4598, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.868 [4.000, 208.000], mean observation: 0.325 [0.000, 76.000], loss: 7.815550, mean_absolute_error: 0.824253, mean_q: 1.196669, mean_eps: 0.100000\n",
      " 164639/175000: episode: 4599, duration: 0.924s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.689 [4.000, 208.000], mean observation: 0.241 [0.000, 90.000], loss: 16.572005, mean_absolute_error: 0.869039, mean_q: 1.093485, mean_eps: 0.100000\n",
      " 164667/175000: episode: 4600, duration: 0.538s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 56.786 [32.000, 195.000], mean observation: 0.076 [0.000, 56.000], loss: 1.146444, mean_absolute_error: 0.802566, mean_q: 1.128655, mean_eps: 0.100000\n",
      " 164710/175000: episode: 4601, duration: 0.810s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.302 [21.000, 191.000], mean observation: 0.211 [0.000, 86.000], loss: 3.957365, mean_absolute_error: 0.830745, mean_q: 1.191435, mean_eps: 0.100000\n",
      " 164744/175000: episode: 4602, duration: 0.684s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.618 [22.000, 189.000], mean observation: 0.140 [0.000, 68.000], loss: 10.972416, mean_absolute_error: 0.879036, mean_q: 1.257881, mean_eps: 0.100000\n",
      " 164778/175000: episode: 4603, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.118 [9.000, 189.000], mean observation: 0.190 [0.000, 68.000], loss: 36.065494, mean_absolute_error: 1.025753, mean_q: 1.436070, mean_eps: 0.100000\n",
      " 164819/175000: episode: 4604, duration: 0.756s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.098 [3.000, 224.000], mean observation: 0.455 [0.000, 82.000], loss: 4.341290, mean_absolute_error: 0.895063, mean_q: 1.037702, mean_eps: 0.100000\n",
      " 164844/175000: episode: 4605, duration: 0.531s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.920 [5.000, 189.000], mean observation: 0.273 [0.000, 50.000], loss: 0.616606, mean_absolute_error: 0.882534, mean_q: 1.172843, mean_eps: 0.100000\n",
      " 164876/175000: episode: 4606, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 55.375 [2.000, 154.000], mean observation: 0.104 [0.000, 64.000], loss: 15.355731, mean_absolute_error: 0.964645, mean_q: 1.043174, mean_eps: 0.100000\n",
      " 164922/175000: episode: 4607, duration: 0.932s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.391 [6.000, 207.000], mean observation: 0.286 [0.000, 92.000], loss: 1.418590, mean_absolute_error: 0.896336, mean_q: 0.862537, mean_eps: 0.100000\n",
      " 164962/175000: episode: 4608, duration: 0.790s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 119.125 [6.000, 210.000], mean observation: 0.310 [0.000, 80.000], loss: 4.687939, mean_absolute_error: 0.868893, mean_q: 0.598319, mean_eps: 0.100000\n",
      " 164994/175000: episode: 4609, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.531 [27.000, 207.000], mean observation: 0.145 [0.000, 64.000], loss: 2.109453, mean_absolute_error: 0.843858, mean_q: 0.718316, mean_eps: 0.100000\n",
      " 165031/175000: episode: 4610, duration: 0.683s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.892 [1.000, 223.000], mean observation: 0.371 [0.000, 74.000], loss: 1.371105, mean_absolute_error: 0.854379, mean_q: 0.878109, mean_eps: 0.100000\n",
      " 165069/175000: episode: 4611, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 60.605 [1.000, 187.000], mean observation: 0.363 [0.000, 76.000], loss: 0.479507, mean_absolute_error: 0.843773, mean_q: 0.770625, mean_eps: 0.100000\n",
      " 165115/175000: episode: 4612, duration: 0.832s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.457 [3.000, 217.000], mean observation: 0.453 [0.000, 92.000], loss: 2.006275, mean_absolute_error: 0.848706, mean_q: 0.689997, mean_eps: 0.100000\n",
      " 165165/175000: episode: 4613, duration: 1.007s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.380 [1.000, 212.000], mean observation: 0.796 [0.000, 100.000], loss: 2.136853, mean_absolute_error: 0.867064, mean_q: 0.738339, mean_eps: 0.100000\n",
      " 165199/175000: episode: 4614, duration: 0.596s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.765 [1.000, 197.000], mean observation: 0.189 [0.000, 68.000], loss: 4.310176, mean_absolute_error: 0.912874, mean_q: 0.715039, mean_eps: 0.100000\n",
      " 165227/175000: episode: 4615, duration: 0.529s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.786 [1.000, 210.000], mean observation: 0.147 [0.000, 56.000], loss: 12.625326, mean_absolute_error: 0.976972, mean_q: 0.794885, mean_eps: 0.100000\n",
      " 165254/175000: episode: 4616, duration: 0.513s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.259 [40.000, 210.000], mean observation: 0.182 [0.000, 54.000], loss: 3.240299, mean_absolute_error: 0.948167, mean_q: 0.773388, mean_eps: 0.100000\n",
      " 165294/175000: episode: 4617, duration: 0.740s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 63.650 [1.000, 210.000], mean observation: 0.439 [0.000, 80.000], loss: 12.652662, mean_absolute_error: 1.003827, mean_q: 0.730246, mean_eps: 0.100000\n",
      " 165336/175000: episode: 4618, duration: 0.818s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.429 [1.000, 174.000], mean observation: 0.280 [0.000, 84.000], loss: 3.505297, mean_absolute_error: 0.979640, mean_q: 0.688345, mean_eps: 0.100000\n",
      " 165375/175000: episode: 4619, duration: 0.766s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.385 [1.000, 174.000], mean observation: 0.351 [0.000, 78.000], loss: 2.719232, mean_absolute_error: 0.990089, mean_q: 0.617394, mean_eps: 0.100000\n",
      " 165409/175000: episode: 4620, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.853 [1.000, 178.000], mean observation: 0.344 [0.000, 68.000], loss: 0.703296, mean_absolute_error: 0.975119, mean_q: 0.576949, mean_eps: 0.100000\n",
      " 165438/175000: episode: 4621, duration: 0.533s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.034 [1.000, 198.000], mean observation: 0.196 [0.000, 58.000], loss: 0.703592, mean_absolute_error: 0.970955, mean_q: 0.529189, mean_eps: 0.100000\n",
      " 165458/175000: episode: 4622, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.400 [33.000, 206.000], mean observation: 0.099 [0.000, 40.000], loss: 0.602270, mean_absolute_error: 0.971942, mean_q: 0.493624, mean_eps: 0.100000\n",
      " 165478/175000: episode: 4623, duration: 0.396s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 101.550 [3.000, 206.000], mean observation: 0.105 [0.000, 40.000], loss: 0.384990, mean_absolute_error: 0.971941, mean_q: 0.473366, mean_eps: 0.100000\n",
      " 165520/175000: episode: 4624, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.143 [1.000, 206.000], mean observation: 0.430 [0.000, 84.000], loss: 4.195090, mean_absolute_error: 0.984660, mean_q: 0.495778, mean_eps: 0.100000\n",
      " 165564/175000: episode: 4625, duration: 0.879s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.682 [45.000, 216.000], mean observation: 0.455 [0.000, 88.000], loss: 38.332237, mean_absolute_error: 1.142648, mean_q: 0.545374, mean_eps: 0.100000\n",
      " 165590/175000: episode: 4626, duration: 0.569s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 149.808 [90.000, 216.000], mean observation: 0.176 [0.000, 52.000], loss: 0.304577, mean_absolute_error: 0.966754, mean_q: 0.475061, mean_eps: 0.100000\n",
      " 165618/175000: episode: 4627, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 145.821 [66.000, 216.000], mean observation: 0.213 [0.000, 56.000], loss: 1.232529, mean_absolute_error: 0.966594, mean_q: 0.478812, mean_eps: 0.100000\n",
      " 165663/175000: episode: 4628, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 163.822 [5.000, 216.000], mean observation: 0.400 [0.000, 90.000], loss: 43.903990, mean_absolute_error: 1.151097, mean_q: 0.524619, mean_eps: 0.100000\n",
      " 165684/175000: episode: 4629, duration: 0.491s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.810 [91.000, 203.000], mean observation: 0.085 [0.000, 42.000], loss: 27.101199, mean_absolute_error: 1.074810, mean_q: 0.593405, mean_eps: 0.100000\n",
      " 165712/175000: episode: 4630, duration: 0.643s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.607 [5.000, 211.000], mean observation: 0.270 [0.000, 56.000], loss: 199.257524, mean_absolute_error: 1.849329, mean_q: 0.639272, mean_eps: 0.100000\n",
      " 165742/175000: episode: 4631, duration: 0.655s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 72.500 [16.000, 206.000], mean observation: 0.283 [0.000, 60.000], loss: 1.838853, mean_absolute_error: 0.982107, mean_q: 0.421209, mean_eps: 0.100000\n",
      " 165763/175000: episode: 4632, duration: 0.395s, episode steps: 21, steps per second: 53, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.762 [77.000, 210.000], mean observation: 0.105 [0.000, 42.000], loss: 7.234879, mean_absolute_error: 1.035426, mean_q: 0.472362, mean_eps: 0.100000\n",
      " 165795/175000: episode: 4633, duration: 0.635s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.594 [1.000, 206.000], mean observation: 0.161 [0.000, 64.000], loss: 2.315915, mean_absolute_error: 1.032919, mean_q: 0.618701, mean_eps: 0.100000\n",
      " 165841/175000: episode: 4634, duration: 0.869s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.500 [1.000, 203.000], mean observation: 0.507 [0.000, 92.000], loss: 2.379533, mean_absolute_error: 1.040805, mean_q: 0.414245, mean_eps: 0.100000\n",
      " 165888/175000: episode: 4635, duration: 1.101s, episode steps: 47, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 71.149 [10.000, 191.000], mean observation: 0.585 [0.000, 94.000], loss: 74.290713, mean_absolute_error: 1.373187, mean_q: 0.472631, mean_eps: 0.100000\n",
      " 165921/175000: episode: 4636, duration: 0.675s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.818 [1.000, 215.000], mean observation: 0.217 [0.000, 66.000], loss: 2.940843, mean_absolute_error: 1.083850, mean_q: 0.364444, mean_eps: 0.100000\n",
      " 165962/175000: episode: 4637, duration: 0.776s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.220 [1.000, 215.000], mean observation: 0.271 [0.000, 82.000], loss: 39.899846, mean_absolute_error: 1.287472, mean_q: 0.422282, mean_eps: 0.100000\n",
      " 166005/175000: episode: 4638, duration: 0.862s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 105.977 [1.000, 202.000], mean observation: 0.323 [0.000, 86.000], loss: 1.508577, mean_absolute_error: 1.143249, mean_q: 0.322457, mean_eps: 0.100000\n",
      " 166046/175000: episode: 4639, duration: 0.813s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 138.366 [0.000, 214.000], mean observation: 0.409 [0.000, 82.000], loss: 67.944348, mean_absolute_error: 1.456973, mean_q: 0.274386, mean_eps: 0.100000\n",
      " 166080/175000: episode: 4640, duration: 0.717s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.794 [77.000, 202.000], mean observation: 0.106 [0.000, 68.000], loss: 2.211805, mean_absolute_error: 1.170514, mean_q: 0.256182, mean_eps: 0.100000\n",
      " 166122/175000: episode: 4641, duration: 0.869s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.333 [3.000, 219.000], mean observation: 0.553 [0.000, 84.000], loss: 28730.102638, mean_absolute_error: 128.977882, mean_q: 1.974341, mean_eps: 0.100000\n",
      " 166159/175000: episode: 4642, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 131.676 [1.000, 203.000], mean observation: 0.197 [0.000, 74.000], loss: 1.297155, mean_absolute_error: 1.139159, mean_q: 0.370366, mean_eps: 0.100000\n",
      " 166187/175000: episode: 4643, duration: 0.603s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 106.857 [1.000, 202.000], mean observation: 0.111 [0.000, 56.000], loss: 2.068538, mean_absolute_error: 1.125790, mean_q: 0.465597, mean_eps: 0.100000\n",
      " 166207/175000: episode: 4644, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 84.750 [1.000, 208.000], mean observation: 0.126 [0.000, 40.000], loss: 60.426548, mean_absolute_error: 1.373813, mean_q: 0.504058, mean_eps: 0.100000\n",
      " 166234/175000: episode: 4645, duration: 0.552s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 84.741 [1.000, 176.000], mean observation: 0.123 [0.000, 54.000], loss: 6.707946, mean_absolute_error: 1.119916, mean_q: 0.448424, mean_eps: 0.100000\n",
      " 166276/175000: episode: 4646, duration: 0.952s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.714 [1.000, 211.000], mean observation: 0.364 [0.000, 84.000], loss: 0.349422, mean_absolute_error: 1.073141, mean_q: 0.470644, mean_eps: 0.100000\n",
      " 166296/175000: episode: 4647, duration: 0.431s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 98.950 [1.000, 218.000], mean observation: 0.157 [0.000, 40.000], loss: 97.810613, mean_absolute_error: 1.491759, mean_q: 0.494596, mean_eps: 0.100000\n",
      " 166324/175000: episode: 4648, duration: 0.629s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.107 [1.000, 202.000], mean observation: 0.239 [0.000, 56.000], loss: 38.916686, mean_absolute_error: 1.209533, mean_q: 0.423512, mean_eps: 0.100000\n",
      " 166364/175000: episode: 4649, duration: 0.821s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 78.325 [1.000, 214.000], mean observation: 0.591 [0.000, 80.000], loss: 0.769172, mean_absolute_error: 1.027465, mean_q: 0.330564, mean_eps: 0.100000\n",
      " 166414/175000: episode: 4650, duration: 1.029s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 146.380 [1.000, 211.000], mean observation: 0.653 [0.000, 100.000], loss: 1.456896, mean_absolute_error: 1.012017, mean_q: 0.428410, mean_eps: 0.100000\n",
      " 166448/175000: episode: 4651, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.382 [1.000, 223.000], mean observation: 0.301 [0.000, 68.000], loss: 89.805239, mean_absolute_error: 1.383565, mean_q: 0.492776, mean_eps: 0.100000\n",
      " 166501/175000: episode: 4652, duration: 1.072s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 76.226 [1.000, 148.000], mean observation: 0.437 [0.000, 106.000], loss: 1828.292970, mean_absolute_error: 9.193675, mean_q: 1.830412, mean_eps: 0.100000\n",
      " 166545/175000: episode: 4653, duration: 0.917s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.386 [1.000, 212.000], mean observation: 0.362 [0.000, 88.000], loss: 5.199815, mean_absolute_error: 0.972794, mean_q: 0.526857, mean_eps: 0.100000\n",
      " 166555/175000: episode: 4654, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 100.400 [1.000, 156.000], mean observation: 0.052 [0.000, 20.000], loss: 0.211236, mean_absolute_error: 0.946952, mean_q: 0.392058, mean_eps: 0.100000\n",
      " 166611/175000: episode: 4655, duration: 1.006s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 90.286 [1.000, 174.000], mean observation: 0.760 [0.000, 112.000], loss: 4.704614, mean_absolute_error: 0.960328, mean_q: 0.505260, mean_eps: 0.100000\n",
      " 166637/175000: episode: 4656, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 127.000 [25.000, 177.000], mean observation: 0.173 [0.000, 52.000], loss: 6.187030, mean_absolute_error: 0.949712, mean_q: 0.442380, mean_eps: 0.100000\n",
      " 166680/175000: episode: 4657, duration: 0.881s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 127.488 [35.000, 167.000], mean observation: 0.324 [0.000, 86.000], loss: 5.904755, mean_absolute_error: 0.938740, mean_q: 0.440851, mean_eps: 0.100000\n",
      " 166725/175000: episode: 4658, duration: 0.887s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.178 [70.000, 173.000], mean observation: 0.364 [0.000, 90.000], loss: 1.996813, mean_absolute_error: 0.906176, mean_q: 0.409410, mean_eps: 0.100000\n",
      " 166777/175000: episode: 4659, duration: 1.007s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.154 [6.000, 212.000], mean observation: 0.463 [0.000, 104.000], loss: 2436.986847, mean_absolute_error: 11.830051, mean_q: 1.804141, mean_eps: 0.100000\n",
      " 166812/175000: episode: 4660, duration: 0.740s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 121.657 [22.000, 188.000], mean observation: 0.283 [0.000, 70.000], loss: 0.836253, mean_absolute_error: 0.872393, mean_q: 0.409708, mean_eps: 0.100000\n",
      " 166845/175000: episode: 4661, duration: 0.711s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 118.697 [1.000, 196.000], mean observation: 0.334 [0.000, 66.000], loss: 0.479758, mean_absolute_error: 0.863224, mean_q: 0.452285, mean_eps: 0.100000\n",
      " 166881/175000: episode: 4662, duration: 0.717s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 135.000 [18.000, 188.000], mean observation: 0.279 [0.000, 72.000], loss: 3.764192, mean_absolute_error: 0.864556, mean_q: 0.445273, mean_eps: 0.100000\n",
      " 166924/175000: episode: 4663, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 120.535 [5.000, 222.000], mean observation: 0.312 [0.000, 86.000], loss: 2.019753, mean_absolute_error: 0.841498, mean_q: 0.508865, mean_eps: 0.100000\n",
      " 166977/175000: episode: 4664, duration: 1.024s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.547 [3.000, 188.000], mean observation: 0.480 [0.000, 106.000], loss: 0.292927, mean_absolute_error: 0.825944, mean_q: 0.411475, mean_eps: 0.100000\n",
      " 167015/175000: episode: 4665, duration: 0.733s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.342 [1.000, 148.000], mean observation: 0.251 [0.000, 76.000], loss: 1.880998, mean_absolute_error: 0.828102, mean_q: 0.541071, mean_eps: 0.100000\n",
      " 167055/175000: episode: 4666, duration: 0.790s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.850 [18.000, 203.000], mean observation: 0.445 [0.000, 80.000], loss: 85.391898, mean_absolute_error: 1.191511, mean_q: 0.483902, mean_eps: 0.100000\n",
      " 167107/175000: episode: 4667, duration: 1.039s, episode steps: 52, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 137.596 [18.000, 201.000], mean observation: 0.750 [0.000, 104.000], loss: 0.540820, mean_absolute_error: 0.808820, mean_q: 0.453147, mean_eps: 0.100000\n",
      " 167157/175000: episode: 4668, duration: 0.986s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 154.780 [3.000, 212.000], mean observation: 0.546 [0.000, 100.000], loss: 16.844973, mean_absolute_error: 0.880186, mean_q: 0.384059, mean_eps: 0.100000\n",
      " 167183/175000: episode: 4669, duration: 0.491s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 115.077 [39.000, 188.000], mean observation: 0.202 [0.000, 52.000], loss: 56.940318, mean_absolute_error: 1.316008, mean_q: 3.369770, mean_eps: 0.100000\n",
      " 167219/175000: episode: 4670, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.639 [20.000, 190.000], mean observation: 0.361 [0.000, 72.000], loss: 3.622281, mean_absolute_error: 0.818942, mean_q: 0.302636, mean_eps: 0.100000\n",
      " 167273/175000: episode: 4671, duration: 1.028s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 141.481 [20.000, 188.000], mean observation: 0.593 [0.000, 108.000], loss: 1.135626, mean_absolute_error: 0.811720, mean_q: 0.374074, mean_eps: 0.100000\n",
      " 167315/175000: episode: 4672, duration: 0.845s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.214 [0.000, 178.000], mean observation: 0.508 [0.000, 84.000], loss: 45.016581, mean_absolute_error: 1.002868, mean_q: 0.359356, mean_eps: 0.100000\n",
      " 167343/175000: episode: 4673, duration: 0.584s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 124.643 [1.000, 219.000], mean observation: 0.231 [0.000, 56.000], loss: 3.917212, mean_absolute_error: 0.816430, mean_q: 0.330209, mean_eps: 0.100000\n",
      " 167383/175000: episode: 4674, duration: 0.793s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 140.425 [52.000, 209.000], mean observation: 0.181 [0.000, 80.000], loss: 68.326347, mean_absolute_error: 1.107547, mean_q: 0.396357, mean_eps: 0.100000\n",
      " 167399/175000: episode: 4675, duration: 0.347s, episode steps: 16, steps per second: 46, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 127.750 [52.000, 153.000], mean observation: 0.041 [0.000, 32.000], loss: 3.812967, mean_absolute_error: 0.814548, mean_q: 0.374624, mean_eps: 0.100000\n",
      " 167423/175000: episode: 4676, duration: 0.478s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 135.750 [15.000, 192.000], mean observation: 0.174 [0.000, 48.000], loss: 23.306253, mean_absolute_error: 0.897828, mean_q: 0.429297, mean_eps: 0.100000\n",
      " 167463/175000: episode: 4677, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 122.925 [52.000, 195.000], mean observation: 0.384 [0.000, 80.000], loss: 0.108517, mean_absolute_error: 0.794526, mean_q: 0.484734, mean_eps: 0.100000\n",
      " 167499/175000: episode: 4678, duration: 0.781s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.889 [32.000, 222.000], mean observation: 0.386 [0.000, 72.000], loss: 3339.237036, mean_absolute_error: 15.800526, mean_q: 2.531916, mean_eps: 0.100000\n",
      " 167558/175000: episode: 4679, duration: 1.275s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 132.593 [1.000, 192.000], mean observation: 0.692 [0.000, 118.000], loss: 1.726909, mean_absolute_error: 0.799553, mean_q: 0.414231, mean_eps: 0.100000\n",
      " 167592/175000: episode: 4680, duration: 0.759s, episode steps: 34, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.794 [1.000, 219.000], mean observation: 0.350 [0.000, 68.000], loss: 0.109582, mean_absolute_error: 0.794902, mean_q: 0.382519, mean_eps: 0.100000\n",
      " 167610/175000: episode: 4681, duration: 0.430s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 132.222 [15.000, 219.000], mean observation: 0.123 [0.000, 36.000], loss: 0.108360, mean_absolute_error: 0.800075, mean_q: 0.442799, mean_eps: 0.100000\n",
      " 167644/175000: episode: 4682, duration: 0.723s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.059 [1.000, 219.000], mean observation: 0.375 [0.000, 68.000], loss: 4.582470, mean_absolute_error: 0.816037, mean_q: 0.415626, mean_eps: 0.100000\n",
      " 167675/175000: episode: 4683, duration: 0.631s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 121.161 [64.000, 201.000], mean observation: 0.282 [0.000, 62.000], loss: 0.119390, mean_absolute_error: 0.784846, mean_q: 0.370189, mean_eps: 0.100000\n",
      " 167708/175000: episode: 4684, duration: 0.691s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 106.697 [64.000, 158.000], mean observation: 0.278 [0.000, 66.000], loss: 3.780850, mean_absolute_error: 0.798199, mean_q: 0.399785, mean_eps: 0.100000\n",
      " 167729/175000: episode: 4685, duration: 0.426s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.095 [64.000, 148.000], mean observation: 0.153 [0.000, 42.000], loss: 0.226189, mean_absolute_error: 0.776673, mean_q: 0.367832, mean_eps: 0.100000\n",
      " 167753/175000: episode: 4686, duration: 0.490s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.708 [18.000, 212.000], mean observation: 0.203 [0.000, 48.000], loss: 0.288732, mean_absolute_error: 0.778039, mean_q: 0.389627, mean_eps: 0.100000\n",
      " 167785/175000: episode: 4687, duration: 0.622s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 108.125 [0.000, 156.000], mean observation: 0.337 [0.000, 64.000], loss: 0.878970, mean_absolute_error: 0.784183, mean_q: 0.311327, mean_eps: 0.100000\n",
      " 167799/175000: episode: 4688, duration: 0.283s, episode steps: 14, steps per second: 50, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 84.714 [70.000, 144.000], mean observation: 0.036 [0.000, 28.000], loss: 0.116837, mean_absolute_error: 0.787018, mean_q: 0.413145, mean_eps: 0.100000\n",
      " 167827/175000: episode: 4689, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 106.036 [7.000, 147.000], mean observation: 0.137 [0.000, 56.000], loss: 0.730559, mean_absolute_error: 0.791177, mean_q: 0.326150, mean_eps: 0.100000\n",
      " 167840/175000: episode: 4690, duration: 0.281s, episode steps: 13, steps per second: 46, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 88.000 [16.000, 145.000], mean observation: 0.063 [0.000, 26.000], loss: 0.180094, mean_absolute_error: 0.795175, mean_q: 0.217066, mean_eps: 0.100000\n",
      " 167857/175000: episode: 4691, duration: 0.411s, episode steps: 17, steps per second: 41, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 80.765 [16.000, 212.000], mean observation: 0.142 [0.000, 34.000], loss: 0.384149, mean_absolute_error: 0.807436, mean_q: 0.373525, mean_eps: 0.100000\n",
      " 167873/175000: episode: 4692, duration: 0.326s, episode steps: 16, steps per second: 49, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 82.188 [16.000, 190.000], mean observation: 0.084 [0.000, 32.000], loss: 2.047388, mean_absolute_error: 0.817062, mean_q: 0.252724, mean_eps: 0.100000\n",
      " 167933/175000: episode: 4693, duration: 1.237s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 93.833 [0.000, 192.000], mean observation: 0.622 [0.000, 120.000], loss: 641.281681, mean_absolute_error: 3.756592, mean_q: 1.425491, mean_eps: 0.100000\n",
      " 167962/175000: episode: 4694, duration: 0.580s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.552 [1.000, 189.000], mean observation: 0.230 [0.000, 58.000], loss: 0.162193, mean_absolute_error: 0.805858, mean_q: 0.229830, mean_eps: 0.100000\n",
      " 168014/175000: episode: 4695, duration: 1.097s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.192 [16.000, 191.000], mean observation: 0.567 [0.000, 104.000], loss: 83.321189, mean_absolute_error: 1.172685, mean_q: 0.185138, mean_eps: 0.100000\n",
      " 168055/175000: episode: 4696, duration: 0.825s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 127.195 [43.000, 213.000], mean observation: 0.281 [0.000, 82.000], loss: 59.890776, mean_absolute_error: 1.213805, mean_q: 2.009785, mean_eps: 0.100000\n",
      " 168082/175000: episode: 4697, duration: 0.582s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 99.444 [1.000, 199.000], mean observation: 0.136 [0.000, 54.000], loss: 0.528998, mean_absolute_error: 0.790441, mean_q: 0.074229, mean_eps: 0.100000\n",
      " 168122/175000: episode: 4698, duration: 0.799s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 64.050 [1.000, 158.000], mean observation: 0.386 [0.000, 80.000], loss: 2.609060, mean_absolute_error: 0.804834, mean_q: 0.178861, mean_eps: 0.100000\n",
      " 168158/175000: episode: 4699, duration: 0.708s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 118.917 [17.000, 218.000], mean observation: 0.405 [0.000, 72.000], loss: 3.285428, mean_absolute_error: 0.802371, mean_q: 0.135595, mean_eps: 0.100000\n",
      " 168208/175000: episode: 4700, duration: 1.025s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.060 [44.000, 221.000], mean observation: 0.579 [0.000, 100.000], loss: 0.594667, mean_absolute_error: 0.780075, mean_q: 0.175178, mean_eps: 0.100000\n",
      " 168244/175000: episode: 4701, duration: 0.821s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 128.111 [6.000, 221.000], mean observation: 0.422 [0.000, 72.000], loss: 9.761795, mean_absolute_error: 0.815984, mean_q: 0.261057, mean_eps: 0.100000\n",
      " 168294/175000: episode: 4702, duration: 0.955s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.880 [4.000, 186.000], mean observation: 0.504 [0.000, 100.000], loss: 23.252765, mean_absolute_error: 0.885880, mean_q: 0.239891, mean_eps: 0.100000\n",
      " 168354/175000: episode: 4703, duration: 1.245s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 145.767 [40.000, 223.000], mean observation: 1.039 [0.000, 120.000], loss: 1.029297, mean_absolute_error: 0.787219, mean_q: 0.168931, mean_eps: 0.100000\n",
      " 168385/175000: episode: 4704, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.387 [66.000, 207.000], mean observation: 0.281 [0.000, 62.000], loss: 0.263284, mean_absolute_error: 0.785000, mean_q: 0.126663, mean_eps: 0.100000\n",
      " 168415/175000: episode: 4705, duration: 0.674s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 141.400 [33.000, 207.000], mean observation: 0.279 [0.000, 60.000], loss: 0.103006, mean_absolute_error: 0.788677, mean_q: 0.191697, mean_eps: 0.100000\n",
      " 168454/175000: episode: 4706, duration: 0.855s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.641 [18.000, 202.000], mean observation: 0.437 [0.000, 78.000], loss: 3.665994, mean_absolute_error: 0.797418, mean_q: 0.272832, mean_eps: 0.100000\n",
      " 168488/175000: episode: 4707, duration: 0.688s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 41.147 [18.000, 207.000], mean observation: 0.110 [0.000, 68.000], loss: 10.558116, mean_absolute_error: 0.834368, mean_q: 0.282397, mean_eps: 0.100000\n",
      " 168524/175000: episode: 4708, duration: 0.773s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.861 [14.000, 223.000], mean observation: 0.470 [0.000, 72.000], loss: 0.441574, mean_absolute_error: 0.792998, mean_q: 0.100496, mean_eps: 0.100000\n",
      " 168550/175000: episode: 4709, duration: 0.580s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 108.769 [62.000, 207.000], mean observation: 0.248 [0.000, 52.000], loss: 0.097875, mean_absolute_error: 0.792972, mean_q: 0.162135, mean_eps: 0.100000\n",
      " 168591/175000: episode: 4710, duration: 0.817s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.756 [20.000, 202.000], mean observation: 0.469 [0.000, 82.000], loss: 6.502237, mean_absolute_error: 0.815191, mean_q: 0.181732, mean_eps: 0.100000\n",
      " 168644/175000: episode: 4711, duration: 1.170s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.396 [6.000, 215.000], mean observation: 0.689 [0.000, 106.000], loss: 0.526380, mean_absolute_error: 0.794725, mean_q: 0.335032, mean_eps: 0.100000\n",
      " 168665/175000: episode: 4712, duration: 0.465s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 69.095 [6.000, 163.000], mean observation: 0.115 [0.000, 42.000], loss: 0.283359, mean_absolute_error: 0.781668, mean_q: 0.174997, mean_eps: 0.100000\n",
      " 168700/175000: episode: 4713, duration: 0.706s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.314 [6.000, 212.000], mean observation: 0.244 [0.000, 70.000], loss: 1.707822, mean_absolute_error: 0.791559, mean_q: 0.223077, mean_eps: 0.100000\n",
      " 168768/175000: episode: 4714, duration: 1.358s, episode steps: 68, steps per second: 50, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 139.353 [9.000, 224.000], mean observation: 0.668 [0.000, 136.000], loss: 2.327025, mean_absolute_error: 0.793403, mean_q: 0.198433, mean_eps: 0.100000\n",
      " 168799/175000: episode: 4715, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.387 [45.000, 224.000], mean observation: 0.309 [0.000, 62.000], loss: 3.771527, mean_absolute_error: 0.800812, mean_q: 0.184680, mean_eps: 0.100000\n",
      " 168834/175000: episode: 4716, duration: 0.699s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 175.229 [45.000, 224.000], mean observation: 0.435 [0.000, 70.000], loss: 678.982045, mean_absolute_error: 3.812993, mean_q: 0.367642, mean_eps: 0.100000\n",
      " 168866/175000: episode: 4717, duration: 0.656s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 35.250 [1.000, 203.000], mean observation: 0.080 [0.000, 64.000], loss: 0.756607, mean_absolute_error: 0.786858, mean_q: 0.202668, mean_eps: 0.100000\n",
      " 168915/175000: episode: 4718, duration: 0.921s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 176.449 [1.000, 224.000], mean observation: 0.661 [0.000, 98.000], loss: 3.078496, mean_absolute_error: 0.791141, mean_q: 0.315708, mean_eps: 0.100000\n",
      " 168950/175000: episode: 4719, duration: 0.691s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.000 [25.000, 224.000], mean observation: 0.462 [0.000, 70.000], loss: 4.364840, mean_absolute_error: 0.815025, mean_q: 0.210786, mean_eps: 0.100000\n",
      " 168978/175000: episode: 4720, duration: 0.567s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 167.429 [25.000, 224.000], mean observation: 0.197 [0.000, 56.000], loss: 0.237419, mean_absolute_error: 0.801273, mean_q: 0.146156, mean_eps: 0.100000\n",
      " 169006/175000: episode: 4721, duration: 0.618s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 143.964 [25.000, 224.000], mean observation: 0.265 [0.000, 56.000], loss: 0.594280, mean_absolute_error: 0.793284, mean_q: 0.120004, mean_eps: 0.100000\n",
      " 169041/175000: episode: 4722, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.829 [25.000, 214.000], mean observation: 0.380 [0.000, 70.000], loss: 0.351070, mean_absolute_error: 0.790562, mean_q: 0.252332, mean_eps: 0.100000\n",
      " 169100/175000: episode: 4723, duration: 1.286s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 101.712 [0.000, 214.000], mean observation: 0.714 [0.000, 118.000], loss: 17.625794, mean_absolute_error: 0.881159, mean_q: 0.228688, mean_eps: 0.100000\n",
      " 169158/175000: episode: 4724, duration: 1.230s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 117.586 [1.000, 207.000], mean observation: 0.902 [0.000, 116.000], loss: 1.684203, mean_absolute_error: 0.834080, mean_q: 0.242647, mean_eps: 0.100000\n",
      " 169209/175000: episode: 4725, duration: 1.061s, episode steps: 51, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 145.902 [32.000, 217.000], mean observation: 0.655 [0.000, 102.000], loss: 5.872917, mean_absolute_error: 0.849809, mean_q: 0.169347, mean_eps: 0.100000\n",
      " 169253/175000: episode: 4726, duration: 0.895s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 65.023 [25.000, 223.000], mean observation: 0.597 [0.000, 88.000], loss: 0.757717, mean_absolute_error: 0.827899, mean_q: 0.305286, mean_eps: 0.100000\n",
      " 169284/175000: episode: 4727, duration: 0.654s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 83.968 [25.000, 207.000], mean observation: 0.316 [0.000, 62.000], loss: 5.579128, mean_absolute_error: 0.871438, mean_q: 0.329250, mean_eps: 0.100000\n",
      " 169311/175000: episode: 4728, duration: 0.554s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 106.852 [25.000, 214.000], mean observation: 0.196 [0.000, 54.000], loss: 1.133588, mean_absolute_error: 0.856760, mean_q: 0.368268, mean_eps: 0.100000\n",
      " 169359/175000: episode: 4729, duration: 0.934s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.000 [25.000, 212.000], mean observation: 0.492 [0.000, 96.000], loss: 9.968524, mean_absolute_error: 0.899280, mean_q: 0.449335, mean_eps: 0.100000\n",
      " 169401/175000: episode: 4730, duration: 1.044s, episode steps: 42, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.476 [10.000, 211.000], mean observation: 0.540 [0.000, 84.000], loss: 3.295643, mean_absolute_error: 0.860966, mean_q: 0.460876, mean_eps: 0.100000\n",
      " 169435/175000: episode: 4731, duration: 0.821s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.500 [10.000, 201.000], mean observation: 0.286 [0.000, 68.000], loss: 553.710890, mean_absolute_error: 3.316449, mean_q: 0.228829, mean_eps: 0.100000\n",
      " 169482/175000: episode: 4732, duration: 1.129s, episode steps: 47, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 119.319 [11.000, 223.000], mean observation: 0.526 [0.000, 94.000], loss: 3.037851, mean_absolute_error: 0.879486, mean_q: 0.162004, mean_eps: 0.100000\n",
      " 169519/175000: episode: 4733, duration: 0.808s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.649 [16.000, 223.000], mean observation: 0.375 [0.000, 74.000], loss: 3617.362249, mean_absolute_error: 17.122542, mean_q: 2.181973, mean_eps: 0.100000\n",
      " 169564/175000: episode: 4734, duration: 0.860s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.822 [4.000, 223.000], mean observation: 0.468 [0.000, 90.000], loss: 1.775872, mean_absolute_error: 0.893996, mean_q: 0.329302, mean_eps: 0.100000\n",
      " 169597/175000: episode: 4735, duration: 0.647s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 115.667 [34.000, 221.000], mean observation: 0.298 [0.000, 66.000], loss: 0.158827, mean_absolute_error: 0.879160, mean_q: 0.231904, mean_eps: 0.100000\n",
      " 169644/175000: episode: 4736, duration: 0.925s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 86.830 [17.000, 201.000], mean observation: 0.397 [0.000, 94.000], loss: 0.217256, mean_absolute_error: 0.874025, mean_q: 0.220244, mean_eps: 0.100000\n",
      " 169669/175000: episode: 4737, duration: 0.628s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 83.920 [45.000, 202.000], mean observation: 0.167 [0.000, 50.000], loss: 0.148792, mean_absolute_error: 0.861437, mean_q: 0.222005, mean_eps: 0.100000\n",
      " 169687/175000: episode: 4738, duration: 0.339s, episode steps: 18, steps per second: 53, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 103.667 [1.000, 202.000], mean observation: 0.094 [0.000, 36.000], loss: 0.668120, mean_absolute_error: 0.859254, mean_q: 0.221169, mean_eps: 0.100000\n",
      " 169721/175000: episode: 4739, duration: 0.694s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 80.882 [1.000, 210.000], mean observation: 0.288 [0.000, 68.000], loss: 0.398479, mean_absolute_error: 0.850589, mean_q: 0.277640, mean_eps: 0.100000\n",
      " 169768/175000: episode: 4740, duration: 0.919s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.191 [3.000, 210.000], mean observation: 0.356 [0.000, 94.000], loss: 0.139859, mean_absolute_error: 0.842187, mean_q: 0.297049, mean_eps: 0.100000\n",
      " 169820/175000: episode: 4741, duration: 1.072s, episode steps: 52, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 71.385 [3.000, 222.000], mean observation: 0.607 [0.000, 104.000], loss: 0.939580, mean_absolute_error: 0.829955, mean_q: 0.221499, mean_eps: 0.100000\n",
      " 169866/175000: episode: 4742, duration: 0.912s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 99.739 [3.000, 210.000], mean observation: 0.569 [0.000, 92.000], loss: 2.403009, mean_absolute_error: 0.828578, mean_q: 0.178030, mean_eps: 0.100000\n",
      " 169918/175000: episode: 4743, duration: 1.238s, episode steps: 52, steps per second: 42, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.731 [4.000, 204.000], mean observation: 0.382 [0.000, 104.000], loss: 2.085800, mean_absolute_error: 0.827618, mean_q: 0.292123, mean_eps: 0.100000\n",
      " 169957/175000: episode: 4744, duration: 1.051s, episode steps: 39, steps per second: 37, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.487 [16.000, 204.000], mean observation: 0.267 [0.000, 78.000], loss: 3.767671, mean_absolute_error: 0.838511, mean_q: 0.377053, mean_eps: 0.100000\n",
      " 170005/175000: episode: 4745, duration: 1.158s, episode steps: 48, steps per second: 41, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.938 [16.000, 204.000], mean observation: 0.417 [0.000, 96.000], loss: 0.589524, mean_absolute_error: 0.832611, mean_q: 0.250473, mean_eps: 0.100000\n",
      " 170041/175000: episode: 4746, duration: 1.309s, episode steps: 36, steps per second: 28, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.361 [4.000, 204.000], mean observation: 0.335 [0.000, 72.000], loss: 0.502966, mean_absolute_error: 0.831289, mean_q: 0.206919, mean_eps: 0.100000\n",
      " 170071/175000: episode: 4747, duration: 1.182s, episode steps: 30, steps per second: 25, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 84.633 [26.000, 194.000], mean observation: 0.241 [0.000, 60.000], loss: 25.556195, mean_absolute_error: 0.928546, mean_q: 0.284765, mean_eps: 0.100000\n",
      " 170086/175000: episode: 4748, duration: 0.727s, episode steps: 15, steps per second: 21, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 99.733 [11.000, 212.000], mean observation: 0.107 [0.000, 30.000], loss: 0.552783, mean_absolute_error: 0.812058, mean_q: 0.427192, mean_eps: 0.100000\n",
      " 170144/175000: episode: 4749, duration: 2.333s, episode steps: 58, steps per second: 25, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 105.190 [56.000, 212.000], mean observation: 0.818 [0.000, 116.000], loss: 0.567788, mean_absolute_error: 0.800851, mean_q: 0.405567, mean_eps: 0.100000\n",
      " 170179/175000: episode: 4750, duration: 1.359s, episode steps: 35, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.257 [2.000, 216.000], mean observation: 0.568 [0.000, 70.000], loss: 1.831680, mean_absolute_error: 0.792037, mean_q: 0.437788, mean_eps: 0.100000\n",
      " 170214/175000: episode: 4751, duration: 1.375s, episode steps: 35, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 77.543 [48.000, 217.000], mean observation: 0.352 [0.000, 70.000], loss: 0.965638, mean_absolute_error: 0.788520, mean_q: 0.392458, mean_eps: 0.100000\n",
      " 170267/175000: episode: 4752, duration: 2.043s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 94.000 [29.000, 221.000], mean observation: 0.759 [0.000, 106.000], loss: 5.779631, mean_absolute_error: 0.802542, mean_q: 0.416114, mean_eps: 0.100000\n",
      " 170311/175000: episode: 4753, duration: 1.802s, episode steps: 44, steps per second: 24, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.795 [8.000, 202.000], mean observation: 0.688 [0.000, 88.000], loss: 2272.761287, mean_absolute_error: 11.020992, mean_q: 2.193906, mean_eps: 0.100000\n",
      " 170338/175000: episode: 4754, duration: 1.024s, episode steps: 27, steps per second: 26, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.333 [5.000, 191.000], mean observation: 0.270 [0.000, 54.000], loss: 0.867956, mean_absolute_error: 0.782608, mean_q: 0.593769, mean_eps: 0.100000\n",
      " 170360/175000: episode: 4755, duration: 0.920s, episode steps: 22, steps per second: 24, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 123.909 [62.000, 223.000], mean observation: 0.200 [0.000, 44.000], loss: 3.465965, mean_absolute_error: 0.786870, mean_q: 0.620195, mean_eps: 0.100000\n",
      " 170373/175000: episode: 4756, duration: 0.589s, episode steps: 13, steps per second: 22, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 136.231 [62.000, 204.000], mean observation: 0.109 [0.000, 26.000], loss: 0.638434, mean_absolute_error: 0.770543, mean_q: 0.793651, mean_eps: 0.100000\n",
      " 170428/175000: episode: 4757, duration: 2.087s, episode steps: 55, steps per second: 26, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 129.945 [10.000, 216.000], mean observation: 0.622 [0.000, 110.000], loss: 0.721808, mean_absolute_error: 0.761523, mean_q: 0.708368, mean_eps: 0.100000\n",
      " 170472/175000: episode: 4758, duration: 1.857s, episode steps: 44, steps per second: 24, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 144.045 [34.000, 204.000], mean observation: 0.571 [0.000, 88.000], loss: 13.516958, mean_absolute_error: 0.816029, mean_q: 0.666567, mean_eps: 0.100000\n",
      " 170499/175000: episode: 4759, duration: 1.069s, episode steps: 27, steps per second: 25, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.370 [5.000, 213.000], mean observation: 0.280 [0.000, 54.000], loss: 5.200204, mean_absolute_error: 0.777066, mean_q: 0.535137, mean_eps: 0.100000\n",
      " 170545/175000: episode: 4760, duration: 1.924s, episode steps: 46, steps per second: 24, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.891 [0.000, 209.000], mean observation: 0.636 [0.000, 92.000], loss: 5.173532, mean_absolute_error: 0.779711, mean_q: 0.486135, mean_eps: 0.100000\n",
      " 170572/175000: episode: 4761, duration: 1.074s, episode steps: 27, steps per second: 25, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 128.407 [0.000, 204.000], mean observation: 0.200 [0.000, 54.000], loss: 0.489992, mean_absolute_error: 0.763531, mean_q: 0.526517, mean_eps: 0.100000\n",
      " 170610/175000: episode: 4762, duration: 1.581s, episode steps: 38, steps per second: 24, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.605 [0.000, 223.000], mean observation: 0.391 [0.000, 76.000], loss: 0.686459, mean_absolute_error: 0.761243, mean_q: 0.507968, mean_eps: 0.100000\n",
      " 170657/175000: episode: 4763, duration: 1.871s, episode steps: 47, steps per second: 25, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 79.851 [0.000, 210.000], mean observation: 0.592 [0.000, 94.000], loss: 7.461236, mean_absolute_error: 0.781346, mean_q: 0.573177, mean_eps: 0.100000\n",
      " 170698/175000: episode: 4764, duration: 1.675s, episode steps: 41, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 150.732 [13.000, 210.000], mean observation: 0.485 [0.000, 82.000], loss: 9.851178, mean_absolute_error: 0.790217, mean_q: 0.738908, mean_eps: 0.100000\n",
      " 170740/175000: episode: 4765, duration: 1.669s, episode steps: 42, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 104.262 [1.000, 210.000], mean observation: 0.434 [0.000, 84.000], loss: 3.889342, mean_absolute_error: 0.772989, mean_q: 0.783358, mean_eps: 0.100000\n",
      " 170763/175000: episode: 4766, duration: 0.910s, episode steps: 23, steps per second: 25, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 131.087 [10.000, 210.000], mean observation: 0.120 [0.000, 46.000], loss: 0.294128, mean_absolute_error: 0.768682, mean_q: 0.693766, mean_eps: 0.100000\n",
      " 170796/175000: episode: 4767, duration: 1.382s, episode steps: 33, steps per second: 24, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 143.182 [7.000, 210.000], mean observation: 0.284 [0.000, 66.000], loss: 2.922376, mean_absolute_error: 0.790501, mean_q: 0.681860, mean_eps: 0.100000\n",
      " 170841/175000: episode: 4768, duration: 1.813s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 158.378 [38.000, 204.000], mean observation: 0.512 [0.000, 90.000], loss: 400.214503, mean_absolute_error: 2.692209, mean_q: 1.952002, mean_eps: 0.100000\n",
      " 170872/175000: episode: 4769, duration: 1.286s, episode steps: 31, steps per second: 24, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.548 [45.000, 212.000], mean observation: 0.311 [0.000, 62.000], loss: 7.062956, mean_absolute_error: 0.837932, mean_q: 0.406722, mean_eps: 0.100000\n",
      " 170902/175000: episode: 4770, duration: 1.250s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.067 [1.000, 212.000], mean observation: 0.319 [0.000, 60.000], loss: 0.642916, mean_absolute_error: 0.811524, mean_q: 0.415955, mean_eps: 0.100000\n",
      " 170943/175000: episode: 4771, duration: 1.591s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.902 [1.000, 212.000], mean observation: 0.529 [0.000, 82.000], loss: 18.380707, mean_absolute_error: 0.888519, mean_q: 0.396819, mean_eps: 0.100000\n",
      " 170972/175000: episode: 4772, duration: 1.204s, episode steps: 29, steps per second: 24, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 116.517 [1.000, 184.000], mean observation: 0.295 [0.000, 58.000], loss: 22.721098, mean_absolute_error: 0.915771, mean_q: 0.407458, mean_eps: 0.100000\n",
      " 171008/175000: episode: 4773, duration: 1.516s, episode steps: 36, steps per second: 24, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 139.917 [0.000, 173.000], mean observation: 0.312 [0.000, 72.000], loss: 5.013429, mean_absolute_error: 0.844079, mean_q: 0.529024, mean_eps: 0.100000\n",
      " 171042/175000: episode: 4774, duration: 1.391s, episode steps: 34, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 124.735 [1.000, 210.000], mean observation: 0.496 [0.000, 68.000], loss: 0.459832, mean_absolute_error: 0.806945, mean_q: 0.433879, mean_eps: 0.100000\n",
      " 171077/175000: episode: 4775, duration: 1.394s, episode steps: 35, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.571 [1.000, 184.000], mean observation: 0.337 [0.000, 70.000], loss: 0.099549, mean_absolute_error: 0.807297, mean_q: 0.377584, mean_eps: 0.100000\n",
      " 171105/175000: episode: 4776, duration: 1.075s, episode steps: 28, steps per second: 26, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.679 [4.000, 219.000], mean observation: 0.264 [0.000, 56.000], loss: 0.824411, mean_absolute_error: 0.808016, mean_q: 0.354831, mean_eps: 0.100000\n",
      " 171137/175000: episode: 4777, duration: 1.321s, episode steps: 32, steps per second: 24, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 136.656 [57.000, 173.000], mean observation: 0.279 [0.000, 64.000], loss: 10.648420, mean_absolute_error: 0.857465, mean_q: 0.314591, mean_eps: 0.100000\n",
      " 171191/175000: episode: 4778, duration: 2.062s, episode steps: 54, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 129.278 [19.000, 195.000], mean observation: 0.659 [0.000, 108.000], loss: 0.375876, mean_absolute_error: 0.799082, mean_q: 0.348384, mean_eps: 0.100000\n",
      " 171235/175000: episode: 4779, duration: 1.765s, episode steps: 44, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.909 [24.000, 214.000], mean observation: 0.499 [0.000, 88.000], loss: 1.020821, mean_absolute_error: 0.793574, mean_q: 0.346270, mean_eps: 0.100000\n",
      " 171273/175000: episode: 4780, duration: 1.533s, episode steps: 38, steps per second: 25, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.289 [26.000, 176.000], mean observation: 0.340 [0.000, 76.000], loss: 1.366284, mean_absolute_error: 0.793882, mean_q: 0.412890, mean_eps: 0.100000\n",
      " 171310/175000: episode: 4781, duration: 1.384s, episode steps: 37, steps per second: 27, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 139.514 [10.000, 216.000], mean observation: 0.399 [0.000, 74.000], loss: 0.147571, mean_absolute_error: 0.781256, mean_q: 0.389678, mean_eps: 0.100000\n",
      " 171349/175000: episode: 4782, duration: 1.529s, episode steps: 39, steps per second: 26, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.385 [2.000, 218.000], mean observation: 0.296 [0.000, 78.000], loss: 61.137783, mean_absolute_error: 1.050374, mean_q: 0.265506, mean_eps: 0.100000\n",
      " 171388/175000: episode: 4783, duration: 1.774s, episode steps: 39, steps per second: 22, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 132.000 [5.000, 214.000], mean observation: 0.313 [0.000, 78.000], loss: 0.164172, mean_absolute_error: 0.778949, mean_q: 0.267895, mean_eps: 0.100000\n",
      " 171405/175000: episode: 4784, duration: 0.812s, episode steps: 17, steps per second: 21, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 133.294 [71.000, 204.000], mean observation: 0.084 [0.000, 34.000], loss: 0.115155, mean_absolute_error: 0.790883, mean_q: 0.375626, mean_eps: 0.100000\n",
      " 171436/175000: episode: 4785, duration: 1.378s, episode steps: 31, steps per second: 23, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 146.484 [59.000, 204.000], mean observation: 0.291 [0.000, 62.000], loss: 1.059881, mean_absolute_error: 0.789286, mean_q: 0.311826, mean_eps: 0.100000\n",
      " 171461/175000: episode: 4786, duration: 1.118s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.960 [11.000, 217.000], mean observation: 0.211 [0.000, 50.000], loss: 0.335496, mean_absolute_error: 0.786732, mean_q: 0.369186, mean_eps: 0.100000\n",
      " 171506/175000: episode: 4787, duration: 1.806s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 137.133 [5.000, 218.000], mean observation: 0.547 [0.000, 90.000], loss: 66.913189, mean_absolute_error: 1.084651, mean_q: 0.407003, mean_eps: 0.100000\n",
      " 171527/175000: episode: 4788, duration: 0.848s, episode steps: 21, steps per second: 25, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 101.571 [10.000, 211.000], mean observation: 0.183 [0.000, 42.000], loss: 66.900895, mean_absolute_error: 1.087214, mean_q: 0.390790, mean_eps: 0.100000\n",
      " 171577/175000: episode: 4789, duration: 1.902s, episode steps: 50, steps per second: 26, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 117.660 [10.000, 196.000], mean observation: 0.267 [0.000, 100.000], loss: 2.699298, mean_absolute_error: 0.795954, mean_q: 0.495133, mean_eps: 0.100000\n",
      " 171610/175000: episode: 4790, duration: 1.237s, episode steps: 33, steps per second: 27, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 152.182 [29.000, 207.000], mean observation: 0.389 [0.000, 66.000], loss: 51097.783776, mean_absolute_error: 228.074267, mean_q: 2.626136, mean_eps: 0.100000\n",
      " 171634/175000: episode: 4791, duration: 0.969s, episode steps: 24, steps per second: 25, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 130.792 [119.000, 198.000], mean observation: 0.078 [0.000, 48.000], loss: 9.501136, mean_absolute_error: 0.833447, mean_q: 0.351961, mean_eps: 0.100000\n",
      " 171653/175000: episode: 4792, duration: 0.771s, episode steps: 19, steps per second: 25, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 142.632 [119.000, 218.000], mean observation: 0.097 [0.000, 38.000], loss: 18.341618, mean_absolute_error: 0.874020, mean_q: 0.441382, mean_eps: 0.100000\n",
      " 171698/175000: episode: 4793, duration: 1.781s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.133 [27.000, 223.000], mean observation: 0.290 [0.000, 90.000], loss: 2.034029, mean_absolute_error: 0.807437, mean_q: 0.429678, mean_eps: 0.100000\n",
      " 171727/175000: episode: 4794, duration: 1.097s, episode steps: 29, steps per second: 26, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 122.862 [7.000, 204.000], mean observation: 0.218 [0.000, 58.000], loss: 0.155083, mean_absolute_error: 0.794009, mean_q: 0.455289, mean_eps: 0.100000\n",
      " 171749/175000: episode: 4795, duration: 0.907s, episode steps: 22, steps per second: 24, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 140.273 [27.000, 191.000], mean observation: 0.124 [0.000, 44.000], loss: 18.414282, mean_absolute_error: 0.874781, mean_q: 0.470245, mean_eps: 0.100000\n",
      " 171785/175000: episode: 4796, duration: 1.404s, episode steps: 36, steps per second: 26, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 143.944 [68.000, 192.000], mean observation: 0.188 [0.000, 72.000], loss: 1.431179, mean_absolute_error: 0.794278, mean_q: 0.464200, mean_eps: 0.100000\n",
      " 171826/175000: episode: 4797, duration: 1.577s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.659 [13.000, 191.000], mean observation: 0.226 [0.000, 82.000], loss: 80.233409, mean_absolute_error: 1.146329, mean_q: 0.409573, mean_eps: 0.100000\n",
      " 171847/175000: episode: 4798, duration: 0.829s, episode steps: 21, steps per second: 25, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 98.429 [38.000, 191.000], mean observation: 0.167 [0.000, 42.000], loss: 0.166318, mean_absolute_error: 0.785947, mean_q: 0.247024, mean_eps: 0.100000\n",
      " 171898/175000: episode: 4799, duration: 1.996s, episode steps: 51, steps per second: 26, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 99.529 [22.000, 191.000], mean observation: 0.423 [0.000, 102.000], loss: 24528.666386, mean_absolute_error: 109.934151, mean_q: 1.607160, mean_eps: 0.100000\n",
      " 171934/175000: episode: 4800, duration: 1.405s, episode steps: 36, steps per second: 26, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 130.333 [68.000, 191.000], mean observation: 0.312 [0.000, 72.000], loss: 0.702934, mean_absolute_error: 0.807620, mean_q: 0.231815, mean_eps: 0.100000\n",
      " 171977/175000: episode: 4801, duration: 1.635s, episode steps: 43, steps per second: 26, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.744 [62.000, 204.000], mean observation: 0.563 [0.000, 86.000], loss: 9.067812, mean_absolute_error: 0.851483, mean_q: 0.189851, mean_eps: 0.100000\n",
      " 172016/175000: episode: 4802, duration: 1.595s, episode steps: 39, steps per second: 24, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.077 [52.000, 191.000], mean observation: 0.284 [0.000, 78.000], loss: 2.058925, mean_absolute_error: 0.819644, mean_q: 0.207615, mean_eps: 0.100000\n",
      " 172040/175000: episode: 4803, duration: 1.026s, episode steps: 24, steps per second: 23, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 101.667 [52.000, 191.000], mean observation: 0.184 [0.000, 48.000], loss: 0.153787, mean_absolute_error: 0.805299, mean_q: 0.248377, mean_eps: 0.100000\n",
      " 172086/175000: episode: 4804, duration: 1.833s, episode steps: 46, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.065 [52.000, 195.000], mean observation: 0.369 [0.000, 92.000], loss: 0.077067, mean_absolute_error: 0.801067, mean_q: 0.245538, mean_eps: 0.100000\n",
      " 172117/175000: episode: 4805, duration: 1.221s, episode steps: 31, steps per second: 25, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 182.194 [12.000, 202.000], mean observation: 0.093 [0.000, 62.000], loss: 0.101600, mean_absolute_error: 0.802784, mean_q: 0.448405, mean_eps: 0.100000\n",
      " 172158/175000: episode: 4806, duration: 1.544s, episode steps: 41, steps per second: 27, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 130.146 [58.000, 202.000], mean observation: 0.196 [0.000, 82.000], loss: 0.093213, mean_absolute_error: 0.786046, mean_q: 0.373804, mean_eps: 0.100000\n",
      " 172181/175000: episode: 4807, duration: 0.901s, episode steps: 23, steps per second: 26, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 128.130 [7.000, 207.000], mean observation: 0.109 [0.000, 46.000], loss: 0.095870, mean_absolute_error: 0.783937, mean_q: 0.453480, mean_eps: 0.100000\n",
      " 172215/175000: episode: 4808, duration: 1.290s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 156.471 [64.000, 204.000], mean observation: 0.263 [0.000, 68.000], loss: 67.311270, mean_absolute_error: 1.077533, mean_q: 0.340417, mean_eps: 0.100000\n",
      " 172239/175000: episode: 4809, duration: 0.967s, episode steps: 24, steps per second: 25, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.583 [38.000, 191.000], mean observation: 0.143 [0.000, 48.000], loss: 17.950079, mean_absolute_error: 0.853970, mean_q: 0.401788, mean_eps: 0.100000\n",
      " 172266/175000: episode: 4810, duration: 1.346s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 110.074 [16.000, 202.000], mean observation: 0.096 [0.000, 54.000], loss: 1.137724, mean_absolute_error: 0.773950, mean_q: 0.367637, mean_eps: 0.100000\n",
      " 172291/175000: episode: 4811, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.480 [7.000, 202.000], mean observation: 0.112 [0.000, 50.000], loss: 120.941390, mean_absolute_error: 1.313425, mean_q: 0.421225, mean_eps: 0.100000\n",
      " 172322/175000: episode: 4812, duration: 1.431s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 68.903 [7.000, 176.000], mean observation: 0.109 [0.000, 62.000], loss: 0.216355, mean_absolute_error: 0.771770, mean_q: 0.524278, mean_eps: 0.100000\n",
      " 172350/175000: episode: 4813, duration: 1.195s, episode steps: 28, steps per second: 23, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.321 [7.000, 191.000], mean observation: 0.190 [0.000, 56.000], loss: 2.534148, mean_absolute_error: 0.787915, mean_q: 0.503151, mean_eps: 0.100000\n",
      " 172402/175000: episode: 4814, duration: 2.145s, episode steps: 52, steps per second: 24, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.058 [7.000, 197.000], mean observation: 0.450 [0.000, 104.000], loss: 1.234781, mean_absolute_error: 0.776801, mean_q: 0.438228, mean_eps: 0.100000\n",
      " 172445/175000: episode: 4815, duration: 2.013s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.465 [7.000, 192.000], mean observation: 0.347 [0.000, 86.000], loss: 0.121793, mean_absolute_error: 0.770761, mean_q: 0.319447, mean_eps: 0.100000\n",
      " 172496/175000: episode: 4816, duration: 2.193s, episode steps: 51, steps per second: 23, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 166.078 [1.000, 211.000], mean observation: 0.543 [0.000, 102.000], loss: 7.987081, mean_absolute_error: 0.797805, mean_q: 0.338336, mean_eps: 0.100000\n",
      " 172527/175000: episode: 4817, duration: 1.301s, episode steps: 31, steps per second: 24, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 142.645 [27.000, 211.000], mean observation: 0.209 [0.000, 62.000], loss: 24.550784, mean_absolute_error: 0.875651, mean_q: 0.373577, mean_eps: 0.100000\n",
      " 172561/175000: episode: 4818, duration: 1.436s, episode steps: 34, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.382 [27.000, 211.000], mean observation: 0.287 [0.000, 68.000], loss: 0.091857, mean_absolute_error: 0.769036, mean_q: 0.466855, mean_eps: 0.100000\n",
      " 172596/175000: episode: 4819, duration: 1.474s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 139.800 [12.000, 212.000], mean observation: 0.331 [0.000, 70.000], loss: 0.116252, mean_absolute_error: 0.778766, mean_q: 0.330866, mean_eps: 0.100000\n",
      " 172619/175000: episode: 4820, duration: 0.975s, episode steps: 23, steps per second: 24, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 168.957 [0.000, 212.000], mean observation: 0.209 [0.000, 46.000], loss: 2.914585, mean_absolute_error: 0.795827, mean_q: 0.353932, mean_eps: 0.100000\n",
      " 172651/175000: episode: 4821, duration: 1.424s, episode steps: 32, steps per second: 22, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 145.000 [0.000, 212.000], mean observation: 0.327 [0.000, 64.000], loss: 0.088267, mean_absolute_error: 0.785297, mean_q: 0.332784, mean_eps: 0.100000\n",
      " 172677/175000: episode: 4822, duration: 1.187s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 152.808 [44.000, 212.000], mean observation: 0.279 [0.000, 52.000], loss: 6.807835, mean_absolute_error: 0.821561, mean_q: 0.331432, mean_eps: 0.100000\n",
      " 172708/175000: episode: 4823, duration: 1.255s, episode steps: 31, steps per second: 25, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.645 [43.000, 212.000], mean observation: 0.332 [0.000, 62.000], loss: 1.900135, mean_absolute_error: 0.792701, mean_q: 0.315865, mean_eps: 0.100000\n",
      " 172744/175000: episode: 4824, duration: 1.577s, episode steps: 36, steps per second: 23, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 119.833 [7.000, 212.000], mean observation: 0.377 [0.000, 72.000], loss: 0.353966, mean_absolute_error: 0.791345, mean_q: 0.370961, mean_eps: 0.100000\n",
      " 172776/175000: episode: 4825, duration: 1.380s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.438 [36.000, 195.000], mean observation: 0.318 [0.000, 64.000], loss: 21.101715, mean_absolute_error: 0.882650, mean_q: 0.318922, mean_eps: 0.100000\n",
      " 172806/175000: episode: 4826, duration: 1.267s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.567 [7.000, 213.000], mean observation: 0.341 [0.000, 60.000], loss: 0.448015, mean_absolute_error: 0.798124, mean_q: 0.442012, mean_eps: 0.100000\n",
      " 172838/175000: episode: 4827, duration: 1.318s, episode steps: 32, steps per second: 24, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.812 [10.000, 209.000], mean observation: 0.321 [0.000, 64.000], loss: 0.164526, mean_absolute_error: 0.802869, mean_q: 0.395943, mean_eps: 0.100000\n",
      " 172863/175000: episode: 4828, duration: 1.006s, episode steps: 25, steps per second: 25, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 103.720 [11.000, 213.000], mean observation: 0.243 [0.000, 50.000], loss: 33.409360, mean_absolute_error: 0.945231, mean_q: 0.344603, mean_eps: 0.100000\n",
      " 172905/175000: episode: 4829, duration: 1.782s, episode steps: 42, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.738 [2.000, 213.000], mean observation: 0.470 [0.000, 84.000], loss: 0.138453, mean_absolute_error: 0.807750, mean_q: 0.320474, mean_eps: 0.100000\n",
      " 172925/175000: episode: 4830, duration: 0.896s, episode steps: 20, steps per second: 22, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 108.900 [66.000, 222.000], mean observation: 0.216 [0.000, 40.000], loss: 213.877720, mean_absolute_error: 1.756543, mean_q: 0.390386, mean_eps: 0.100000\n",
      " 172951/175000: episode: 4831, duration: 1.122s, episode steps: 26, steps per second: 23, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.231 [26.000, 222.000], mean observation: 0.272 [0.000, 52.000], loss: 0.111595, mean_absolute_error: 0.801989, mean_q: 0.354920, mean_eps: 0.100000\n",
      " 172992/175000: episode: 4832, duration: 1.685s, episode steps: 41, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.195 [19.000, 221.000], mean observation: 0.535 [0.000, 82.000], loss: 1.716206, mean_absolute_error: 0.810360, mean_q: 0.316344, mean_eps: 0.100000\n",
      " 173022/175000: episode: 4833, duration: 1.248s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 158.300 [111.000, 191.000], mean observation: 0.221 [0.000, 60.000], loss: 2.808070, mean_absolute_error: 0.819851, mean_q: 0.354676, mean_eps: 0.100000\n",
      " 173057/175000: episode: 4834, duration: 1.434s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 161.429 [111.000, 203.000], mean observation: 0.304 [0.000, 70.000], loss: 0.101595, mean_absolute_error: 0.789339, mean_q: 0.314778, mean_eps: 0.100000\n",
      " 173084/175000: episode: 4835, duration: 1.186s, episode steps: 27, steps per second: 23, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 149.630 [53.000, 183.000], mean observation: 0.239 [0.000, 54.000], loss: 28.234457, mean_absolute_error: 0.921230, mean_q: 0.287249, mean_eps: 0.100000\n",
      " 173124/175000: episode: 4836, duration: 1.649s, episode steps: 40, steps per second: 24, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 151.775 [29.000, 183.000], mean observation: 0.394 [0.000, 80.000], loss: 3.853963, mean_absolute_error: 0.818361, mean_q: 0.323253, mean_eps: 0.100000\n",
      " 173168/175000: episode: 4837, duration: 1.978s, episode steps: 44, steps per second: 22, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.773 [43.000, 221.000], mean observation: 0.475 [0.000, 88.000], loss: 0.181204, mean_absolute_error: 0.797311, mean_q: 0.373247, mean_eps: 0.100000\n",
      " 173206/175000: episode: 4838, duration: 1.539s, episode steps: 38, steps per second: 25, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.658 [39.000, 221.000], mean observation: 0.419 [0.000, 76.000], loss: 27.069514, mean_absolute_error: 0.926199, mean_q: 0.384402, mean_eps: 0.100000\n",
      " 173241/175000: episode: 4839, duration: 1.434s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 151.514 [23.000, 221.000], mean observation: 0.336 [0.000, 70.000], loss: 0.631977, mean_absolute_error: 0.825782, mean_q: 0.398987, mean_eps: 0.100000\n",
      " 173301/175000: episode: 4840, duration: 2.432s, episode steps: 60, steps per second: 25, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 102.350 [10.000, 223.000], mean observation: 0.741 [0.000, 120.000], loss: 3.826705, mean_absolute_error: 0.867639, mean_q: 0.640171, mean_eps: 0.100000\n",
      " 173333/175000: episode: 4841, duration: 1.378s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.938 [10.000, 199.000], mean observation: 0.185 [0.000, 64.000], loss: 1.400576, mean_absolute_error: 0.885291, mean_q: 0.741365, mean_eps: 0.100000\n",
      " 173360/175000: episode: 4842, duration: 1.135s, episode steps: 27, steps per second: 24, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 89.296 [10.000, 199.000], mean observation: 0.183 [0.000, 54.000], loss: 1.202340, mean_absolute_error: 0.909851, mean_q: 0.862157, mean_eps: 0.100000\n",
      " 173401/175000: episode: 4843, duration: 1.656s, episode steps: 41, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.512 [28.000, 199.000], mean observation: 0.238 [0.000, 82.000], loss: 5.284052, mean_absolute_error: 0.966694, mean_q: 0.759488, mean_eps: 0.100000\n",
      " 173447/175000: episode: 4844, duration: 1.793s, episode steps: 46, steps per second: 26, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 64.587 [27.000, 207.000], mean observation: 0.621 [0.000, 92.000], loss: 15.788327, mean_absolute_error: 1.041267, mean_q: 0.643672, mean_eps: 0.100000\n",
      " 173497/175000: episode: 4845, duration: 2.039s, episode steps: 50, steps per second: 25, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 87.900 [12.000, 219.000], mean observation: 0.587 [0.000, 100.000], loss: 1.404954, mean_absolute_error: 0.984169, mean_q: 0.698051, mean_eps: 0.100000\n",
      " 173538/175000: episode: 4846, duration: 1.583s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.634 [12.000, 202.000], mean observation: 0.319 [0.000, 82.000], loss: 0.816984, mean_absolute_error: 1.005261, mean_q: 0.734096, mean_eps: 0.100000\n",
      " 173571/175000: episode: 4847, duration: 1.312s, episode steps: 33, steps per second: 25, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 62.758 [12.000, 212.000], mean observation: 0.304 [0.000, 66.000], loss: 0.353538, mean_absolute_error: 1.018125, mean_q: 0.595370, mean_eps: 0.100000\n",
      " 173617/175000: episode: 4848, duration: 1.867s, episode steps: 46, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 59.370 [12.000, 172.000], mean observation: 0.515 [0.000, 92.000], loss: 3296.073355, mean_absolute_error: 15.782140, mean_q: 2.045535, mean_eps: 0.100000\n",
      " 173660/175000: episode: 4849, duration: 1.742s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 58.488 [12.000, 201.000], mean observation: 0.494 [0.000, 86.000], loss: 0.196204, mean_absolute_error: 0.998946, mean_q: 0.589172, mean_eps: 0.100000\n",
      " 173705/175000: episode: 4850, duration: 1.933s, episode steps: 45, steps per second: 23, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 65.822 [12.000, 223.000], mean observation: 0.518 [0.000, 90.000], loss: 0.163925, mean_absolute_error: 0.981616, mean_q: 0.445390, mean_eps: 0.100000\n",
      " 173771/175000: episode: 4851, duration: 2.601s, episode steps: 66, steps per second: 25, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 70.924 [10.000, 214.000], mean observation: 0.975 [0.000, 132.000], loss: 1.114500, mean_absolute_error: 0.964969, mean_q: 0.295869, mean_eps: 0.100000\n",
      " 173813/175000: episode: 4852, duration: 1.735s, episode steps: 42, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.738 [12.000, 222.000], mean observation: 0.252 [0.000, 84.000], loss: 0.167379, mean_absolute_error: 0.936364, mean_q: 0.308988, mean_eps: 0.100000\n",
      " 173856/175000: episode: 4853, duration: 1.708s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.349 [12.000, 220.000], mean observation: 0.397 [0.000, 86.000], loss: 0.140854, mean_absolute_error: 0.927036, mean_q: 0.301269, mean_eps: 0.100000\n",
      " 173882/175000: episode: 4854, duration: 1.060s, episode steps: 26, steps per second: 25, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 73.615 [21.000, 220.000], mean observation: 0.256 [0.000, 52.000], loss: 432.343676, mean_absolute_error: 3.273870, mean_q: 5.489378, mean_eps: 0.100000\n",
      " 173914/175000: episode: 4855, duration: 1.249s, episode steps: 32, steps per second: 26, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 163.500 [12.000, 218.000], mean observation: 0.124 [0.000, 64.000], loss: 35.756470, mean_absolute_error: 1.075611, mean_q: 0.539653, mean_eps: 0.100000\n",
      " 173942/175000: episode: 4856, duration: 1.103s, episode steps: 28, steps per second: 25, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 82.714 [6.000, 223.000], mean observation: 0.313 [0.000, 56.000], loss: 7.295796, mean_absolute_error: 0.950207, mean_q: 0.581462, mean_eps: 0.100000\n",
      " 173957/175000: episode: 4857, duration: 0.632s, episode steps: 15, steps per second: 24, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 160.333 [96.000, 223.000], mean observation: 0.098 [0.000, 30.000], loss: 0.090477, mean_absolute_error: 0.909846, mean_q: 0.590049, mean_eps: 0.100000\n",
      " 174000/175000: episode: 4858, duration: 1.694s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.953 [11.000, 223.000], mean observation: 0.412 [0.000, 86.000], loss: 2.257458, mean_absolute_error: 0.906952, mean_q: 0.497584, mean_eps: 0.100000\n",
      " 174045/175000: episode: 4859, duration: 1.842s, episode steps: 45, steps per second: 24, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.111 [21.000, 223.000], mean observation: 0.441 [0.000, 90.000], loss: 25.603784, mean_absolute_error: 0.998857, mean_q: 0.561578, mean_eps: 0.100000\n",
      " 174088/175000: episode: 4860, duration: 1.622s, episode steps: 43, steps per second: 27, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 140.512 [16.000, 223.000], mean observation: 0.495 [0.000, 86.000], loss: 32.546565, mean_absolute_error: 1.027187, mean_q: 0.518644, mean_eps: 0.100000\n",
      " 174130/175000: episode: 4861, duration: 1.689s, episode steps: 42, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.762 [21.000, 223.000], mean observation: 0.382 [0.000, 84.000], loss: 0.148499, mean_absolute_error: 0.887947, mean_q: 0.416445, mean_eps: 0.100000\n",
      " 174166/175000: episode: 4862, duration: 1.450s, episode steps: 36, steps per second: 25, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.167 [21.000, 223.000], mean observation: 0.321 [0.000, 72.000], loss: 0.105481, mean_absolute_error: 0.886527, mean_q: 0.313162, mean_eps: 0.100000\n",
      " 174219/175000: episode: 4863, duration: 2.032s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.981 [20.000, 215.000], mean observation: 0.621 [0.000, 106.000], loss: 0.133662, mean_absolute_error: 0.898131, mean_q: 0.359103, mean_eps: 0.100000\n",
      " 174268/175000: episode: 4864, duration: 2.048s, episode steps: 49, steps per second: 24, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.327 [21.000, 215.000], mean observation: 0.470 [0.000, 98.000], loss: 0.567729, mean_absolute_error: 0.898289, mean_q: 0.286421, mean_eps: 0.100000\n",
      " 174297/175000: episode: 4865, duration: 1.180s, episode steps: 29, steps per second: 25, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 64.655 [21.000, 154.000], mean observation: 0.128 [0.000, 58.000], loss: 0.516515, mean_absolute_error: 0.890948, mean_q: 0.244901, mean_eps: 0.100000\n",
      " 174331/175000: episode: 4866, duration: 1.314s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.000 [21.000, 215.000], mean observation: 0.278 [0.000, 68.000], loss: 0.075693, mean_absolute_error: 0.885914, mean_q: 0.168258, mean_eps: 0.100000\n",
      " 174385/175000: episode: 4867, duration: 2.235s, episode steps: 54, steps per second: 24, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 160.611 [3.000, 221.000], mean observation: 0.598 [0.000, 108.000], loss: 13.329325, mean_absolute_error: 0.947654, mean_q: 0.102058, mean_eps: 0.100000\n",
      " 174433/175000: episode: 4868, duration: 1.856s, episode steps: 48, steps per second: 26, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 165.542 [3.000, 221.000], mean observation: 0.491 [0.000, 96.000], loss: 4.131916, mean_absolute_error: 0.902447, mean_q: 0.089602, mean_eps: 0.100000\n",
      " 174486/175000: episode: 4869, duration: 2.022s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 64.415 [21.000, 191.000], mean observation: 0.543 [0.000, 106.000], loss: 3.695922, mean_absolute_error: 0.889822, mean_q: 0.078346, mean_eps: 0.100000\n",
      " 174520/175000: episode: 4870, duration: 1.332s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.412 [25.000, 200.000], mean observation: 0.262 [0.000, 68.000], loss: 0.386380, mean_absolute_error: 0.869039, mean_q: 0.135576, mean_eps: 0.100000\n",
      " 174569/175000: episode: 4871, duration: 2.021s, episode steps: 49, steps per second: 24, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.245 [18.000, 189.000], mean observation: 0.465 [0.000, 98.000], loss: 6.800269, mean_absolute_error: 0.901599, mean_q: 0.255886, mean_eps: 0.100000\n",
      " 174601/175000: episode: 4872, duration: 1.292s, episode steps: 32, steps per second: 25, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 94.344 [25.000, 189.000], mean observation: 0.293 [0.000, 64.000], loss: 0.137794, mean_absolute_error: 0.881650, mean_q: 0.322293, mean_eps: 0.100000\n",
      " 174637/175000: episode: 4873, duration: 1.420s, episode steps: 36, steps per second: 25, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 110.694 [33.000, 208.000], mean observation: 0.345 [0.000, 72.000], loss: 2.040972, mean_absolute_error: 0.889029, mean_q: 0.465575, mean_eps: 0.100000\n",
      " 174670/175000: episode: 4874, duration: 1.229s, episode steps: 33, steps per second: 27, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.879 [33.000, 202.000], mean observation: 0.270 [0.000, 66.000], loss: 4.456309, mean_absolute_error: 0.892913, mean_q: 0.451376, mean_eps: 0.100000\n",
      " 174721/175000: episode: 4875, duration: 2.063s, episode steps: 51, steps per second: 25, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.941 [33.000, 189.000], mean observation: 0.462 [0.000, 102.000], loss: 0.369816, mean_absolute_error: 0.863120, mean_q: 0.341565, mean_eps: 0.100000\n",
      " 174759/175000: episode: 4876, duration: 1.463s, episode steps: 38, steps per second: 26, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 160.500 [33.000, 211.000], mean observation: 0.266 [0.000, 76.000], loss: 0.085133, mean_absolute_error: 0.866423, mean_q: 0.458974, mean_eps: 0.100000\n",
      " 174788/175000: episode: 4877, duration: 1.206s, episode steps: 29, steps per second: 24, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 136.621 [46.000, 189.000], mean observation: 0.183 [0.000, 58.000], loss: 0.115012, mean_absolute_error: 0.847566, mean_q: 0.380788, mean_eps: 0.100000\n",
      " 174813/175000: episode: 4878, duration: 1.041s, episode steps: 25, steps per second: 24, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 120.320 [13.000, 213.000], mean observation: 0.236 [0.000, 50.000], loss: 0.192087, mean_absolute_error: 0.838429, mean_q: 0.357407, mean_eps: 0.100000\n",
      " 174842/175000: episode: 4879, duration: 1.139s, episode steps: 29, steps per second: 25, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 120.724 [33.000, 218.000], mean observation: 0.309 [0.000, 58.000], loss: 6.427044, mean_absolute_error: 0.861638, mean_q: 0.347207, mean_eps: 0.100000\n",
      " 174885/175000: episode: 4880, duration: 1.731s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 87.814 [11.000, 195.000], mean observation: 0.499 [0.000, 86.000], loss: 1.791559, mean_absolute_error: 0.861799, mean_q: 0.205183, mean_eps: 0.100000\n",
      " 174894/175000: episode: 4881, duration: 0.318s, episode steps: 9, steps per second: 28, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 126.889 [29.000, 197.000], mean observation: 0.042 [0.000, 18.000], loss: 0.500019, mean_absolute_error: 0.853906, mean_q: 0.131513, mean_eps: 0.100000\n",
      " 174928/175000: episode: 4882, duration: 1.380s, episode steps: 34, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 144.676 [29.000, 221.000], mean observation: 0.315 [0.000, 68.000], loss: 0.243343, mean_absolute_error: 0.871459, mean_q: 0.187529, mean_eps: 0.100000\n",
      " 174971/175000: episode: 4883, duration: 1.704s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 152.419 [3.000, 211.000], mean observation: 0.347 [0.000, 86.000], loss: 302.914660, mean_absolute_error: 2.372906, mean_q: 1.691216, mean_eps: 0.100000\n",
      "done, took 3574.997 seconds\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X _ _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ O _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 30\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 45\n",
      "Episode 2: reward: -1.000, steps: 34\n",
      "Episode 3: reward: -1.000, steps: 34\n",
      "Episode 4: reward: -1.000, steps: 32\n",
      "Episode 5: reward: -1.000, steps: 47\n",
      "Episode 6: reward: -1.000, steps: 38\n",
      "Episode 7: reward: -1.000, steps: 16\n",
      "Episode 8: reward: -1.000, steps: 21\n",
      "Episode 9: reward: -1.000, steps: 31\n",
      "Episode 10: reward: -1.000, steps: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ccba2ca58>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=175000, log_interval=10000, verbose = 2 )\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights('latest_dqn', overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 10 episodes.\n",
    "dqn.test(env, nb_episodes=1, visualize=True)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SarsaAgent(model=model_policy, nb_actions=225)\n",
    "sarsa.compile(Adam(), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarsa.load_weights(\"sarsa_Renju9_weights.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = RenjuTEST(1, 'kn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ O _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ O _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ O _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 48\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 45\n",
      "Episode 2: reward: -1.000, steps: 44\n",
      "Episode 3: reward: -1.000, steps: 25\n",
      "Episode 4: reward: -1.000, steps: 41\n",
      "Episode 5: reward: -1.000, steps: 22\n",
      "Episode 6: reward: -1.000, steps: 20\n",
      "Episode 7: reward: -1.000, steps: 33\n",
      "Episode 8: reward: -1.000, steps: 19\n",
      "Episode 9: reward: -1.000, steps: 45\n",
      "Episode 10: reward: -1.000, steps: 56\n",
      "Opponent: kn\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O _ _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ O _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O O O _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 53\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 34\n",
      "Episode 2: reward: -1.000, steps: 36\n",
      "Episode 3: reward: -1.000, steps: 26\n",
      "Episode 4: reward: -1.000, steps: 36\n",
      "Episode 5: reward: -1.000, steps: 39\n",
      "Episode 6: reward: -1.000, steps: 34\n",
      "Episode 7: reward: -1.000, steps: 39\n",
      "Episode 8: reward: -1.000, steps: 41\n",
      "Episode 9: reward: -1.000, steps: 34\n",
      "Episode 10: reward: -1.000, steps: 57\n",
      "Opponent: kn\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d27f3e4b2383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRenjuTEST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kn'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neural'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3e186e96ea69>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, mode)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 320\u001b[0;31m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    dqn.fit(env, nb_steps=17500, log_interval=10000, verbose = 0 )\n",
    "\n",
    "    # After training is done, we save the final weights.\n",
    "    dqn.save_weights('dqn', overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    dqn.test(env, nb_episodes=1, visualize=True)\n",
    "    env = RenjuTEST(1, 'kn' if random.randint(0,1) == 0 else 'neural')\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "    print('Opponent:', env.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
