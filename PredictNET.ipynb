{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train-1.renju') as f:\n",
    "    content = f.readlines()\n",
    "data = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "' '.join(list(filter(lambda a: 'p' not in a, data[34])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('renju-mod', 'w')\n",
    "for elem in data:\n",
    "    list(filter(lambda a: 'p' not in a, elem))\n",
    "    f.write(' '.join(list(filter(lambda a: 'p' not in a, elem))) +'\\n')  # python will convert \\n to os.linesep\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number(letter):\n",
    "    #lst = {'a':0, 'b':1,}\n",
    "\n",
    "    return ord(letter) - ord('a')\n",
    "\n",
    "def get_pos(elem):\n",
    "    w = get_number(elem[0])\n",
    "    h = int(elem[1:]) - 1\n",
    "    return w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data[int(0.7 * len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[: int(0.7 * len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def parallel_game(_):\n",
    "    elem = data[np.random.randint(len(data))]\n",
    "    cur_pos = np.zeros((15,15, 3))\n",
    "    game = elem.split()\n",
    "    for iterator in range(1, len(game) - 1):\n",
    "        elem = game[iterator]\n",
    "        w, h = get_pos(elem)\n",
    "        if iterator % 2 == 1:\n",
    "            cur_pos[w][h][0] = 1 \n",
    "        else:\n",
    "            cur_pos[w][h][1] = 1\n",
    "\n",
    "\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if cur_pos[i][j][0] > 0:\n",
    "                    cur_pos[i][j][2] += 1\n",
    "                if cur_pos[i][j][1] > 0:\n",
    "                    cur_pos[i][j][2] += 1\n",
    "\n",
    "\n",
    "        cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "        w, h = get_pos(game[iterator + 1])\n",
    "        ans = h * 15 + w\n",
    "        #print(cur_pos)\n",
    "        return (cur_pos_to_app, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen(data, test=False):\n",
    "    part_size = len(data)//30\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        while(len(neural_games) < part_size):\n",
    "            elem = data[np.random.randint(len(data))]\n",
    "            cur_pos = np.zeros((15,15, 3))\n",
    "            game = elem.split()\n",
    "            for iterator in range(1, len(game) - 1):\n",
    "                elem = game[iterator]\n",
    "                w, h = get_pos(elem)\n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos[w][h][0] = 1 \n",
    "                else:\n",
    "                    cur_pos[w][h][1] = 1 \n",
    "                \n",
    "                cur_pos[:, :, 2:] = cur_pos.sum(2).reshape((15,15,1))\n",
    "                        \n",
    "                \n",
    "                \n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "                    w, h = get_pos(game[iterator + 1])\n",
    "                    ans = h * 15 + w\n",
    "                    #print(cur_pos)\n",
    "\n",
    "                    neural_ans.append(ans)\n",
    "                    neural_games.append(cur_pos_to_app)\n",
    "                    if (len(neural_games) >= part_size):\n",
    "                        break\n",
    "\n",
    "\n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        neural_games.resize((len(neural_games), 1, 15,15,3))\n",
    "        yield (neural_games, neural_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def data_gen_lr(data, test=False):\n",
    "    part_size = len(data) // 4100\n",
    "    while(1):\n",
    "        neural_games = []\n",
    "        neural_ans = []\n",
    "        while(len(neural_games) < part_size):\n",
    "            elem = data[np.random.randint(len(data))]\n",
    "            cur_pos = np.zeros(225)\n",
    "            game = elem.split()\n",
    "            for iterator in range(1, len(game) - 1):\n",
    "                elem = game[iterator]\n",
    "                w, h = get_pos(elem)\n",
    "                if iterator % 2 == 1:\n",
    "                    cur_pos[h * 15 + w] = 1 \n",
    "                else:\n",
    "                    cur_pos[h * 15 + w] = 2 \n",
    "                \n",
    "                cur_pos_to_app = copy.deepcopy(cur_pos)\n",
    "                w, h = get_pos(game[iterator + 1])\n",
    "                ans = h * 15 + w\n",
    "                #print(cur_pos)\n",
    "                neural_ans.append(ans)\n",
    "                neural_games.append(cur_pos_to_app)\n",
    "                if (len(neural_games) >= part_size):\n",
    "                    break\n",
    "\n",
    "\n",
    "        neural_games = np.array(neural_games)\n",
    "        neural_ans = np.array(neural_ans)\n",
    "        #neural_ans = label_binarize(neural_ans, classes=[i for i in range(225)])\n",
    "        #neural_games.resize((part_size, 1, 15,15,3))\n",
    "        yield (neural_games, neural_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=8,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "#clf = LinearRegression(n_jobs=8)\n",
    "#clf = KNeighborsClassifier(n_jobs = 4, n_neighbors=3)\n",
    "clf = LogisticRegression(n_jobs = 8)\n",
    "#clf = Ridge()\n",
    "\n",
    "X, y = next(data_gen_lr(data))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7 µs ± 1.66 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = clf.predict([tester_1.lr_pos])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979 ns ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "random.randint(0,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 µs ± 19.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.random.randint(0,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = 'LinearClass'\n",
    "_ = joblib.dump(clf, filename, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('LinearClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020689655172413793"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_test, y_true = next(data_gen_lr(test_data))\n",
    "y_pred = clf.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred[i] = int(y_pred[i])\n",
    "    \n",
    "\"\"\" \n",
    "for elem in clf.predict(X_test):\n",
    "    if elem > int(elem) + 0.5:\n",
    "        y_pred.append(int(elem) + 1)\n",
    "    else:\n",
    "        y_pred.append(int(elem))\n",
    "\"\"\"        \n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:379: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of layers/models '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "reshape_3 (Reshape)              (None, 15, 15, 3)     0           reshape_input_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 11, 11, 4)     304         reshape_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 11, 11, 4)     16          convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 7, 7, 8)       808         batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 5, 5, 16)      1168        convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 5, 5, 16)      64          convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 3, 3, 32)      4640        batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 288)           0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bayesiandense_4 (BayesianDense)  (None, 512)           295936      flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 512)           2048        bayesiandense_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bayesiandense_5 (BayesianDense)  (None, 256)           262656      batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 225)           57825       bayesiandense_5[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 625,465\n",
      "Trainable params: 624,401\n",
      "Non-trainable params: 1,064\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "#from keras.regularizers import WeightRegularizer\n",
    "\n",
    "model_policy = Sequential()\n",
    "\n",
    "model_policy.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model_policy.add(Convolution2D(4, 5, 5, activation='relu'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(Convolution2D(8, 5, 5, activation='relu'))\n",
    "model_policy.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "\n",
    "model_policy.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "#model_policy.add(Dense(1100, activation='relu', name='fczero'))\n",
    "model_policy.add(BayesianDense(output_dim=512, activation='relu'))\n",
    "model_policy.add(BatchNormalization())\n",
    "model_policy.add(BayesianDense(output_dim=256, activation='relu'))\n",
    "#model_policy.add(Dropout(0.1))\n",
    "model_policy.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model_policy.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.load_weights(\"weights.11-0.283181.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_policy = load_model(\"zeros_policy_44\")\n",
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/axcel/.theano/compiledir_Linux-4.10--generic-x86_64-with-Ubuntu-17.04-zesty-x86_64-3.5.3-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 46309 samples, validate on 19846 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91979bc6caf4d0a87dab330aaa506e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae5a0e769ed409a9c2df9d2973d66a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "rng_mrg gpu implementation does not support more than (2**31 -1) samples\nApply node that caused the error: GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}(<CudaNdarrayType(float32, vector)>, MakeVector{dtype='int64'}.0)\nToposort index: 320\nInputs types: [CudaNdarrayType(float32, vector), TensorType(int64, vector)]\nInputs shapes: [(92160,), (1,)]\nInputs strides: [(1,), (8,)]\nInputs values: ['not shown', array([2963275776])]\nOutputs clients: [['output'], [Shape_i{0}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1), GpuSubtensor{:int64:}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1, ScalarFromTensor.0), GpuSubtensor{int64::}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1, ScalarFromTensor.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: rng_mrg gpu implementation does not support more than (2**31 -1) samples",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-311f46df3035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     model_policy.fit(neural_games, neural_ans, batch_size=20096, nb_epoch=100, verbose=2,\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_saver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      validation_data = next(data_gen(test_data, test=False)))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: rng_mrg gpu implementation does not support more than (2**31 -1) samples\nApply node that caused the error: GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}(<CudaNdarrayType(float32, vector)>, MakeVector{dtype='int64'}.0)\nToposort index: 320\nInputs types: [CudaNdarrayType(float32, vector), TensorType(int64, vector)]\nInputs shapes: [(92160,), (1,)]\nInputs strides: [(1,), (8,)]\nInputs values: ['not shown', array([2963275776])]\nOutputs clients: [['output'], [Shape_i{0}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1), GpuSubtensor{:int64:}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1, ScalarFromTensor.0), GpuSubtensor{int64::}(GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.1, ScalarFromTensor.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):\n",
    "    neural_games, neural_ans = next(data_gen(data))\n",
    "    model_policy.fit(neural_games, neural_ans, batch_size=20096, nb_epoch=100, verbose=2,\n",
    "                    callbacks=[model_saver, TQDMNotebookCallback()],\n",
    "                     validation_data = next(data_gen(test_data, test=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ntensorboard = keras.callbacks.TensorBoard(log_dir='./logs', \\n                                          histogram_freq=0, \\n                                          write_graph=True,\\n                                          write_images=True)\\n                                          \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback, TQDMCallback\n",
    "# keras, model definition...\n",
    "model_saver = ModelCheckpoint(\"weights.{epoch:02d}-{val_acc:.6f}.hdf5\",\n",
    "                                              monitor='val_acc', verbose=0,\n",
    "                                              save_best_only=True, save_weights_only=False,\n",
    "                                              mode='auto')\n",
    "\n",
    "\"\"\" \n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                          histogram_freq=0, \n",
    "                                          write_graph=True,\n",
    "                                          write_images=True)\n",
    "                                          \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.save('zeros_policy_44')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n\nCaused by op 'Variable_15/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-965bb7b58cb9>\", line 5, in <module>\n    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\", line 1097, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1774, in fit_generator\n    self._make_train_function()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1001, in _make_train_function\n    self.total_loss)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in <listcomp>\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 520, in zeros\n    dtype, name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 286, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 305, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-965bb7b58cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n\u001b[0m",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1803\u001b[0m         callbacks.set_params({\n\u001b[1;32m   1804\u001b[0m             \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n\nCaused by op 'Variable_15/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/axcel/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-965bb7b58cb9>\", line 5, in <module>\n    callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/models.py\", line 1097, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1774, in fit_generator\n    self._make_train_function()\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1001, in _make_train_function\n    self.total_loss)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in get_updates\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 393, in <listcomp>\n    ms = [K.zeros(shape) for shape in shapes]\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 520, in zeros\n    dtype, name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 286, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 305, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/axcel/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,2200]\n\t [[Node: Variable_15/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_15\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_15, Const_21)]]\n"
     ]
    }
   ],
   "source": [
    "model_policy.fit_generator(data_gen(data, True),\n",
    "        steps_per_epoch=500, epochs=350, verbose=0,\n",
    "                   validation_data = data_gen(test_data, test=True), validation_steps=2,\n",
    "                   callbacks=[model_saver, tensorboard, TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_check_env = load_model(\"ReshapedModelsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model, save_model\n",
    "\n",
    "#save_model(model, \"med_modelv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy.load_weights(\"weights.04-0.400417.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"weights.11-0.283181.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_policy = load_model(\"ReshapedModel0.46-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"Med5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.18-0.381543.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#clf = joblib.load('LinearClass')\n",
    "\n",
    "#model = load_model('zeros_policy_44')\n",
    "\n",
    "class RenjuTEST(object): \n",
    "    def __init__(self, player, mode):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = 1\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        self.mode = mode\n",
    "        self.obl_action = None\n",
    "        self.win_action = None\n",
    "        self.latest_move = (7,7)\n",
    "        \n",
    "        \n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        self.moves_done += 1\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "            self.lr_pos[action] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "            self.lr_pos[action] = 2\n",
    "            \n",
    "            \n",
    "            \n",
    "        w = (action % 15) + 5\n",
    "        h = (action // 15) + 5\n",
    "        \n",
    "        self.latest_move = (w - 5, h - 5)\n",
    "        \n",
    "        self.ext_pos[w][h] = self.cur_player\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0 or self.cur_pos[i][j][1] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.cur_pos[:, :, 2:] = self.cur_pos.sum(2).reshape((15,15,1))\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        rdiag = [0, 0, 0]\n",
    "        rdiaginv = [0, 0, 0]\n",
    "        ldiag = [0, 0, 0]\n",
    "        ldiaginv = [0, 0, 0]\n",
    "        rrow = [0, 0, 0]\n",
    "        lrow = [0, 0, 0]\n",
    "        rcol = [0, 0, 0]\n",
    "        lcol = [0, 0, 0]\n",
    "        \n",
    "        broken = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            if self.ext_pos[w + i][h + i]:\n",
    "                if not broken[0]:\n",
    "                    rdiag[int(self.ext_pos[w + i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[0] = 1\n",
    "                \n",
    "            if self.ext_pos[w][h + i]:\n",
    "                if not broken[1]:\n",
    "                    rcol[int(self.ext_pos[w][h + i])] += 1\n",
    "                else:\n",
    "                    broken[1] = 1\n",
    "                \n",
    "            if self.ext_pos[w - i][h + i]:\n",
    "                if not broken[2]:\n",
    "                    ldiag[int(self.ext_pos[w - i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[2] = 1\n",
    "                \n",
    "            \n",
    "            if self.ext_pos[w - i][h]:\n",
    "                if not broken[3]:\n",
    "                    lrow[int(self.ext_pos[w - i][h])] += 1\n",
    "                else:\n",
    "                    broken[3] = 1\n",
    "            \n",
    "            if self.ext_pos[w - i][h - i]:\n",
    "                if not broken[4]:\n",
    "                    rdiaginv[int(self.ext_pos[w - i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[4] = 1\n",
    "            \n",
    "            if self.ext_pos[w][h - i]:\n",
    "                if not broken[5]:\n",
    "                    lcol[int(self.ext_pos[w][h - i])] += 1\n",
    "                else:\n",
    "                    broken[5] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h - i]:\n",
    "                if not broken[6]:\n",
    "                    ldiaginv[int(self.ext_pos[w + i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[6] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h]:\n",
    "                if not broken[7]:\n",
    "                    rrow[int(self.ext_pos[w + i][h])] += 1\n",
    "                else:\n",
    "                    broken[7] = 1\n",
    "                    \n",
    "        \n",
    "        self.obl_action = None\n",
    "        first_obl_action = None\n",
    "        second_obl_action = None\n",
    "        \n",
    "        rightdiag = rdiag[self.cur_player] + rdiaginv[self.cur_player]\n",
    "        \n",
    "        if rightdiag >= 4:\n",
    "            if rightdiag >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w + rdiag[self.cur_player]][h + rdiag[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 + rdiag[self.cur_player] < 15 and 0 <= h - 5 + rdiag[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 + rdiag[self.cur_player]) * 15 + w - 5 + rdiag[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w - rdiaginv[self.cur_player]][h - rdiaginv[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 - rdiaginv[self.cur_player] < 15 and 0 <= h - 5 - rdiaginv[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 - rdiaginv[self.cur_player]) * 15 + w - 5 - rdiaginv[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if rightdiag == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "        leftdiag = ldiag[self.cur_player] + ldiaginv[self.cur_player]\n",
    "        \n",
    "        if leftdiag >= 4:\n",
    "            if leftdiag >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w - ldiag[self.cur_player]][h + ldiag[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 - ldiag[self.cur_player] < 15 and 0 <= h - 5 + ldiag[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 + ldiag[self.cur_player]) * 15 + w - 5 - ldiag[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w + ldiaginv[self.cur_player]][h - ldiaginv[self.cur_player]] == 0 and \n",
    "                 0 <= w - 5 + ldiaginv[self.cur_player] < 15 and 0 <= h - 5 - ldiaginv[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 - ldiaginv[self.cur_player]) * 15 + w - 5 + ldiaginv[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if leftdiag == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "        \n",
    "        row = rrow[self.cur_player] + lrow[self.cur_player]\n",
    "        \n",
    "        if row >= 4:\n",
    "            if row >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w - lrow[self.cur_player]][h] == 0 and \n",
    "                 0 <= w - 5 - lrow[self.cur_player] < 15):\n",
    "                    first_obl_action = (h - 5) * 15 + w - 5 - lrow[self.cur_player]\n",
    "                \n",
    "                if (self.ext_pos[w + rrow[self.cur_player]][h] == 0 and \n",
    "                 0 <= w - 5 + rrow[self.cur_player] < 15):\n",
    "                    second_obl_action = (h - 5) * 15 + w - 5 + rrow[self.cur_player]\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if row == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "        \n",
    "        \n",
    "        col = lcol[self.cur_player] + rcol[self.cur_player]\n",
    "        \n",
    "        if col >= 4:\n",
    "            if col >= 6:\n",
    "                if self.player == self.cur_player:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                if (self.ext_pos[w][h - lcol[self.cur_player]] == 0 and \n",
    "                 0 <= h - 5 - lcol[self.cur_player] <= 15):\n",
    "                    first_obl_action = (h - 5 - lcol[self.cur_player]) * 15 + w - 5\n",
    "                \n",
    "                if (self.ext_pos[w][h + rcol[self.cur_player]] == 0 and \n",
    "                 0 <= h - 5 + rcol[self.cur_player] <= 15):\n",
    "                    second_obl_action = (h - 5 + rcol[self.cur_player]) * 15 + w - 5\n",
    "                \n",
    "                if first_obl_action or second_obl_action:\n",
    "                    if col == 5 and self.player == self.cur_player:\n",
    "                        self.win_action = (first_obl_action, second_obl_action)\n",
    "                        \n",
    "                    if first_obl_action:\n",
    "                        self.obl_action = first_obl_action\n",
    "                    else:\n",
    "                        self.obl_action = second_obl_action\n",
    "                    \n",
    "                    \n",
    "        #if self.win_action != None:\n",
    "        #    print(\"WARNING, WIN POSITION:\", self.win_action)\n",
    "            \n",
    "        #print(\"pure\", first_obl_action, second_obl_action)\n",
    "        #print(self.win_action)\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        done = True if (self.moves_done == 225 or reward != 0) else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if self.moves_done == 225:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def net_ans(self, model, mode = 'all'):\n",
    "        s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        if mode == 'one':\n",
    "            return np.argmax(s)\n",
    "        else:\n",
    "            #return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "            return np.argsort(s)\n",
    "    \n",
    "    def simulation(self, model, mode):\n",
    "        fake_env = RenjuTEST(1, 'neural')\n",
    "        fake_env.cur_pos = np.copy(self.cur_pos)\n",
    "        fake_env.cur_player = self.cur_player\n",
    "        fake_env.player = self.player\n",
    "        fake_env.action_space = self.action_space = 225\n",
    "        fake_env.moves_done = self.moves_done\n",
    "        fake_env.ext_pos = np.copy(self.ext_pos)\n",
    "        fake_env.lr_pos = np.copy(self.lr_pos)\n",
    "        fake_env.mode = self.mode\n",
    "        reward = 0\n",
    "        while reward == 0:\n",
    "            \"\"\"\n",
    "            action = random.randint(0,224)\n",
    "            while fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = random.randint(0,224)\n",
    "                \n",
    "            \"\"\" \n",
    "            #if mode == 'kn':\n",
    "            s = clf.predict_proba([fake_env.lr_pos])[0]\n",
    "            #else:\n",
    "            #    s = model.predict(np.array([[fake_env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if fake_env.cur_pos[act % 15][act // 15][0] == 0 and fake_env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "            #\"\"\"\n",
    "            cur_pos, reward, done, info = fake_env.in_step(action)\n",
    "            \n",
    "        if reward == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def step(self, action, mode = 'kn'):\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        \n",
    "        if self.win_action != None:\n",
    "            #print(\"WIN ACTION\")\n",
    "            if self.win_action[0]:\n",
    "                if self.ext_pos[(self.win_action[0] % 15) + 5][(self.win_action[0] // 15) + 5] == 0:\n",
    "                    action = self.win_action[0]\n",
    "                    cur_pos, reward, done, info = self.in_step(self.win_action[0])\n",
    "                    return cur_pos, reward, done, action\n",
    "            if self.win_action[1]:\n",
    "                if self.ext_pos[(self.win_action[1] % 15) + 5][(self.win_action[1] // 15) + 5] == 0:\n",
    "                    action = self.win_action[1]\n",
    "                    cur_pos, reward, done, info = self.in_step(self.win_action[1])\n",
    "                    return cur_pos, reward, done, action\n",
    "            \n",
    "        \n",
    "        if self.obl_action:\n",
    "            action = self.obl_action\n",
    "            cur_pos, reward, done, info = self.in_step(self.obl_action)\n",
    "            return cur_pos, reward, done, action\n",
    "        \n",
    "        if reward != 0:\n",
    "            #self.render()\n",
    "            if done:\n",
    "                self.reset()\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            if self.mode == 'neural':\n",
    "                s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "            else:\n",
    "                s = clf.predict_proba([self.lr_pos])[0]\n",
    "            #plt.figure()\n",
    "            #k = s.reshape((15,15))\n",
    "            #plt.imshow(k, cmap='hot', interpolation='nearest')\n",
    "            #plt.show()\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                \n",
    "                action = 0\n",
    "                #print(net_move)\n",
    "                for act in net_move:\n",
    "                    if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "                #print(\"Net:\", action)\n",
    "\n",
    "            rnd = np.random.randint(1, 100)\n",
    "            if rnd < -10:\n",
    "                action = np.random.randint(0, 224)\n",
    "                while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                    action = np.random.randint(0, 224)\n",
    "            print('Net action:', action)\n",
    "            cur_pos, reward, done, info = self.in_step(action)\n",
    "            return cur_pos, reward, done, action\n",
    "    \n",
    "    def learning(self, opponent):\n",
    "        if self.moves_done % 2 == 0:\n",
    "            s = opponent.predict(np.array([[self.cur_pos]]))[0]\n",
    "        else:\n",
    "            s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        action = np.argmax(s)\n",
    "        if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "            net_move = np.argsort(s)[::-1]\n",
    "\n",
    "            action = 0\n",
    "            #print(net_move)\n",
    "            for act in net_move:\n",
    "                if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                    action = act\n",
    "                    break\n",
    "            #print(\"Net:\", action)\n",
    "        rnd = np.random.randint(1, 100)\n",
    "        if rnd < 5:\n",
    "            action = np.random.randint(0, 224)\n",
    "            while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = np.random.randint(0, 224)\n",
    "\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        return cur_pos, reward, done, action\n",
    "            \n",
    "            \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in reversed(range(15)):\n",
    "                for i in range(15):\n",
    "                    flag = 0\n",
    "                    if self.cur_pos[i][j][0] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.cur_pos[i][j][1] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        if mode == 'debug':\n",
    "            for j in reversed(range(25)):\n",
    "                for i in range(25):\n",
    "                    flag = 0\n",
    "                    if self.ext_pos[i][j] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.ext_pos[i][j] == 2:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "\n",
    "        return self.cur_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opponent = load_model('cross_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look(cur_pos):\n",
    "    for j in reversed(range(15)):\n",
    "        for i in range(15):\n",
    "            flag = 0\n",
    "            if cur_pos[i][j] == 1:\n",
    "                flag = 1\n",
    "                print(\"X\", end=' ')\n",
    "            if not flag:\n",
    "                print(\"_\", end=' ')\n",
    "        print('\\n', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class MCSTNode():\n",
    "    def __init__(self, env, len_nodes = 0):\n",
    "        self.wins = 0\n",
    "        self.all_games = 0\n",
    "        self.childs = []\n",
    "        self.env = env\n",
    "        self.parent = None\n",
    "        self.action = None\n",
    "        self.index = len_nodes\n",
    "        self.child_actions = []\n",
    "        \n",
    "\n",
    "class UCT():\n",
    "    def __init__(self, env, model, mode):        \n",
    "        self.root = MCSTNode(env)\n",
    "        self.all_games = 0\n",
    "        self.constant = 1.1\n",
    "        self.cur_node = self.root\n",
    "        self.model = model\n",
    "        self.cur_pos = self.root\n",
    "        self.mode = mode\n",
    "        self.len_nodes = 1\n",
    "        self.edges = []\n",
    "        \n",
    "    \n",
    "    def get_child_UCT_list(self, node):\n",
    "        return list(map(self.get_UCT_stat, node.childs))\n",
    "        \n",
    "    def explore(self, number):\n",
    "        self.cur_node = self.root\n",
    "        temp_look = np.zeros((15,15))\n",
    "        if number == 0:\n",
    "            print(\"EXPLORE ZERO NUM\")\n",
    "            overview = 4\n",
    "            w, h = self.root.env.latest_move\n",
    "            w_plus = min(15 - w, overview)\n",
    "            w_minus = min(w, overview)\n",
    "            \n",
    "            h_plus = min(15 - h, overview)\n",
    "            h_minus = min(h, overview)\n",
    "            \n",
    "            print(\"original\", w, h)\n",
    "            print(\"w-range\", w_minus, w_plus)\n",
    "            print(\"h-range\", h_minus, h_plus)\n",
    "            \n",
    "            for j in range(h - h_minus, h + h_plus):\n",
    "                for i in range(w - w_minus, w + w_plus):\n",
    "                    if ((i,j) != (w,h)) and self.cur_node.env.ext_pos[i + 5][j + 5] == 0:\n",
    "                        temp_look[i][j] = 1\n",
    "                        new_node_action = (j * 15) + i\n",
    "                        #print('new_node action', new_node_action)\n",
    "\n",
    "                        new_env = deepcopy(self.cur_node.env)\n",
    "                        new_env.cur_player = 1 if new_env.cur_player == 2 else 2\n",
    "                        cur_pos, reward, done, action = new_env.in_step(new_node_action)\n",
    "                        #print(\"REWARD NEW ROOT NODE\", reward)\n",
    "                        #new_env.render()\n",
    "\n",
    "                        new_node = MCSTNode(new_env, self.len_nodes)\n",
    "                        \n",
    "                        if reward != 0:\n",
    "                            new_node.wins = 999999999999999999\n",
    "                            print(\"PIZDEC\")\n",
    "                        \n",
    "                        new_node.env.cur_player = 1 if new_env.cur_player == 2 else 2\n",
    "\n",
    "                        new_node.parent = self.cur_node\n",
    "                        new_node.action = new_node_action\n",
    "                        self.cur_node.child_actions.append(new_node_action)\n",
    "                        self.len_nodes += 1\n",
    "\n",
    "                        self.edges.append((self.cur_node.index, new_node.index))\n",
    "\n",
    "                        run = new_node.env.simulation(self.model, self.mode)\n",
    "                        self.all_games += 1\n",
    "\n",
    "                        self.cur_node.childs.append(new_node)\n",
    "                        self.cur_node = new_node\n",
    "\n",
    "                        while(self.cur_node.parent != None):\n",
    "                            self.cur_node.wins += run\n",
    "                            self.cur_node.all_games += 1\n",
    "                            self.cur_node = self.cur_node.parent\n",
    "                        else:\n",
    "                            self.cur_node.wins += run\n",
    "                            self.cur_node.all_games += 1\n",
    "            look(temp_look)                    \n",
    "        UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "        \n",
    "        while len(UCT_list) > 0 and  max(UCT_list) > self.get_UCT_stat(self.cur_node):\n",
    "            #print(UCT_list, \"I am at \", self.cur_node.index)\n",
    "            #print(UCT_list)\n",
    "            best_node = np.argmax(UCT_list)\n",
    "            self.cur_node = self.cur_node.childs[best_node]\n",
    "            UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "    \n",
    "    def expand(self):\n",
    "    \n",
    "        \n",
    "        #if self.mode == 'kn':\n",
    "        #    s = clf.predict_proba([self.cur_node.env.lr_pos])[0]\n",
    "        #else:\n",
    "        s = model.predict(np.array([[self.cur_node.env.cur_pos]]))[0]\n",
    "        action = np.argmax(s)\n",
    "        if self.cur_node.env.cur_pos[action % 15][action // 15][0] != 0 or self.cur_node.env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "            net_move = np.argsort(s)[::-1]\n",
    "            action = 0\n",
    "            for act in net_move:\n",
    "                if act not in self.cur_node.child_actions and self.cur_node.env.cur_pos[act % 15][act // 15][0] == 0 and self.cur_node.env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                    action = act\n",
    "                    break        \n",
    "\n",
    "        new_node_action = action\n",
    "        #print('New Node!')\n",
    "\n",
    "        new_env = deepcopy(self.cur_node.env)\n",
    "        cur_pos, reward, done, action = new_env.in_step(new_node_action)\n",
    "\n",
    "        new_node = MCSTNode(new_env, self.len_nodes)\n",
    "\n",
    "        if reward != 0:\n",
    "            new_node.wins = 999999999999999999\n",
    "        new_node.parent = self.cur_node\n",
    "        new_node.action = new_node_action\n",
    "        self.cur_node.child_actions.append(new_node_action)\n",
    "        self.len_nodes += 1\n",
    "\n",
    "        self.edges.append((self.cur_node.index, new_node.index))\n",
    "\n",
    "        run = new_node.env.simulation(self.model, self.mode)\n",
    "        self.all_games += 1\n",
    "\n",
    "        self.cur_node.childs.append(new_node)\n",
    "        self.cur_node = new_node\n",
    "\n",
    "        while(self.cur_node.parent != None):\n",
    "            self.cur_node.wins += run\n",
    "            self.cur_node.all_games += 1\n",
    "            self.cur_node = self.cur_node.parent\n",
    "        else:\n",
    "            self.cur_node.wins += run\n",
    "            self.cur_node.all_games += 1\n",
    "\n",
    "        \"\"\"       \n",
    "        else:\n",
    "            run = self.cur_node.env.simulation(self.model, self.model)\n",
    "            self.all_games += 1\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "        \"\"\"  \n",
    "            \n",
    "            \n",
    "    def search(self, time_limit):\n",
    "        begin = time()\n",
    "        point_action = np.zeros((225))\n",
    "        cur_pos = self.root.env.cur_pos\n",
    "        ext_pos = self.root.env.ext_pos\n",
    "        cur_player = 1 if self.root.env.cur_player == 2 else 2\n",
    "        \"\"\"\n",
    "        w,h = self.root.env.latest_move\n",
    "        w += 5\n",
    "        h += 5\n",
    "        \"\"\" \n",
    "        \n",
    "        for h_iter in range(15):\n",
    "            for w_iter in range(15):\n",
    "                w = w_iter + 5\n",
    "                h = h_iter + 5\n",
    "                rdiag = \"\"\n",
    "                rdiaginv = \"\"\n",
    "                ldiag = \"\"\n",
    "                ldiaginv = \"\"\n",
    "                rrow = \"\"\n",
    "                lrow = \"\"\n",
    "                rcol = \"\"\n",
    "                lcol = \"\"\n",
    "\n",
    "\n",
    "                rdiag_def = \"\"\n",
    "                rdiaginv_def = \"\"\n",
    "                ldiag_def = \"\"\n",
    "                ldiaginv_def = \"\"\n",
    "                rrow_def = \"\"\n",
    "                lrow_def = \"\"\n",
    "                rcol_def = \"\"\n",
    "                lcol_def = \"\"\n",
    "\n",
    "\n",
    "                rdiag_pos = []\n",
    "                rdiaginv_pos = []\n",
    "                ldiag_pos = []\n",
    "                ldiaginv_pos = []\n",
    "                rrow_pos = []\n",
    "                lrow_pos = []\n",
    "                rcol_pos = []\n",
    "                lcol_pos = []\n",
    "\n",
    "                rdiaginv_pos.append((w,h))\n",
    "                ldiaginv_pos.append((w,h))\n",
    "                lrow_pos.append((w,h))\n",
    "                lcol_pos.append((w,h))\n",
    "\n",
    "\n",
    "                for i in range(5):            \n",
    "                    if ext_pos[w + i][h + i] == 0:\n",
    "                        rdiag += \"0\"\n",
    "                        rdiag_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w + i][h + i] == cur_player:\n",
    "                            rdiag += \"1\"\n",
    "                            rdiag_def += \"2\"\n",
    "                        else:\n",
    "                            rdiag_def += \"1\"\n",
    "                            rdiag += \"2\"\n",
    "                    rdiag_pos.append((w + i, h + i))\n",
    "\n",
    "                    if ext_pos[w][h + i] == 0:\n",
    "                        rcol += \"0\"\n",
    "                        rcol_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w][h + i] == cur_player:\n",
    "                            rcol += \"1\"\n",
    "                            rcol_def += \"2\"\n",
    "                        else:\n",
    "                            rcol_def += \"1\"\n",
    "                            rcol += \"2\"\n",
    "\n",
    "                    rcol_pos.append((w, h + i))        \n",
    "\n",
    "                    if ext_pos[w - i][h + i] == 0:\n",
    "                        ldiag += \"0\"\n",
    "                        ldiag_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w - i][h + i] == cur_player:\n",
    "                            ldiag += \"1\"\n",
    "                            ldiag_def += \"2\"\n",
    "                        else:\n",
    "                            ldiag_def += \"1\"\n",
    "                            ldiag += \"2\"\n",
    "                    ldiag_pos.append((w - i, h + i))\n",
    "\n",
    "                    if ext_pos[w - i][h] == 0:\n",
    "                        lrow += \"0\"\n",
    "                        lrow_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w - i][h] == cur_player:\n",
    "                            lrow += \"1\"\n",
    "                            lrow_def += \"2\"\n",
    "                        else:\n",
    "                            lrow_def += \"1\"\n",
    "                            lrow += \"2\"\n",
    "\n",
    "                    lrow_pos.append((w - i, h))\n",
    "\n",
    "                    if ext_pos[w - i][h - i] == 0:\n",
    "                        rdiaginv += \"0\"\n",
    "                        rdiaginv_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w - i][h - i] == cur_player:\n",
    "                            rdiaginv += \"1\"\n",
    "                            rdiaginv_def += \"2\"\n",
    "                        else:\n",
    "                            rdiaginv_def += \"1\"\n",
    "                            rdiaginv += \"2\"\n",
    "                    rdiaginv_pos.append((w - i, h - i))\n",
    "\n",
    "                    if ext_pos[w][h - i] == 0:\n",
    "                        lcol += \"0\"\n",
    "                        lcol_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w][h - i] == cur_player:\n",
    "                            lcol += \"1\"\n",
    "                            lcol_def += \"2\"\n",
    "                        else:\n",
    "                            lcol_def += \"1\"\n",
    "                            lcol += \"2\"\n",
    "\n",
    "                    lcol_pos.append((w, h - i))\n",
    "\n",
    "                    if ext_pos[w + i][h - i] == 0:\n",
    "                        ldiaginv += \"0\"\n",
    "                        ldiaginv_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w + i][h - i] == cur_player:\n",
    "                            ldiaginv += \"1\"\n",
    "                            ldiaginv_def += \"2\"\n",
    "                        else:\n",
    "                            ldiaginv_def += \"1\"\n",
    "                            ldiaginv += \"2\"\n",
    "\n",
    "                    ldiaginv_pos.append((w + i, h - i))\n",
    "                    if ext_pos[w + i][h] == 0:\n",
    "                        rrow += \"0\"\n",
    "                        rrow_def += \"0\"\n",
    "                    else:\n",
    "                        if ext_pos[w + i][h] == cur_player:\n",
    "                            rrow += \"1\"\n",
    "                            rrow_def += \"2\"\n",
    "                        else:\n",
    "                            rrow_def += \"1\"\n",
    "                            rrow += \"2\"\n",
    "                    rrow_pos.append((w + i, h))\n",
    "\n",
    "\n",
    "\n",
    "                left_diag_full = ldiag[::-1] + ldiaginv[1:]\n",
    "                left_diag_full_pos = ldiag_pos[::-1] + ldiaginv_pos[1:]\n",
    "                left_diag_full_def = ldiag_def[::-1] + ldiaginv_def[1:]\n",
    "\n",
    "                right_diag_full = rdiag[::-1] + rdiaginv[1:]\n",
    "                right_diag_full_pos = rdiag_pos[::-1] + rdiaginv_pos[1:]\n",
    "                right_diag_full_def = rdiag_def[::-1] + rdiaginv_def[1:]\n",
    "\n",
    "                col_full = rcol[::-1] + lcol[1:]\n",
    "                col_full_pos = rcol_pos[::-1] + lcol_pos[1:]\n",
    "                col_full_def = rcol_def[::-1] + lcol_def[1:]\n",
    "\n",
    "\n",
    "                row_full = lrow[::-1] + rrow[1:]\n",
    "                row_full_pos = lrow_pos[::-1] + rrow_pos[1:]\n",
    "                row_full_def = lrow_def[::-1] + rrow_def[1:]\n",
    "\n",
    "                #print(\"left_diag\", left_diag_full)\n",
    "                #print(\"right_diag\", right_diag_full)\n",
    "                #print(\"column\", col_full)\n",
    "                #print(\"row\", row_full)\n",
    "\n",
    "                def find_occurances(what, where):\n",
    "                    import re\n",
    "                    return [m.start() for m in re.finditer(what, where)]\n",
    "\n",
    "                pattern = [ \n",
    "                ['11111', 99999],\n",
    "                ['011110', 7000], \n",
    "                ['01111', 4000], \n",
    "                ['11110', 4000],\n",
    "                ['010111', 2000],\n",
    "                ['011011', 2000],\n",
    "                ['011101', 2000],\n",
    "                ['111010', 2000],\n",
    "                ['110110', 2000],\n",
    "                ['101110', 2000],\n",
    "                ['01110', 3000],\n",
    "                ['0111', 1500],\n",
    "                ['1110', 1500],\n",
    "                ['01101', 800],\n",
    "                ['01011', 800],\n",
    "                ['11010', 800],\n",
    "                ['10110', 800],\n",
    "                ['0110', 200],\n",
    "                ['10', 100],\n",
    "                ['01', 100]\n",
    "                ]\n",
    "\n",
    "\n",
    "                comb_set = [[left_diag_full, left_diag_full_pos, left_diag_full_def], \n",
    "                            [right_diag_full, right_diag_full_pos, right_diag_full_def],\n",
    "                            [col_full, col_full_pos, col_full_def],\n",
    "                            [row_full, row_full_pos, row_full_def]]\n",
    "\n",
    "\n",
    "                for match in pattern:\n",
    "                    for comb in comb_set:\n",
    "                        #print(\"I FOUND\", match[0], \"IN\", comb[0], \"AND\", comb[2])\n",
    "                        positions_attack = find_occurances(match[0], comb[0])\n",
    "                        positions_def = find_occurances(match[0], comb[2])\n",
    "                        #print(\"FOUND\", positions_attack), positions_def\n",
    "\n",
    "                        #print(\"pos_attack\", positions_attack, match[0])\n",
    "                        #print(\"pos_def\", positions_def, match[0])\n",
    "\n",
    "                        for left in positions_attack:\n",
    "                            for i in range(left, left + len(match[0]) + 1):\n",
    "                                #print(\"BONUSES AT\", comb[1][i])\n",
    "                                point_action[comb[1][i][0] - 5 + (comb[1][i][1] - 5) * 15] += match[1] \n",
    "\n",
    "                        for left in positions_def:\n",
    "                            for i in range(left, left + len(match[0]) + 1):\n",
    "                                point_action[comb[1][i][0] - 5 + (comb[1][i][1] - 5) * 15] += match[1] * 1.1\n",
    "\n",
    "        for i in range(225):\n",
    "            if ext_pos[(i % 15) + 5][(i // 15) + 5]:\n",
    "                point_action[i] = -1\n",
    "        \n",
    "        for j in reversed(range(15)):\n",
    "            for i in range(15):\n",
    "                flag = 0\n",
    "                if point_action[j * 15 + i] != 0:\n",
    "                    flag = 1\n",
    "                    print(point_action[j * 15 + i], end=' ')\n",
    "                if not flag:\n",
    "                    print(\"_\", end=' ')\n",
    "            print('\\n', end='')\n",
    "\n",
    "        \n",
    "        number = 0\n",
    "        while (time() - begin) < time_limit - (time() - begin) * 0.95:\n",
    "            self.explore(number)\n",
    "            self.expand()\n",
    "            number += 1\n",
    "        root_values = list(map(self.get_stat, self.root.childs))\n",
    "        #print(root_values)\n",
    "        #print(root_UCT_values, len(self.root.childs))\n",
    "        if len(root_values) != 0:\n",
    "            best_child = np.argmax(root_values)\n",
    "        else:\n",
    "            self.root = self.root.childs[0]\n",
    "            return self.root.action\n",
    "        print(\"I CHOOSE\", self.root.childs[best_child].action)\n",
    "        self.root = self.root.childs[best_child]\n",
    "        \n",
    "        if max(point_action) < 100:\n",
    "            return self.root.action\n",
    "        else:\n",
    "            return np.argmax(point_action)\n",
    "                    \n",
    "    def get_UCT_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(self.all_games) + self.constant * math.sqrt(math.log(self.all_games / node.all_games))\n",
    "    \n",
    "    def get_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(self.all_games)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Example program to illustrate my issue with dynamic buttons.\n",
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "\n",
    "class my_app(Frame):\n",
    "    \"\"\"Basic Frame\"\"\"\n",
    "    def __init__(self, master, mode, time_for_search):\n",
    "        self.buttons = []\n",
    "        \"\"\"Init the Frame\"\"\"\n",
    "        Frame.__init__(self,master)\n",
    "        self.grid()\n",
    "        self.Create_Widgets()\n",
    "        self.tester = RenjuTEST(2 if mode == 'me' else 1, mode)\n",
    "        self.label = tkinter.Label(self, text='')\n",
    "        self.label.grid(column=7, row=16, sticky=W);   #creates label for image on window \n",
    "        self.tree = UCT(self.tester, model, mode)\n",
    "        self.mode = mode\n",
    "        self.time_for_search = time_for_search\n",
    "\n",
    "\n",
    "    def Create_Widgets(self):\n",
    "        for i in range(15):\n",
    "            for j in range(15): #Start creating buttons\n",
    "\n",
    "                button_id = i * 15 + j \n",
    "                #print(self.button_id)\n",
    "\n",
    "                self.newmessage = Button(self, #I want to bind the self.button_id to each button, so that it prints its number when clicked.\n",
    "                                         text = '',\n",
    "                                         anchor = W, command = lambda button_id=button_id: self.access(button_id))#Run the method\n",
    "\n",
    "                #Placing\n",
    "                self.newmessage.config(height = 1, width = 1)\n",
    "                self.newmessage.grid(row = 15 - i, column = j, sticky = NW)\n",
    "                self.buttons.append(self.newmessage)\n",
    "        \n",
    "    def access(self, b_id): #This is one of the areas where I need help. I want this to return the number of the button clicked.\n",
    "        self.b_id = b_id\n",
    "        print(b_id)\n",
    "        self.buttons[b_id].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.step(b_id)\n",
    "        self.tester.render(mode='human')\n",
    "        \n",
    "        self.buttons[info].config(text = 'X' if self.tester.cur_player == 2 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.in_step(b_id)\n",
    "        tree_act_right = None\n",
    "        if self.tester.obl_action:\n",
    "            tree_act_right = self.tester.obl_action\n",
    "            print(\"WARNING\")\n",
    "            \n",
    "        if self.tester.win_action != None:\n",
    "            print('Nyyyak')\n",
    "            if self.tester.win_action[0]:\n",
    "                if self.tester.ext_pos[(self.tester.win_action[0] % 15) + 5][(self.tester.win_action[0] // 15) + 5] == 0:\n",
    "                    tree_act_right = self.tester.win_action[0]\n",
    "            if self.tester.win_action[1]:\n",
    "                if self.tester.ext_pos[(self.tester.win_action[1] % 15) + 5][(self.tester.win_action[1] // 15) + 5] == 0:\n",
    "                    tree_act_right = self.tester.win_action[1]\n",
    "\n",
    "        \n",
    "        self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if b_id in self.tree.root.child_actions:\n",
    "            for node in self.tree.root.childs:\n",
    "                if node.action == b_id:\n",
    "                    self.tree.root = node\n",
    "                    break\n",
    "        else:\n",
    "        \"\"\"\n",
    "        self.tree = UCT(self.tester, model, self.mode)\n",
    "        #tree = UCT(self.tester, model, 'kn' if random.randint(0,1) == 0 else 'neuron')\n",
    "        self.tree.edges = []\n",
    "        tree_act = self.tree.search(self.time_for_search)\n",
    "        \n",
    "        #print(tree_act, tree.root.all_games)\n",
    "        #if tree_act_right:\n",
    "        #    tree_act = tree_act_right\n",
    "        self.buttons[tree_act].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        print(self.tree.root.all_games, tree_act)\n",
    "        cur_pos, reward, done, info = self.tester.in_step(tree_act)\n",
    "        self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 7 7\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X _ X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 48\n",
      "112\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ 300.0 _ 300.0 _ 300.0 _ _ _ _ _ \n",
      "_ _ _ _ _ _ 1200.0 1200.0 1200.0 _ _ _ _ _ _ \n",
      "_ _ _ _ _ 300.0 1200.0 -1.0 1200.0 300.0 _ _ _ _ _ \n",
      "_ _ _ _ _ _ 1200.0 1200.0 1200.0 _ _ _ _ _ _ \n",
      "_ 220.0 _ 330.0 _ 630.0 _ 300.0 _ 300.0 _ _ _ _ _ \n",
      "_ _ 1210.0 1320.0 1320.0 _ _ _ _ _ _ _ _ _ _ \n",
      "_ 220.0 1210.0 -1.0 1320.0 330.0 _ _ _ _ _ _ _ _ _ \n",
      "_ _ 1210.0 1210.0 1210.0 _ _ _ _ _ _ _ _ _ _ \n",
      "_ 220.0 _ 220.0 _ 220.0 _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 7 7\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X _ X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ X X X X X X X X _ _ _ _ \n",
      "_ _ _ _ X X X X X X X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 49\n",
      "2 49\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "113\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ 300.0 300.0 300.0 300.0 300.0 300.0 _ _ _ _ \n",
      "_ _ _ _ _ _ 1200.0 2400.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ _ _ _ _ 500.0 2300.0 -1.0 -1.0 2300.0 500.0 _ _ _ _ \n",
      "_ _ _ _ _ _ 1200.0 2400.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 220.0 330.0 330.0 330.0 630.0 630.0 300.0 300.0 300.0 300.0 _ _ _ _ \n",
      "_ _ 1210.0 2640.0 2640.0 1320.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 440.0 2420.0 -1.0 -1.0 2530.0 550.0 _ _ _ _ _ _ _ _ \n",
      "_ _ 1210.0 2420.0 2420.0 1210.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 220.0 220.0 220.0 220.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 8 7\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ X X X _ _ X X X _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ X X X X X X X X _ _ _ \n",
      "_ _ _ _ _ X X X X X X X _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 50\n",
      "2 63\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "110\n",
      "WARNING\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 _ 600.0 300.0 600.0 300.0 300.0 300.0 _ _ _ _ \n",
      "_ _ _ _ 1200.0 1200.0 2400.0 2400.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ _ _ 300.0 6000.0 -1.0 13100.0 -1.0 -1.0 7100.0 500.0 _ _ _ _ \n",
      "_ 440.0 _ 550.0 1200.0 1530.0 2400.0 2400.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 220.0 2420.0 2830.0 1650.0 930.0 630.0 600.0 300.0 300.0 300.0 _ _ _ _ \n",
      "_ 220.0 2420.0 -1.0 3960.0 1650.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 440.0 3630.0 -1.0 -1.0 2530.0 550.0 _ _ _ _ _ _ _ _ \n",
      "_ 220.0 1210.0 3630.0 2420.0 2420.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 440.0 220.0 220.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 5 7\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X _ X _ _ _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X _ X X X X X _ _ _ _ _ _ \n",
      "_ X X _ _ X X X X _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 51\n",
      "2 111\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "78\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 330.0 600.0 630.0 600.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ _ _ _ 1200.0 2520.0 3720.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 200.0 _ 600.0 900.0 -1.0 -1.0 -1.0 -1.0 900.0 300.0 _ _ _ _ \n",
      "_ 440.0 1100.0 900.0 2400.0 2850.0 3720.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 420.0 3520.0 -1.0 3180.0 1230.0 960.0 600.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ 220.0 3520.0 -1.0 5160.0 1650.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 640.0 3630.0 -1.0 -1.0 2830.0 550.0 _ _ _ _ _ _ _ _ \n",
      "_ 220.0 1210.0 2090.0 2420.0 2420.0 _ _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 220.0 220.0 220.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 3 5\n",
      "w-range 3 4\n",
      "h-range 4 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X X _ _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X X _ _ _ _ _ _ _ _ \n",
      "X X X _ X X X _ _ _ _ _ _ _ _ \n",
      "X X X _ X X X _ _ _ _ _ _ _ _ \n",
      "X X X _ _ X X _ _ _ _ _ _ _ _ \n",
      "X X X X X X X _ _ _ _ _ _ _ _ \n",
      "X X X X X X X _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 15\n",
      "2 64\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "80\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 330.0 600.0 630.0 600.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ _ _ _ 1200.0 2520.0 3720.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 200.0 _ 900.0 900.0 -1.0 -1.0 -1.0 -1.0 900.0 300.0 _ _ _ _ \n",
      "_ 440.0 800.0 900.0 4150.0 4050.0 4620.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 420.0 3520.0 -1.0 6580.0 -1.0 2160.0 900.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ 440.0 4730.0 -1.0 -1.0 5050.0 1750.0 _ _ _ _ _ _ _ _ \n",
      "_ 640.0 3630.0 -1.0 -1.0 3820.0 550.0 300.0 _ _ _ _ _ _ _ \n",
      "_ 220.0 880.0 2090.0 3630.0 2420.0 330.0 _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 220.0 440.0 220.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 5 5\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X _ X _ X X X _ _ _ _ _ _ \n",
      "_ X X _ _ X X X X _ _ _ _ _ _ \n",
      "_ X X _ _ X X X X _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ X X X X X X X X _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 17\n",
      "2 79\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "94\n",
      "WARNING\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 330.0 600.0 630.0 2100.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ _ 500.0 _ 1500.0 2520.0 33420.0 9000.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 200.0 330.0 2900.0 1800.0 -1.0 -1.0 -1.0 -1.0 900.0 300.0 _ _ _ _ \n",
      "_ 440.0 1100.0 3420.0 -1.0 18010.0 4920.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 420.0 3220.0 -1.0 -1.0 -1.0 1860.0 900.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ 440.0 34430.0 -1.0 -1.0 6370.0 2850.0 _ _ _ _ _ _ _ _ \n",
      "_ 2140.0 10120.0 -1.0 -1.0 3820.0 880.0 500.0 _ _ _ _ _ _ _ \n",
      "_ 440.0 880.0 2090.0 13640.0 2420.0 330.0 _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 220.0 1870.0 220.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 4 6\n",
      "w-range 4 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X X X _ _ _ _ _ _ _ \n",
      "X X X X X X X X _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X _ X X X _ _ _ _ _ _ _ \n",
      "X X X _ _ _ X X _ _ _ _ _ _ _ \n",
      "X X X _ _ X X X _ _ _ _ _ _ _ \n",
      "X X X _ _ X X X _ _ _ _ _ _ _ \n",
      "X X X X X X X X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 30\n",
      "2 62\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "61\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ O X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 330.0 600.0 630.0 2100.0 630.0 300.0 300.0 _ _ _ _ \n",
      "_ _ 500.0 _ 1500.0 2520.0 13920.0 9000.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 200.0 330.0 2900.0 1800.0 -1.0 -1.0 -1.0 -1.0 900.0 300.0 _ _ _ _ \n",
      "330.0 740.0 1430.0 3720.0 -1.0 18010.0 4920.0 3720.0 2400.0 1200.0 _ _ _ _ _ \n",
      "700.0 3710.0 5540.0 -1.0 -1.0 -1.0 1860.0 900.0 630.0 300.0 300.0 _ _ _ _ \n",
      "600.0 -1.0 -1.0 -1.0 -1.0 16380.0 4280.0 _ _ _ _ _ _ _ _ \n",
      "700.0 2410.0 12440.0 -1.0 -1.0 3820.0 880.0 500.0 _ _ _ _ _ _ _ \n",
      "110.0 740.0 1210.0 2390.0 14850.0 2420.0 330.0 _ _ _ _ _ _ _ _ \n",
      "_ 220.0 220.0 220.0 1870.0 440.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 1 4\n",
      "w-range 1 4\n",
      "h-range 4 4\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X X _ _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 1\n",
      "2 95\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ O X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "47\n",
      "Nyyyak\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ O X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ 300.0 330.0 600.0 630.0 2100.0 2280.0 300.0 300.0 _ _ _ _ \n",
      "_ _ 500.0 330.0 1500.0 2220.0 13920.0 41340.0 2400.0 1200.0 _ _ _ _ _ \n",
      "_ 200.0 330.0 2900.0 3120.0 -1.0 -1.0 -1.0 -1.0 900.0 300.0 _ _ _ _ \n",
      "330.0 740.0 1430.0 3420.0 -1.0 -1.0 5610.0 4050.0 2400.0 1200.0 _ _ _ _ _ \n",
      "1800.0 3710.0 5210.0 -1.0 -1.0 -1.0 3180.0 900.0 630.0 300.0 300.0 _ _ _ _ \n",
      "600.0 -1.0 -1.0 -1.0 -1.0 16080.0 4280.0 330.0 _ _ _ _ _ _ _ \n",
      "800.0 2670.0 -1.0 -1.0 -1.0 2280.0 660.0 500.0 _ _ _ _ _ _ _ \n",
      "110.0 1000.0 1680.0 3990.0 14850.0 2420.0 330.0 _ _ _ _ _ _ _ _ \n",
      "100.0 220.0 420.0 220.0 2270.0 440.0 440.0 _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "EXPLORE ZERO NUM\n",
      "original 2 3\n",
      "w-range 2 4\n",
      "h-range 3 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X X _ _ _ _ _ _ _ _ _ _ _ \n",
      "X X X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "X _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "X X _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X _ _ _ _ _ _ _ _ _ \n",
      "X X X X X X _ _ _ _ _ _ _ _ _ \n",
      "I CHOOSE 0\n",
      "2 127\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O X O O _ _ _ _ _ _ \n",
      "_ _ _ _ O X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O X O _ _ _ _ _ _ _ _ _ \n",
      "_ O X X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O X X _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "opponent = 'neural'\n",
    "first_move = 'opp'\n",
    "time_for_search = 0.5\n",
    "\n",
    "edges = []\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Tic-Tac-Toe\")\n",
    "root.geometry(\"542x500\")\n",
    "app = my_app(root, first_move, time_for_search)\n",
    "\n",
    "\n",
    "if first_move != 'me':\n",
    "    model_policy = load_model('cross_40v4')\n",
    "    tree_act = app.tree.search(time_for_search)\n",
    "    #print(tree_act, tree.root.all_games)\n",
    "    app.buttons[tree_act].config(text = 'X' if app.tester.cur_player == 1 else 'O')\n",
    "    cur_pos, reward, done, info = app.tester.in_step(tree_act)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(app.tree.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_occurances(what, where):\n",
    "    import re\n",
    "    return [m.start() for m in re.finditer(what, where)]\n",
    "    \n",
    "find_occurances(\"0111\", \"011100111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g=nx.DiGraph()\n",
    "g.add_edges_from(app.tree.edges)\n",
    "p=nx.drawing.nx_pydot.to_pydot(g)\n",
    "p.write_png('example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'Cython'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f36a8d41893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext Cython'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2052\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-63>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'Cython'"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "clf = joblib.load('LinearClass')\n",
    "\n",
    "model = load_model('zeros_policy_44')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('LinearClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "4111 127\n",
      "142\n",
      "3906 129\n",
      "157\n",
      "4381 126\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "cimport numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "import tkinter\n",
    "import random\n",
    "from tkinter import *\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import numba\n",
    "\n",
    "\n",
    "model = load_model('zeros_policy_44')\n",
    "clf = joblib.load('LinearClass')\n",
    "\n",
    "class RenjuTEST(): \n",
    "    def __init__(self, player, mode):\n",
    "        \"\"\"\n",
    "        player : 1 for black, 2 for white\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15, 3))\n",
    "        self.cur_player = 1\n",
    "        self.player = player\n",
    "        self.action_space = 225\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        self.mode = mode\n",
    "        \n",
    "\n",
    "    def in_step(self, action):\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics. When end of episode\n",
    "        is reached, reset() should be called to reset the environment's internal state.\n",
    "        Input\n",
    "        -----\n",
    "        action : an action provided by the environment\n",
    "        Outputs\n",
    "        -------\n",
    "        (observation, reward, done, info)\n",
    "        observation : agent's observation of the current environment\n",
    "        reward [Float] : amount of reward due to the previous action\n",
    "        done : a boolean, indicating whether the episode has ended\n",
    "        info : a dictionary containing other diagnostic information from the previous action\n",
    "        \"\"\"\n",
    "        self.moves_done += 1\n",
    "        if self.cur_player == 1:\n",
    "            self.cur_pos[action % 15][action // 15][0] = 1\n",
    "            self.lr_pos[action] = 1\n",
    "        else:\n",
    "            self.cur_pos[action % 15][action // 15][1] = 1\n",
    "            self.lr_pos[action] = 2\n",
    "            \n",
    "            \n",
    "        w = (action % 15) + 5\n",
    "        h = (action // 15) + 5\n",
    "        \n",
    "        self.ext_pos[w][h] = self.cur_player\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                if self.cur_pos[i][j][0] > 0 or self.cur_pos[i][j][1] > 0:\n",
    "                    self.cur_pos[i][j][2] += 1\n",
    "        \"\"\"\n",
    "        self.cur_pos[:, :, 2:] = self.cur_pos.sum(2).reshape((15,15,1))\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        rdiag = [0, 0, 0]\n",
    "        rdiaginv = [0, 0, 0]\n",
    "        ldiag = [0, 0, 0]\n",
    "        ldiaginv = [0, 0, 0]\n",
    "        rrow = [0, 0, 0]\n",
    "        lrow = [0, 0, 0]\n",
    "        rcol = [0, 0, 0]\n",
    "        lcol = [0, 0, 0]\n",
    "        \n",
    "        broken = [0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        for i in range(5):\n",
    "            \n",
    "            if self.ext_pos[w + i][h + i]:\n",
    "                if not broken[0]:\n",
    "                    rdiag[int(self.ext_pos[w + i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[0] = 1\n",
    "                \n",
    "            if self.ext_pos[w][h + i]:\n",
    "                if not broken[1]:\n",
    "                    rcol[int(self.ext_pos[w][h + i])] += 1\n",
    "                else:\n",
    "                    broken[1] = 1\n",
    "                \n",
    "            if self.ext_pos[w - i][h + i]:\n",
    "                if not broken[2]:\n",
    "                    ldiag[int(self.ext_pos[w - i][h + i])] += 1\n",
    "                else:\n",
    "                    broken[2] = 1\n",
    "                \n",
    "            \n",
    "            if self.ext_pos[w - i][h]:\n",
    "                if not broken[3]:\n",
    "                    lrow[int(self.ext_pos[w - i][h])] += 1\n",
    "                else:\n",
    "                    broken[3] = 1\n",
    "            \n",
    "            if self.ext_pos[w - i][h - i]:\n",
    "                if not broken[4]:\n",
    "                    rdiaginv[int(self.ext_pos[w - i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[4] = 1\n",
    "            \n",
    "            if self.ext_pos[w][h - i]:\n",
    "                if not broken[5]:\n",
    "                    lcol[int(self.ext_pos[w][h - i])] += 1\n",
    "                else:\n",
    "                    broken[5] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h - i]:\n",
    "                if not broken[6]:\n",
    "                    ldiaginv[int(self.ext_pos[w + i][h - i])] += 1\n",
    "                else:\n",
    "                    broken[6] = 1\n",
    "            \n",
    "            if self.ext_pos[w + i][h]:\n",
    "                if not broken[7]:\n",
    "                    rrow[int(self.ext_pos[w + i][h])] += 1\n",
    "                else:\n",
    "                    broken[7] = 1\n",
    "        \n",
    "        \n",
    "        if rdiag[self.cur_player] + rdiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if ldiag[self.cur_player] + ldiaginv[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \n",
    "        if rrow[self.cur_player] + lrow[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                \n",
    "        if lcol[self.cur_player] + rcol[self.cur_player] >= 6:\n",
    "            if self.player == self.cur_player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        \"\"\"\n",
    "        print('rdiag', rdiag)\n",
    "        print('rdiaginv', rdiaginv)\n",
    "        print('ldiag', ldiag)\n",
    "        print('ldiaginv', ldiaginv)\n",
    "        print('rrow', rrow)\n",
    "        print('lrow', lrow)\n",
    "        print('rcol', rcol)\n",
    "        print('lcol', lcol)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.cur_player == 1:\n",
    "            self.cur_player = 2\n",
    "        else:\n",
    "            self.cur_player = 1\n",
    "        \n",
    "        done = True if (self.moves_done == 225 or reward != 0) else False\n",
    "        cur_pos = self.cur_pos\n",
    "        if self.moves_done == 225:\n",
    "            self.reset()\n",
    "        info = dict()\n",
    "        return (cur_pos, reward, done, info)\n",
    "    \n",
    "    def net_ans(self, model, mode = 'all'):\n",
    "        s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        if mode == 'one':\n",
    "            return np.argmax(s)\n",
    "        else:\n",
    "            #return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "            return np.argsort(s)\n",
    "    \n",
    "    def simulation(self, model, mode):\n",
    "        fake_env = RenjuTEST(1, 'neural')\n",
    "        fake_env.cur_pos = np.copy(self.cur_pos)\n",
    "        fake_env.cur_player = self.cur_player\n",
    "        fake_env.player = self.player\n",
    "        fake_env.action_space = self.action_space = 225\n",
    "        fake_env.moves_done = self.moves_done\n",
    "        fake_env.ext_pos = np.copy(self.ext_pos)\n",
    "        fake_env.lr_pos = np.copy(self.lr_pos)\n",
    "        fake_env.mode = self.mode\n",
    "        reward = 0\n",
    "        while reward == 0:\n",
    "            \n",
    "            action = random.randint(0,224)\n",
    "            while fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = random.randint(0,224)\n",
    "                \n",
    "            \"\"\" \n",
    "            #if mode == 'kn':\n",
    "            s = clf.predict_proba([fake_env.lr_pos])[0]\n",
    "            #else:\n",
    "            #    s = model.predict(np.array([[fake_env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if fake_env.cur_pos[action % 15][action // 15][0] != 0 or fake_env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if fake_env.cur_pos[act % 15][act // 15][0] == 0 and fake_env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "            \"\"\"\n",
    "            cur_pos, reward, done, info = fake_env.in_step(action)\n",
    "            \n",
    "        if reward == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action, mode = 'kn'):\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "\n",
    "        if reward != 0:\n",
    "            #self.render()\n",
    "            if done:\n",
    "                self.reset()\n",
    "            return cur_pos, reward, done, info\n",
    "        else:\n",
    "            if self.mode == 'neural':\n",
    "                s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "            else:\n",
    "                s = clf.predict_proba([self.lr_pos])[0]\n",
    "            #plt.figure()\n",
    "            #k = s.reshape((15,15))\n",
    "            #plt.imshow(k, cmap='hot', interpolation='nearest')\n",
    "            #plt.show()\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                \n",
    "                action = 0\n",
    "                #print(net_move)\n",
    "                for act in net_move:\n",
    "                    if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break\n",
    "                #print(\"Net:\", action)\n",
    "\n",
    "            rnd = np.random.randint(1, 100)\n",
    "            if rnd < -10:\n",
    "                action = np.random.randint(0, 224)\n",
    "                while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                    action = np.random.randint(0, 224)\n",
    "            print('Net action:', action)\n",
    "            cur_pos, reward, done, info = self.in_step(action)\n",
    "            return cur_pos, reward, done, action\n",
    "    \n",
    "    def learning(self, opponent):\n",
    "        if self.moves_done % 2 == 0:\n",
    "            s = opponent.predict(np.array([[self.cur_pos]]))[0]\n",
    "        else:\n",
    "            s = model.predict(np.array([[self.cur_pos]]))[0]\n",
    "        action = np.argmax(s)\n",
    "        if self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "            net_move = np.argsort(s)[::-1]\n",
    "\n",
    "            action = 0\n",
    "            #print(net_move)\n",
    "            for act in net_move:\n",
    "                if self.cur_pos[act % 15][act // 15][0] == 0 and self.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                    action = act\n",
    "                    break\n",
    "            #print(\"Net:\", action)\n",
    "        rnd = np.random.randint(1, 100)\n",
    "        if rnd < 5:\n",
    "            action = np.random.randint(0, 224)\n",
    "            while self.cur_pos[action % 15][action // 15][0] != 0 or self.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                action = np.random.randint(0, 224)\n",
    "\n",
    "        cur_pos, reward, done, info = self.in_step(action)\n",
    "        return cur_pos, reward, done, action\n",
    "            \n",
    "            \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in reversed(range(15)):\n",
    "                for i in range(15):\n",
    "                    flag = 0\n",
    "                    if self.cur_pos[i][j][0] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.cur_pos[i][j][1] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        if mode == 'debug':\n",
    "            for j in reversed(range(25)):\n",
    "                for i in range(25):\n",
    "                    flag = 0\n",
    "                    if self.ext_pos[i][j] == 1:\n",
    "                        flag = 1\n",
    "                        print(\"X\", end=' ')\n",
    "                    if self.ext_pos[i][j] == 2:\n",
    "                        flag = 1\n",
    "                        print(\"O\", end=' ')\n",
    "                    if not flag:\n",
    "                        print(\"_\", end=' ')\n",
    "                print('\\n', end='')\n",
    "            print(\"------------------------------------------------\\n\")\n",
    "        \n",
    "     \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment, returning an initial observation.\n",
    "        Outputs\n",
    "        -------\n",
    "        observation : the initial observation of the space. (Initial reward is assumed to be 0.)\n",
    "        \"\"\"\n",
    "        self.cur_pos = np.zeros((15,15,3))\n",
    "        self.cur_player = 1\n",
    "        self.moves_done = 0\n",
    "        self.ext_pos = np.zeros((25, 25))\n",
    "        self.lr_pos = np.zeros(225)\n",
    "        return self.cur_pos\n",
    "\n",
    "\n",
    "class MCSTNode():\n",
    "    def __init__(self, env):\n",
    "        self.wins = 0\n",
    "        self.all_games = 0\n",
    "        self.childs = []\n",
    "        self.env = env\n",
    "        self.parent = None\n",
    "        self.action = None\n",
    "        self.index = np.random.randint(0, 10000000)\n",
    "        self.child_actions = []\n",
    "       \n",
    "\n",
    "class UCT():\n",
    "    def __init__(self, env, model, mode):        \n",
    "        self.root = MCSTNode(env)\n",
    "        self.all_games = 0\n",
    "        self.constant = math.sqrt(2)\n",
    "        self.cur_node = self.root\n",
    "        self.model = model\n",
    "        self.cur_pos = self.root\n",
    "        self.mode = mode\n",
    "\n",
    "    \n",
    "    def get_child_UCT_list(self, node):\n",
    "        return list(map(self.get_UCT_stat, node.childs))\n",
    "       \n",
    "    def explore(self):\n",
    "        self.cur_node = self.root\n",
    "        UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "        while len(UCT_list) > 0 and  max(UCT_list) > self.get_UCT_stat(self.cur_node):\n",
    "            #print(UCT_list, \"I am at \", self.cur_node.index)\n",
    "            #print(UCT_list)\n",
    "            best_node = np.argmax(UCT_list)\n",
    "            self.cur_node = self.cur_node.childs[best_node]\n",
    "            UCT_list = self.get_child_UCT_list(self.cur_node)\n",
    "    \n",
    "    def expand(self):\n",
    "        if self.cur_node.all_games > 1:\n",
    "            #if self.mode == 'kn':\n",
    "                #s = clf.predict_proba([self.cur_node.env.lr_pos])[0]\n",
    "            #else:\n",
    "            s = model.predict(np.array([[self.cur_node.env.cur_pos]]))[0]\n",
    "            action = np.argmax(s)\n",
    "            if self.cur_node.env.cur_pos[action % 15][action // 15][0] != 0 or self.cur_node.env.cur_pos[action % 15][action // 15][1] != 0:\n",
    "                net_move = np.argsort(s)[::-1]\n",
    "                action = 0\n",
    "                for act in net_move:\n",
    "                    if act not in self.cur_node.child_actions and self.cur_node.env.cur_pos[act % 15][act // 15][0] == 0 and self.cur_node.env.cur_pos[act % 15][act // 15][1] == 0:\n",
    "                        action = act\n",
    "                        break        \n",
    "\n",
    "            new_node_action = action\n",
    "            #print('New Node!')\n",
    "\n",
    "            new_env = deepcopy(self.cur_node.env)\n",
    "            new_env.in_step(new_node_action)\n",
    "\n",
    "            new_node = MCSTNode(new_env)\n",
    "            new_node.parent = self.cur_node\n",
    "            new_node.action = new_node_action\n",
    "            self.cur_node.child_actions.append(new_node_action)\n",
    "\n",
    "            run = new_node.env.simulation(self.model, self.mode)\n",
    "            self.all_games += 1\n",
    "\n",
    "            self.cur_node.childs.append(new_node)\n",
    "            self.cur_node = new_node\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "        else:\n",
    "            run = self.cur_node.env.simulation(self.model, self.model)\n",
    "            self.all_games += 1\n",
    "\n",
    "            while(self.cur_node.parent != None):\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "                self.cur_node = self.cur_node.parent\n",
    "            else:\n",
    "                self.cur_node.wins += run\n",
    "                self.cur_node.all_games += 1\n",
    "            \n",
    "            \n",
    "    def search(self, time_limit):\n",
    "        begin = time()\n",
    "        while (time() - begin) < time_limit * 0.95:\n",
    "            self.explore()\n",
    "            self.expand()\n",
    "        root_values = list(map(self.get_stat, self.root.childs))\n",
    "        #print(root_UCT_values, len(self.root.childs))\n",
    "        if len(root_values) != 0:\n",
    "            best_child = np.argmax(root_values)\n",
    "        else:\n",
    "            self.root = self.root.childs[0]\n",
    "            return self.root.action\n",
    "        self.root = self.root.childs[best_child]\n",
    "        return self.root.action\n",
    "                    \n",
    "    def get_UCT_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games) + self.constant * math.sqrt(math.log(self.all_games / node.all_games))\n",
    "    \n",
    "    def get_stat(self, node):\n",
    "        if node.all_games == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(node.wins) / float(node.all_games)\n",
    "\n",
    "\n",
    "class my_app(Frame):\n",
    "    \"\"\"Basic Frame\"\"\"\n",
    "    def __init__(self, master, mode, time_for_search):\n",
    "        self.buttons = []\n",
    "        \"\"\"Init the Frame\"\"\"\n",
    "        Frame.__init__(self,master)\n",
    "        self.grid()\n",
    "        self.Create_Widgets()\n",
    "        self.tester = RenjuTEST(2 if mode == 'me' else 1, mode)\n",
    "        self.label = tkinter.Label(self, text='')\n",
    "        self.label.grid(column=7, row=16, sticky=W);   #creates label for image on window \n",
    "        self.tree = UCT(self.tester, model, mode)\n",
    "        self.mode = mode\n",
    "        self.time_for_search = time_for_search\n",
    "\n",
    "    \n",
    "    def Create_Widgets(self):\n",
    "        for i in range(15):\n",
    "            for j in range(15): #Start creating buttons\n",
    "\n",
    "                button_id = i * 15 + j \n",
    "                #print(self.button_id)\n",
    "\n",
    "                self.newmessage = Button(self, #I want to bind the self.button_id to each button, so that it prints its number when clicked.\n",
    "                                         text = '',\n",
    "                                         anchor = W, command = lambda button_id=button_id: self.access(button_id))#Run the method\n",
    "\n",
    "                #Placing\n",
    "                self.newmessage.config(height = 1, width = 1)\n",
    "                self.newmessage.grid(row = 15 - i, column = j, sticky = NW)\n",
    "                self.buttons.append(self.newmessage)\n",
    "      \n",
    "    def access(self, b_id): #This is one of the areas where I need help. I want this to return the number of the button clicked.\n",
    "        self.b_id = b_id\n",
    "        print(b_id)\n",
    "        self.buttons[b_id].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        \n",
    "        \"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.step(b_id)\n",
    "        self.tester.render(mode='human')\n",
    "        \n",
    "        self.buttons[info].config(text = 'X' if self.tester.cur_player == 2 else 'O')\n",
    "        \n",
    "        #\"\"\" \n",
    "        cur_pos, reward, done, info = self.tester.in_step(b_id)\n",
    "        \n",
    "        #self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        \n",
    "        if b_id in self.tree.root.child_actions:\n",
    "            for node in self.tree.root.childs:\n",
    "                if node.action == b_id:\n",
    "                    self.tree.root = node\n",
    "                    break\n",
    "        else:\n",
    "            self.tree = UCT(self.tester, model, self.mode)\n",
    "        #tree = UCT(self.tester, model, 'kn' if random.randint(0,1) == 0 else 'neuron')\n",
    "        tree_act = self.tree.search(self.time_for_search)\n",
    "        print(self.tree.root.all_games, tree_act)\n",
    "        #print(tree_act, tree.root.all_games)\n",
    "        self.buttons[tree_act].config(text = 'X' if self.tester.cur_player == 1 else 'O')\n",
    "        cur_pos, reward, done, info = self.tester.in_step(tree_act)\n",
    "        #self.tester.render(mode='human')\n",
    "        if reward != 0:\n",
    "            ch = 'X' if self.tester.cur_player == 2 else 'O'\n",
    "            self.label.config(text = str(ch + ' win'))\n",
    "        #\"\"\"\n",
    "        \n",
    "\n",
    "opponent = 'neural'\n",
    "first_move = 'me'\n",
    "time_for_search = 10\n",
    "\n",
    "\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Tic-Tac-Toe\")\n",
    "root.geometry(\"542x500\")\n",
    "app = my_app(root, opponent, time_for_search)\n",
    "\n",
    "\n",
    "if first_move != 'me':\n",
    "    tree_act = app.tree.search(time_for_search)\n",
    "    #print(tree_act, tree.root.all_games)\n",
    "    app.buttons[tree_act].config(text = 'X' if app.tester.cur_player == 1 else 'O')\n",
    "    cur_pos, reward, done, info = app.tester.in_step(tree_act)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 µs ± 15.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = clf.predict([tester_1.lr_pos])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([95]),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(tester_1.lr_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 ms ± 184 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = model.predict(np.array([[tester_1.cur_pos]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ec29a02d0db1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtree_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcur_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.UCT.search (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:10679)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.UCT.expand (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:10080)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.RenjuTEST.simulation (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:5176)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.pyx\u001b[0m in \u001b[0;36m_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.RenjuTEST.in_step (/home/axcel/.cache/ipython/cython/_cython_magic_3cef77f9208d3ab03df2eeb95adbefca.c:2618)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tester_1 = RenjuTEST(1, 'neural')\n",
    "tester_2 = RenjuTEST(2, 'kn')\n",
    "\n",
    "win = 0\n",
    "loose = 0\n",
    "all_game = 0\n",
    "\n",
    "mode_1 = 'kn'\n",
    "mode_2 = 'neuron'\n",
    "\n",
    "tree_1 = UCT(tester_1, model, mode_1)\n",
    "tree_2 = UCT(tester_2, model, mode_2)\n",
    "\n",
    "for i in tqdm(range(150)):\n",
    "    \n",
    "    tree_act = tree_1.search(5)\n",
    "    print(tree_act, tree_1.root.all_games)\n",
    "    cur_pos, reward, done, info = tester_1.in_step(tree_act)\n",
    "    tester_2.in_step(tree_act)\n",
    "    \n",
    "    if tree_act in tree_2.root.child_actions:\n",
    "        for node in tree_2.root.childs:\n",
    "            if node.action == tree_act:\n",
    "                tree_2.root = node\n",
    "                break\n",
    "    else:\n",
    "        tree_2 = UCT(tester_2, model, mode_2)\n",
    "        \n",
    "        \n",
    "    if reward == 1:\n",
    "        win += 1\n",
    "        all_game += 1\n",
    "    if reward == -1:\n",
    "        loose += 1\n",
    "        all_game += 1\n",
    "        \n",
    "    \n",
    "    tree_act = tree_2.search(5)\n",
    "    print(tree_act, tree_2.root.all_games)\n",
    "    cur_pos, reward, done, info = tester_2.in_step(tree_act)\n",
    "    tester_1.in_step(tree_act)\n",
    "    if tree_act in tree_1.root.child_actions:\n",
    "        for node in tree_1.root.childs:\n",
    "            if node.action == tree_act:\n",
    "                tree_1.root = node\n",
    "                break\n",
    "    else:\n",
    "        tree_1 = UCT(tester_1, model, mode_1)\n",
    "        \n",
    "    if reward == -1:\n",
    "        win += 1\n",
    "        all_game += 1\n",
    "    if reward == 1:\n",
    "        loose += 1\n",
    "        all_game += 1\n",
    "        \n",
    "    #tester.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 7 loose: 12 all rate: 0.3684210526315789\n"
     ]
    }
   ],
   "source": [
    "print(\"win:\", win, \"loose:\", loose, \"all rate:\", win / all_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f  tester.step tester.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_check_env.predict(np.array([[env.cur_pos]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205,\n",
       " 157,\n",
       " 79,\n",
       " 191,\n",
       " 63,\n",
       " 202,\n",
       " 22,\n",
       " 175,\n",
       " 111,\n",
       " 91,\n",
       " 189,\n",
       " 152,\n",
       " 142,\n",
       " 34,\n",
       " 25,\n",
       " 160,\n",
       " 72,\n",
       " 33,\n",
       " 204,\n",
       " 36,\n",
       " 176,\n",
       " 19,\n",
       " 57,\n",
       " 48,\n",
       " 137,\n",
       " 178,\n",
       " 133,\n",
       " 52,\n",
       " 136,\n",
       " 106,\n",
       " 151,\n",
       " 132,\n",
       " 77,\n",
       " 187,\n",
       " 102,\n",
       " 103,\n",
       " 92,\n",
       " 26,\n",
       " 118,\n",
       " 88,\n",
       " 20,\n",
       " 42,\n",
       " 109,\n",
       " 35,\n",
       " 67,\n",
       " 172,\n",
       " 121,\n",
       " 126,\n",
       " 78,\n",
       " 93,\n",
       " 119,\n",
       " 169,\n",
       " 61,\n",
       " 39,\n",
       " 177,\n",
       " 153,\n",
       " 71,\n",
       " 24,\n",
       " 148,\n",
       " 59,\n",
       " 201,\n",
       " 18,\n",
       " 162,\n",
       " 107,\n",
       " 161,\n",
       " 104,\n",
       " 139,\n",
       " 122,\n",
       " 146,\n",
       " 64,\n",
       " 62,\n",
       " 190,\n",
       " 127,\n",
       " 186,\n",
       " 194,\n",
       " 199,\n",
       " 41,\n",
       " 167,\n",
       " 198,\n",
       " 74,\n",
       " 166,\n",
       " 56,\n",
       " 124,\n",
       " 40,\n",
       " 215,\n",
       " 131,\n",
       " 134,\n",
       " 174,\n",
       " 159,\n",
       " 164,\n",
       " 51,\n",
       " 182,\n",
       " 87,\n",
       " 117,\n",
       " 37,\n",
       " 140,\n",
       " 110,\n",
       " 155,\n",
       " 185,\n",
       " 168,\n",
       " 76,\n",
       " 54,\n",
       " 171,\n",
       " 154,\n",
       " 7,\n",
       " 206,\n",
       " 49,\n",
       " 193,\n",
       " 149,\n",
       " 73,\n",
       " 90,\n",
       " 123,\n",
       " 163,\n",
       " 31,\n",
       " 147,\n",
       " 183,\n",
       " 192,\n",
       " 138,\n",
       " 108,\n",
       " 58,\n",
       " 89,\n",
       " 4,\n",
       " 170,\n",
       " 65,\n",
       " 69,\n",
       " 116,\n",
       " 70,\n",
       " 94,\n",
       " 145,\n",
       " 66,\n",
       " 47,\n",
       " 50,\n",
       " 184,\n",
       " 200,\n",
       " 44,\n",
       " 21,\n",
       " 5,\n",
       " 2,\n",
       " 27,\n",
       " 86,\n",
       " 125,\n",
       " 80,\n",
       " 105,\n",
       " 97,\n",
       " 101,\n",
       " 209,\n",
       " 141,\n",
       " 156,\n",
       " 220,\n",
       " 179,\n",
       " 181,\n",
       " 55,\n",
       " 17,\n",
       " 75,\n",
       " 208,\n",
       " 43,\n",
       " 219,\n",
       " 216,\n",
       " 30,\n",
       " 14,\n",
       " 96,\n",
       " 6,\n",
       " 223,\n",
       " 46,\n",
       " 120,\n",
       " 150,\n",
       " 32,\n",
       " 12,\n",
       " 213,\n",
       " 135,\n",
       " 212,\n",
       " 217,\n",
       " 95,\n",
       " 195,\n",
       " 222,\n",
       " 15,\n",
       " 3,\n",
       " 196,\n",
       " 10,\n",
       " 214,\n",
       " 13,\n",
       " 144,\n",
       " 130,\n",
       " 221,\n",
       " 1,\n",
       " 28,\n",
       " 81,\n",
       " 197,\n",
       " 100,\n",
       " 115,\n",
       " 84,\n",
       " 11,\n",
       " 45,\n",
       " 207,\n",
       " 9,\n",
       " 82,\n",
       " 180,\n",
       " 165,\n",
       " 60,\n",
       " 211,\n",
       " 29,\n",
       " 224,\n",
       " 85,\n",
       " 99,\n",
       " 128,\n",
       " 16,\n",
       " 129,\n",
       " 0,\n",
       " 38,\n",
       " 114,\n",
       " 210,\n",
       " 53,\n",
       " 173,\n",
       " 83,\n",
       " 112,\n",
       " 188,\n",
       " 218,\n",
       " 68,\n",
       " 98,\n",
       " 143,\n",
       " 23,\n",
       " 158,\n",
       " 8,\n",
       " 203,\n",
       " 113]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def net_ans(env):\n",
    "    s = model_check_env.predict(np.array([[env.cur_pos]]))[0]\n",
    "    return sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "\n",
    "net_ans(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(model_normal.layers)):\n",
    "    model.layers[i + 1].set_weights(model_normal.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 15, 15, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 8)         224       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 550)               744150    \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 225)               123975    \n",
      "=================================================================\n",
      "Total params: 868,349.0\n",
      "Trainable params: 868,349.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, Convolution2D, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "model_policy = Sequential()\n",
    "\n",
    "model_policy.add(Reshape((15,15,3), input_shape=(1,15,15,3)))\n",
    "model_policy.add(Convolution2D(8, (3,3), input_shape=(15, 15, 4), activation='relu'))\n",
    "\n",
    "model_policy.add(Flatten(name='flatten'))\n",
    "\n",
    "\n",
    "model_policy.add(Dense(550, activation='relu', name='fc1'))\n",
    "\n",
    "model_policy.add(Dense(225, activation='softmax', name='fc2'))\n",
    "model_policy.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.agents.sarsa import SarsaAgent\n",
    "\n",
    "\n",
    "ENV_NAME = 'Renju'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = RenjuTEST(1, 'kn')\n",
    "np.random.seed(123)\n",
    "nb_actions = 225\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=1)\n",
    "\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=10000)\n",
    "\n",
    "\n",
    "dqn = DQNAgent(model=model_policy, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "                nb_steps_warmup=5000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 175000 steps ...\n",
      "     56/175000: episode: 1, duration: 0.512s, episode steps: 56, steps per second: 109, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.411 [3.000, 222.000], mean observation: 1.602 [0.000, 112.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     87/175000: episode: 2, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 129.194 [1.000, 219.000], mean observation: 0.564 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    124/175000: episode: 3, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 109.162 [3.000, 224.000], mean observation: 0.784 [0.000, 73.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    152/175000: episode: 4, duration: 0.213s, episode steps: 28, steps per second: 132, episode reward: 1.000, mean reward: 0.036 [0.000, 1.000], mean action: 110.179 [1.000, 224.000], mean observation: 0.461 [0.000, 55.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    176/175000: episode: 5, duration: 0.201s, episode steps: 24, steps per second: 120, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.083 [3.000, 223.000], mean observation: 0.358 [0.000, 48.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    222/175000: episode: 6, duration: 0.376s, episode steps: 46, steps per second: 122, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.826 [8.000, 211.000], mean observation: 1.061 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    255/175000: episode: 7, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 115.818 [0.000, 213.000], mean observation: 0.596 [0.000, 65.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    288/175000: episode: 8, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 136.303 [2.000, 217.000], mean observation: 0.626 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    316/175000: episode: 9, duration: 0.120s, episode steps: 28, steps per second: 233, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 100.321 [8.000, 223.000], mean observation: 0.440 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    347/175000: episode: 10, duration: 0.191s, episode steps: 31, steps per second: 162, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 113.000 [0.000, 216.000], mean observation: 0.527 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    373/175000: episode: 11, duration: 0.118s, episode steps: 26, steps per second: 220, episode reward: 1.000, mean reward: 0.038 [0.000, 1.000], mean action: 95.615 [12.000, 219.000], mean observation: 0.412 [0.000, 51.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    402/175000: episode: 12, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 86.207 [5.000, 204.000], mean observation: 0.496 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    448/175000: episode: 13, duration: 0.361s, episode steps: 46, steps per second: 128, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 117.043 [4.000, 216.000], mean observation: 1.177 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    479/175000: episode: 14, duration: 0.230s, episode steps: 31, steps per second: 135, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 116.516 [7.000, 219.000], mean observation: 0.567 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    499/175000: episode: 15, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 120.050 [38.000, 187.000], mean observation: 0.224 [0.000, 40.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    530/175000: episode: 16, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 126.548 [8.000, 215.000], mean observation: 0.533 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    565/175000: episode: 17, duration: 0.147s, episode steps: 35, steps per second: 237, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.086 [14.000, 217.000], mean observation: 0.651 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    610/175000: episode: 18, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 126.400 [5.000, 224.000], mean observation: 1.022 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    638/175000: episode: 19, duration: 0.132s, episode steps: 28, steps per second: 213, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 111.536 [7.000, 221.000], mean observation: 0.467 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    669/175000: episode: 20, duration: 0.178s, episode steps: 31, steps per second: 175, episode reward: 1.000, mean reward: 0.032 [0.000, 1.000], mean action: 113.194 [0.000, 215.000], mean observation: 0.548 [0.000, 61.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    701/175000: episode: 21, duration: 0.149s, episode steps: 32, steps per second: 215, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.031 [1.000, 213.000], mean observation: 0.575 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    733/175000: episode: 22, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.844 [6.000, 219.000], mean observation: 0.584 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    768/175000: episode: 23, duration: 0.150s, episode steps: 35, steps per second: 233, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.629 [2.000, 218.000], mean observation: 0.709 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    819/175000: episode: 24, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.000 [2.000, 218.000], mean observation: 1.285 [0.000, 102.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    855/175000: episode: 25, duration: 0.162s, episode steps: 36, steps per second: 223, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 110.417 [0.000, 217.000], mean observation: 0.684 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    892/175000: episode: 26, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.324 [12.000, 219.000], mean observation: 0.764 [0.000, 74.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    927/175000: episode: 27, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 94.029 [7.000, 209.000], mean observation: 0.634 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    959/175000: episode: 28, duration: 0.302s, episode steps: 32, steps per second: 106, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 93.938 [3.000, 210.000], mean observation: 0.575 [0.000, 63.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    991/175000: episode: 29, duration: 0.262s, episode steps: 32, steps per second: 122, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 100.156 [1.000, 216.000], mean observation: 0.575 [0.000, 63.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1027/175000: episode: 30, duration: 0.155s, episode steps: 36, steps per second: 232, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 94.361 [5.000, 181.000], mean observation: 0.686 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1058/175000: episode: 31, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.968 [7.000, 210.000], mean observation: 0.555 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1082/175000: episode: 32, duration: 0.102s, episode steps: 24, steps per second: 235, episode reward: 1.000, mean reward: 0.042 [0.000, 1.000], mean action: 112.000 [3.000, 219.000], mean observation: 0.340 [0.000, 47.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1117/175000: episode: 33, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.971 [0.000, 219.000], mean observation: 0.689 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1160/175000: episode: 34, duration: 0.397s, episode steps: 43, steps per second: 108, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.744 [11.000, 218.000], mean observation: 0.874 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1197/175000: episode: 35, duration: 0.166s, episode steps: 37, steps per second: 224, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 111.838 [11.000, 221.000], mean observation: 0.777 [0.000, 73.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1247/175000: episode: 36, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.740 [8.000, 221.000], mean observation: 1.243 [0.000, 100.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1296/175000: episode: 37, duration: 0.253s, episode steps: 49, steps per second: 193, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 123.653 [4.000, 221.000], mean observation: 1.210 [0.000, 97.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1339/175000: episode: 38, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 115.930 [2.000, 217.000], mean observation: 0.956 [0.000, 85.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1369/175000: episode: 39, duration: 0.126s, episode steps: 30, steps per second: 238, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 131.400 [13.000, 217.000], mean observation: 0.481 [0.000, 60.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1395/175000: episode: 40, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 120.769 [15.000, 220.000], mean observation: 0.409 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1431/175000: episode: 41, duration: 0.157s, episode steps: 36, steps per second: 230, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 119.056 [1.000, 217.000], mean observation: 0.720 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1470/175000: episode: 42, duration: 0.323s, episode steps: 39, steps per second: 121, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.282 [1.000, 221.000], mean observation: 0.821 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1514/175000: episode: 43, duration: 0.384s, episode steps: 44, steps per second: 115, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.068 [5.000, 210.000], mean observation: 0.891 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1560/175000: episode: 44, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 113.717 [0.000, 224.000], mean observation: 1.100 [0.000, 91.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1600/175000: episode: 45, duration: 0.295s, episode steps: 40, steps per second: 136, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.725 [7.000, 223.000], mean observation: 0.829 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1643/175000: episode: 46, duration: 0.377s, episode steps: 43, steps per second: 114, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.326 [1.000, 224.000], mean observation: 0.888 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1684/175000: episode: 47, duration: 0.173s, episode steps: 41, steps per second: 237, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.805 [10.000, 223.000], mean observation: 0.851 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1713/175000: episode: 48, duration: 0.283s, episode steps: 29, steps per second: 102, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.517 [7.000, 222.000], mean observation: 0.503 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1754/175000: episode: 49, duration: 0.295s, episode steps: 41, steps per second: 139, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.537 [6.000, 218.000], mean observation: 0.885 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1793/175000: episode: 50, duration: 0.162s, episode steps: 39, steps per second: 240, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.590 [2.000, 221.000], mean observation: 0.820 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1841/175000: episode: 51, duration: 0.361s, episode steps: 48, steps per second: 133, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.708 [5.000, 216.000], mean observation: 1.187 [0.000, 96.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1883/175000: episode: 52, duration: 0.321s, episode steps: 42, steps per second: 131, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.071 [3.000, 205.000], mean observation: 0.960 [0.000, 84.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1912/175000: episode: 53, duration: 0.128s, episode steps: 29, steps per second: 227, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 103.138 [20.000, 189.000], mean observation: 0.389 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1934/175000: episode: 54, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 106.091 [8.000, 197.000], mean observation: 0.236 [0.000, 44.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1963/175000: episode: 55, duration: 0.222s, episode steps: 29, steps per second: 131, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.897 [9.000, 219.000], mean observation: 0.477 [0.000, 58.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   1996/175000: episode: 56, duration: 0.300s, episode steps: 33, steps per second: 110, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 122.333 [5.000, 216.000], mean observation: 0.540 [0.000, 65.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2043/175000: episode: 57, duration: 0.274s, episode steps: 47, steps per second: 172, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.851 [0.000, 224.000], mean observation: 1.198 [0.000, 94.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2082/175000: episode: 58, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 106.256 [2.000, 220.000], mean observation: 0.811 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2118/175000: episode: 59, duration: 0.345s, episode steps: 36, steps per second: 104, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 112.361 [1.000, 224.000], mean observation: 0.697 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2154/175000: episode: 60, duration: 0.236s, episode steps: 36, steps per second: 152, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 90.556 [3.000, 213.000], mean observation: 0.497 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2219/175000: episode: 61, duration: 0.322s, episode steps: 65, steps per second: 202, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 124.369 [16.000, 224.000], mean observation: 2.050 [0.000, 130.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2266/175000: episode: 62, duration: 0.355s, episode steps: 47, steps per second: 132, episode reward: 1.000, mean reward: 0.021 [0.000, 1.000], mean action: 108.468 [1.000, 216.000], mean observation: 1.147 [0.000, 93.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2296/175000: episode: 63, duration: 0.247s, episode steps: 30, steps per second: 121, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 118.233 [4.000, 222.000], mean observation: 0.471 [0.000, 60.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2351/175000: episode: 64, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 1.000, mean reward: 0.018 [0.000, 1.000], mean action: 102.855 [21.000, 223.000], mean observation: 1.342 [0.000, 109.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2401/175000: episode: 65, duration: 0.435s, episode steps: 50, steps per second: 115, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.380 [2.000, 219.000], mean observation: 1.062 [0.000, 100.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2444/175000: episode: 66, duration: 0.199s, episode steps: 43, steps per second: 216, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 110.419 [0.000, 223.000], mean observation: 0.835 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2478/175000: episode: 67, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.353 [10.000, 224.000], mean observation: 0.567 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2518/175000: episode: 68, duration: 0.312s, episode steps: 40, steps per second: 128, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 101.975 [2.000, 219.000], mean observation: 0.645 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2549/175000: episode: 69, duration: 0.288s, episode steps: 31, steps per second: 108, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.935 [5.000, 211.000], mean observation: 0.514 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2576/175000: episode: 70, duration: 0.117s, episode steps: 27, steps per second: 231, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 124.296 [11.000, 224.000], mean observation: 0.366 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2609/175000: episode: 71, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.455 [1.000, 210.000], mean observation: 0.560 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2652/175000: episode: 72, duration: 0.428s, episode steps: 43, steps per second: 100, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.767 [5.000, 224.000], mean observation: 0.976 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2684/175000: episode: 73, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 125.250 [1.000, 214.000], mean observation: 0.559 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2727/175000: episode: 74, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.837 [11.000, 222.000], mean observation: 0.978 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2753/175000: episode: 75, duration: 0.212s, episode steps: 26, steps per second: 122, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 111.077 [4.000, 221.000], mean observation: 0.413 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2785/175000: episode: 76, duration: 0.282s, episode steps: 32, steps per second: 114, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.781 [25.000, 210.000], mean observation: 0.420 [0.000, 64.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2812/175000: episode: 77, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 111.704 [18.000, 212.000], mean observation: 0.410 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2850/175000: episode: 78, duration: 0.166s, episode steps: 38, steps per second: 229, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.316 [17.000, 223.000], mean observation: 0.712 [0.000, 76.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2904/175000: episode: 79, duration: 0.416s, episode steps: 54, steps per second: 130, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 109.556 [5.000, 213.000], mean observation: 1.302 [0.000, 108.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2950/175000: episode: 80, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 135.304 [22.000, 223.000], mean observation: 1.164 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   2976/175000: episode: 81, duration: 0.123s, episode steps: 26, steps per second: 212, episode reward: 1.000, mean reward: 0.038 [0.000, 1.000], mean action: 101.308 [0.000, 222.000], mean observation: 0.369 [0.000, 51.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3016/175000: episode: 82, duration: 0.311s, episode steps: 40, steps per second: 128, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 126.650 [18.000, 221.000], mean observation: 0.717 [0.000, 80.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3052/175000: episode: 83, duration: 0.219s, episode steps: 36, steps per second: 165, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.306 [7.000, 216.000], mean observation: 0.617 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3087/175000: episode: 84, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.086 [8.000, 223.000], mean observation: 0.643 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3144/175000: episode: 85, duration: 0.501s, episode steps: 57, steps per second: 114, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.772 [0.000, 223.000], mean observation: 1.341 [0.000, 114.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3188/175000: episode: 86, duration: 0.151s, episode steps: 44, steps per second: 291, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.955 [0.000, 215.000], mean observation: 1.005 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3223/175000: episode: 87, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.057 [1.000, 223.000], mean observation: 0.632 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3267/175000: episode: 88, duration: 0.232s, episode steps: 44, steps per second: 189, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.432 [0.000, 216.000], mean observation: 0.984 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3306/175000: episode: 89, duration: 0.323s, episode steps: 39, steps per second: 121, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 102.000 [9.000, 210.000], mean observation: 0.764 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3357/175000: episode: 90, duration: 0.310s, episode steps: 51, steps per second: 165, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.510 [2.000, 211.000], mean observation: 1.252 [0.000, 102.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3398/175000: episode: 91, duration: 0.195s, episode steps: 41, steps per second: 211, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.049 [25.000, 210.000], mean observation: 0.774 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3439/175000: episode: 92, duration: 0.373s, episode steps: 41, steps per second: 110, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.878 [15.000, 223.000], mean observation: 0.821 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3473/175000: episode: 93, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.500 [2.000, 224.000], mean observation: 0.524 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3509/175000: episode: 94, duration: 0.217s, episode steps: 36, steps per second: 166, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.194 [37.000, 224.000], mean observation: 0.653 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3558/175000: episode: 95, duration: 0.228s, episode steps: 49, steps per second: 215, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 125.878 [14.000, 222.000], mean observation: 0.914 [0.000, 97.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3586/175000: episode: 96, duration: 0.222s, episode steps: 28, steps per second: 126, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 111.321 [11.000, 215.000], mean observation: 0.319 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3631/175000: episode: 97, duration: 0.351s, episode steps: 45, steps per second: 128, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.622 [0.000, 210.000], mean observation: 0.944 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3658/175000: episode: 98, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 99.556 [19.000, 210.000], mean observation: 0.428 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3701/175000: episode: 99, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.558 [0.000, 219.000], mean observation: 0.769 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3723/175000: episode: 100, duration: 0.098s, episode steps: 22, steps per second: 226, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.909 [2.000, 216.000], mean observation: 0.282 [0.000, 44.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3748/175000: episode: 101, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 111.880 [12.000, 210.000], mean observation: 0.295 [0.000, 50.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3779/175000: episode: 102, duration: 0.229s, episode steps: 31, steps per second: 135, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 113.065 [24.000, 224.000], mean observation: 0.467 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3809/175000: episode: 103, duration: 0.225s, episode steps: 30, steps per second: 133, episode reward: 1.000, mean reward: 0.033 [0.000, 1.000], mean action: 107.067 [10.000, 216.000], mean observation: 0.253 [0.000, 59.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3854/175000: episode: 104, duration: 0.281s, episode steps: 45, steps per second: 160, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 101.311 [2.000, 216.000], mean observation: 1.073 [0.000, 89.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3897/175000: episode: 105, duration: 0.188s, episode steps: 43, steps per second: 229, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.116 [0.000, 222.000], mean observation: 0.789 [0.000, 86.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3938/175000: episode: 106, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 99.146 [0.000, 220.000], mean observation: 0.736 [0.000, 81.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3969/175000: episode: 107, duration: 0.132s, episode steps: 31, steps per second: 234, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.677 [20.000, 217.000], mean observation: 0.382 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   3985/175000: episode: 108, duration: 0.069s, episode steps: 16, steps per second: 232, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 105.562 [11.000, 216.000], mean observation: 0.106 [0.000, 32.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4023/175000: episode: 109, duration: 0.248s, episode steps: 38, steps per second: 153, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.342 [3.000, 212.000], mean observation: 0.694 [0.000, 76.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4054/175000: episode: 110, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 128.355 [34.000, 217.000], mean observation: 0.307 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4098/175000: episode: 111, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.773 [11.000, 215.000], mean observation: 0.823 [0.000, 88.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4124/175000: episode: 112, duration: 0.113s, episode steps: 26, steps per second: 231, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 102.115 [24.000, 167.000], mean observation: 0.223 [0.000, 52.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4160/175000: episode: 113, duration: 0.222s, episode steps: 36, steps per second: 162, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.639 [10.000, 221.000], mean observation: 0.621 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4212/175000: episode: 114, duration: 0.439s, episode steps: 52, steps per second: 119, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 113.865 [9.000, 214.000], mean observation: 1.134 [0.000, 104.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4248/175000: episode: 115, duration: 0.165s, episode steps: 36, steps per second: 219, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 107.056 [9.000, 212.000], mean observation: 0.476 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4283/175000: episode: 116, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.829 [2.000, 223.000], mean observation: 0.416 [0.000, 70.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4335/175000: episode: 117, duration: 0.259s, episode steps: 52, steps per second: 201, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 124.692 [8.000, 224.000], mean observation: 1.229 [0.000, 104.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4358/175000: episode: 118, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 110.783 [7.000, 223.000], mean observation: 0.217 [0.000, 46.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4391/175000: episode: 119, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.939 [16.000, 210.000], mean observation: 0.406 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4427/175000: episode: 120, duration: 0.158s, episode steps: 36, steps per second: 228, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 119.306 [3.000, 210.000], mean observation: 0.508 [0.000, 71.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4473/175000: episode: 121, duration: 0.245s, episode steps: 46, steps per second: 188, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.696 [2.000, 224.000], mean observation: 0.832 [0.000, 92.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4514/175000: episode: 122, duration: 0.177s, episode steps: 41, steps per second: 232, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 107.366 [7.000, 215.000], mean observation: 0.668 [0.000, 81.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4562/175000: episode: 123, duration: 0.228s, episode steps: 48, steps per second: 211, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 101.729 [15.000, 219.000], mean observation: 0.858 [0.000, 96.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4579/175000: episode: 124, duration: 0.204s, episode steps: 17, steps per second: 83, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 98.588 [24.000, 206.000], mean observation: 0.142 [0.000, 34.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4620/175000: episode: 125, duration: 0.377s, episode steps: 41, steps per second: 109, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 110.049 [10.000, 224.000], mean observation: 0.784 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4651/175000: episode: 126, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 112.226 [8.000, 210.000], mean observation: 0.372 [0.000, 62.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4684/175000: episode: 127, duration: 0.255s, episode steps: 33, steps per second: 129, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 109.576 [9.000, 210.000], mean observation: 0.393 [0.000, 66.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4698/175000: episode: 128, duration: 0.145s, episode steps: 14, steps per second: 97, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 110.071 [12.000, 208.000], mean observation: 0.106 [0.000, 28.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4732/175000: episode: 129, duration: 0.216s, episode steps: 34, steps per second: 158, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.971 [10.000, 215.000], mean observation: 0.481 [0.000, 68.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4757/175000: episode: 130, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 108.760 [14.000, 210.000], mean observation: 0.234 [0.000, 50.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4796/175000: episode: 131, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 108.615 [4.000, 215.000], mean observation: 0.781 [0.000, 77.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4835/175000: episode: 132, duration: 0.354s, episode steps: 39, steps per second: 110, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 115.487 [25.000, 210.000], mean observation: 0.466 [0.000, 78.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4876/175000: episode: 133, duration: 0.190s, episode steps: 41, steps per second: 216, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.561 [13.000, 216.000], mean observation: 0.510 [0.000, 82.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4912/175000: episode: 134, duration: 0.214s, episode steps: 36, steps per second: 168, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.778 [10.000, 216.000], mean observation: 0.594 [0.000, 72.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4957/175000: episode: 135, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.289 [4.000, 214.000], mean observation: 0.845 [0.000, 90.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   4984/175000: episode: 136, duration: 0.218s, episode steps: 27, steps per second: 124, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 109.296 [15.000, 214.000], mean observation: 0.307 [0.000, 54.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   5026/175000: episode: 137, duration: 2.573s, episode steps: 42, steps per second: 16, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.595 [14.000, 223.000], mean observation: 0.652 [0.000, 84.000], loss: 0.443550, mean_absolute_error: 0.319872, mean_q: 1.193915, mean_eps: 0.548740\n",
      "   5087/175000: episode: 138, duration: 1.166s, episode steps: 61, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 103.148 [4.000, 220.000], mean observation: 1.491 [0.000, 122.000], loss: 0.335851, mean_absolute_error: 0.373059, mean_q: 1.247759, mean_eps: 0.544960\n",
      "   5112/175000: episode: 139, duration: 0.535s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 84.920 [1.000, 212.000], mean observation: 0.245 [0.000, 50.000], loss: 0.320287, mean_absolute_error: 0.449429, mean_q: 1.432621, mean_eps: 0.541180\n",
      "   5164/175000: episode: 140, duration: 1.052s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.654 [2.000, 222.000], mean observation: 1.003 [0.000, 104.000], loss: 0.353149, mean_absolute_error: 0.447526, mean_q: 1.287315, mean_eps: 0.537760\n",
      "   5219/175000: episode: 141, duration: 1.097s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 104.964 [7.000, 212.000], mean observation: 0.990 [0.000, 110.000], loss: 0.191706, mean_absolute_error: 0.433508, mean_q: 1.263952, mean_eps: 0.532900\n",
      "   5246/175000: episode: 142, duration: 0.550s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 92.815 [35.000, 212.000], mean observation: 0.186 [0.000, 54.000], loss: 0.200162, mean_absolute_error: 0.388345, mean_q: 1.237132, mean_eps: 0.529120\n",
      "   5266/175000: episode: 143, duration: 0.376s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 129.600 [19.000, 212.000], mean observation: 0.184 [0.000, 40.000], loss: 0.187205, mean_absolute_error: 0.367172, mean_q: 1.193088, mean_eps: 0.526960\n",
      "   5292/175000: episode: 144, duration: 0.506s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 107.808 [33.000, 223.000], mean observation: 0.348 [0.000, 52.000], loss: 0.210597, mean_absolute_error: 0.324375, mean_q: 1.053521, mean_eps: 0.524980\n",
      "   5332/175000: episode: 145, duration: 0.756s, episode steps: 40, steps per second: 53, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 103.700 [1.000, 222.000], mean observation: 0.655 [0.000, 79.000], loss: 0.117854, mean_absolute_error: 0.314486, mean_q: 1.089087, mean_eps: 0.522100\n",
      "   5373/175000: episode: 146, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.829 [1.000, 220.000], mean observation: 0.588 [0.000, 82.000], loss: 0.185525, mean_absolute_error: 0.299029, mean_q: 1.053195, mean_eps: 0.518320\n",
      "   5393/175000: episode: 147, duration: 0.344s, episode steps: 20, steps per second: 58, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 74.800 [0.000, 181.000], mean observation: 0.144 [0.000, 40.000], loss: 0.130733, mean_absolute_error: 0.306731, mean_q: 1.151301, mean_eps: 0.515440\n",
      "   5419/175000: episode: 148, duration: 0.445s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 68.808 [1.000, 196.000], mean observation: 0.337 [0.000, 52.000], loss: 0.172016, mean_absolute_error: 0.306390, mean_q: 1.142769, mean_eps: 0.513460\n",
      "   5453/175000: episode: 149, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.412 [8.000, 221.000], mean observation: 0.414 [0.000, 68.000], loss: 0.126104, mean_absolute_error: 0.295012, mean_q: 1.122403, mean_eps: 0.510760\n",
      "   5504/175000: episode: 150, duration: 0.978s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.392 [5.000, 221.000], mean observation: 1.237 [0.000, 102.000], loss: 0.123636, mean_absolute_error: 0.293767, mean_q: 1.200138, mean_eps: 0.506980\n",
      "   5532/175000: episode: 151, duration: 0.714s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 115.714 [26.000, 222.000], mean observation: 0.348 [0.000, 56.000], loss: 0.136955, mean_absolute_error: 0.262875, mean_q: 0.986355, mean_eps: 0.503560\n",
      "   5570/175000: episode: 152, duration: 0.933s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.500 [12.000, 214.000], mean observation: 0.587 [0.000, 76.000], loss: 0.118528, mean_absolute_error: 0.266896, mean_q: 1.022014, mean_eps: 0.500500\n",
      "   5631/175000: episode: 153, duration: 1.355s, episode steps: 61, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 113.426 [6.000, 215.000], mean observation: 1.349 [0.000, 122.000], loss: 0.095236, mean_absolute_error: 0.288875, mean_q: 1.092171, mean_eps: 0.496000\n",
      "   5655/175000: episode: 154, duration: 0.538s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 156.333 [36.000, 214.000], mean observation: 0.251 [0.000, 48.000], loss: 0.160988, mean_absolute_error: 0.310668, mean_q: 1.174143, mean_eps: 0.492220\n",
      "   5679/175000: episode: 155, duration: 0.612s, episode steps: 24, steps per second: 39, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 125.458 [17.000, 193.000], mean observation: 0.253 [0.000, 48.000], loss: 0.135548, mean_absolute_error: 0.290457, mean_q: 1.112301, mean_eps: 0.490060\n",
      "   5711/175000: episode: 156, duration: 0.630s, episode steps: 32, steps per second: 51, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 121.562 [4.000, 223.000], mean observation: 0.527 [0.000, 63.000], loss: 0.169889, mean_absolute_error: 0.284844, mean_q: 1.190255, mean_eps: 0.487540\n",
      "   5736/175000: episode: 157, duration: 0.584s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 108.960 [2.000, 201.000], mean observation: 0.341 [0.000, 50.000], loss: 0.114169, mean_absolute_error: 0.264324, mean_q: 1.148090, mean_eps: 0.485020\n",
      "   5789/175000: episode: 158, duration: 0.989s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 141.717 [17.000, 223.000], mean observation: 1.121 [0.000, 106.000], loss: 0.135553, mean_absolute_error: 0.272364, mean_q: 1.049968, mean_eps: 0.481420\n",
      "   5828/175000: episode: 159, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 108.513 [23.000, 221.000], mean observation: 0.503 [0.000, 78.000], loss: 0.091180, mean_absolute_error: 0.260359, mean_q: 0.889761, mean_eps: 0.477280\n",
      "   5852/175000: episode: 160, duration: 0.474s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 94.583 [19.000, 215.000], mean observation: 0.195 [0.000, 48.000], loss: 0.099804, mean_absolute_error: 0.246749, mean_q: 0.892359, mean_eps: 0.474580\n",
      "   5875/175000: episode: 161, duration: 0.445s, episode steps: 23, steps per second: 52, episode reward: 1.000, mean reward: 0.043 [0.000, 1.000], mean action: 101.478 [12.000, 219.000], mean observation: 0.234 [0.000, 45.000], loss: 0.149405, mean_absolute_error: 0.253427, mean_q: 0.905617, mean_eps: 0.472420\n",
      "   5897/175000: episode: 162, duration: 0.445s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 157.182 [13.000, 215.000], mean observation: 0.224 [0.000, 44.000], loss: 0.112518, mean_absolute_error: 0.255132, mean_q: 0.914953, mean_eps: 0.470260\n",
      "   5932/175000: episode: 163, duration: 0.674s, episode steps: 35, steps per second: 52, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 150.743 [0.000, 223.000], mean observation: 0.479 [0.000, 69.000], loss: 0.081262, mean_absolute_error: 0.239708, mean_q: 0.854257, mean_eps: 0.467740\n",
      "   5981/175000: episode: 164, duration: 1.173s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.918 [8.000, 221.000], mean observation: 0.771 [0.000, 98.000], loss: 0.091262, mean_absolute_error: 0.240236, mean_q: 0.801649, mean_eps: 0.463960\n",
      "   6005/175000: episode: 165, duration: 0.561s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 96.375 [11.000, 188.000], mean observation: 0.198 [0.000, 48.000], loss: 0.112897, mean_absolute_error: 0.232586, mean_q: 0.758231, mean_eps: 0.460540\n",
      "   6056/175000: episode: 166, duration: 1.307s, episode steps: 51, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.510 [10.000, 223.000], mean observation: 0.774 [0.000, 102.000], loss: 0.175853, mean_absolute_error: 0.240680, mean_q: 0.819738, mean_eps: 0.457300\n",
      "   6081/175000: episode: 167, duration: 0.600s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 146.600 [14.000, 221.000], mean observation: 0.276 [0.000, 50.000], loss: 0.137409, mean_absolute_error: 0.266731, mean_q: 0.953706, mean_eps: 0.453880\n",
      "   6129/175000: episode: 168, duration: 0.931s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 142.646 [4.000, 221.000], mean observation: 0.669 [0.000, 96.000], loss: 0.153055, mean_absolute_error: 0.269667, mean_q: 0.905342, mean_eps: 0.450460\n",
      "   6153/175000: episode: 169, duration: 0.534s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 153.375 [20.000, 199.000], mean observation: 0.164 [0.000, 48.000], loss: 0.185325, mean_absolute_error: 0.304252, mean_q: 1.037824, mean_eps: 0.447220\n",
      "   6170/175000: episode: 170, duration: 0.299s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 159.000 [8.000, 192.000], mean observation: 0.100 [0.000, 34.000], loss: 0.291013, mean_absolute_error: 0.288244, mean_q: 0.980248, mean_eps: 0.445420\n",
      "   6200/175000: episode: 171, duration: 0.635s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.133 [33.000, 218.000], mean observation: 0.383 [0.000, 60.000], loss: 0.288639, mean_absolute_error: 0.283883, mean_q: 1.037884, mean_eps: 0.443440\n",
      "   6228/175000: episode: 172, duration: 0.724s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 104.036 [19.000, 218.000], mean observation: 0.316 [0.000, 56.000], loss: 0.281411, mean_absolute_error: 0.303226, mean_q: 1.267328, mean_eps: 0.440920\n",
      "   6255/175000: episode: 173, duration: 0.687s, episode steps: 27, steps per second: 39, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.000 [6.000, 180.000], mean observation: 0.313 [0.000, 54.000], loss: 0.256985, mean_absolute_error: 0.381513, mean_q: 1.269541, mean_eps: 0.438400\n",
      "   6300/175000: episode: 174, duration: 0.948s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.844 [0.000, 224.000], mean observation: 0.655 [0.000, 90.000], loss: 0.299728, mean_absolute_error: 0.414544, mean_q: 1.151420, mean_eps: 0.435160\n",
      "   6322/175000: episode: 175, duration: 0.449s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.409 [3.000, 190.000], mean observation: 0.131 [0.000, 44.000], loss: 0.312578, mean_absolute_error: 0.407707, mean_q: 1.219926, mean_eps: 0.432100\n",
      "   6345/175000: episode: 176, duration: 0.427s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 43.652 [1.000, 207.000], mean observation: 0.195 [0.000, 46.000], loss: 0.261672, mean_absolute_error: 0.395509, mean_q: 1.257832, mean_eps: 0.429940\n",
      "   6391/175000: episode: 177, duration: 0.794s, episode steps: 46, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 105.478 [1.000, 221.000], mean observation: 0.754 [0.000, 92.000], loss: 0.192967, mean_absolute_error: 0.359960, mean_q: 1.084343, mean_eps: 0.426880\n",
      "   6425/175000: episode: 178, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 59.559 [1.000, 213.000], mean observation: 0.298 [0.000, 68.000], loss: 0.159912, mean_absolute_error: 0.359342, mean_q: 1.120786, mean_eps: 0.423280\n",
      "   6453/175000: episode: 179, duration: 0.505s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 81.893 [1.000, 224.000], mean observation: 0.268 [0.000, 56.000], loss: 0.112122, mean_absolute_error: 0.360923, mean_q: 1.087797, mean_eps: 0.420400\n",
      "   6504/175000: episode: 180, duration: 1.001s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 100.941 [1.000, 201.000], mean observation: 0.760 [0.000, 102.000], loss: 0.184638, mean_absolute_error: 0.372163, mean_q: 1.037890, mean_eps: 0.416980\n",
      "   6545/175000: episode: 181, duration: 0.888s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.146 [8.000, 224.000], mean observation: 0.719 [0.000, 82.000], loss: 0.173612, mean_absolute_error: 0.339382, mean_q: 1.009316, mean_eps: 0.412840\n",
      "   6570/175000: episode: 182, duration: 0.559s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 100.000 [4.000, 219.000], mean observation: 0.222 [0.000, 50.000], loss: 0.181901, mean_absolute_error: 0.308106, mean_q: 1.069818, mean_eps: 0.409780\n",
      "   6591/175000: episode: 183, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 100.667 [3.000, 171.000], mean observation: 0.118 [0.000, 42.000], loss: 0.174335, mean_absolute_error: 0.286439, mean_q: 1.106847, mean_eps: 0.407800\n",
      "   6617/175000: episode: 184, duration: 0.671s, episode steps: 26, steps per second: 39, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.500 [11.000, 222.000], mean observation: 0.235 [0.000, 52.000], loss: 0.209713, mean_absolute_error: 0.286569, mean_q: 1.148111, mean_eps: 0.405640\n",
      "   6651/175000: episode: 185, duration: 0.740s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.676 [16.000, 219.000], mean observation: 0.275 [0.000, 68.000], loss: 0.200950, mean_absolute_error: 0.298268, mean_q: 1.181644, mean_eps: 0.402940\n",
      "   6676/175000: episode: 186, duration: 0.609s, episode steps: 25, steps per second: 41, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 145.120 [54.000, 219.000], mean observation: 0.224 [0.000, 50.000], loss: 0.187008, mean_absolute_error: 0.306083, mean_q: 1.142547, mean_eps: 0.400420\n",
      "   6715/175000: episode: 187, duration: 0.782s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.436 [25.000, 219.000], mean observation: 0.410 [0.000, 78.000], loss: 0.162892, mean_absolute_error: 0.295234, mean_q: 0.976413, mean_eps: 0.397540\n",
      "   6726/175000: episode: 188, duration: 0.247s, episode steps: 11, steps per second: 44, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 146.727 [11.000, 213.000], mean observation: 0.065 [0.000, 22.000], loss: 0.196278, mean_absolute_error: 0.291309, mean_q: 0.949550, mean_eps: 0.395200\n",
      "   6787/175000: episode: 189, duration: 1.357s, episode steps: 61, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 144.984 [41.000, 219.000], mean observation: 0.810 [0.000, 122.000], loss: 0.134164, mean_absolute_error: 0.280306, mean_q: 0.935903, mean_eps: 0.391960\n",
      "   6817/175000: episode: 190, duration: 0.694s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 141.233 [13.000, 218.000], mean observation: 0.304 [0.000, 60.000], loss: 0.162682, mean_absolute_error: 0.266051, mean_q: 0.903618, mean_eps: 0.387820\n",
      "   6833/175000: episode: 191, duration: 0.288s, episode steps: 16, steps per second: 56, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 168.875 [58.000, 218.000], mean observation: 0.084 [0.000, 32.000], loss: 0.172154, mean_absolute_error: 0.262160, mean_q: 0.912572, mean_eps: 0.385660\n",
      "   6864/175000: episode: 192, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 111.516 [13.000, 202.000], mean observation: 0.329 [0.000, 62.000], loss: 0.159295, mean_absolute_error: 0.263464, mean_q: 0.924406, mean_eps: 0.383680\n",
      "   6890/175000: episode: 193, duration: 0.584s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.769 [13.000, 207.000], mean observation: 0.287 [0.000, 52.000], loss: 0.190581, mean_absolute_error: 0.254990, mean_q: 0.930271, mean_eps: 0.381160\n",
      "   6918/175000: episode: 194, duration: 0.611s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 91.857 [16.000, 219.000], mean observation: 0.186 [0.000, 56.000], loss: 0.163445, mean_absolute_error: 0.275532, mean_q: 1.131693, mean_eps: 0.378640\n",
      "   6956/175000: episode: 195, duration: 0.849s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 98.395 [13.000, 215.000], mean observation: 0.510 [0.000, 76.000], loss: 0.129758, mean_absolute_error: 0.270789, mean_q: 1.142558, mean_eps: 0.375760\n",
      "   6994/175000: episode: 196, duration: 0.912s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 127.868 [3.000, 216.000], mean observation: 0.393 [0.000, 76.000], loss: 0.192595, mean_absolute_error: 0.261106, mean_q: 1.143386, mean_eps: 0.372340\n",
      "   7035/175000: episode: 197, duration: 0.842s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.341 [13.000, 204.000], mean observation: 0.549 [0.000, 82.000], loss: 0.109564, mean_absolute_error: 0.245955, mean_q: 1.029922, mean_eps: 0.368740\n",
      "   7074/175000: episode: 198, duration: 0.871s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.513 [8.000, 205.000], mean observation: 0.504 [0.000, 78.000], loss: 0.157885, mean_absolute_error: 0.247806, mean_q: 0.920912, mean_eps: 0.365140\n",
      "   7102/175000: episode: 199, duration: 0.655s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.286 [8.000, 193.000], mean observation: 0.244 [0.000, 56.000], loss: 0.127396, mean_absolute_error: 0.251976, mean_q: 0.889514, mean_eps: 0.362080\n",
      "   7141/175000: episode: 200, duration: 0.796s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.590 [10.000, 224.000], mean observation: 0.411 [0.000, 78.000], loss: 0.130220, mean_absolute_error: 0.273926, mean_q: 0.866171, mean_eps: 0.359020\n",
      "   7191/175000: episode: 201, duration: 0.937s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.780 [9.000, 224.000], mean observation: 0.709 [0.000, 100.000], loss: 0.102803, mean_absolute_error: 0.276815, mean_q: 0.976539, mean_eps: 0.355060\n",
      "   7233/175000: episode: 202, duration: 0.995s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.262 [1.000, 210.000], mean observation: 0.713 [0.000, 84.000], loss: 0.173555, mean_absolute_error: 0.271609, mean_q: 1.058298, mean_eps: 0.350920\n",
      "   7279/175000: episode: 203, duration: 0.976s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.565 [6.000, 194.000], mean observation: 0.617 [0.000, 92.000], loss: 0.118471, mean_absolute_error: 0.254431, mean_q: 1.012366, mean_eps: 0.346960\n",
      "   7297/175000: episode: 204, duration: 0.395s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 120.056 [15.000, 208.000], mean observation: 0.131 [0.000, 36.000], loss: 0.097084, mean_absolute_error: 0.249931, mean_q: 0.973721, mean_eps: 0.344080\n",
      "   7336/175000: episode: 205, duration: 0.810s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.359 [16.000, 219.000], mean observation: 0.397 [0.000, 78.000], loss: 0.140767, mean_absolute_error: 0.278126, mean_q: 1.073120, mean_eps: 0.341560\n",
      "   7356/175000: episode: 206, duration: 0.548s, episode steps: 20, steps per second: 36, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 53.200 [35.000, 208.000], mean observation: 0.099 [0.000, 40.000], loss: 0.176212, mean_absolute_error: 0.251836, mean_q: 0.984148, mean_eps: 0.339040\n",
      "   7378/175000: episode: 207, duration: 0.536s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 65.455 [0.000, 219.000], mean observation: 0.194 [0.000, 44.000], loss: 0.113747, mean_absolute_error: 0.265094, mean_q: 0.932902, mean_eps: 0.337060\n",
      "   7418/175000: episode: 208, duration: 0.817s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.925 [5.000, 218.000], mean observation: 0.459 [0.000, 80.000], loss: 0.132454, mean_absolute_error: 0.250189, mean_q: 0.906028, mean_eps: 0.334180\n",
      "   7451/175000: episode: 209, duration: 0.690s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.545 [5.000, 200.000], mean observation: 0.296 [0.000, 66.000], loss: 0.117431, mean_absolute_error: 0.265944, mean_q: 1.001228, mean_eps: 0.330940\n",
      "   7471/175000: episode: 210, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 169.200 [102.000, 196.000], mean observation: 0.059 [0.000, 40.000], loss: 0.188718, mean_absolute_error: 0.279645, mean_q: 1.004620, mean_eps: 0.328600\n",
      "   7522/175000: episode: 211, duration: 1.134s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.255 [4.000, 224.000], mean observation: 0.997 [0.000, 102.000], loss: 0.142229, mean_absolute_error: 0.265131, mean_q: 0.901242, mean_eps: 0.325360\n",
      "   7565/175000: episode: 212, duration: 1.040s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.488 [5.000, 217.000], mean observation: 0.473 [0.000, 86.000], loss: 0.194123, mean_absolute_error: 0.268260, mean_q: 0.945093, mean_eps: 0.321040\n",
      "   7603/175000: episode: 213, duration: 0.921s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.605 [5.000, 219.000], mean observation: 0.375 [0.000, 76.000], loss: 0.153758, mean_absolute_error: 0.294260, mean_q: 1.009258, mean_eps: 0.317440\n",
      "   7637/175000: episode: 214, duration: 0.827s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 162.441 [0.000, 223.000], mean observation: 0.261 [0.000, 68.000], loss: 0.194759, mean_absolute_error: 0.323260, mean_q: 1.066288, mean_eps: 0.314200\n",
      "   7679/175000: episode: 215, duration: 0.870s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 94.786 [5.000, 210.000], mean observation: 0.346 [0.000, 84.000], loss: 0.201748, mean_absolute_error: 0.311263, mean_q: 1.074490, mean_eps: 0.310780\n",
      "   7693/175000: episode: 216, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 41.929 [11.000, 161.000], mean observation: 0.089 [0.000, 28.000], loss: 0.187729, mean_absolute_error: 0.317419, mean_q: 1.097854, mean_eps: 0.308260\n",
      "   7748/175000: episode: 217, duration: 1.187s, episode steps: 55, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 112.673 [5.000, 215.000], mean observation: 0.640 [0.000, 110.000], loss: 0.124410, mean_absolute_error: 0.304749, mean_q: 1.084975, mean_eps: 0.305200\n",
      "   7786/175000: episode: 218, duration: 0.856s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.211 [6.000, 203.000], mean observation: 0.248 [0.000, 76.000], loss: 0.144748, mean_absolute_error: 0.283058, mean_q: 0.958887, mean_eps: 0.301060\n",
      "   7831/175000: episode: 219, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.711 [6.000, 223.000], mean observation: 0.377 [0.000, 90.000], loss: 0.164704, mean_absolute_error: 0.260764, mean_q: 0.924505, mean_eps: 0.297280\n",
      "   7862/175000: episode: 220, duration: 0.637s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 62.484 [6.000, 211.000], mean observation: 0.270 [0.000, 62.000], loss: 0.170170, mean_absolute_error: 0.267249, mean_q: 1.001953, mean_eps: 0.293860\n",
      "   7905/175000: episode: 221, duration: 1.039s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.023 [2.000, 223.000], mean observation: 0.628 [0.000, 86.000], loss: 0.116276, mean_absolute_error: 0.269948, mean_q: 0.928175, mean_eps: 0.290440\n",
      "   7939/175000: episode: 222, duration: 0.686s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.941 [22.000, 221.000], mean observation: 0.212 [0.000, 68.000], loss: 0.179496, mean_absolute_error: 0.287189, mean_q: 0.957501, mean_eps: 0.287020\n",
      "   7980/175000: episode: 223, duration: 0.994s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.195 [2.000, 204.000], mean observation: 0.374 [0.000, 82.000], loss: 0.131892, mean_absolute_error: 0.270718, mean_q: 0.897746, mean_eps: 0.283780\n",
      "   8016/175000: episode: 224, duration: 0.810s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.861 [20.000, 191.000], mean observation: 0.381 [0.000, 72.000], loss: 0.108474, mean_absolute_error: 0.282691, mean_q: 0.955576, mean_eps: 0.280360\n",
      "   8063/175000: episode: 225, duration: 0.993s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 127.234 [6.000, 223.000], mean observation: 0.581 [0.000, 94.000], loss: 0.180948, mean_absolute_error: 0.277318, mean_q: 1.010722, mean_eps: 0.276580\n",
      "   8098/175000: episode: 226, duration: 0.803s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.171 [3.000, 192.000], mean observation: 0.254 [0.000, 70.000], loss: 0.295070, mean_absolute_error: 0.277714, mean_q: 1.049309, mean_eps: 0.272800\n",
      "   8131/175000: episode: 227, duration: 0.610s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 146.576 [8.000, 223.000], mean observation: 0.258 [0.000, 66.000], loss: 0.158416, mean_absolute_error: 0.312738, mean_q: 1.138158, mean_eps: 0.269740\n",
      "   8166/175000: episode: 228, duration: 0.743s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.600 [2.000, 223.000], mean observation: 0.311 [0.000, 70.000], loss: 0.154123, mean_absolute_error: 0.326897, mean_q: 1.176337, mean_eps: 0.266680\n",
      "   8207/175000: episode: 229, duration: 0.807s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.951 [1.000, 223.000], mean observation: 0.428 [0.000, 82.000], loss: 0.134960, mean_absolute_error: 0.339843, mean_q: 1.218082, mean_eps: 0.263260\n",
      "   8256/175000: episode: 230, duration: 1.039s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.735 [14.000, 215.000], mean observation: 0.484 [0.000, 98.000], loss: 0.134159, mean_absolute_error: 0.313701, mean_q: 1.186722, mean_eps: 0.259300\n",
      "   8265/175000: episode: 231, duration: 0.197s, episode steps: 9, steps per second: 46, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 60.556 [34.000, 209.000], mean observation: 0.032 [0.000, 18.000], loss: 0.127386, mean_absolute_error: 0.296933, mean_q: 1.194912, mean_eps: 0.256600\n",
      "   8294/175000: episode: 232, duration: 0.633s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 52.345 [34.000, 208.000], mean observation: 0.195 [0.000, 58.000], loss: 0.099185, mean_absolute_error: 0.304766, mean_q: 1.290602, mean_eps: 0.254800\n",
      "   8327/175000: episode: 233, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 49.939 [19.000, 220.000], mean observation: 0.193 [0.000, 66.000], loss: 0.222265, mean_absolute_error: 0.289992, mean_q: 1.251227, mean_eps: 0.252100\n",
      "   8369/175000: episode: 234, duration: 0.958s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.048 [2.000, 211.000], mean observation: 0.482 [0.000, 84.000], loss: 0.185608, mean_absolute_error: 0.301938, mean_q: 1.197379, mean_eps: 0.248680\n",
      "   8419/175000: episode: 235, duration: 0.912s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.520 [14.000, 205.000], mean observation: 0.586 [0.000, 100.000], loss: 0.179241, mean_absolute_error: 0.275627, mean_q: 1.081553, mean_eps: 0.244540\n",
      "   8459/175000: episode: 236, duration: 0.951s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 133.175 [23.000, 202.000], mean observation: 0.487 [0.000, 80.000], loss: 0.193300, mean_absolute_error: 0.293368, mean_q: 1.029658, mean_eps: 0.240580\n",
      "   8483/175000: episode: 237, duration: 0.574s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 133.333 [64.000, 197.000], mean observation: 0.166 [0.000, 48.000], loss: 0.157011, mean_absolute_error: 0.310715, mean_q: 1.061909, mean_eps: 0.237700\n",
      "   8499/175000: episode: 238, duration: 0.406s, episode steps: 16, steps per second: 39, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 189.812 [118.000, 223.000], mean observation: 0.086 [0.000, 32.000], loss: 0.249088, mean_absolute_error: 0.301915, mean_q: 1.065089, mean_eps: 0.235900\n",
      "   8531/175000: episode: 239, duration: 0.725s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 146.906 [8.000, 217.000], mean observation: 0.319 [0.000, 64.000], loss: 0.216053, mean_absolute_error: 0.303154, mean_q: 1.076979, mean_eps: 0.233740\n",
      "   8574/175000: episode: 240, duration: 0.864s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.349 [26.000, 197.000], mean observation: 0.516 [0.000, 86.000], loss: 0.157383, mean_absolute_error: 0.313493, mean_q: 1.009709, mean_eps: 0.230320\n",
      "   8624/175000: episode: 241, duration: 1.105s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 137.780 [16.000, 218.000], mean observation: 0.590 [0.000, 100.000], loss: 0.254332, mean_absolute_error: 0.308965, mean_q: 1.004328, mean_eps: 0.226180\n",
      "   8654/175000: episode: 242, duration: 0.695s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 100.367 [23.000, 223.000], mean observation: 0.288 [0.000, 60.000], loss: 0.147099, mean_absolute_error: 0.309080, mean_q: 1.019996, mean_eps: 0.222580\n",
      "   8684/175000: episode: 243, duration: 0.791s, episode steps: 30, steps per second: 38, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 81.967 [4.000, 157.000], mean observation: 0.288 [0.000, 60.000], loss: 0.199655, mean_absolute_error: 0.288057, mean_q: 0.972589, mean_eps: 0.219880\n",
      "   8724/175000: episode: 244, duration: 0.935s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 79.400 [14.000, 208.000], mean observation: 0.427 [0.000, 80.000], loss: 0.174847, mean_absolute_error: 0.310995, mean_q: 1.106217, mean_eps: 0.216820\n",
      "   8775/175000: episode: 245, duration: 1.161s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.471 [4.000, 210.000], mean observation: 0.596 [0.000, 102.000], loss: 0.227587, mean_absolute_error: 0.313625, mean_q: 1.104249, mean_eps: 0.212680\n",
      "   8798/175000: episode: 246, duration: 0.539s, episode steps: 23, steps per second: 43, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 127.391 [12.000, 199.000], mean observation: 0.134 [0.000, 46.000], loss: 0.227294, mean_absolute_error: 0.341414, mean_q: 1.203568, mean_eps: 0.209260\n",
      "   8840/175000: episode: 247, duration: 1.075s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.190 [58.000, 205.000], mean observation: 0.369 [0.000, 84.000], loss: 0.144351, mean_absolute_error: 0.307742, mean_q: 1.053599, mean_eps: 0.206380\n",
      "   8861/175000: episode: 248, duration: 0.549s, episode steps: 21, steps per second: 38, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.286 [33.000, 204.000], mean observation: 0.130 [0.000, 42.000], loss: 0.155186, mean_absolute_error: 0.299245, mean_q: 0.997489, mean_eps: 0.203500\n",
      "   8906/175000: episode: 249, duration: 0.962s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.178 [53.000, 218.000], mean observation: 0.411 [0.000, 90.000], loss: 0.144108, mean_absolute_error: 0.310033, mean_q: 1.066221, mean_eps: 0.200440\n",
      "   8917/175000: episode: 250, duration: 0.199s, episode steps: 11, steps per second: 55, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 85.818 [37.000, 118.000], mean observation: 0.044 [0.000, 22.000], loss: 0.220395, mean_absolute_error: 0.299558, mean_q: 1.046247, mean_eps: 0.197920\n",
      "   8948/175000: episode: 251, duration: 0.745s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.194 [0.000, 198.000], mean observation: 0.230 [0.000, 62.000], loss: 0.120045, mean_absolute_error: 0.288340, mean_q: 1.004611, mean_eps: 0.196120\n",
      "   8993/175000: episode: 252, duration: 0.961s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 111.511 [12.000, 205.000], mean observation: 0.509 [0.000, 90.000], loss: 0.166506, mean_absolute_error: 0.307789, mean_q: 1.036176, mean_eps: 0.192700\n",
      "   9026/175000: episode: 253, duration: 0.764s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.515 [22.000, 166.000], mean observation: 0.162 [0.000, 66.000], loss: 0.253380, mean_absolute_error: 0.288915, mean_q: 1.052505, mean_eps: 0.189100\n",
      "   9062/175000: episode: 254, duration: 0.673s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.278 [36.000, 224.000], mean observation: 0.348 [0.000, 72.000], loss: 0.219202, mean_absolute_error: 0.310682, mean_q: 1.145633, mean_eps: 0.186040\n",
      "   9105/175000: episode: 255, duration: 1.038s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.814 [11.000, 171.000], mean observation: 0.413 [0.000, 86.000], loss: 0.175335, mean_absolute_error: 0.291706, mean_q: 1.039622, mean_eps: 0.182440\n",
      "   9131/175000: episode: 256, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.538 [17.000, 224.000], mean observation: 0.084 [0.000, 52.000], loss: 0.124898, mean_absolute_error: 0.279509, mean_q: 0.973697, mean_eps: 0.179380\n",
      "   9177/175000: episode: 257, duration: 1.086s, episode steps: 46, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.739 [27.000, 224.000], mean observation: 0.389 [0.000, 92.000], loss: 0.151061, mean_absolute_error: 0.287170, mean_q: 1.037016, mean_eps: 0.176140\n",
      "   9215/175000: episode: 258, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.921 [25.000, 224.000], mean observation: 0.204 [0.000, 76.000], loss: 0.149756, mean_absolute_error: 0.277656, mean_q: 1.019909, mean_eps: 0.172360\n",
      "   9252/175000: episode: 259, duration: 0.891s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.541 [7.000, 224.000], mean observation: 0.408 [0.000, 74.000], loss: 0.231850, mean_absolute_error: 0.300715, mean_q: 1.118582, mean_eps: 0.169120\n",
      "   9290/175000: episode: 260, duration: 0.806s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.395 [7.000, 224.000], mean observation: 0.307 [0.000, 76.000], loss: 0.139407, mean_absolute_error: 0.296097, mean_q: 1.036399, mean_eps: 0.165700\n",
      "   9336/175000: episode: 261, duration: 0.882s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 171.783 [3.000, 221.000], mean observation: 0.310 [0.000, 92.000], loss: 0.243416, mean_absolute_error: 0.322668, mean_q: 1.105279, mean_eps: 0.161920\n",
      "   9370/175000: episode: 262, duration: 0.738s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.794 [2.000, 221.000], mean observation: 0.317 [0.000, 68.000], loss: 0.230377, mean_absolute_error: 0.314605, mean_q: 1.165094, mean_eps: 0.158320\n",
      "   9404/175000: episode: 263, duration: 0.673s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 149.353 [71.000, 221.000], mean observation: 0.247 [0.000, 68.000], loss: 0.218448, mean_absolute_error: 0.324238, mean_q: 1.218250, mean_eps: 0.155260\n",
      "   9421/175000: episode: 264, duration: 0.456s, episode steps: 17, steps per second: 37, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 177.118 [119.000, 203.000], mean observation: 0.072 [0.000, 34.000], loss: 0.171108, mean_absolute_error: 0.284586, mean_q: 1.052221, mean_eps: 0.152920\n",
      "   9457/175000: episode: 265, duration: 0.850s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.167 [11.000, 221.000], mean observation: 0.305 [0.000, 72.000], loss: 0.168585, mean_absolute_error: 0.314525, mean_q: 1.174939, mean_eps: 0.150400\n",
      "   9478/175000: episode: 266, duration: 0.515s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 123.429 [6.000, 221.000], mean observation: 0.180 [0.000, 42.000], loss: 0.209352, mean_absolute_error: 0.303974, mean_q: 1.055472, mean_eps: 0.147880\n",
      "   9526/175000: episode: 267, duration: 1.080s, episode steps: 48, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 28.312 [6.000, 221.000], mean observation: 0.257 [0.000, 96.000], loss: 0.261936, mean_absolute_error: 0.300726, mean_q: 1.027675, mean_eps: 0.144820\n",
      "   9571/175000: episode: 268, duration: 0.973s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 149.844 [6.000, 182.000], mean observation: 0.340 [0.000, 90.000], loss: 0.172336, mean_absolute_error: 0.321132, mean_q: 1.032094, mean_eps: 0.140680\n",
      "   9610/175000: episode: 269, duration: 0.790s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.821 [12.000, 210.000], mean observation: 0.389 [0.000, 78.000], loss: 0.151602, mean_absolute_error: 0.318360, mean_q: 1.085466, mean_eps: 0.136900\n",
      "   9641/175000: episode: 270, duration: 0.711s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 84.032 [33.000, 208.000], mean observation: 0.194 [0.000, 62.000], loss: 0.160345, mean_absolute_error: 0.297438, mean_q: 1.129116, mean_eps: 0.133660\n",
      "   9680/175000: episode: 271, duration: 0.821s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.949 [15.000, 175.000], mean observation: 0.359 [0.000, 78.000], loss: 0.145844, mean_absolute_error: 0.281658, mean_q: 0.990258, mean_eps: 0.130600\n",
      "   9726/175000: episode: 272, duration: 0.958s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.717 [27.000, 204.000], mean observation: 0.306 [0.000, 92.000], loss: 0.183389, mean_absolute_error: 0.264470, mean_q: 0.960034, mean_eps: 0.126820\n",
      "   9749/175000: episode: 273, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 119.609 [21.000, 217.000], mean observation: 0.177 [0.000, 46.000], loss: 0.261066, mean_absolute_error: 0.279306, mean_q: 1.075439, mean_eps: 0.123580\n",
      "   9786/175000: episode: 274, duration: 0.690s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 98.054 [27.000, 184.000], mean observation: 0.236 [0.000, 74.000], loss: 0.160478, mean_absolute_error: 0.256274, mean_q: 0.892033, mean_eps: 0.120880\n",
      "   9820/175000: episode: 275, duration: 0.774s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.088 [17.000, 209.000], mean observation: 0.133 [0.000, 68.000], loss: 0.200970, mean_absolute_error: 0.250138, mean_q: 0.813890, mean_eps: 0.117820\n",
      "   9859/175000: episode: 276, duration: 0.827s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 71.949 [27.000, 206.000], mean observation: 0.302 [0.000, 78.000], loss: 0.136371, mean_absolute_error: 0.268528, mean_q: 0.886586, mean_eps: 0.114580\n",
      "   9883/175000: episode: 277, duration: 0.472s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 73.667 [8.000, 202.000], mean observation: 0.085 [0.000, 48.000], loss: 0.164800, mean_absolute_error: 0.255217, mean_q: 0.880753, mean_eps: 0.111700\n",
      "   9915/175000: episode: 278, duration: 0.707s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.438 [6.000, 206.000], mean observation: 0.206 [0.000, 64.000], loss: 0.094525, mean_absolute_error: 0.259850, mean_q: 0.932823, mean_eps: 0.109180\n",
      "   9958/175000: episode: 279, duration: 0.918s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.837 [10.000, 215.000], mean observation: 0.308 [0.000, 86.000], loss: 0.122813, mean_absolute_error: 0.253576, mean_q: 0.871526, mean_eps: 0.105760\n",
      "   9996/175000: episode: 280, duration: 0.857s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.579 [10.000, 203.000], mean observation: 0.176 [0.000, 76.000], loss: 0.094657, mean_absolute_error: 0.261543, mean_q: 0.926039, mean_eps: 0.102160\n",
      "  10056/175000: episode: 281, duration: 1.421s, episode steps: 60, steps per second: 42, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 83.067 [0.000, 214.000], mean observation: 0.724 [0.000, 120.000], loss: 0.591978, mean_absolute_error: 0.261225, mean_q: 0.867033, mean_eps: 0.100024\n",
      "  10086/175000: episode: 282, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 105.233 [27.000, 162.000], mean observation: 0.095 [0.000, 60.000], loss: 0.447050, mean_absolute_error: 0.288863, mean_q: 0.958709, mean_eps: 0.100000\n",
      "  10106/175000: episode: 283, duration: 0.480s, episode steps: 20, steps per second: 42, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 101.250 [40.000, 162.000], mean observation: 0.066 [0.000, 40.000], loss: 0.214927, mean_absolute_error: 0.302144, mean_q: 0.986950, mean_eps: 0.100000\n",
      "  10143/175000: episode: 284, duration: 0.804s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 126.811 [34.000, 203.000], mean observation: 0.274 [0.000, 74.000], loss: 0.415060, mean_absolute_error: 0.307224, mean_q: 1.078963, mean_eps: 0.100000\n",
      "  10202/175000: episode: 285, duration: 1.339s, episode steps: 59, steps per second: 44, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 53.356 [27.000, 203.000], mean observation: 0.545 [0.000, 118.000], loss: 0.302851, mean_absolute_error: 0.292722, mean_q: 1.111227, mean_eps: 0.100000\n",
      "  10239/175000: episode: 286, duration: 0.742s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 45.081 [30.000, 219.000], mean observation: 0.190 [0.000, 74.000], loss: 0.303554, mean_absolute_error: 0.280105, mean_q: 1.136334, mean_eps: 0.100000\n",
      "  10269/175000: episode: 287, duration: 0.609s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 48.467 [44.000, 134.000], mean observation: 0.106 [0.000, 60.000], loss: 0.204598, mean_absolute_error: 0.248804, mean_q: 1.071238, mean_eps: 0.100000\n",
      "  10318/175000: episode: 288, duration: 0.916s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 72.653 [18.000, 137.000], mean observation: 0.208 [0.000, 98.000], loss: 0.154382, mean_absolute_error: 0.253875, mean_q: 1.158982, mean_eps: 0.100000\n",
      "  10346/175000: episode: 289, duration: 0.560s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 67.893 [27.000, 203.000], mean observation: 0.142 [0.000, 56.000], loss: 0.156721, mean_absolute_error: 0.253903, mean_q: 1.096212, mean_eps: 0.100000\n",
      "  10400/175000: episode: 290, duration: 1.241s, episode steps: 54, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.389 [78.000, 209.000], mean observation: 0.390 [0.000, 108.000], loss: 0.176180, mean_absolute_error: 0.258059, mean_q: 1.139706, mean_eps: 0.100000\n",
      "  10425/175000: episode: 291, duration: 0.558s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 48.600 [9.000, 199.000], mean observation: 0.102 [0.000, 50.000], loss: 0.133372, mean_absolute_error: 0.267475, mean_q: 1.322829, mean_eps: 0.100000\n",
      "  10451/175000: episode: 292, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 9.000 [9.000, 9.000], mean observation: 0.061 [0.000, 52.000], loss: 0.149868, mean_absolute_error: 0.271516, mean_q: 1.373930, mean_eps: 0.100000\n",
      "  10499/175000: episode: 293, duration: 1.054s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.979 [9.000, 205.000], mean observation: 0.175 [0.000, 96.000], loss: 0.138450, mean_absolute_error: 0.281499, mean_q: 1.337101, mean_eps: 0.100000\n",
      "  10546/175000: episode: 294, duration: 1.153s, episode steps: 47, steps per second: 41, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 83.447 [4.000, 209.000], mean observation: 0.322 [0.000, 94.000], loss: 0.132217, mean_absolute_error: 0.261495, mean_q: 1.191576, mean_eps: 0.100000\n",
      "  10590/175000: episode: 295, duration: 1.004s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.273 [19.000, 205.000], mean observation: 0.302 [0.000, 88.000], loss: 0.108647, mean_absolute_error: 0.252781, mean_q: 1.069751, mean_eps: 0.100000\n",
      "  10627/175000: episode: 296, duration: 0.787s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 81.946 [33.000, 130.000], mean observation: 0.205 [0.000, 74.000], loss: 0.058878, mean_absolute_error: 0.248261, mean_q: 1.078348, mean_eps: 0.100000\n",
      "  10654/175000: episode: 297, duration: 0.640s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 134.037 [9.000, 216.000], mean observation: 0.080 [0.000, 54.000], loss: 0.125071, mean_absolute_error: 0.242984, mean_q: 1.011419, mean_eps: 0.100000\n",
      "  10691/175000: episode: 298, duration: 0.863s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 82.189 [9.000, 216.000], mean observation: 0.207 [0.000, 74.000], loss: 0.060359, mean_absolute_error: 0.246788, mean_q: 1.024306, mean_eps: 0.100000\n",
      "  10718/175000: episode: 299, duration: 0.600s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.889 [9.000, 114.000], mean observation: 0.076 [0.000, 54.000], loss: 0.132397, mean_absolute_error: 0.233276, mean_q: 0.979701, mean_eps: 0.100000\n",
      "  10758/175000: episode: 300, duration: 0.906s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 123.650 [36.000, 222.000], mean observation: 0.247 [0.000, 80.000], loss: 0.122418, mean_absolute_error: 0.228583, mean_q: 1.029897, mean_eps: 0.100000\n",
      "  10787/175000: episode: 301, duration: 0.805s, episode steps: 29, steps per second: 36, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.345 [22.000, 198.000], mean observation: 0.151 [0.000, 58.000], loss: 0.146358, mean_absolute_error: 0.246408, mean_q: 1.142318, mean_eps: 0.100000\n",
      "  10826/175000: episode: 302, duration: 0.974s, episode steps: 39, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 136.077 [54.000, 222.000], mean observation: 0.270 [0.000, 78.000], loss: 0.146318, mean_absolute_error: 0.248330, mean_q: 1.040984, mean_eps: 0.100000\n",
      "  10875/175000: episode: 303, duration: 1.171s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 163.837 [29.000, 222.000], mean observation: 0.312 [0.000, 98.000], loss: 0.117388, mean_absolute_error: 0.269144, mean_q: 1.020811, mean_eps: 0.100000\n",
      "  10899/175000: episode: 304, duration: 0.538s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 79.250 [65.000, 222.000], mean observation: 0.139 [0.000, 48.000], loss: 0.075121, mean_absolute_error: 0.298480, mean_q: 1.117725, mean_eps: 0.100000\n",
      "  10947/175000: episode: 305, duration: 0.939s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.979 [3.000, 222.000], mean observation: 0.260 [0.000, 96.000], loss: 0.114696, mean_absolute_error: 0.302900, mean_q: 1.063863, mean_eps: 0.100000\n",
      "  10978/175000: episode: 306, duration: 0.669s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 106.161 [27.000, 210.000], mean observation: 0.149 [0.000, 62.000], loss: 0.131573, mean_absolute_error: 0.295028, mean_q: 0.994473, mean_eps: 0.100000\n",
      "  11003/175000: episode: 307, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 172.400 [96.000, 180.000], mean observation: 0.071 [0.000, 50.000], loss: 0.090437, mean_absolute_error: 0.290807, mean_q: 0.994804, mean_eps: 0.100000\n",
      "  11046/175000: episode: 308, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.419 [3.000, 205.000], mean observation: 0.317 [0.000, 86.000], loss: 0.101893, mean_absolute_error: 0.278423, mean_q: 0.970685, mean_eps: 0.100000\n",
      "  11087/175000: episode: 309, duration: 0.866s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.512 [3.000, 224.000], mean observation: 0.286 [0.000, 82.000], loss: 0.109473, mean_absolute_error: 0.257960, mean_q: 0.867641, mean_eps: 0.100000\n",
      "  11132/175000: episode: 310, duration: 1.042s, episode steps: 45, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 170.511 [29.000, 214.000], mean observation: 0.319 [0.000, 90.000], loss: 0.070617, mean_absolute_error: 0.253014, mean_q: 0.847118, mean_eps: 0.100000\n",
      "  11159/175000: episode: 311, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 175.630 [7.000, 200.000], mean observation: 0.180 [0.000, 54.000], loss: 0.064717, mean_absolute_error: 0.256763, mean_q: 0.947570, mean_eps: 0.100000\n",
      "  11174/175000: episode: 312, duration: 0.287s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 183.667 [86.000, 197.000], mean observation: 0.078 [0.000, 30.000], loss: 0.080185, mean_absolute_error: 0.241568, mean_q: 0.923067, mean_eps: 0.100000\n",
      "  11202/175000: episode: 313, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 192.500 [118.000, 210.000], mean observation: 0.087 [0.000, 56.000], loss: 0.097698, mean_absolute_error: 0.266827, mean_q: 1.018215, mean_eps: 0.100000\n",
      "  11236/175000: episode: 314, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 189.029 [42.000, 198.000], mean observation: 0.159 [0.000, 68.000], loss: 0.157746, mean_absolute_error: 0.245719, mean_q: 0.987873, mean_eps: 0.100000\n",
      "  11279/175000: episode: 315, duration: 0.867s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 166.651 [12.000, 200.000], mean observation: 0.226 [0.000, 86.000], loss: 0.157778, mean_absolute_error: 0.286396, mean_q: 1.041141, mean_eps: 0.100000\n",
      "  11325/175000: episode: 316, duration: 0.993s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.674 [3.000, 198.000], mean observation: 0.411 [0.000, 92.000], loss: 0.130091, mean_absolute_error: 0.337886, mean_q: 1.055946, mean_eps: 0.100000\n",
      "  11358/175000: episode: 317, duration: 0.669s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.939 [60.000, 168.000], mean observation: 0.101 [0.000, 66.000], loss: 0.111758, mean_absolute_error: 0.334182, mean_q: 1.034944, mean_eps: 0.100000\n",
      "  11379/175000: episode: 318, duration: 0.451s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 71.619 [4.000, 223.000], mean observation: 0.110 [0.000, 42.000], loss: 0.145209, mean_absolute_error: 0.324382, mean_q: 1.029811, mean_eps: 0.100000\n",
      "  11407/175000: episode: 319, duration: 0.560s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.679 [30.000, 139.000], mean observation: 0.151 [0.000, 56.000], loss: 0.118677, mean_absolute_error: 0.282059, mean_q: 0.910269, mean_eps: 0.100000\n",
      "  11461/175000: episode: 320, duration: 1.116s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.019 [23.000, 214.000], mean observation: 0.402 [0.000, 108.000], loss: 0.144175, mean_absolute_error: 0.290859, mean_q: 1.025345, mean_eps: 0.100000\n",
      "  11499/175000: episode: 321, duration: 0.847s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.026 [30.000, 218.000], mean observation: 0.335 [0.000, 76.000], loss: 0.117987, mean_absolute_error: 0.267239, mean_q: 1.001749, mean_eps: 0.100000\n",
      "  11539/175000: episode: 322, duration: 0.796s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 80.300 [7.000, 110.000], mean observation: 0.195 [0.000, 80.000], loss: 0.149490, mean_absolute_error: 0.265163, mean_q: 0.992374, mean_eps: 0.100000\n",
      "  11564/175000: episode: 323, duration: 0.512s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 123.600 [9.000, 156.000], mean observation: 0.089 [0.000, 50.000], loss: 0.242910, mean_absolute_error: 0.246519, mean_q: 0.901165, mean_eps: 0.100000\n",
      "  11604/175000: episode: 324, duration: 0.812s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 75.100 [7.000, 182.000], mean observation: 0.263 [0.000, 80.000], loss: 0.211537, mean_absolute_error: 0.247018, mean_q: 0.969406, mean_eps: 0.100000\n",
      "  11651/175000: episode: 325, duration: 0.896s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 79.255 [7.000, 205.000], mean observation: 0.214 [0.000, 94.000], loss: 0.180865, mean_absolute_error: 0.253533, mean_q: 1.068143, mean_eps: 0.100000\n",
      "  11675/175000: episode: 326, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 159.667 [117.000, 205.000], mean observation: 0.068 [0.000, 48.000], loss: 0.173545, mean_absolute_error: 0.294092, mean_q: 1.127890, mean_eps: 0.100000\n",
      "  11703/175000: episode: 327, duration: 0.527s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.786 [54.000, 205.000], mean observation: 0.150 [0.000, 56.000], loss: 0.187224, mean_absolute_error: 0.258076, mean_q: 0.909547, mean_eps: 0.100000\n",
      "  11736/175000: episode: 328, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 87.273 [8.000, 172.000], mean observation: 0.151 [0.000, 66.000], loss: 0.140535, mean_absolute_error: 0.249463, mean_q: 0.861931, mean_eps: 0.100000\n",
      "  11771/175000: episode: 329, duration: 0.768s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 90.686 [43.000, 145.000], mean observation: 0.133 [0.000, 70.000], loss: 0.190291, mean_absolute_error: 0.236412, mean_q: 0.867105, mean_eps: 0.100000\n",
      "  11806/175000: episode: 330, duration: 0.748s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.057 [43.000, 126.000], mean observation: 0.178 [0.000, 70.000], loss: 0.196167, mean_absolute_error: 0.270510, mean_q: 0.995933, mean_eps: 0.100000\n",
      "  11835/175000: episode: 331, duration: 0.654s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 138.034 [31.000, 149.000], mean observation: 0.131 [0.000, 58.000], loss: 0.153542, mean_absolute_error: 0.250594, mean_q: 0.955302, mean_eps: 0.100000\n",
      "  11887/175000: episode: 332, duration: 1.151s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.385 [29.000, 149.000], mean observation: 0.336 [0.000, 104.000], loss: 0.099841, mean_absolute_error: 0.247171, mean_q: 1.002131, mean_eps: 0.100000\n",
      "  11939/175000: episode: 333, duration: 0.967s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.096 [20.000, 200.000], mean observation: 0.272 [0.000, 104.000], loss: 0.088796, mean_absolute_error: 0.242219, mean_q: 0.863584, mean_eps: 0.100000\n",
      "  11955/175000: episode: 334, duration: 0.308s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 103.312 [17.000, 110.000], mean observation: 0.047 [0.000, 32.000], loss: 0.179644, mean_absolute_error: 0.275896, mean_q: 1.049858, mean_eps: 0.100000\n",
      "  12005/175000: episode: 335, duration: 0.996s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.500 [29.000, 133.000], mean observation: 0.209 [0.000, 100.000], loss: 0.141139, mean_absolute_error: 0.254752, mean_q: 1.095563, mean_eps: 0.100000\n",
      "  12046/175000: episode: 336, duration: 0.727s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 205.805 [198.000, 218.000], mean observation: 0.119 [0.000, 82.000], loss: 0.155018, mean_absolute_error: 0.262224, mean_q: 1.056698, mean_eps: 0.100000\n",
      "  12094/175000: episode: 337, duration: 0.889s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 159.125 [37.000, 218.000], mean observation: 0.433 [0.000, 96.000], loss: 0.160288, mean_absolute_error: 0.310173, mean_q: 1.195976, mean_eps: 0.100000\n",
      "  12123/175000: episode: 338, duration: 0.549s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 37.000 [37.000, 37.000], mean observation: 0.068 [0.000, 58.000], loss: 0.282825, mean_absolute_error: 0.307999, mean_q: 1.206933, mean_eps: 0.100000\n",
      "  12153/175000: episode: 339, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.633 [37.000, 218.000], mean observation: 0.150 [0.000, 60.000], loss: 0.167892, mean_absolute_error: 0.285643, mean_q: 1.131370, mean_eps: 0.100000\n",
      "  12193/175000: episode: 340, duration: 0.715s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 52.300 [37.000, 221.000], mean observation: 0.150 [0.000, 80.000], loss: 0.309218, mean_absolute_error: 0.318690, mean_q: 1.278199, mean_eps: 0.100000\n",
      "  12225/175000: episode: 341, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 81.625 [37.000, 134.000], mean observation: 0.115 [0.000, 64.000], loss: 0.278833, mean_absolute_error: 0.325743, mean_q: 1.117375, mean_eps: 0.100000\n",
      "  12241/175000: episode: 342, duration: 0.292s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 157.312 [21.000, 178.000], mean observation: 0.087 [0.000, 32.000], loss: 0.147036, mean_absolute_error: 0.327826, mean_q: 1.138951, mean_eps: 0.100000\n",
      "  12263/175000: episode: 343, duration: 0.382s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 125.545 [37.000, 176.000], mean observation: 0.113 [0.000, 44.000], loss: 0.315796, mean_absolute_error: 0.329928, mean_q: 1.257687, mean_eps: 0.100000\n",
      "  12289/175000: episode: 344, duration: 0.537s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 175.077 [164.000, 176.000], mean observation: 0.062 [0.000, 52.000], loss: 0.205363, mean_absolute_error: 0.293764, mean_q: 1.298121, mean_eps: 0.100000\n",
      "  12330/175000: episode: 345, duration: 0.722s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.780 [37.000, 199.000], mean observation: 0.165 [0.000, 82.000], loss: 0.305183, mean_absolute_error: 0.300083, mean_q: 1.397478, mean_eps: 0.100000\n",
      "  12356/175000: episode: 346, duration: 0.576s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 155.115 [29.000, 164.000], mean observation: 0.097 [0.000, 52.000], loss: 0.380995, mean_absolute_error: 0.294557, mean_q: 1.208987, mean_eps: 0.100000\n",
      "  12401/175000: episode: 347, duration: 0.853s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 154.533 [38.000, 199.000], mean observation: 0.395 [0.000, 90.000], loss: 0.174477, mean_absolute_error: 0.271035, mean_q: 1.011896, mean_eps: 0.100000\n",
      "  12441/175000: episode: 348, duration: 0.916s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 162.175 [5.000, 203.000], mean observation: 0.334 [0.000, 80.000], loss: 0.167172, mean_absolute_error: 0.277632, mean_q: 1.089150, mean_eps: 0.100000\n",
      "  12462/175000: episode: 349, duration: 0.481s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 148.190 [88.000, 164.000], mean observation: 0.085 [0.000, 42.000], loss: 0.478757, mean_absolute_error: 0.272463, mean_q: 1.145521, mean_eps: 0.100000\n",
      "  12482/175000: episode: 350, duration: 0.438s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 145.350 [59.000, 191.000], mean observation: 0.090 [0.000, 40.000], loss: 0.267177, mean_absolute_error: 0.245920, mean_q: 1.038725, mean_eps: 0.100000\n",
      "  12512/175000: episode: 351, duration: 0.732s, episode steps: 30, steps per second: 41, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 85.367 [37.000, 208.000], mean observation: 0.162 [0.000, 60.000], loss: 0.218246, mean_absolute_error: 0.274317, mean_q: 1.063617, mean_eps: 0.100000\n",
      "  12555/175000: episode: 352, duration: 0.948s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.721 [2.000, 182.000], mean observation: 0.262 [0.000, 86.000], loss: 0.206090, mean_absolute_error: 0.270936, mean_q: 1.072049, mean_eps: 0.100000\n",
      "  12604/175000: episode: 353, duration: 1.037s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 155.469 [1.000, 219.000], mean observation: 0.369 [0.000, 98.000], loss: 0.221700, mean_absolute_error: 0.283842, mean_q: 1.151397, mean_eps: 0.100000\n",
      "  12625/175000: episode: 354, duration: 0.568s, episode steps: 21, steps per second: 37, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 90.190 [19.000, 210.000], mean observation: 0.138 [0.000, 42.000], loss: 0.243806, mean_absolute_error: 0.287741, mean_q: 1.114314, mean_eps: 0.100000\n",
      "  12664/175000: episode: 355, duration: 0.796s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.128 [10.000, 178.000], mean observation: 0.192 [0.000, 78.000], loss: 0.191899, mean_absolute_error: 0.308387, mean_q: 1.153073, mean_eps: 0.100000\n",
      "  12695/175000: episode: 356, duration: 0.704s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 61.839 [10.000, 178.000], mean observation: 0.160 [0.000, 62.000], loss: 0.288527, mean_absolute_error: 0.297116, mean_q: 1.067363, mean_eps: 0.100000\n",
      "  12734/175000: episode: 357, duration: 0.866s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 46.103 [10.000, 178.000], mean observation: 0.191 [0.000, 78.000], loss: 0.324677, mean_absolute_error: 0.317379, mean_q: 1.182537, mean_eps: 0.100000\n",
      "  12756/175000: episode: 358, duration: 0.490s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 44.409 [10.000, 216.000], mean observation: 0.140 [0.000, 44.000], loss: 0.373289, mean_absolute_error: 0.314916, mean_q: 1.080853, mean_eps: 0.100000\n",
      "  12815/175000: episode: 359, duration: 1.163s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 85.881 [9.000, 165.000], mean observation: 0.308 [0.000, 118.000], loss: 0.377734, mean_absolute_error: 0.360409, mean_q: 1.174454, mean_eps: 0.100000\n",
      "  12867/175000: episode: 360, duration: 1.015s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.635 [11.000, 203.000], mean observation: 0.382 [0.000, 104.000], loss: 0.277614, mean_absolute_error: 0.361302, mean_q: 1.252862, mean_eps: 0.100000\n",
      "  12917/175000: episode: 361, duration: 0.932s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 143.300 [54.000, 223.000], mean observation: 0.326 [0.000, 100.000], loss: 0.241632, mean_absolute_error: 0.331285, mean_q: 1.110418, mean_eps: 0.100000\n",
      "  12961/175000: episode: 362, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 177.523 [47.000, 213.000], mean observation: 0.272 [0.000, 88.000], loss: 0.170155, mean_absolute_error: 0.344261, mean_q: 1.291608, mean_eps: 0.100000\n",
      "  12987/175000: episode: 363, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 210.385 [145.000, 213.000], mean observation: 0.065 [0.000, 52.000], loss: 0.167614, mean_absolute_error: 0.356569, mean_q: 1.275940, mean_eps: 0.100000\n",
      "  13016/175000: episode: 364, duration: 0.662s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 105.103 [35.000, 219.000], mean observation: 0.229 [0.000, 58.000], loss: 0.123374, mean_absolute_error: 0.333794, mean_q: 1.162972, mean_eps: 0.100000\n",
      "  13057/175000: episode: 365, duration: 0.834s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.415 [15.000, 139.000], mean observation: 0.204 [0.000, 82.000], loss: 0.166560, mean_absolute_error: 0.335943, mean_q: 1.170158, mean_eps: 0.100000\n",
      "  13092/175000: episode: 366, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.886 [19.000, 171.000], mean observation: 0.280 [0.000, 70.000], loss: 0.126287, mean_absolute_error: 0.318394, mean_q: 1.096325, mean_eps: 0.100000\n",
      "  13117/175000: episode: 367, duration: 0.592s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 154.440 [28.000, 171.000], mean observation: 0.075 [0.000, 50.000], loss: 0.165427, mean_absolute_error: 0.301540, mean_q: 1.084662, mean_eps: 0.100000\n",
      "  13170/175000: episode: 368, duration: 1.184s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 144.264 [25.000, 171.000], mean observation: 0.342 [0.000, 106.000], loss: 0.176557, mean_absolute_error: 0.321059, mean_q: 1.063383, mean_eps: 0.100000\n",
      "  13197/175000: episode: 369, duration: 0.557s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 76.852 [4.000, 188.000], mean observation: 0.117 [0.000, 54.000], loss: 0.132056, mean_absolute_error: 0.304170, mean_q: 0.949231, mean_eps: 0.100000\n",
      "  13224/175000: episode: 370, duration: 0.542s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 163.111 [70.000, 184.000], mean observation: 0.092 [0.000, 54.000], loss: 0.173725, mean_absolute_error: 0.301344, mean_q: 0.946284, mean_eps: 0.100000\n",
      "  13269/175000: episode: 371, duration: 0.910s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 159.889 [23.000, 163.000], mean observation: 0.123 [0.000, 90.000], loss: 0.129879, mean_absolute_error: 0.298610, mean_q: 1.017200, mean_eps: 0.100000\n",
      "  13305/175000: episode: 372, duration: 0.673s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.806 [4.000, 221.000], mean observation: 0.208 [0.000, 72.000], loss: 0.117579, mean_absolute_error: 0.276863, mean_q: 1.078631, mean_eps: 0.100000\n",
      "  13336/175000: episode: 373, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 104.806 [4.000, 206.000], mean observation: 0.133 [0.000, 62.000], loss: 0.196843, mean_absolute_error: 0.294261, mean_q: 1.112841, mean_eps: 0.100000\n",
      "  13372/175000: episode: 374, duration: 0.712s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 71.444 [4.000, 206.000], mean observation: 0.254 [0.000, 72.000], loss: 0.283921, mean_absolute_error: 0.319296, mean_q: 1.142717, mean_eps: 0.100000\n",
      "  13402/175000: episode: 375, duration: 0.630s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 40.267 [20.000, 186.000], mean observation: 0.141 [0.000, 60.000], loss: 0.223058, mean_absolute_error: 0.365308, mean_q: 1.138513, mean_eps: 0.100000\n",
      "  13425/175000: episode: 376, duration: 0.465s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.348 [19.000, 103.000], mean observation: 0.066 [0.000, 46.000], loss: 0.158353, mean_absolute_error: 0.364541, mean_q: 1.104974, mean_eps: 0.100000\n",
      "  13460/175000: episode: 377, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.000 [67.000, 212.000], mean observation: 0.247 [0.000, 70.000], loss: 0.215036, mean_absolute_error: 0.380901, mean_q: 1.211195, mean_eps: 0.100000\n",
      "  13500/175000: episode: 378, duration: 0.789s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 157.650 [64.000, 212.000], mean observation: 0.277 [0.000, 80.000], loss: 0.268109, mean_absolute_error: 0.342270, mean_q: 1.103623, mean_eps: 0.100000\n",
      "  13525/175000: episode: 379, duration: 0.509s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 8.960 [4.000, 128.000], mean observation: 0.066 [0.000, 50.000], loss: 0.327863, mean_absolute_error: 0.337725, mean_q: 1.209783, mean_eps: 0.100000\n",
      "  13568/175000: episode: 380, duration: 0.856s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 26.907 [2.000, 151.000], mean observation: 0.189 [0.000, 86.000], loss: 0.222592, mean_absolute_error: 0.327294, mean_q: 1.139334, mean_eps: 0.100000\n",
      "  13600/175000: episode: 381, duration: 0.681s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.750 [2.000, 204.000], mean observation: 0.209 [0.000, 64.000], loss: 0.306626, mean_absolute_error: 0.329119, mean_q: 1.269778, mean_eps: 0.100000\n",
      "  13645/175000: episode: 382, duration: 0.888s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.689 [3.000, 204.000], mean observation: 0.324 [0.000, 90.000], loss: 0.278883, mean_absolute_error: 0.297589, mean_q: 1.214664, mean_eps: 0.100000\n",
      "  13672/175000: episode: 383, duration: 0.542s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.259 [25.000, 217.000], mean observation: 0.151 [0.000, 54.000], loss: 0.310873, mean_absolute_error: 0.319748, mean_q: 1.313991, mean_eps: 0.100000\n",
      "  13725/175000: episode: 384, duration: 1.260s, episode steps: 53, steps per second: 42, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 157.264 [48.000, 185.000], mean observation: 0.359 [0.000, 106.000], loss: 0.238647, mean_absolute_error: 0.298088, mean_q: 1.264202, mean_eps: 0.100000\n",
      "  13748/175000: episode: 385, duration: 0.453s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 91.391 [12.000, 154.000], mean observation: 0.072 [0.000, 46.000], loss: 0.324566, mean_absolute_error: 0.269052, mean_q: 1.016295, mean_eps: 0.100000\n",
      "  13796/175000: episode: 386, duration: 0.947s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 22.000 [7.000, 224.000], mean observation: 0.189 [0.000, 96.000], loss: 0.273829, mean_absolute_error: 0.281492, mean_q: 1.044806, mean_eps: 0.100000\n",
      "  13834/175000: episode: 387, duration: 0.971s, episode steps: 38, steps per second: 39, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 47.211 [7.000, 172.000], mean observation: 0.184 [0.000, 76.000], loss: 0.159358, mean_absolute_error: 0.262748, mean_q: 1.066921, mean_eps: 0.100000\n",
      "  13861/175000: episode: 388, duration: 0.585s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 119.185 [7.000, 158.000], mean observation: 0.093 [0.000, 54.000], loss: 0.236766, mean_absolute_error: 0.268134, mean_q: 1.026931, mean_eps: 0.100000\n",
      "  13898/175000: episode: 389, duration: 0.769s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 19.946 [10.000, 63.000], mean observation: 0.110 [0.000, 74.000], loss: 0.240818, mean_absolute_error: 0.272483, mean_q: 1.091190, mean_eps: 0.100000\n",
      "  13948/175000: episode: 390, duration: 1.125s, episode steps: 50, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 22.300 [7.000, 204.000], mean observation: 0.338 [0.000, 100.000], loss: 0.383812, mean_absolute_error: 0.257405, mean_q: 1.049064, mean_eps: 0.100000\n",
      "  13995/175000: episode: 391, duration: 0.961s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.574 [15.000, 206.000], mean observation: 0.201 [0.000, 94.000], loss: 0.205813, mean_absolute_error: 0.268604, mean_q: 1.090655, mean_eps: 0.100000\n",
      "  14023/175000: episode: 392, duration: 0.603s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 58.857 [7.000, 198.000], mean observation: 0.165 [0.000, 56.000], loss: 0.185287, mean_absolute_error: 0.265832, mean_q: 0.976675, mean_eps: 0.100000\n",
      "  14075/175000: episode: 393, duration: 1.104s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 66.038 [7.000, 208.000], mean observation: 0.293 [0.000, 104.000], loss: 0.245817, mean_absolute_error: 0.278248, mean_q: 1.012257, mean_eps: 0.100000\n",
      "  14127/175000: episode: 394, duration: 0.931s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 116.250 [7.000, 200.000], mean observation: 0.303 [0.000, 104.000], loss: 0.153571, mean_absolute_error: 0.294504, mean_q: 1.303818, mean_eps: 0.100000\n",
      "  14156/175000: episode: 395, duration: 0.762s, episode steps: 29, steps per second: 38, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 83.069 [60.000, 188.000], mean observation: 0.080 [0.000, 58.000], loss: 0.092362, mean_absolute_error: 0.320779, mean_q: 1.449899, mean_eps: 0.100000\n",
      "  14182/175000: episode: 396, duration: 0.657s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.385 [77.000, 174.000], mean observation: 0.067 [0.000, 52.000], loss: 0.194907, mean_absolute_error: 0.303281, mean_q: 1.296679, mean_eps: 0.100000\n",
      "  14231/175000: episode: 397, duration: 0.926s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 147.102 [8.000, 216.000], mean observation: 0.466 [0.000, 98.000], loss: 0.185844, mean_absolute_error: 0.307080, mean_q: 1.098395, mean_eps: 0.100000\n",
      "  14273/175000: episode: 398, duration: 0.840s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 22.357 [5.000, 223.000], mean observation: 0.173 [0.000, 84.000], loss: 0.187280, mean_absolute_error: 0.309457, mean_q: 1.097479, mean_eps: 0.100000\n",
      "  14313/175000: episode: 399, duration: 0.925s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 84.625 [5.000, 214.000], mean observation: 0.377 [0.000, 80.000], loss: 0.183415, mean_absolute_error: 0.300820, mean_q: 1.104501, mean_eps: 0.100000\n",
      "  14326/175000: episode: 400, duration: 0.243s, episode steps: 13, steps per second: 53, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 50.154 [5.000, 209.000], mean observation: 0.053 [0.000, 26.000], loss: 0.110521, mean_absolute_error: 0.260943, mean_q: 0.959559, mean_eps: 0.100000\n",
      "  14354/175000: episode: 401, duration: 0.564s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.464 [66.000, 196.000], mean observation: 0.108 [0.000, 56.000], loss: 0.156893, mean_absolute_error: 0.275403, mean_q: 1.031654, mean_eps: 0.100000\n",
      "  14390/175000: episode: 402, duration: 0.649s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.167 [8.000, 217.000], mean observation: 0.319 [0.000, 72.000], loss: 0.270396, mean_absolute_error: 0.285789, mean_q: 1.021196, mean_eps: 0.100000\n",
      "  14419/175000: episode: 403, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 213.172 [206.000, 214.000], mean observation: 0.105 [0.000, 58.000], loss: 0.371132, mean_absolute_error: 0.312631, mean_q: 1.030130, mean_eps: 0.100000\n",
      "  14451/175000: episode: 404, duration: 0.708s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 184.312 [5.000, 214.000], mean observation: 0.112 [0.000, 64.000], loss: 0.318439, mean_absolute_error: 0.325346, mean_q: 1.009655, mean_eps: 0.100000\n",
      "  14477/175000: episode: 405, duration: 0.649s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 157.077 [36.000, 218.000], mean observation: 0.123 [0.000, 52.000], loss: 0.284791, mean_absolute_error: 0.333708, mean_q: 1.038886, mean_eps: 0.100000\n",
      "  14517/175000: episode: 406, duration: 0.901s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.675 [7.000, 214.000], mean observation: 0.247 [0.000, 80.000], loss: 0.292890, mean_absolute_error: 0.328398, mean_q: 1.001587, mean_eps: 0.100000\n",
      "  14547/175000: episode: 407, duration: 0.660s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 19.100 [7.000, 177.000], mean observation: 0.134 [0.000, 60.000], loss: 0.166923, mean_absolute_error: 0.325821, mean_q: 1.050458, mean_eps: 0.100000\n",
      "  14574/175000: episode: 408, duration: 0.626s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 55.148 [19.000, 202.000], mean observation: 0.114 [0.000, 54.000], loss: 0.194963, mean_absolute_error: 0.320094, mean_q: 1.087207, mean_eps: 0.100000\n",
      "  14599/175000: episode: 409, duration: 0.471s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.480 [1.000, 216.000], mean observation: 0.156 [0.000, 50.000], loss: 0.136648, mean_absolute_error: 0.295593, mean_q: 0.984699, mean_eps: 0.100000\n",
      "  14657/175000: episode: 410, duration: 1.150s, episode steps: 58, steps per second: 50, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 65.828 [1.000, 206.000], mean observation: 0.468 [0.000, 116.000], loss: 0.261710, mean_absolute_error: 0.297274, mean_q: 0.948125, mean_eps: 0.100000\n",
      "  14683/175000: episode: 411, duration: 0.569s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 9.346 [1.000, 193.000], mean observation: 0.065 [0.000, 52.000], loss: 0.160765, mean_absolute_error: 0.277265, mean_q: 0.947666, mean_eps: 0.100000\n",
      "  14702/175000: episode: 412, duration: 0.387s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 41.316 [1.000, 137.000], mean observation: 0.056 [0.000, 38.000], loss: 0.156814, mean_absolute_error: 0.297949, mean_q: 1.177012, mean_eps: 0.100000\n",
      "  14730/175000: episode: 413, duration: 0.544s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.321 [84.000, 211.000], mean observation: 0.094 [0.000, 56.000], loss: 0.157794, mean_absolute_error: 0.268422, mean_q: 1.072977, mean_eps: 0.100000\n",
      "  14769/175000: episode: 414, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.538 [20.000, 189.000], mean observation: 0.342 [0.000, 78.000], loss: 0.081975, mean_absolute_error: 0.263237, mean_q: 1.028248, mean_eps: 0.100000\n",
      "  14803/175000: episode: 415, duration: 0.650s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 150.647 [45.000, 218.000], mean observation: 0.234 [0.000, 68.000], loss: 0.115433, mean_absolute_error: 0.264994, mean_q: 1.205273, mean_eps: 0.100000\n",
      "  14840/175000: episode: 416, duration: 0.767s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 145.541 [38.000, 207.000], mean observation: 0.265 [0.000, 74.000], loss: 0.101563, mean_absolute_error: 0.270772, mean_q: 1.243899, mean_eps: 0.100000\n",
      "  14886/175000: episode: 417, duration: 0.871s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.761 [29.000, 216.000], mean observation: 0.369 [0.000, 92.000], loss: 0.106499, mean_absolute_error: 0.260772, mean_q: 1.139226, mean_eps: 0.100000\n",
      "  14912/175000: episode: 418, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.308 [16.000, 215.000], mean observation: 0.192 [0.000, 52.000], loss: 0.197572, mean_absolute_error: 0.259056, mean_q: 1.117681, mean_eps: 0.100000\n",
      "  14939/175000: episode: 419, duration: 0.519s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 142.630 [80.000, 154.000], mean observation: 0.090 [0.000, 54.000], loss: 0.148015, mean_absolute_error: 0.265869, mean_q: 0.992045, mean_eps: 0.100000\n",
      "  14964/175000: episode: 420, duration: 0.499s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 144.600 [42.000, 192.000], mean observation: 0.067 [0.000, 50.000], loss: 0.146599, mean_absolute_error: 0.237200, mean_q: 0.902295, mean_eps: 0.100000\n",
      "  15009/175000: episode: 421, duration: 0.966s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 143.311 [36.000, 148.000], mean observation: 0.133 [0.000, 90.000], loss: 0.103980, mean_absolute_error: 0.246818, mean_q: 0.927032, mean_eps: 0.100000\n",
      "  15043/175000: episode: 422, duration: 0.723s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.382 [52.000, 209.000], mean observation: 0.239 [0.000, 68.000], loss: 0.168491, mean_absolute_error: 0.253386, mean_q: 0.988389, mean_eps: 0.100000\n",
      "  15077/175000: episode: 423, duration: 0.729s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.029 [5.000, 179.000], mean observation: 0.203 [0.000, 68.000], loss: 0.121435, mean_absolute_error: 0.262526, mean_q: 1.033601, mean_eps: 0.100000\n",
      "  15119/175000: episode: 424, duration: 0.818s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.357 [21.000, 199.000], mean observation: 0.223 [0.000, 84.000], loss: 0.107344, mean_absolute_error: 0.251628, mean_q: 0.960889, mean_eps: 0.100000\n",
      "  15146/175000: episode: 425, duration: 0.576s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 123.926 [5.000, 189.000], mean observation: 0.124 [0.000, 54.000], loss: 0.129339, mean_absolute_error: 0.261033, mean_q: 0.941922, mean_eps: 0.100000\n",
      "  15204/175000: episode: 426, duration: 1.241s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 43.931 [9.000, 147.000], mean observation: 0.242 [0.000, 116.000], loss: 0.145862, mean_absolute_error: 0.272668, mean_q: 0.934793, mean_eps: 0.100000\n",
      "  15244/175000: episode: 427, duration: 0.838s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 152.550 [3.000, 216.000], mean observation: 0.416 [0.000, 80.000], loss: 0.137921, mean_absolute_error: 0.280218, mean_q: 0.987167, mean_eps: 0.100000\n",
      "  15281/175000: episode: 428, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 180.027 [45.000, 217.000], mean observation: 0.159 [0.000, 74.000], loss: 0.143158, mean_absolute_error: 0.265857, mean_q: 0.995800, mean_eps: 0.100000\n",
      "  15302/175000: episode: 429, duration: 0.422s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 193.524 [101.000, 203.000], mean observation: 0.074 [0.000, 42.000], loss: 0.122651, mean_absolute_error: 0.269372, mean_q: 1.041820, mean_eps: 0.100000\n",
      "  15334/175000: episode: 430, duration: 0.637s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 170.344 [15.000, 203.000], mean observation: 0.142 [0.000, 64.000], loss: 0.133824, mean_absolute_error: 0.254725, mean_q: 0.934054, mean_eps: 0.100000\n",
      "  15395/175000: episode: 431, duration: 1.077s, episode steps: 61, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 128.164 [45.000, 222.000], mean observation: 0.289 [0.000, 122.000], loss: 0.146915, mean_absolute_error: 0.254177, mean_q: 0.918821, mean_eps: 0.100000\n",
      "  15421/175000: episode: 432, duration: 0.536s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.731 [19.000, 220.000], mean observation: 0.127 [0.000, 52.000], loss: 0.136417, mean_absolute_error: 0.239133, mean_q: 1.097303, mean_eps: 0.100000\n",
      "  15446/175000: episode: 433, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 45.680 [6.000, 105.000], mean observation: 0.089 [0.000, 50.000], loss: 0.114318, mean_absolute_error: 0.253972, mean_q: 1.234239, mean_eps: 0.100000\n",
      "  15490/175000: episode: 434, duration: 0.834s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 82.409 [26.000, 217.000], mean observation: 0.277 [0.000, 88.000], loss: 0.145590, mean_absolute_error: 0.252755, mean_q: 1.107142, mean_eps: 0.100000\n",
      "  15526/175000: episode: 435, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 150.306 [12.000, 211.000], mean observation: 0.202 [0.000, 72.000], loss: 0.176160, mean_absolute_error: 0.266847, mean_q: 0.988659, mean_eps: 0.100000\n",
      "  15551/175000: episode: 436, duration: 0.463s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 169.320 [26.000, 211.000], mean observation: 0.114 [0.000, 50.000], loss: 0.219221, mean_absolute_error: 0.278660, mean_q: 1.013739, mean_eps: 0.100000\n",
      "  15582/175000: episode: 437, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.871 [75.000, 211.000], mean observation: 0.113 [0.000, 62.000], loss: 0.173067, mean_absolute_error: 0.288941, mean_q: 1.062579, mean_eps: 0.100000\n",
      "  15617/175000: episode: 438, duration: 0.683s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 138.343 [120.000, 211.000], mean observation: 0.114 [0.000, 70.000], loss: 0.185371, mean_absolute_error: 0.293367, mean_q: 1.092059, mean_eps: 0.100000\n",
      "  15644/175000: episode: 439, duration: 0.526s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 145.815 [38.000, 224.000], mean observation: 0.186 [0.000, 54.000], loss: 0.179838, mean_absolute_error: 0.317650, mean_q: 1.175358, mean_eps: 0.100000\n",
      "  15665/175000: episode: 440, duration: 0.452s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 128.476 [45.000, 217.000], mean observation: 0.087 [0.000, 42.000], loss: 0.239213, mean_absolute_error: 0.308892, mean_q: 1.035832, mean_eps: 0.100000\n",
      "  15696/175000: episode: 441, duration: 0.602s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 193.097 [94.000, 222.000], mean observation: 0.262 [0.000, 62.000], loss: 0.228478, mean_absolute_error: 0.303278, mean_q: 0.953749, mean_eps: 0.100000\n",
      "  15723/175000: episode: 442, duration: 0.580s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 110.407 [59.000, 209.000], mean observation: 0.157 [0.000, 54.000], loss: 0.179789, mean_absolute_error: 0.326793, mean_q: 1.067803, mean_eps: 0.100000\n",
      "  15749/175000: episode: 443, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 133.769 [45.000, 209.000], mean observation: 0.128 [0.000, 52.000], loss: 0.139730, mean_absolute_error: 0.321728, mean_q: 1.085518, mean_eps: 0.100000\n",
      "  15762/175000: episode: 444, duration: 0.235s, episode steps: 13, steps per second: 55, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 119.615 [5.000, 163.000], mean observation: 0.043 [0.000, 26.000], loss: 0.279703, mean_absolute_error: 0.318340, mean_q: 1.070270, mean_eps: 0.100000\n",
      "  15801/175000: episode: 445, duration: 0.783s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.103 [24.000, 209.000], mean observation: 0.284 [0.000, 78.000], loss: 0.147897, mean_absolute_error: 0.321831, mean_q: 1.078934, mean_eps: 0.100000\n",
      "  15854/175000: episode: 446, duration: 0.971s, episode steps: 53, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.453 [24.000, 209.000], mean observation: 0.404 [0.000, 106.000], loss: 0.155641, mean_absolute_error: 0.308782, mean_q: 1.074328, mean_eps: 0.100000\n",
      "  15899/175000: episode: 447, duration: 0.818s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.467 [80.000, 199.000], mean observation: 0.203 [0.000, 90.000], loss: 0.092879, mean_absolute_error: 0.271050, mean_q: 0.959959, mean_eps: 0.100000\n",
      "  15937/175000: episode: 448, duration: 0.762s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 132.763 [17.000, 199.000], mean observation: 0.256 [0.000, 76.000], loss: 0.217047, mean_absolute_error: 0.264108, mean_q: 0.919368, mean_eps: 0.100000\n",
      "  15977/175000: episode: 449, duration: 0.747s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.100 [30.000, 217.000], mean observation: 0.296 [0.000, 80.000], loss: 0.189972, mean_absolute_error: 0.284894, mean_q: 1.083203, mean_eps: 0.100000\n",
      "  16007/175000: episode: 450, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.467 [47.000, 217.000], mean observation: 0.135 [0.000, 60.000], loss: 0.202112, mean_absolute_error: 0.317823, mean_q: 1.444260, mean_eps: 0.100000\n",
      "  16048/175000: episode: 451, duration: 0.785s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 146.049 [21.000, 221.000], mean observation: 0.188 [0.000, 82.000], loss: 0.213001, mean_absolute_error: 0.326748, mean_q: 1.425570, mean_eps: 0.100000\n",
      "  16077/175000: episode: 452, duration: 0.613s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.517 [0.000, 215.000], mean observation: 0.180 [0.000, 58.000], loss: 0.157793, mean_absolute_error: 0.298427, mean_q: 1.254617, mean_eps: 0.100000\n",
      "  16119/175000: episode: 453, duration: 0.773s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.452 [39.000, 215.000], mean observation: 0.308 [0.000, 84.000], loss: 0.161327, mean_absolute_error: 0.287445, mean_q: 1.235862, mean_eps: 0.100000\n",
      "  16148/175000: episode: 454, duration: 0.588s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 186.414 [70.000, 221.000], mean observation: 0.133 [0.000, 58.000], loss: 0.183996, mean_absolute_error: 0.271922, mean_q: 1.215902, mean_eps: 0.100000\n",
      "  16191/175000: episode: 455, duration: 0.830s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 173.953 [111.000, 213.000], mean observation: 0.251 [0.000, 86.000], loss: 0.251460, mean_absolute_error: 0.266520, mean_q: 1.130898, mean_eps: 0.100000\n",
      "  16228/175000: episode: 456, duration: 0.728s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 103.676 [70.000, 224.000], mean observation: 0.099 [0.000, 74.000], loss: 0.212779, mean_absolute_error: 0.279337, mean_q: 1.183398, mean_eps: 0.100000\n",
      "  16264/175000: episode: 457, duration: 0.731s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 147.083 [42.000, 222.000], mean observation: 0.260 [0.000, 72.000], loss: 0.163251, mean_absolute_error: 0.282644, mean_q: 1.184162, mean_eps: 0.100000\n",
      "  16303/175000: episode: 458, duration: 0.719s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.590 [5.000, 222.000], mean observation: 0.294 [0.000, 78.000], loss: 0.288466, mean_absolute_error: 0.282795, mean_q: 1.208655, mean_eps: 0.100000\n",
      "  16357/175000: episode: 459, duration: 1.040s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 180.907 [41.000, 224.000], mean observation: 0.297 [0.000, 108.000], loss: 0.175037, mean_absolute_error: 0.317283, mean_q: 1.363765, mean_eps: 0.100000\n",
      "  16408/175000: episode: 460, duration: 0.982s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 183.098 [49.000, 224.000], mean observation: 0.255 [0.000, 102.000], loss: 0.265301, mean_absolute_error: 0.356049, mean_q: 1.245762, mean_eps: 0.100000\n",
      "  16440/175000: episode: 461, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 160.938 [55.000, 168.000], mean observation: 0.092 [0.000, 64.000], loss: 0.226944, mean_absolute_error: 0.331587, mean_q: 1.230312, mean_eps: 0.100000\n",
      "  16478/175000: episode: 462, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 168.500 [11.000, 224.000], mean observation: 0.155 [0.000, 76.000], loss: 0.182570, mean_absolute_error: 0.335493, mean_q: 1.435473, mean_eps: 0.100000\n",
      "  16509/175000: episode: 463, duration: 0.594s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 203.032 [168.000, 217.000], mean observation: 0.108 [0.000, 62.000], loss: 0.228300, mean_absolute_error: 0.327248, mean_q: 1.429965, mean_eps: 0.100000\n",
      "  16553/175000: episode: 464, duration: 0.828s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 185.091 [5.000, 217.000], mean observation: 0.165 [0.000, 88.000], loss: 0.220222, mean_absolute_error: 0.375791, mean_q: 1.410841, mean_eps: 0.100000\n",
      "  16578/175000: episode: 465, duration: 0.503s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 164.600 [90.000, 168.000], mean observation: 0.092 [0.000, 50.000], loss: 0.154371, mean_absolute_error: 0.385546, mean_q: 1.243673, mean_eps: 0.100000\n",
      "  16618/175000: episode: 466, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.500 [24.000, 214.000], mean observation: 0.370 [0.000, 80.000], loss: 0.205960, mean_absolute_error: 0.375118, mean_q: 1.190037, mean_eps: 0.100000\n",
      "  16668/175000: episode: 467, duration: 1.225s, episode steps: 50, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 175.860 [64.000, 224.000], mean observation: 0.294 [0.000, 100.000], loss: 0.174757, mean_absolute_error: 0.346732, mean_q: 1.209846, mean_eps: 0.100000\n",
      "  16681/175000: episode: 468, duration: 0.295s, episode steps: 13, steps per second: 44, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 190.000 [190.000, 190.000], mean observation: 0.033 [0.000, 26.000], loss: 0.144016, mean_absolute_error: 0.287701, mean_q: 1.205585, mean_eps: 0.100000\n",
      "  16716/175000: episode: 469, duration: 0.747s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 148.343 [25.000, 224.000], mean observation: 0.175 [0.000, 70.000], loss: 0.110684, mean_absolute_error: 0.313433, mean_q: 1.230349, mean_eps: 0.100000\n",
      "  16739/175000: episode: 470, duration: 0.498s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 138.826 [43.000, 166.000], mean observation: 0.107 [0.000, 46.000], loss: 0.094502, mean_absolute_error: 0.285637, mean_q: 1.105711, mean_eps: 0.100000\n",
      "  16783/175000: episode: 471, duration: 1.048s, episode steps: 44, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.886 [18.000, 188.000], mean observation: 0.300 [0.000, 88.000], loss: 0.087830, mean_absolute_error: 0.299964, mean_q: 1.188636, mean_eps: 0.100000\n",
      "  16815/175000: episode: 472, duration: 0.672s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 141.219 [36.000, 182.000], mean observation: 0.176 [0.000, 64.000], loss: 0.094186, mean_absolute_error: 0.283509, mean_q: 1.122860, mean_eps: 0.100000\n",
      "  16855/175000: episode: 473, duration: 0.784s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 165.725 [4.000, 187.000], mean observation: 0.211 [0.000, 80.000], loss: 0.160666, mean_absolute_error: 0.272017, mean_q: 1.081895, mean_eps: 0.100000\n",
      "  16901/175000: episode: 474, duration: 0.963s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 184.848 [50.000, 224.000], mean observation: 0.335 [0.000, 92.000], loss: 0.136206, mean_absolute_error: 0.283626, mean_q: 1.137024, mean_eps: 0.100000\n",
      "  16933/175000: episode: 475, duration: 0.624s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 169.438 [25.000, 220.000], mean observation: 0.153 [0.000, 64.000], loss: 0.096654, mean_absolute_error: 0.303613, mean_q: 1.231644, mean_eps: 0.100000\n",
      "  16981/175000: episode: 476, duration: 0.941s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 166.542 [34.000, 224.000], mean observation: 0.217 [0.000, 96.000], loss: 0.170363, mean_absolute_error: 0.347351, mean_q: 1.352304, mean_eps: 0.100000\n",
      "  17011/175000: episode: 477, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 161.067 [46.000, 224.000], mean observation: 0.156 [0.000, 60.000], loss: 0.077809, mean_absolute_error: 0.321863, mean_q: 1.237014, mean_eps: 0.100000\n",
      "  17048/175000: episode: 478, duration: 0.763s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.054 [25.000, 222.000], mean observation: 0.177 [0.000, 74.000], loss: 0.065389, mean_absolute_error: 0.317479, mean_q: 1.177558, mean_eps: 0.100000\n",
      "  17086/175000: episode: 479, duration: 0.785s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.263 [46.000, 198.000], mean observation: 0.246 [0.000, 76.000], loss: 0.110832, mean_absolute_error: 0.293675, mean_q: 1.152345, mean_eps: 0.100000\n",
      "  17120/175000: episode: 480, duration: 0.674s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.412 [44.000, 175.000], mean observation: 0.134 [0.000, 68.000], loss: 0.144149, mean_absolute_error: 0.291900, mean_q: 1.211507, mean_eps: 0.100000\n",
      "  17162/175000: episode: 481, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.381 [12.000, 216.000], mean observation: 0.360 [0.000, 84.000], loss: 0.128171, mean_absolute_error: 0.275816, mean_q: 1.129716, mean_eps: 0.100000\n",
      "  17186/175000: episode: 482, duration: 0.500s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 168.667 [168.000, 184.000], mean observation: 0.067 [0.000, 48.000], loss: 0.130374, mean_absolute_error: 0.278670, mean_q: 1.118325, mean_eps: 0.100000\n",
      "  17211/175000: episode: 483, duration: 0.536s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 175.320 [29.000, 203.000], mean observation: 0.117 [0.000, 50.000], loss: 0.174102, mean_absolute_error: 0.279828, mean_q: 1.154206, mean_eps: 0.100000\n",
      "  17245/175000: episode: 484, duration: 0.713s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 33.912 [1.000, 70.000], mean observation: 0.102 [0.000, 68.000], loss: 0.129625, mean_absolute_error: 0.263510, mean_q: 1.042938, mean_eps: 0.100000\n",
      "  17267/175000: episode: 485, duration: 0.411s, episode steps: 22, steps per second: 53, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 41.500 [1.000, 216.000], mean observation: 0.116 [0.000, 44.000], loss: 0.070884, mean_absolute_error: 0.285569, mean_q: 1.130210, mean_eps: 0.100000\n",
      "  17283/175000: episode: 486, duration: 0.384s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 7.250 [1.000, 101.000], mean observation: 0.060 [0.000, 32.000], loss: 0.110667, mean_absolute_error: 0.248219, mean_q: 0.959733, mean_eps: 0.100000\n",
      "  17308/175000: episode: 487, duration: 0.631s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 8.040 [1.000, 177.000], mean observation: 0.061 [0.000, 50.000], loss: 0.121055, mean_absolute_error: 0.261489, mean_q: 1.020399, mean_eps: 0.100000\n",
      "  17331/175000: episode: 488, duration: 0.451s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 82.348 [1.000, 168.000], mean observation: 0.119 [0.000, 46.000], loss: 0.125435, mean_absolute_error: 0.255939, mean_q: 1.081656, mean_eps: 0.100000\n",
      "  17352/175000: episode: 489, duration: 0.491s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 123.333 [1.000, 217.000], mean observation: 0.087 [0.000, 42.000], loss: 0.176207, mean_absolute_error: 0.269243, mean_q: 1.154294, mean_eps: 0.100000\n",
      "  17376/175000: episode: 490, duration: 0.668s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 163.958 [12.000, 217.000], mean observation: 0.137 [0.000, 48.000], loss: 0.128240, mean_absolute_error: 0.256473, mean_q: 1.106416, mean_eps: 0.100000\n",
      "  17416/175000: episode: 491, duration: 0.941s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.300 [5.000, 168.000], mean observation: 0.285 [0.000, 80.000], loss: 0.096204, mean_absolute_error: 0.246297, mean_q: 1.058063, mean_eps: 0.100000\n",
      "  17434/175000: episode: 492, duration: 0.473s, episode steps: 18, steps per second: 38, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 41.222 [8.000, 62.000], mean observation: 0.094 [0.000, 36.000], loss: 0.100162, mean_absolute_error: 0.238804, mean_q: 0.979280, mean_eps: 0.100000\n",
      "  17477/175000: episode: 493, duration: 0.912s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 42.907 [29.000, 158.000], mean observation: 0.157 [0.000, 86.000], loss: 0.172630, mean_absolute_error: 0.249188, mean_q: 1.027559, mean_eps: 0.100000\n",
      "  17527/175000: episode: 494, duration: 0.891s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.580 [38.000, 185.000], mean observation: 0.289 [0.000, 100.000], loss: 0.158822, mean_absolute_error: 0.273237, mean_q: 0.949943, mean_eps: 0.100000\n",
      "  17581/175000: episode: 495, duration: 1.125s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.926 [4.000, 175.000], mean observation: 0.272 [0.000, 108.000], loss: 0.212355, mean_absolute_error: 0.317800, mean_q: 1.013949, mean_eps: 0.100000\n",
      "  17642/175000: episode: 496, duration: 1.287s, episode steps: 61, steps per second: 47, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 38.361 [4.000, 161.000], mean observation: 0.348 [0.000, 122.000], loss: 0.126611, mean_absolute_error: 0.329265, mean_q: 1.034115, mean_eps: 0.100000\n",
      "  17673/175000: episode: 497, duration: 0.645s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 77.355 [4.000, 211.000], mean observation: 0.162 [0.000, 62.000], loss: 0.150378, mean_absolute_error: 0.321583, mean_q: 1.013974, mean_eps: 0.100000\n",
      "  17687/175000: episode: 498, duration: 0.261s, episode steps: 14, steps per second: 54, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 153.571 [93.000, 205.000], mean observation: 0.069 [0.000, 28.000], loss: 0.180209, mean_absolute_error: 0.279643, mean_q: 0.890032, mean_eps: 0.100000\n",
      "  17723/175000: episode: 499, duration: 0.721s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.583 [12.000, 203.000], mean observation: 0.221 [0.000, 72.000], loss: 0.096467, mean_absolute_error: 0.252881, mean_q: 0.879503, mean_eps: 0.100000\n",
      "  17759/175000: episode: 500, duration: 0.747s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 73.944 [13.000, 209.000], mean observation: 0.184 [0.000, 72.000], loss: 0.131287, mean_absolute_error: 0.258645, mean_q: 0.897354, mean_eps: 0.100000\n",
      "  17778/175000: episode: 501, duration: 0.456s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 66.789 [39.000, 223.000], mean observation: 0.080 [0.000, 38.000], loss: 0.141667, mean_absolute_error: 0.258274, mean_q: 0.912745, mean_eps: 0.100000\n",
      "  17825/175000: episode: 502, duration: 1.121s, episode steps: 47, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 51.851 [14.000, 185.000], mean observation: 0.289 [0.000, 94.000], loss: 0.100607, mean_absolute_error: 0.249853, mean_q: 0.902227, mean_eps: 0.100000\n",
      "  17870/175000: episode: 503, duration: 0.937s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 45.156 [4.000, 223.000], mean observation: 0.336 [0.000, 90.000], loss: 0.116010, mean_absolute_error: 0.247130, mean_q: 0.860526, mean_eps: 0.100000\n",
      "  17893/175000: episode: 504, duration: 0.496s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 146.261 [64.000, 150.000], mean observation: 0.066 [0.000, 46.000], loss: 0.106070, mean_absolute_error: 0.286888, mean_q: 1.020153, mean_eps: 0.100000\n",
      "  17913/175000: episode: 505, duration: 0.391s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 162.550 [28.000, 175.000], mean observation: 0.097 [0.000, 40.000], loss: 0.077582, mean_absolute_error: 0.239533, mean_q: 0.943504, mean_eps: 0.100000\n",
      "  17956/175000: episode: 506, duration: 0.884s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.930 [20.000, 175.000], mean observation: 0.222 [0.000, 86.000], loss: 0.083552, mean_absolute_error: 0.241852, mean_q: 1.040996, mean_eps: 0.100000\n",
      "  17987/175000: episode: 507, duration: 0.661s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 164.097 [5.000, 175.000], mean observation: 0.127 [0.000, 62.000], loss: 0.109159, mean_absolute_error: 0.245982, mean_q: 0.937267, mean_eps: 0.100000\n",
      "  18036/175000: episode: 508, duration: 1.031s, episode steps: 49, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.245 [33.000, 209.000], mean observation: 0.599 [0.000, 98.000], loss: 0.157380, mean_absolute_error: 0.259956, mean_q: 0.942627, mean_eps: 0.100000\n",
      "  18077/175000: episode: 509, duration: 0.836s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 151.293 [52.000, 223.000], mean observation: 0.224 [0.000, 82.000], loss: 0.106468, mean_absolute_error: 0.263586, mean_q: 0.998101, mean_eps: 0.100000\n",
      "  18119/175000: episode: 510, duration: 0.790s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.786 [52.000, 220.000], mean observation: 0.347 [0.000, 84.000], loss: 0.222130, mean_absolute_error: 0.274206, mean_q: 1.075836, mean_eps: 0.100000\n",
      "  18173/175000: episode: 511, duration: 1.122s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 140.648 [12.000, 223.000], mean observation: 0.442 [0.000, 108.000], loss: 0.171351, mean_absolute_error: 0.264035, mean_q: 1.049580, mean_eps: 0.100000\n",
      "  18218/175000: episode: 512, duration: 0.833s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.778 [37.000, 223.000], mean observation: 0.348 [0.000, 90.000], loss: 0.151612, mean_absolute_error: 0.272034, mean_q: 1.077665, mean_eps: 0.100000\n",
      "  18258/175000: episode: 513, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.375 [39.000, 203.000], mean observation: 0.193 [0.000, 80.000], loss: 0.242234, mean_absolute_error: 0.257966, mean_q: 0.999928, mean_eps: 0.100000\n",
      "  18289/175000: episode: 514, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 142.935 [118.000, 175.000], mean observation: 0.108 [0.000, 62.000], loss: 0.206159, mean_absolute_error: 0.255828, mean_q: 1.027550, mean_eps: 0.100000\n",
      "  18330/175000: episode: 515, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.049 [1.000, 208.000], mean observation: 0.185 [0.000, 82.000], loss: 0.241723, mean_absolute_error: 0.282424, mean_q: 1.253072, mean_eps: 0.100000\n",
      "  18375/175000: episode: 516, duration: 0.915s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.156 [2.000, 208.000], mean observation: 0.113 [0.000, 90.000], loss: 0.378948, mean_absolute_error: 0.280167, mean_q: 1.236795, mean_eps: 0.100000\n",
      "  18410/175000: episode: 517, duration: 0.692s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.600 [49.000, 208.000], mean observation: 0.199 [0.000, 70.000], loss: 0.342982, mean_absolute_error: 0.279382, mean_q: 1.096607, mean_eps: 0.100000\n",
      "  18451/175000: episode: 518, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.854 [24.000, 208.000], mean observation: 0.144 [0.000, 82.000], loss: 0.196798, mean_absolute_error: 0.279744, mean_q: 1.112750, mean_eps: 0.100000\n",
      "  18497/175000: episode: 519, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.543 [26.000, 213.000], mean observation: 0.367 [0.000, 92.000], loss: 0.262599, mean_absolute_error: 0.281171, mean_q: 1.115357, mean_eps: 0.100000\n",
      "  18528/175000: episode: 520, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 86.452 [42.000, 223.000], mean observation: 0.147 [0.000, 62.000], loss: 0.417579, mean_absolute_error: 0.279212, mean_q: 1.123532, mean_eps: 0.100000\n",
      "  18563/175000: episode: 521, duration: 0.791s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 184.657 [32.000, 223.000], mean observation: 0.188 [0.000, 70.000], loss: 0.274387, mean_absolute_error: 0.269234, mean_q: 1.065093, mean_eps: 0.100000\n",
      "  18590/175000: episode: 522, duration: 0.635s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 200.111 [9.000, 223.000], mean observation: 0.101 [0.000, 54.000], loss: 0.202154, mean_absolute_error: 0.270071, mean_q: 1.068365, mean_eps: 0.100000\n",
      "  18645/175000: episode: 523, duration: 1.295s, episode steps: 55, steps per second: 42, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 134.982 [19.000, 223.000], mean observation: 0.457 [0.000, 110.000], loss: 0.179638, mean_absolute_error: 0.273675, mean_q: 1.032870, mean_eps: 0.100000\n",
      "  18692/175000: episode: 524, duration: 1.042s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.191 [50.000, 209.000], mean observation: 0.258 [0.000, 94.000], loss: 0.242041, mean_absolute_error: 0.286365, mean_q: 1.049611, mean_eps: 0.100000\n",
      "  18737/175000: episode: 525, duration: 0.916s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 98.267 [18.000, 154.000], mean observation: 0.323 [0.000, 90.000], loss: 0.263881, mean_absolute_error: 0.306317, mean_q: 1.096301, mean_eps: 0.100000\n",
      "  18763/175000: episode: 526, duration: 0.490s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 90.038 [75.000, 148.000], mean observation: 0.080 [0.000, 52.000], loss: 0.450117, mean_absolute_error: 0.284020, mean_q: 1.088834, mean_eps: 0.100000\n",
      "  18817/175000: episode: 527, duration: 1.265s, episode steps: 54, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.444 [75.000, 183.000], mean observation: 0.353 [0.000, 108.000], loss: 0.271466, mean_absolute_error: 0.292785, mean_q: 1.159824, mean_eps: 0.100000\n",
      "  18865/175000: episode: 528, duration: 0.970s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.479 [66.000, 216.000], mean observation: 0.331 [0.000, 96.000], loss: 0.226457, mean_absolute_error: 0.278766, mean_q: 1.029312, mean_eps: 0.100000\n",
      "  18885/175000: episode: 529, duration: 0.434s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 128.050 [2.000, 150.000], mean observation: 0.076 [0.000, 40.000], loss: 0.065588, mean_absolute_error: 0.280392, mean_q: 1.081957, mean_eps: 0.100000\n",
      "  18917/175000: episode: 530, duration: 0.621s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 127.812 [17.000, 174.000], mean observation: 0.197 [0.000, 64.000], loss: 0.170034, mean_absolute_error: 0.266251, mean_q: 1.057301, mean_eps: 0.100000\n",
      "  18957/175000: episode: 531, duration: 0.829s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 134.925 [19.000, 220.000], mean observation: 0.369 [0.000, 80.000], loss: 0.182062, mean_absolute_error: 0.277409, mean_q: 1.126102, mean_eps: 0.100000\n",
      "  18971/175000: episode: 532, duration: 0.238s, episode steps: 14, steps per second: 59, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 117.214 [42.000, 148.000], mean observation: 0.064 [0.000, 28.000], loss: 0.188490, mean_absolute_error: 0.314646, mean_q: 1.234862, mean_eps: 0.100000\n",
      "  19010/175000: episode: 533, duration: 0.826s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.487 [7.000, 150.000], mean observation: 0.147 [0.000, 78.000], loss: 0.125724, mean_absolute_error: 0.301362, mean_q: 1.138351, mean_eps: 0.100000\n",
      "  19046/175000: episode: 534, duration: 0.725s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.833 [32.000, 219.000], mean observation: 0.193 [0.000, 72.000], loss: 0.112421, mean_absolute_error: 0.314064, mean_q: 1.144223, mean_eps: 0.100000\n",
      "  19091/175000: episode: 535, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.089 [52.000, 220.000], mean observation: 0.312 [0.000, 90.000], loss: 0.197041, mean_absolute_error: 0.304219, mean_q: 1.095794, mean_eps: 0.100000\n",
      "  19114/175000: episode: 536, duration: 0.468s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 183.174 [182.000, 209.000], mean observation: 0.069 [0.000, 46.000], loss: 0.094665, mean_absolute_error: 0.288858, mean_q: 1.032939, mean_eps: 0.100000\n",
      "  19166/175000: episode: 537, duration: 1.017s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 143.519 [30.000, 190.000], mean observation: 0.447 [0.000, 104.000], loss: 0.172790, mean_absolute_error: 0.312838, mean_q: 1.156146, mean_eps: 0.100000\n",
      "  19201/175000: episode: 538, duration: 0.741s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.514 [66.000, 219.000], mean observation: 0.365 [0.000, 70.000], loss: 0.113396, mean_absolute_error: 0.321009, mean_q: 1.175625, mean_eps: 0.100000\n",
      "  19243/175000: episode: 539, duration: 0.833s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.738 [9.000, 214.000], mean observation: 0.427 [0.000, 84.000], loss: 0.142302, mean_absolute_error: 0.316601, mean_q: 1.084190, mean_eps: 0.100000\n",
      "  19285/175000: episode: 540, duration: 0.883s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 159.357 [79.000, 179.000], mean observation: 0.288 [0.000, 84.000], loss: 0.134527, mean_absolute_error: 0.343442, mean_q: 1.113266, mean_eps: 0.100000\n",
      "  19301/175000: episode: 541, duration: 0.303s, episode steps: 16, steps per second: 53, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 160.562 [58.000, 180.000], mean observation: 0.047 [0.000, 32.000], loss: 0.180859, mean_absolute_error: 0.351425, mean_q: 1.084539, mean_eps: 0.100000\n",
      "  19329/175000: episode: 542, duration: 0.549s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 161.536 [29.000, 180.000], mean observation: 0.130 [0.000, 56.000], loss: 0.177803, mean_absolute_error: 0.370709, mean_q: 1.137235, mean_eps: 0.100000\n",
      "  19351/175000: episode: 543, duration: 0.371s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 168.000 [68.000, 190.000], mean observation: 0.090 [0.000, 44.000], loss: 0.211563, mean_absolute_error: 0.351812, mean_q: 1.075164, mean_eps: 0.100000\n",
      "  19376/175000: episode: 544, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.160 [28.000, 219.000], mean observation: 0.085 [0.000, 50.000], loss: 0.114552, mean_absolute_error: 0.320592, mean_q: 0.996271, mean_eps: 0.100000\n",
      "  19414/175000: episode: 545, duration: 0.757s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 153.237 [30.000, 186.000], mean observation: 0.251 [0.000, 76.000], loss: 0.155652, mean_absolute_error: 0.320320, mean_q: 1.041898, mean_eps: 0.100000\n",
      "  19454/175000: episode: 546, duration: 0.830s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.675 [39.000, 195.000], mean observation: 0.175 [0.000, 80.000], loss: 0.142738, mean_absolute_error: 0.306934, mean_q: 1.089576, mean_eps: 0.100000\n",
      "  19484/175000: episode: 547, duration: 0.710s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 139.800 [1.000, 152.000], mean observation: 0.105 [0.000, 60.000], loss: 0.121293, mean_absolute_error: 0.296893, mean_q: 1.074113, mean_eps: 0.100000\n",
      "  19527/175000: episode: 548, duration: 0.882s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 153.884 [32.000, 180.000], mean observation: 0.333 [0.000, 86.000], loss: 0.185359, mean_absolute_error: 0.309172, mean_q: 1.046322, mean_eps: 0.100000\n",
      "  19565/175000: episode: 549, duration: 0.782s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 157.289 [49.000, 180.000], mean observation: 0.199 [0.000, 76.000], loss: 0.124425, mean_absolute_error: 0.325015, mean_q: 1.135434, mean_eps: 0.100000\n",
      "  19603/175000: episode: 550, duration: 0.793s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 39.868 [21.000, 186.000], mean observation: 0.142 [0.000, 76.000], loss: 0.199546, mean_absolute_error: 0.341448, mean_q: 1.218604, mean_eps: 0.100000\n",
      "  19638/175000: episode: 551, duration: 0.737s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.486 [27.000, 187.000], mean observation: 0.182 [0.000, 70.000], loss: 0.147979, mean_absolute_error: 0.350858, mean_q: 1.156299, mean_eps: 0.100000\n",
      "  19677/175000: episode: 552, duration: 0.800s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.744 [10.000, 181.000], mean observation: 0.283 [0.000, 78.000], loss: 0.126835, mean_absolute_error: 0.346023, mean_q: 1.198237, mean_eps: 0.100000\n",
      "  19717/175000: episode: 553, duration: 0.761s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 14.625 [0.000, 15.000], mean observation: 0.094 [0.000, 80.000], loss: 0.169899, mean_absolute_error: 0.331073, mean_q: 1.323743, mean_eps: 0.100000\n",
      "  19753/175000: episode: 554, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.611 [15.000, 192.000], mean observation: 0.175 [0.000, 72.000], loss: 0.158458, mean_absolute_error: 0.305042, mean_q: 1.236903, mean_eps: 0.100000\n",
      "  19791/175000: episode: 555, duration: 0.722s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.605 [33.000, 220.000], mean observation: 0.180 [0.000, 76.000], loss: 0.119252, mean_absolute_error: 0.298868, mean_q: 1.147386, mean_eps: 0.100000\n",
      "  19827/175000: episode: 556, duration: 0.694s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 129.778 [10.000, 190.000], mean observation: 0.255 [0.000, 72.000], loss: 0.169090, mean_absolute_error: 0.318937, mean_q: 1.074313, mean_eps: 0.100000\n",
      "  19857/175000: episode: 557, duration: 0.597s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 132.533 [3.000, 174.000], mean observation: 0.254 [0.000, 60.000], loss: 0.158841, mean_absolute_error: 0.339767, mean_q: 1.072212, mean_eps: 0.100000\n",
      "  19869/175000: episode: 558, duration: 0.207s, episode steps: 12, steps per second: 58, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 150.500 [52.000, 174.000], mean observation: 0.032 [0.000, 24.000], loss: 0.101202, mean_absolute_error: 0.370487, mean_q: 1.158828, mean_eps: 0.100000\n",
      "  19912/175000: episode: 559, duration: 0.867s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.535 [13.000, 186.000], mean observation: 0.320 [0.000, 86.000], loss: 0.145264, mean_absolute_error: 0.363356, mean_q: 1.142929, mean_eps: 0.100000\n",
      "  19965/175000: episode: 560, duration: 1.064s, episode steps: 53, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 138.302 [13.000, 200.000], mean observation: 0.411 [0.000, 106.000], loss: 0.137063, mean_absolute_error: 0.363346, mean_q: 1.085772, mean_eps: 0.100000\n",
      "  19997/175000: episode: 561, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 156.844 [28.000, 205.000], mean observation: 0.235 [0.000, 64.000], loss: 0.141280, mean_absolute_error: 0.358132, mean_q: 1.099644, mean_eps: 0.100000\n",
      "  20022/175000: episode: 562, duration: 0.525s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.400 [59.000, 190.000], mean observation: 0.114 [0.000, 50.000], loss: 0.484973, mean_absolute_error: 0.389654, mean_q: 1.230363, mean_eps: 0.100000\n",
      "  20066/175000: episode: 563, duration: 0.830s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.773 [24.000, 217.000], mean observation: 0.350 [0.000, 88.000], loss: 0.796608, mean_absolute_error: 0.377445, mean_q: 1.205326, mean_eps: 0.100000\n",
      "  20094/175000: episode: 564, duration: 0.575s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.821 [59.000, 217.000], mean observation: 0.084 [0.000, 56.000], loss: 0.403252, mean_absolute_error: 0.409924, mean_q: 1.231298, mean_eps: 0.100000\n",
      "  20150/175000: episode: 565, duration: 1.053s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 110.893 [30.000, 202.000], mean observation: 0.483 [0.000, 112.000], loss: 1.256676, mean_absolute_error: 0.419707, mean_q: 1.283814, mean_eps: 0.100000\n",
      "  20196/175000: episode: 566, duration: 0.897s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.217 [20.000, 211.000], mean observation: 0.547 [0.000, 92.000], loss: 0.547799, mean_absolute_error: 0.442016, mean_q: 1.466383, mean_eps: 0.100000\n",
      "  20243/175000: episode: 567, duration: 0.897s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 105.553 [68.000, 204.000], mean observation: 0.125 [0.000, 94.000], loss: 0.682771, mean_absolute_error: 0.466522, mean_q: 1.320236, mean_eps: 0.100000\n",
      "  20261/175000: episode: 568, duration: 0.361s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 111.167 [6.000, 216.000], mean observation: 0.125 [0.000, 36.000], loss: 0.230293, mean_absolute_error: 0.482938, mean_q: 1.349676, mean_eps: 0.100000\n",
      "  20287/175000: episode: 569, duration: 0.481s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.577 [102.000, 204.000], mean observation: 0.137 [0.000, 52.000], loss: 0.535498, mean_absolute_error: 0.470923, mean_q: 1.341312, mean_eps: 0.100000\n",
      "  20327/175000: episode: 570, duration: 0.792s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 114.750 [33.000, 188.000], mean observation: 0.433 [0.000, 80.000], loss: 0.221186, mean_absolute_error: 0.451663, mean_q: 1.158251, mean_eps: 0.100000\n",
      "  20360/175000: episode: 571, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.030 [5.000, 152.000], mean observation: 0.148 [0.000, 66.000], loss: 0.171391, mean_absolute_error: 0.445640, mean_q: 1.051225, mean_eps: 0.100000\n",
      "  20389/175000: episode: 572, duration: 0.619s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.276 [42.000, 186.000], mean observation: 0.204 [0.000, 58.000], loss: 0.295047, mean_absolute_error: 0.448362, mean_q: 1.089435, mean_eps: 0.100000\n",
      "  20435/175000: episode: 573, duration: 0.861s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 204.261 [0.000, 224.000], mean observation: 0.231 [0.000, 92.000], loss: 0.413560, mean_absolute_error: 0.408762, mean_q: 1.032491, mean_eps: 0.100000\n",
      "  20470/175000: episode: 574, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 177.514 [29.000, 224.000], mean observation: 0.148 [0.000, 70.000], loss: 0.520423, mean_absolute_error: 0.385483, mean_q: 1.086628, mean_eps: 0.100000\n",
      "  20522/175000: episode: 575, duration: 0.955s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 139.904 [49.000, 224.000], mean observation: 0.360 [0.000, 104.000], loss: 0.569729, mean_absolute_error: 0.364905, mean_q: 1.039691, mean_eps: 0.100000\n",
      "  20556/175000: episode: 576, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 170.088 [15.000, 190.000], mean observation: 0.178 [0.000, 68.000], loss: 0.325906, mean_absolute_error: 0.397597, mean_q: 1.064407, mean_eps: 0.100000\n",
      "  20583/175000: episode: 577, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 97.852 [5.000, 105.000], mean observation: 0.120 [0.000, 54.000], loss: 0.275511, mean_absolute_error: 0.425369, mean_q: 1.110518, mean_eps: 0.100000\n",
      "  20635/175000: episode: 578, duration: 1.026s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 182.904 [51.000, 213.000], mean observation: 0.414 [0.000, 104.000], loss: 0.345527, mean_absolute_error: 0.455340, mean_q: 1.282807, mean_eps: 0.100000\n",
      "  20676/175000: episode: 579, duration: 0.918s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.463 [10.000, 190.000], mean observation: 0.510 [0.000, 82.000], loss: 0.485769, mean_absolute_error: 0.439251, mean_q: 1.162117, mean_eps: 0.100000\n",
      "  20716/175000: episode: 580, duration: 0.888s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 49.400 [14.000, 201.000], mean observation: 0.217 [0.000, 80.000], loss: 0.426229, mean_absolute_error: 0.415428, mean_q: 1.171813, mean_eps: 0.100000\n",
      "  20754/175000: episode: 581, duration: 0.753s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 14.289 [9.000, 30.000], mean observation: 0.114 [0.000, 76.000], loss: 0.377616, mean_absolute_error: 0.385302, mean_q: 1.224453, mean_eps: 0.100000\n",
      "  20787/175000: episode: 582, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.242 [14.000, 174.000], mean observation: 0.106 [0.000, 66.000], loss: 0.507505, mean_absolute_error: 0.377726, mean_q: 1.226620, mean_eps: 0.100000\n",
      "  20829/175000: episode: 583, duration: 0.846s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.095 [45.000, 201.000], mean observation: 0.322 [0.000, 84.000], loss: 0.606239, mean_absolute_error: 0.390032, mean_q: 1.321740, mean_eps: 0.100000\n",
      "  20860/175000: episode: 584, duration: 0.621s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.194 [14.000, 180.000], mean observation: 0.209 [0.000, 62.000], loss: 0.323497, mean_absolute_error: 0.403663, mean_q: 1.460509, mean_eps: 0.100000\n",
      "  20902/175000: episode: 585, duration: 0.877s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.810 [14.000, 223.000], mean observation: 0.235 [0.000, 84.000], loss: 0.459446, mean_absolute_error: 0.378726, mean_q: 1.450110, mean_eps: 0.100000\n",
      "  20946/175000: episode: 586, duration: 0.855s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.432 [23.000, 207.000], mean observation: 0.367 [0.000, 88.000], loss: 0.438205, mean_absolute_error: 0.354180, mean_q: 1.407470, mean_eps: 0.100000\n",
      "  20983/175000: episode: 587, duration: 0.710s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 64.568 [25.000, 208.000], mean observation: 0.281 [0.000, 74.000], loss: 0.240642, mean_absolute_error: 0.313651, mean_q: 1.240058, mean_eps: 0.100000\n",
      "  21009/175000: episode: 588, duration: 0.538s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 81.692 [25.000, 128.000], mean observation: 0.130 [0.000, 52.000], loss: 0.888059, mean_absolute_error: 0.339164, mean_q: 1.306192, mean_eps: 0.100000\n",
      "  21048/175000: episode: 589, duration: 0.810s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.513 [35.000, 210.000], mean observation: 0.360 [0.000, 78.000], loss: 0.230465, mean_absolute_error: 0.340090, mean_q: 1.179454, mean_eps: 0.100000\n",
      "  21080/175000: episode: 590, duration: 0.651s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 87.125 [60.000, 177.000], mean observation: 0.105 [0.000, 64.000], loss: 0.975718, mean_absolute_error: 0.387479, mean_q: 1.257296, mean_eps: 0.100000\n",
      "  21118/175000: episode: 591, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.447 [48.000, 212.000], mean observation: 0.332 [0.000, 76.000], loss: 0.443194, mean_absolute_error: 0.415692, mean_q: 1.314206, mean_eps: 0.100000\n",
      "  21142/175000: episode: 592, duration: 0.491s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 97.167 [48.000, 218.000], mean observation: 0.119 [0.000, 48.000], loss: 0.189138, mean_absolute_error: 0.400654, mean_q: 1.310429, mean_eps: 0.100000\n",
      "  21171/175000: episode: 593, duration: 0.540s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.862 [60.000, 153.000], mean observation: 0.103 [0.000, 58.000], loss: 0.262912, mean_absolute_error: 0.402956, mean_q: 1.388345, mean_eps: 0.100000\n",
      "  21228/175000: episode: 594, duration: 1.063s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 76.421 [17.000, 153.000], mean observation: 0.303 [0.000, 114.000], loss: 0.207333, mean_absolute_error: 0.399977, mean_q: 1.336604, mean_eps: 0.100000\n",
      "  21258/175000: episode: 595, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 102.933 [59.000, 211.000], mean observation: 0.209 [0.000, 60.000], loss: 0.459236, mean_absolute_error: 0.400387, mean_q: 1.172754, mean_eps: 0.100000\n",
      "  21299/175000: episode: 596, duration: 0.779s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.341 [52.000, 216.000], mean observation: 0.266 [0.000, 82.000], loss: 0.360706, mean_absolute_error: 0.395003, mean_q: 0.969602, mean_eps: 0.100000\n",
      "  21325/175000: episode: 597, duration: 0.548s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 135.731 [24.000, 208.000], mean observation: 0.117 [0.000, 52.000], loss: 0.447566, mean_absolute_error: 0.391335, mean_q: 1.039810, mean_eps: 0.100000\n",
      "  21364/175000: episode: 598, duration: 0.779s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.487 [25.000, 208.000], mean observation: 0.221 [0.000, 78.000], loss: 0.875223, mean_absolute_error: 0.383894, mean_q: 1.147284, mean_eps: 0.100000\n",
      "  21403/175000: episode: 599, duration: 0.802s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.923 [4.000, 207.000], mean observation: 0.286 [0.000, 78.000], loss: 0.242931, mean_absolute_error: 0.368375, mean_q: 1.220497, mean_eps: 0.100000\n",
      "  21441/175000: episode: 600, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 160.158 [6.000, 224.000], mean observation: 0.191 [0.000, 76.000], loss: 0.257857, mean_absolute_error: 0.384534, mean_q: 1.241030, mean_eps: 0.100000\n",
      "  21474/175000: episode: 601, duration: 0.601s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.667 [48.000, 208.000], mean observation: 0.162 [0.000, 66.000], loss: 0.409988, mean_absolute_error: 0.410040, mean_q: 1.350171, mean_eps: 0.100000\n",
      "  21515/175000: episode: 602, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.098 [48.000, 208.000], mean observation: 0.219 [0.000, 82.000], loss: 0.452975, mean_absolute_error: 0.396367, mean_q: 1.244869, mean_eps: 0.100000\n",
      "  21544/175000: episode: 603, duration: 0.641s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 162.345 [33.000, 208.000], mean observation: 0.134 [0.000, 58.000], loss: 0.417653, mean_absolute_error: 0.417836, mean_q: 1.192603, mean_eps: 0.100000\n",
      "  21600/175000: episode: 604, duration: 1.070s, episode steps: 56, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.911 [2.000, 219.000], mean observation: 0.687 [0.000, 112.000], loss: 0.535392, mean_absolute_error: 0.426820, mean_q: 1.303113, mean_eps: 0.100000\n",
      "  21633/175000: episode: 605, duration: 0.686s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.364 [15.000, 143.000], mean observation: 0.240 [0.000, 66.000], loss: 0.450952, mean_absolute_error: 0.408675, mean_q: 1.344492, mean_eps: 0.100000\n",
      "  21672/175000: episode: 606, duration: 0.751s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.564 [8.000, 193.000], mean observation: 0.212 [0.000, 78.000], loss: 0.607925, mean_absolute_error: 0.422574, mean_q: 1.378331, mean_eps: 0.100000\n",
      "  21709/175000: episode: 607, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 92.162 [39.000, 192.000], mean observation: 0.249 [0.000, 74.000], loss: 0.447521, mean_absolute_error: 0.441036, mean_q: 1.433671, mean_eps: 0.100000\n",
      "  21768/175000: episode: 608, duration: 1.126s, episode steps: 59, steps per second: 52, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 104.729 [33.000, 174.000], mean observation: 0.442 [0.000, 118.000], loss: 0.571916, mean_absolute_error: 0.470216, mean_q: 1.474226, mean_eps: 0.100000\n",
      "  21799/175000: episode: 609, duration: 0.611s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 36.161 [28.000, 197.000], mean observation: 0.090 [0.000, 62.000], loss: 0.433814, mean_absolute_error: 0.476171, mean_q: 1.599402, mean_eps: 0.100000\n",
      "  21822/175000: episode: 610, duration: 0.451s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 43.348 [23.000, 165.000], mean observation: 0.114 [0.000, 46.000], loss: 0.147005, mean_absolute_error: 0.452227, mean_q: 1.669852, mean_eps: 0.100000\n",
      "  21846/175000: episode: 611, duration: 0.495s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 90.083 [28.000, 130.000], mean observation: 0.104 [0.000, 48.000], loss: 0.429337, mean_absolute_error: 0.429041, mean_q: 1.511641, mean_eps: 0.100000\n",
      "  21894/175000: episode: 612, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 80.771 [40.000, 173.000], mean observation: 0.359 [0.000, 96.000], loss: 0.330023, mean_absolute_error: 0.446167, mean_q: 1.580136, mean_eps: 0.100000\n",
      "  21931/175000: episode: 613, duration: 0.713s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 10.270 [0.000, 136.000], mean observation: 0.194 [0.000, 74.000], loss: 0.918728, mean_absolute_error: 0.445647, mean_q: 1.482793, mean_eps: 0.100000\n",
      "  21957/175000: episode: 614, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 35.808 [0.000, 107.000], mean observation: 0.088 [0.000, 52.000], loss: 0.565618, mean_absolute_error: 0.432978, mean_q: 1.529728, mean_eps: 0.100000\n",
      "  21994/175000: episode: 615, duration: 0.689s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.973 [62.000, 224.000], mean observation: 0.179 [0.000, 74.000], loss: 0.380017, mean_absolute_error: 0.416820, mean_q: 1.791916, mean_eps: 0.100000\n",
      "  22023/175000: episode: 616, duration: 0.556s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 145.276 [59.000, 224.000], mean observation: 0.244 [0.000, 58.000], loss: 0.220446, mean_absolute_error: 0.406393, mean_q: 1.912289, mean_eps: 0.100000\n",
      "  22069/175000: episode: 617, duration: 0.884s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.783 [19.000, 204.000], mean observation: 0.265 [0.000, 92.000], loss: 0.480076, mean_absolute_error: 0.396231, mean_q: 1.905486, mean_eps: 0.100000\n",
      "  22109/175000: episode: 618, duration: 0.783s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.575 [27.000, 215.000], mean observation: 0.269 [0.000, 80.000], loss: 0.550771, mean_absolute_error: 0.437605, mean_q: 1.920129, mean_eps: 0.100000\n",
      "  22125/175000: episode: 619, duration: 0.304s, episode steps: 16, steps per second: 53, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 207.500 [204.000, 212.000], mean observation: 0.042 [0.000, 32.000], loss: 0.623437, mean_absolute_error: 0.421966, mean_q: 1.663626, mean_eps: 0.100000\n",
      "  22152/175000: episode: 620, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 166.222 [73.000, 208.000], mean observation: 0.144 [0.000, 54.000], loss: 0.269192, mean_absolute_error: 0.459503, mean_q: 1.878976, mean_eps: 0.100000\n",
      "  22184/175000: episode: 621, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.094 [7.000, 205.000], mean observation: 0.142 [0.000, 64.000], loss: 0.371734, mean_absolute_error: 0.467203, mean_q: 1.694888, mean_eps: 0.100000\n",
      "  22218/175000: episode: 622, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 166.059 [105.000, 211.000], mean observation: 0.124 [0.000, 68.000], loss: 0.473874, mean_absolute_error: 0.477343, mean_q: 1.763392, mean_eps: 0.100000\n",
      "  22256/175000: episode: 623, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.974 [20.000, 196.000], mean observation: 0.208 [0.000, 76.000], loss: 0.574921, mean_absolute_error: 0.434471, mean_q: 1.723557, mean_eps: 0.100000\n",
      "  22284/175000: episode: 624, duration: 0.583s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.679 [36.000, 196.000], mean observation: 0.156 [0.000, 56.000], loss: 0.862660, mean_absolute_error: 0.468583, mean_q: 1.809558, mean_eps: 0.100000\n",
      "  22331/175000: episode: 625, duration: 0.926s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.213 [37.000, 206.000], mean observation: 0.257 [0.000, 94.000], loss: 0.395562, mean_absolute_error: 0.449925, mean_q: 1.819170, mean_eps: 0.100000\n",
      "  22386/175000: episode: 626, duration: 1.088s, episode steps: 55, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 73.982 [1.000, 199.000], mean observation: 0.556 [0.000, 110.000], loss: 0.442510, mean_absolute_error: 0.475562, mean_q: 1.946681, mean_eps: 0.100000\n",
      "  22423/175000: episode: 627, duration: 0.701s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 46.324 [0.000, 209.000], mean observation: 0.278 [0.000, 74.000], loss: 0.371247, mean_absolute_error: 0.452525, mean_q: 1.792516, mean_eps: 0.100000\n",
      "  22467/175000: episode: 628, duration: 0.881s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 132.114 [0.000, 219.000], mean observation: 0.424 [0.000, 88.000], loss: 0.525119, mean_absolute_error: 0.446273, mean_q: 1.685403, mean_eps: 0.100000\n",
      "  22529/175000: episode: 629, duration: 1.251s, episode steps: 62, steps per second: 50, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 127.726 [20.000, 183.000], mean observation: 0.501 [0.000, 124.000], loss: 0.385614, mean_absolute_error: 0.424395, mean_q: 1.669120, mean_eps: 0.100000\n",
      "  22564/175000: episode: 630, duration: 0.716s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.086 [9.000, 182.000], mean observation: 0.285 [0.000, 70.000], loss: 0.615434, mean_absolute_error: 0.437311, mean_q: 1.665091, mean_eps: 0.100000\n",
      "  22619/175000: episode: 631, duration: 1.098s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 173.964 [16.000, 210.000], mean observation: 0.485 [0.000, 110.000], loss: 0.422697, mean_absolute_error: 0.437926, mean_q: 1.589907, mean_eps: 0.100000\n",
      "  22657/175000: episode: 632, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 169.184 [16.000, 192.000], mean observation: 0.276 [0.000, 76.000], loss: 0.741246, mean_absolute_error: 0.395120, mean_q: 1.487657, mean_eps: 0.100000\n",
      "  22677/175000: episode: 633, duration: 0.372s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 146.800 [16.000, 192.000], mean observation: 0.114 [0.000, 40.000], loss: 0.469556, mean_absolute_error: 0.402320, mean_q: 1.487534, mean_eps: 0.100000\n",
      "  22716/175000: episode: 634, duration: 0.795s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 135.821 [8.000, 192.000], mean observation: 0.171 [0.000, 78.000], loss: 0.362959, mean_absolute_error: 0.419681, mean_q: 1.507318, mean_eps: 0.100000\n",
      "  22756/175000: episode: 635, duration: 0.777s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 117.050 [40.000, 209.000], mean observation: 0.396 [0.000, 80.000], loss: 0.224972, mean_absolute_error: 0.361955, mean_q: 1.299322, mean_eps: 0.100000\n",
      "  22806/175000: episode: 636, duration: 1.111s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.320 [16.000, 205.000], mean observation: 0.416 [0.000, 100.000], loss: 0.266739, mean_absolute_error: 0.366804, mean_q: 1.390389, mean_eps: 0.100000\n",
      "  22853/175000: episode: 637, duration: 0.895s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 151.383 [37.000, 190.000], mean observation: 0.552 [0.000, 94.000], loss: 0.249625, mean_absolute_error: 0.384017, mean_q: 1.398373, mean_eps: 0.100000\n",
      "  22888/175000: episode: 638, duration: 0.718s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 144.171 [9.000, 212.000], mean observation: 0.243 [0.000, 70.000], loss: 0.170874, mean_absolute_error: 0.407837, mean_q: 1.486632, mean_eps: 0.100000\n",
      "  22914/175000: episode: 639, duration: 0.542s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.846 [52.000, 170.000], mean observation: 0.187 [0.000, 52.000], loss: 0.277861, mean_absolute_error: 0.432907, mean_q: 1.571377, mean_eps: 0.100000\n",
      "  22953/175000: episode: 640, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 122.179 [31.000, 223.000], mean observation: 0.446 [0.000, 78.000], loss: 0.320528, mean_absolute_error: 0.396552, mean_q: 1.469466, mean_eps: 0.100000\n",
      "  22982/175000: episode: 641, duration: 0.515s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.690 [2.000, 190.000], mean observation: 0.285 [0.000, 58.000], loss: 0.236988, mean_absolute_error: 0.373993, mean_q: 1.435320, mean_eps: 0.100000\n",
      "  23019/175000: episode: 642, duration: 0.711s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 169.432 [35.000, 216.000], mean observation: 0.373 [0.000, 74.000], loss: 0.311329, mean_absolute_error: 0.393071, mean_q: 1.435217, mean_eps: 0.100000\n",
      "  23057/175000: episode: 643, duration: 0.768s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.263 [21.000, 190.000], mean observation: 0.206 [0.000, 76.000], loss: 0.289863, mean_absolute_error: 0.417026, mean_q: 1.436624, mean_eps: 0.100000\n",
      "  23091/175000: episode: 644, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.676 [33.000, 210.000], mean observation: 0.236 [0.000, 68.000], loss: 0.281457, mean_absolute_error: 0.418881, mean_q: 1.508680, mean_eps: 0.100000\n",
      "  23115/175000: episode: 645, duration: 0.461s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 141.125 [20.000, 214.000], mean observation: 0.133 [0.000, 48.000], loss: 0.365080, mean_absolute_error: 0.399666, mean_q: 1.458460, mean_eps: 0.100000\n",
      "  23159/175000: episode: 646, duration: 0.871s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 65.773 [10.000, 176.000], mean observation: 0.210 [0.000, 88.000], loss: 0.335930, mean_absolute_error: 0.376795, mean_q: 1.387793, mean_eps: 0.100000\n",
      "  23177/175000: episode: 647, duration: 0.409s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 29.000 [13.000, 70.000], mean observation: 0.084 [0.000, 36.000], loss: 0.717964, mean_absolute_error: 0.397880, mean_q: 1.498716, mean_eps: 0.100000\n",
      "  23196/175000: episode: 648, duration: 0.375s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 13.000 [13.000, 13.000], mean observation: 0.046 [0.000, 38.000], loss: 0.921456, mean_absolute_error: 0.360526, mean_q: 1.454061, mean_eps: 0.100000\n",
      "  23226/175000: episode: 649, duration: 0.659s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 26.567 [13.000, 90.000], mean observation: 0.121 [0.000, 60.000], loss: 0.448051, mean_absolute_error: 0.361924, mean_q: 1.466739, mean_eps: 0.100000\n",
      "  23258/175000: episode: 650, duration: 0.594s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 76.781 [20.000, 218.000], mean observation: 0.156 [0.000, 64.000], loss: 0.347328, mean_absolute_error: 0.361064, mean_q: 1.438558, mean_eps: 0.100000\n",
      "  23306/175000: episode: 651, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 80.854 [14.000, 185.000], mean observation: 0.265 [0.000, 96.000], loss: 0.358237, mean_absolute_error: 0.365075, mean_q: 1.410686, mean_eps: 0.100000\n",
      "  23344/175000: episode: 652, duration: 0.814s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.526 [5.000, 169.000], mean observation: 0.206 [0.000, 76.000], loss: 0.348252, mean_absolute_error: 0.342851, mean_q: 1.336875, mean_eps: 0.100000\n",
      "  23396/175000: episode: 653, duration: 1.044s, episode steps: 52, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.038 [3.000, 185.000], mean observation: 0.490 [0.000, 104.000], loss: 0.265476, mean_absolute_error: 0.346458, mean_q: 1.412484, mean_eps: 0.100000\n",
      "  23416/175000: episode: 654, duration: 0.449s, episode steps: 20, steps per second: 44, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 62.050 [9.000, 212.000], mean observation: 0.114 [0.000, 40.000], loss: 0.149247, mean_absolute_error: 0.383355, mean_q: 1.516785, mean_eps: 0.100000\n",
      "  23450/175000: episode: 655, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.588 [2.000, 169.000], mean observation: 0.202 [0.000, 68.000], loss: 0.308875, mean_absolute_error: 0.348430, mean_q: 1.402023, mean_eps: 0.100000\n",
      "  23493/175000: episode: 656, duration: 0.799s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.116 [0.000, 197.000], mean observation: 0.178 [0.000, 86.000], loss: 0.309837, mean_absolute_error: 0.312389, mean_q: 1.198591, mean_eps: 0.100000\n",
      "  23512/175000: episode: 657, duration: 0.372s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 106.579 [19.000, 169.000], mean observation: 0.102 [0.000, 38.000], loss: 0.191203, mean_absolute_error: 0.331081, mean_q: 1.240020, mean_eps: 0.100000\n",
      "  23539/175000: episode: 658, duration: 0.546s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 143.037 [25.000, 210.000], mean observation: 0.199 [0.000, 54.000], loss: 0.235708, mean_absolute_error: 0.315273, mean_q: 1.250275, mean_eps: 0.100000\n",
      "  23575/175000: episode: 659, duration: 0.932s, episode steps: 36, steps per second: 39, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 78.417 [12.000, 169.000], mean observation: 0.269 [0.000, 72.000], loss: 0.383871, mean_absolute_error: 0.337827, mean_q: 1.364031, mean_eps: 0.100000\n",
      "  23609/175000: episode: 660, duration: 0.795s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.176 [12.000, 195.000], mean observation: 0.218 [0.000, 68.000], loss: 0.646250, mean_absolute_error: 0.329813, mean_q: 1.313760, mean_eps: 0.100000\n",
      "  23631/175000: episode: 661, duration: 0.448s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 21.182 [11.000, 154.000], mean observation: 0.068 [0.000, 44.000], loss: 0.594997, mean_absolute_error: 0.404482, mean_q: 1.537105, mean_eps: 0.100000\n",
      "  23679/175000: episode: 662, duration: 1.255s, episode steps: 48, steps per second: 38, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 75.583 [0.000, 203.000], mean observation: 0.474 [0.000, 96.000], loss: 0.471179, mean_absolute_error: 0.408422, mean_q: 1.435437, mean_eps: 0.100000\n",
      "  23712/175000: episode: 663, duration: 0.822s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 59.242 [11.000, 216.000], mean observation: 0.172 [0.000, 66.000], loss: 0.293691, mean_absolute_error: 0.380680, mean_q: 1.407300, mean_eps: 0.100000\n",
      "  23761/175000: episode: 664, duration: 1.197s, episode steps: 49, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.449 [11.000, 222.000], mean observation: 0.452 [0.000, 98.000], loss: 0.351583, mean_absolute_error: 0.352898, mean_q: 1.490450, mean_eps: 0.100000\n",
      "  23786/175000: episode: 665, duration: 0.542s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 178.240 [11.000, 206.000], mean observation: 0.138 [0.000, 50.000], loss: 0.580906, mean_absolute_error: 0.368834, mean_q: 1.681643, mean_eps: 0.100000\n",
      "  23812/175000: episode: 666, duration: 0.638s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 191.038 [12.000, 224.000], mean observation: 0.098 [0.000, 52.000], loss: 0.391349, mean_absolute_error: 0.375804, mean_q: 1.629955, mean_eps: 0.100000\n",
      "  23852/175000: episode: 667, duration: 0.960s, episode steps: 40, steps per second: 42, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 167.750 [11.000, 206.000], mean observation: 0.258 [0.000, 80.000], loss: 0.380224, mean_absolute_error: 0.440202, mean_q: 1.801734, mean_eps: 0.100000\n",
      "  23897/175000: episode: 668, duration: 1.073s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.711 [4.000, 216.000], mean observation: 0.282 [0.000, 90.000], loss: 0.257958, mean_absolute_error: 0.468523, mean_q: 1.949751, mean_eps: 0.100000\n",
      "  23930/175000: episode: 669, duration: 0.712s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.485 [32.000, 223.000], mean observation: 0.177 [0.000, 66.000], loss: 0.710060, mean_absolute_error: 0.490726, mean_q: 2.016694, mean_eps: 0.100000\n",
      "  23968/175000: episode: 670, duration: 0.861s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.184 [85.000, 223.000], mean observation: 0.185 [0.000, 76.000], loss: 0.501321, mean_absolute_error: 0.487750, mean_q: 2.030464, mean_eps: 0.100000\n",
      "  23992/175000: episode: 671, duration: 0.665s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 85.083 [11.000, 223.000], mean observation: 0.168 [0.000, 48.000], loss: 0.597603, mean_absolute_error: 0.496760, mean_q: 1.964298, mean_eps: 0.100000\n",
      "  24012/175000: episode: 672, duration: 0.518s, episode steps: 20, steps per second: 39, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 25.950 [9.000, 114.000], mean observation: 0.059 [0.000, 40.000], loss: 0.238296, mean_absolute_error: 0.476814, mean_q: 1.792848, mean_eps: 0.100000\n",
      "  24061/175000: episode: 673, duration: 1.314s, episode steps: 49, steps per second: 37, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.980 [11.000, 212.000], mean observation: 0.591 [0.000, 98.000], loss: 0.418226, mean_absolute_error: 0.427675, mean_q: 1.607404, mean_eps: 0.100000\n",
      "  24086/175000: episode: 674, duration: 0.625s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 141.920 [12.000, 223.000], mean observation: 0.119 [0.000, 50.000], loss: 0.421663, mean_absolute_error: 0.408437, mean_q: 1.602616, mean_eps: 0.100000\n",
      "  24098/175000: episode: 675, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 160.500 [44.000, 223.000], mean observation: 0.056 [0.000, 24.000], loss: 0.541131, mean_absolute_error: 0.418387, mean_q: 1.591682, mean_eps: 0.100000\n",
      "  24136/175000: episode: 676, duration: 0.892s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.947 [11.000, 223.000], mean observation: 0.263 [0.000, 76.000], loss: 0.406184, mean_absolute_error: 0.465309, mean_q: 1.669629, mean_eps: 0.100000\n",
      "  24172/175000: episode: 677, duration: 0.849s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.139 [11.000, 224.000], mean observation: 0.222 [0.000, 72.000], loss: 0.252245, mean_absolute_error: 0.451554, mean_q: 1.437119, mean_eps: 0.100000\n",
      "  24197/175000: episode: 678, duration: 0.560s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 57.520 [19.000, 173.000], mean observation: 0.126 [0.000, 50.000], loss: 0.481245, mean_absolute_error: 0.440614, mean_q: 1.328013, mean_eps: 0.100000\n",
      "  24217/175000: episode: 679, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.650 [19.000, 151.000], mean observation: 0.087 [0.000, 40.000], loss: 0.899316, mean_absolute_error: 0.465290, mean_q: 1.397302, mean_eps: 0.100000\n",
      "  24266/175000: episode: 680, duration: 1.000s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 81.571 [11.000, 219.000], mean observation: 0.330 [0.000, 98.000], loss: 0.174724, mean_absolute_error: 0.453390, mean_q: 1.440912, mean_eps: 0.100000\n",
      "  24279/175000: episode: 681, duration: 0.295s, episode steps: 13, steps per second: 44, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 55.231 [20.000, 104.000], mean observation: 0.053 [0.000, 26.000], loss: 0.191059, mean_absolute_error: 0.482481, mean_q: 1.530111, mean_eps: 0.100000\n",
      "  24313/175000: episode: 682, duration: 0.731s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 57.882 [11.000, 215.000], mean observation: 0.173 [0.000, 68.000], loss: 0.211238, mean_absolute_error: 0.484921, mean_q: 1.480250, mean_eps: 0.100000\n",
      "  24333/175000: episode: 683, duration: 0.465s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 155.900 [47.000, 168.000], mean observation: 0.049 [0.000, 40.000], loss: 0.245451, mean_absolute_error: 0.488000, mean_q: 1.393254, mean_eps: 0.100000\n",
      "  24375/175000: episode: 684, duration: 0.877s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.952 [9.000, 223.000], mean observation: 0.241 [0.000, 84.000], loss: 0.223693, mean_absolute_error: 0.472540, mean_q: 1.419042, mean_eps: 0.100000\n",
      "  24412/175000: episode: 685, duration: 0.875s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 66.595 [19.000, 223.000], mean observation: 0.253 [0.000, 74.000], loss: 0.422503, mean_absolute_error: 0.467006, mean_q: 1.456913, mean_eps: 0.100000\n",
      "  24434/175000: episode: 686, duration: 0.596s, episode steps: 22, steps per second: 37, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 83.455 [19.000, 206.000], mean observation: 0.148 [0.000, 44.000], loss: 0.196796, mean_absolute_error: 0.470335, mean_q: 1.561531, mean_eps: 0.100000\n",
      "  24484/175000: episode: 687, duration: 1.160s, episode steps: 50, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.380 [19.000, 172.000], mean observation: 0.364 [0.000, 100.000], loss: 0.282228, mean_absolute_error: 0.429952, mean_q: 1.538232, mean_eps: 0.100000\n",
      "  24505/175000: episode: 688, duration: 0.460s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 66.190 [14.000, 220.000], mean observation: 0.149 [0.000, 42.000], loss: 0.218127, mean_absolute_error: 0.432370, mean_q: 1.497042, mean_eps: 0.100000\n",
      "  24518/175000: episode: 689, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.033 [0.000, 26.000], loss: 0.280896, mean_absolute_error: 0.404321, mean_q: 1.415525, mean_eps: 0.100000\n",
      "  24558/175000: episode: 690, duration: 0.837s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 35.925 [4.000, 148.000], mean observation: 0.284 [0.000, 80.000], loss: 0.220993, mean_absolute_error: 0.399693, mean_q: 1.438860, mean_eps: 0.100000\n",
      "  24586/175000: episode: 691, duration: 0.656s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.536 [14.000, 204.000], mean observation: 0.195 [0.000, 56.000], loss: 0.384621, mean_absolute_error: 0.405447, mean_q: 1.389456, mean_eps: 0.100000\n",
      "  24619/175000: episode: 692, duration: 0.695s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 92.758 [14.000, 191.000], mean observation: 0.188 [0.000, 66.000], loss: 0.186364, mean_absolute_error: 0.402187, mean_q: 1.367594, mean_eps: 0.100000\n",
      "  24654/175000: episode: 693, duration: 0.764s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.743 [14.000, 193.000], mean observation: 0.271 [0.000, 70.000], loss: 0.230594, mean_absolute_error: 0.384593, mean_q: 1.337357, mean_eps: 0.100000\n",
      "  24681/175000: episode: 694, duration: 0.594s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 140.667 [14.000, 199.000], mean observation: 0.205 [0.000, 54.000], loss: 0.478016, mean_absolute_error: 0.381451, mean_q: 1.380106, mean_eps: 0.100000\n",
      "  24725/175000: episode: 695, duration: 0.947s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 154.886 [22.000, 200.000], mean observation: 0.357 [0.000, 88.000], loss: 0.350057, mean_absolute_error: 0.360874, mean_q: 1.263461, mean_eps: 0.100000\n",
      "  24753/175000: episode: 696, duration: 0.627s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 155.321 [89.000, 212.000], mean observation: 0.100 [0.000, 56.000], loss: 0.174780, mean_absolute_error: 0.379292, mean_q: 1.332038, mean_eps: 0.100000\n",
      "  24778/175000: episode: 697, duration: 0.529s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 30.640 [23.000, 214.000], mean observation: 0.059 [0.000, 50.000], loss: 0.187197, mean_absolute_error: 0.397134, mean_q: 1.446177, mean_eps: 0.100000\n",
      "  24810/175000: episode: 698, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 82.125 [7.000, 211.000], mean observation: 0.150 [0.000, 64.000], loss: 0.305081, mean_absolute_error: 0.395136, mean_q: 1.381528, mean_eps: 0.100000\n",
      "  24846/175000: episode: 699, duration: 0.814s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 167.750 [18.000, 209.000], mean observation: 0.119 [0.000, 72.000], loss: 0.252311, mean_absolute_error: 0.408383, mean_q: 1.566531, mean_eps: 0.100000\n",
      "  24869/175000: episode: 700, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 185.087 [31.000, 200.000], mean observation: 0.056 [0.000, 46.000], loss: 0.597047, mean_absolute_error: 0.372359, mean_q: 1.697574, mean_eps: 0.100000\n",
      "  24889/175000: episode: 701, duration: 0.412s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 190.900 [105.000, 200.000], mean observation: 0.056 [0.000, 40.000], loss: 0.256740, mean_absolute_error: 0.342436, mean_q: 1.685908, mean_eps: 0.100000\n",
      "  24945/175000: episode: 702, duration: 1.214s, episode steps: 56, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 127.125 [19.000, 208.000], mean observation: 0.318 [0.000, 112.000], loss: 0.578487, mean_absolute_error: 0.334905, mean_q: 1.449527, mean_eps: 0.100000\n",
      "  25000/175000: episode: 703, duration: 1.220s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 138.291 [14.000, 194.000], mean observation: 0.364 [0.000, 110.000], loss: 0.251286, mean_absolute_error: 0.343068, mean_q: 1.296125, mean_eps: 0.100000\n",
      "  25017/175000: episode: 704, duration: 0.418s, episode steps: 17, steps per second: 41, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 72.000 [72.000, 72.000], mean observation: 0.041 [0.000, 34.000], loss: 0.203012, mean_absolute_error: 0.329148, mean_q: 1.210918, mean_eps: 0.100000\n",
      "  25063/175000: episode: 705, duration: 0.989s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.087 [39.000, 180.000], mean observation: 0.202 [0.000, 92.000], loss: 0.402508, mean_absolute_error: 0.367660, mean_q: 1.363147, mean_eps: 0.100000\n",
      "  25083/175000: episode: 706, duration: 0.464s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.700 [19.000, 203.000], mean observation: 0.064 [0.000, 40.000], loss: 0.234333, mean_absolute_error: 0.390657, mean_q: 1.398278, mean_eps: 0.100000\n",
      "  25107/175000: episode: 707, duration: 0.507s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 39.125 [19.000, 201.000], mean observation: 0.077 [0.000, 48.000], loss: 0.235138, mean_absolute_error: 0.412951, mean_q: 1.395722, mean_eps: 0.100000\n",
      "  25134/175000: episode: 708, duration: 0.611s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 53.889 [19.000, 207.000], mean observation: 0.098 [0.000, 54.000], loss: 0.186788, mean_absolute_error: 0.435621, mean_q: 1.384673, mean_eps: 0.100000\n",
      "  25170/175000: episode: 709, duration: 0.708s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.389 [17.000, 205.000], mean observation: 0.231 [0.000, 72.000], loss: 0.526575, mean_absolute_error: 0.423536, mean_q: 1.408473, mean_eps: 0.100000\n",
      "  25200/175000: episode: 710, duration: 0.612s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 101.067 [16.000, 169.000], mean observation: 0.137 [0.000, 60.000], loss: 0.427427, mean_absolute_error: 0.399760, mean_q: 1.480171, mean_eps: 0.100000\n",
      "  25230/175000: episode: 711, duration: 0.670s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 87.500 [18.000, 169.000], mean observation: 0.125 [0.000, 60.000], loss: 0.292181, mean_absolute_error: 0.394709, mean_q: 1.524953, mean_eps: 0.100000\n",
      "  25263/175000: episode: 712, duration: 0.654s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.077 [0.000, 66.000], loss: 0.171524, mean_absolute_error: 0.392568, mean_q: 1.496368, mean_eps: 0.100000\n",
      "  25305/175000: episode: 713, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.429 [12.000, 207.000], mean observation: 0.284 [0.000, 84.000], loss: 0.267579, mean_absolute_error: 0.394722, mean_q: 1.516645, mean_eps: 0.100000\n",
      "  25330/175000: episode: 714, duration: 0.492s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 181.720 [81.000, 207.000], mean observation: 0.106 [0.000, 50.000], loss: 0.197217, mean_absolute_error: 0.385332, mean_q: 1.404362, mean_eps: 0.100000\n",
      "  25368/175000: episode: 715, duration: 0.808s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.711 [30.000, 212.000], mean observation: 0.251 [0.000, 76.000], loss: 0.198019, mean_absolute_error: 0.386029, mean_q: 1.406273, mean_eps: 0.100000\n",
      "  25421/175000: episode: 716, duration: 1.028s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 190.547 [27.000, 221.000], mean observation: 0.274 [0.000, 106.000], loss: 0.541138, mean_absolute_error: 0.404848, mean_q: 1.309638, mean_eps: 0.100000\n",
      "  25480/175000: episode: 717, duration: 1.152s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 120.644 [10.000, 212.000], mean observation: 0.495 [0.000, 118.000], loss: 0.311784, mean_absolute_error: 0.454987, mean_q: 1.465623, mean_eps: 0.100000\n",
      "  25522/175000: episode: 718, duration: 0.890s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.476 [18.000, 194.000], mean observation: 0.181 [0.000, 84.000], loss: 0.191289, mean_absolute_error: 0.434200, mean_q: 1.456274, mean_eps: 0.100000\n",
      "  25548/175000: episode: 719, duration: 0.561s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 138.654 [54.000, 155.000], mean observation: 0.153 [0.000, 52.000], loss: 0.731210, mean_absolute_error: 0.403374, mean_q: 1.338929, mean_eps: 0.100000\n",
      "  25581/175000: episode: 720, duration: 0.809s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 134.576 [3.000, 219.000], mean observation: 0.206 [0.000, 66.000], loss: 0.405519, mean_absolute_error: 0.394711, mean_q: 1.368649, mean_eps: 0.100000\n",
      "  25596/175000: episode: 721, duration: 0.315s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 202.267 [62.000, 219.000], mean observation: 0.060 [0.000, 30.000], loss: 0.568499, mean_absolute_error: 0.423454, mean_q: 1.438507, mean_eps: 0.100000\n",
      "  25637/175000: episode: 722, duration: 0.846s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 30.951 [3.000, 219.000], mean observation: 0.228 [0.000, 82.000], loss: 0.402368, mean_absolute_error: 0.375396, mean_q: 1.359935, mean_eps: 0.100000\n",
      "  25676/175000: episode: 723, duration: 0.785s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 18.436 [5.000, 207.000], mean observation: 0.107 [0.000, 78.000], loss: 0.499785, mean_absolute_error: 0.383111, mean_q: 1.383855, mean_eps: 0.100000\n",
      "  25703/175000: episode: 724, duration: 0.608s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 39.296 [5.000, 205.000], mean observation: 0.188 [0.000, 54.000], loss: 0.493845, mean_absolute_error: 0.409125, mean_q: 1.498651, mean_eps: 0.100000\n",
      "  25731/175000: episode: 725, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.036 [5.000, 163.000], mean observation: 0.218 [0.000, 56.000], loss: 0.248367, mean_absolute_error: 0.390090, mean_q: 1.412042, mean_eps: 0.100000\n",
      "  25763/175000: episode: 726, duration: 0.690s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.156 [5.000, 219.000], mean observation: 0.199 [0.000, 64.000], loss: 0.224624, mean_absolute_error: 0.373341, mean_q: 1.537120, mean_eps: 0.100000\n",
      "  25798/175000: episode: 727, duration: 0.873s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.886 [5.000, 219.000], mean observation: 0.226 [0.000, 70.000], loss: 0.771946, mean_absolute_error: 0.376820, mean_q: 1.789783, mean_eps: 0.100000\n",
      "  25829/175000: episode: 728, duration: 0.672s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 199.484 [4.000, 219.000], mean observation: 0.155 [0.000, 62.000], loss: 0.271015, mean_absolute_error: 0.422088, mean_q: 1.922838, mean_eps: 0.100000\n",
      "  25861/175000: episode: 729, duration: 0.672s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 108.656 [5.000, 212.000], mean observation: 0.263 [0.000, 64.000], loss: 0.190757, mean_absolute_error: 0.418540, mean_q: 1.828934, mean_eps: 0.100000\n",
      "  25889/175000: episode: 730, duration: 0.568s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 138.036 [17.000, 212.000], mean observation: 0.166 [0.000, 56.000], loss: 0.486575, mean_absolute_error: 0.405487, mean_q: 1.710166, mean_eps: 0.100000\n",
      "  25944/175000: episode: 731, duration: 1.135s, episode steps: 55, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 114.673 [18.000, 212.000], mean observation: 0.609 [0.000, 110.000], loss: 0.381860, mean_absolute_error: 0.391464, mean_q: 1.564860, mean_eps: 0.100000\n",
      "  25955/175000: episode: 732, duration: 0.219s, episode steps: 11, steps per second: 50, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 34.091 [19.000, 137.000], mean observation: 0.058 [0.000, 22.000], loss: 1.580403, mean_absolute_error: 0.408931, mean_q: 1.617887, mean_eps: 0.100000\n",
      "  25984/175000: episode: 733, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.724 [19.000, 212.000], mean observation: 0.124 [0.000, 58.000], loss: 0.554869, mean_absolute_error: 0.405194, mean_q: 1.687614, mean_eps: 0.100000\n",
      "  26010/175000: episode: 734, duration: 0.530s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 104.962 [84.000, 146.000], mean observation: 0.073 [0.000, 52.000], loss: 0.520151, mean_absolute_error: 0.433734, mean_q: 1.697924, mean_eps: 0.100000\n",
      "  26034/175000: episode: 735, duration: 0.478s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.375 [19.000, 215.000], mean observation: 0.110 [0.000, 48.000], loss: 0.287558, mean_absolute_error: 0.459014, mean_q: 1.762546, mean_eps: 0.100000\n",
      "  26067/175000: episode: 736, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.515 [19.000, 214.000], mean observation: 0.322 [0.000, 66.000], loss: 0.553270, mean_absolute_error: 0.475222, mean_q: 1.800057, mean_eps: 0.100000\n",
      "  26099/175000: episode: 737, duration: 0.700s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 192.750 [19.000, 218.000], mean observation: 0.110 [0.000, 64.000], loss: 0.423570, mean_absolute_error: 0.461263, mean_q: 1.750527, mean_eps: 0.100000\n",
      "  26128/175000: episode: 738, duration: 0.656s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 40.759 [19.000, 222.000], mean observation: 0.128 [0.000, 58.000], loss: 0.318809, mean_absolute_error: 0.433914, mean_q: 1.749749, mean_eps: 0.100000\n",
      "  26171/175000: episode: 739, duration: 0.900s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 55.465 [19.000, 195.000], mean observation: 0.244 [0.000, 86.000], loss: 0.363213, mean_absolute_error: 0.422818, mean_q: 1.873171, mean_eps: 0.100000\n",
      "  26194/175000: episode: 740, duration: 0.492s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 27.957 [19.000, 150.000], mean observation: 0.091 [0.000, 46.000], loss: 0.453179, mean_absolute_error: 0.452480, mean_q: 1.966853, mean_eps: 0.100000\n",
      "  26228/175000: episode: 741, duration: 0.702s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.079 [0.000, 68.000], loss: 0.722679, mean_absolute_error: 0.439658, mean_q: 1.897936, mean_eps: 0.100000\n",
      "  26266/175000: episode: 742, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.947 [19.000, 194.000], mean observation: 0.283 [0.000, 76.000], loss: 0.248715, mean_absolute_error: 0.415683, mean_q: 1.825805, mean_eps: 0.100000\n",
      "  26300/175000: episode: 743, duration: 0.846s, episode steps: 34, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.618 [19.000, 189.000], mean observation: 0.205 [0.000, 68.000], loss: 0.672926, mean_absolute_error: 0.435726, mean_q: 2.015008, mean_eps: 0.100000\n",
      "  26317/175000: episode: 744, duration: 0.462s, episode steps: 17, steps per second: 37, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 102.235 [19.000, 187.000], mean observation: 0.085 [0.000, 34.000], loss: 0.337385, mean_absolute_error: 0.422674, mean_q: 2.033480, mean_eps: 0.100000\n",
      "  26359/175000: episode: 745, duration: 0.807s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.000 [19.000, 199.000], mean observation: 0.214 [0.000, 84.000], loss: 0.954233, mean_absolute_error: 0.434230, mean_q: 1.884501, mean_eps: 0.100000\n",
      "  26385/175000: episode: 746, duration: 0.552s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 90.692 [12.000, 187.000], mean observation: 0.097 [0.000, 52.000], loss: 0.174857, mean_absolute_error: 0.449357, mean_q: 1.870460, mean_eps: 0.100000\n",
      "  26423/175000: episode: 747, duration: 0.738s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.105 [19.000, 169.000], mean observation: 0.274 [0.000, 76.000], loss: 0.369142, mean_absolute_error: 0.474692, mean_q: 1.960413, mean_eps: 0.100000\n",
      "  26468/175000: episode: 748, duration: 0.964s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 74.556 [19.000, 212.000], mean observation: 0.446 [0.000, 90.000], loss: 0.516468, mean_absolute_error: 0.432290, mean_q: 1.828158, mean_eps: 0.100000\n",
      "  26492/175000: episode: 749, duration: 0.625s, episode steps: 24, steps per second: 38, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 103.500 [19.000, 188.000], mean observation: 0.067 [0.000, 48.000], loss: 0.365045, mean_absolute_error: 0.446639, mean_q: 1.866360, mean_eps: 0.100000\n",
      "  26518/175000: episode: 750, duration: 0.550s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 149.038 [6.000, 188.000], mean observation: 0.159 [0.000, 52.000], loss: 0.298891, mean_absolute_error: 0.413987, mean_q: 1.788586, mean_eps: 0.100000\n",
      "  26571/175000: episode: 751, duration: 0.995s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.434 [6.000, 212.000], mean observation: 0.422 [0.000, 106.000], loss: 0.607993, mean_absolute_error: 0.412257, mean_q: 1.787170, mean_eps: 0.100000\n",
      "  26606/175000: episode: 752, duration: 0.626s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 50.257 [19.000, 212.000], mean observation: 0.092 [0.000, 70.000], loss: 0.456203, mean_absolute_error: 0.455335, mean_q: 1.868152, mean_eps: 0.100000\n",
      "  26647/175000: episode: 753, duration: 0.711s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.561 [23.000, 215.000], mean observation: 0.344 [0.000, 82.000], loss: 0.367948, mean_absolute_error: 0.440221, mean_q: 1.861535, mean_eps: 0.100000\n",
      "  26680/175000: episode: 754, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.303 [21.000, 223.000], mean observation: 0.175 [0.000, 66.000], loss: 0.904333, mean_absolute_error: 0.413404, mean_q: 1.733468, mean_eps: 0.100000\n",
      "  26720/175000: episode: 755, duration: 0.799s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 94.475 [8.000, 219.000], mean observation: 0.288 [0.000, 80.000], loss: 0.410787, mean_absolute_error: 0.448848, mean_q: 1.871378, mean_eps: 0.100000\n",
      "  26759/175000: episode: 756, duration: 0.791s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.410 [21.000, 191.000], mean observation: 0.357 [0.000, 78.000], loss: 0.721748, mean_absolute_error: 0.453972, mean_q: 1.712246, mean_eps: 0.100000\n",
      "  26800/175000: episode: 757, duration: 0.895s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.585 [12.000, 200.000], mean observation: 0.309 [0.000, 82.000], loss: 0.811795, mean_absolute_error: 0.455711, mean_q: 1.862847, mean_eps: 0.100000\n",
      "  26833/175000: episode: 758, duration: 0.641s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.667 [6.000, 200.000], mean observation: 0.140 [0.000, 66.000], loss: 0.694646, mean_absolute_error: 0.461086, mean_q: 1.804151, mean_eps: 0.100000\n",
      "  26844/175000: episode: 759, duration: 0.221s, episode steps: 11, steps per second: 50, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 82.727 [28.000, 158.000], mean observation: 0.048 [0.000, 22.000], loss: 0.203105, mean_absolute_error: 0.456788, mean_q: 1.904118, mean_eps: 0.100000\n",
      "  26859/175000: episode: 760, duration: 0.275s, episode steps: 15, steps per second: 54, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 35.000 [28.000, 133.000], mean observation: 0.049 [0.000, 30.000], loss: 0.194441, mean_absolute_error: 0.436012, mean_q: 1.903095, mean_eps: 0.100000\n",
      "  26906/175000: episode: 761, duration: 0.903s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 113.745 [7.000, 188.000], mean observation: 0.356 [0.000, 94.000], loss: 0.357942, mean_absolute_error: 0.436784, mean_q: 1.931246, mean_eps: 0.100000\n",
      "  26967/175000: episode: 762, duration: 1.207s, episode steps: 61, steps per second: 51, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 92.066 [17.000, 219.000], mean observation: 0.399 [0.000, 122.000], loss: 0.804423, mean_absolute_error: 0.429034, mean_q: 1.834657, mean_eps: 0.100000\n",
      "  26998/175000: episode: 763, duration: 0.682s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 120.581 [1.000, 216.000], mean observation: 0.162 [0.000, 62.000], loss: 1.243092, mean_absolute_error: 0.452719, mean_q: 1.822053, mean_eps: 0.100000\n",
      "  27033/175000: episode: 764, duration: 0.622s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 86.543 [41.000, 216.000], mean observation: 0.137 [0.000, 70.000], loss: 0.587506, mean_absolute_error: 0.472287, mean_q: 1.903476, mean_eps: 0.100000\n",
      "  27091/175000: episode: 765, duration: 1.060s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 85.793 [30.000, 202.000], mean observation: 0.330 [0.000, 116.000], loss: 0.984262, mean_absolute_error: 0.480290, mean_q: 1.832969, mean_eps: 0.100000\n",
      "  27122/175000: episode: 766, duration: 0.678s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 175.548 [60.000, 204.000], mean observation: 0.144 [0.000, 62.000], loss: 0.214343, mean_absolute_error: 0.487537, mean_q: 1.921915, mean_eps: 0.100000\n",
      "  27149/175000: episode: 767, duration: 0.482s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.185 [21.000, 217.000], mean observation: 0.157 [0.000, 54.000], loss: 0.208474, mean_absolute_error: 0.461008, mean_q: 1.782471, mean_eps: 0.100000\n",
      "  27192/175000: episode: 768, duration: 0.923s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.744 [14.000, 195.000], mean observation: 0.318 [0.000, 86.000], loss: 0.300064, mean_absolute_error: 0.501673, mean_q: 1.844130, mean_eps: 0.100000\n",
      "  27216/175000: episode: 769, duration: 0.573s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 99.250 [16.000, 185.000], mean observation: 0.086 [0.000, 48.000], loss: 0.421507, mean_absolute_error: 0.472822, mean_q: 1.665245, mean_eps: 0.100000\n",
      "  27245/175000: episode: 770, duration: 0.618s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 72.483 [58.000, 73.000], mean observation: 0.071 [0.000, 58.000], loss: 0.232216, mean_absolute_error: 0.449423, mean_q: 1.672564, mean_eps: 0.100000\n",
      "  27281/175000: episode: 771, duration: 0.624s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 89.917 [16.000, 212.000], mean observation: 0.176 [0.000, 72.000], loss: 0.605964, mean_absolute_error: 0.403435, mean_q: 1.623740, mean_eps: 0.100000\n",
      "  27316/175000: episode: 772, duration: 0.680s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.829 [16.000, 188.000], mean observation: 0.161 [0.000, 70.000], loss: 0.463476, mean_absolute_error: 0.423058, mean_q: 1.823041, mean_eps: 0.100000\n",
      "  27345/175000: episode: 773, duration: 0.621s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 91.586 [16.000, 192.000], mean observation: 0.130 [0.000, 58.000], loss: 0.297276, mean_absolute_error: 0.392309, mean_q: 1.648769, mean_eps: 0.100000\n",
      "  27399/175000: episode: 774, duration: 0.971s, episode steps: 54, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 64.185 [16.000, 219.000], mean observation: 0.401 [0.000, 108.000], loss: 0.462512, mean_absolute_error: 0.401907, mean_q: 1.735920, mean_eps: 0.100000\n",
      "  27428/175000: episode: 775, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 91.207 [7.000, 184.000], mean observation: 0.150 [0.000, 58.000], loss: 0.353451, mean_absolute_error: 0.441213, mean_q: 1.749858, mean_eps: 0.100000\n",
      "  27458/175000: episode: 776, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 88.500 [15.000, 188.000], mean observation: 0.135 [0.000, 60.000], loss: 0.667952, mean_absolute_error: 0.490263, mean_q: 1.900130, mean_eps: 0.100000\n",
      "  27491/175000: episode: 777, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 100.091 [6.000, 187.000], mean observation: 0.242 [0.000, 66.000], loss: 0.182115, mean_absolute_error: 0.476325, mean_q: 1.805757, mean_eps: 0.100000\n",
      "  27514/175000: episode: 778, duration: 0.466s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 81.696 [16.000, 219.000], mean observation: 0.120 [0.000, 46.000], loss: 0.711968, mean_absolute_error: 0.480851, mean_q: 1.841685, mean_eps: 0.100000\n",
      "  27545/175000: episode: 779, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.742 [16.000, 197.000], mean observation: 0.170 [0.000, 62.000], loss: 0.458415, mean_absolute_error: 0.457626, mean_q: 1.804346, mean_eps: 0.100000\n",
      "  27589/175000: episode: 780, duration: 0.785s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 47.114 [13.000, 219.000], mean observation: 0.272 [0.000, 88.000], loss: 0.382906, mean_absolute_error: 0.422184, mean_q: 1.688167, mean_eps: 0.100000\n",
      "  27601/175000: episode: 781, duration: 0.212s, episode steps: 12, steps per second: 57, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 76.000 [76.000, 76.000], mean observation: 0.030 [0.000, 24.000], loss: 0.462540, mean_absolute_error: 0.400663, mean_q: 1.636298, mean_eps: 0.100000\n",
      "  27618/175000: episode: 782, duration: 0.326s, episode steps: 17, steps per second: 52, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 73.235 [16.000, 164.000], mean observation: 0.113 [0.000, 34.000], loss: 0.125811, mean_absolute_error: 0.399348, mean_q: 1.686995, mean_eps: 0.100000\n",
      "  27664/175000: episode: 783, duration: 0.838s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 55.826 [15.000, 220.000], mean observation: 0.207 [0.000, 92.000], loss: 0.426251, mean_absolute_error: 0.409097, mean_q: 1.899676, mean_eps: 0.100000\n",
      "  27688/175000: episode: 784, duration: 0.510s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 88.500 [21.000, 190.000], mean observation: 0.102 [0.000, 48.000], loss: 0.508708, mean_absolute_error: 0.421950, mean_q: 2.037261, mean_eps: 0.100000\n",
      "  27708/175000: episode: 785, duration: 0.421s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 103.300 [14.000, 140.000], mean observation: 0.088 [0.000, 40.000], loss: 0.966994, mean_absolute_error: 0.405778, mean_q: 1.897666, mean_eps: 0.100000\n",
      "  27727/175000: episode: 786, duration: 0.345s, episode steps: 19, steps per second: 55, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.211 [15.000, 221.000], mean observation: 0.113 [0.000, 38.000], loss: 0.369698, mean_absolute_error: 0.413921, mean_q: 1.924074, mean_eps: 0.100000\n",
      "  27762/175000: episode: 787, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.686 [19.000, 223.000], mean observation: 0.310 [0.000, 70.000], loss: 0.663959, mean_absolute_error: 0.403670, mean_q: 1.864919, mean_eps: 0.100000\n",
      "  27800/175000: episode: 788, duration: 0.762s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 122.974 [65.000, 222.000], mean observation: 0.319 [0.000, 76.000], loss: 0.444630, mean_absolute_error: 0.444301, mean_q: 1.932309, mean_eps: 0.100000\n",
      "  27829/175000: episode: 789, duration: 0.606s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 129.931 [51.000, 164.000], mean observation: 0.111 [0.000, 58.000], loss: 0.166682, mean_absolute_error: 0.470252, mean_q: 1.952647, mean_eps: 0.100000\n",
      "  27859/175000: episode: 790, duration: 0.603s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 117.400 [47.000, 135.000], mean observation: 0.073 [0.000, 60.000], loss: 0.101451, mean_absolute_error: 0.502647, mean_q: 1.874369, mean_eps: 0.100000\n",
      "  27910/175000: episode: 791, duration: 1.096s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 100.824 [21.000, 170.000], mean observation: 0.577 [0.000, 102.000], loss: 0.832561, mean_absolute_error: 0.551477, mean_q: 1.972203, mean_eps: 0.100000\n",
      "  27946/175000: episode: 792, duration: 0.644s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.556 [15.000, 193.000], mean observation: 0.212 [0.000, 72.000], loss: 0.581370, mean_absolute_error: 0.512133, mean_q: 1.864611, mean_eps: 0.100000\n",
      "  27982/175000: episode: 793, duration: 0.802s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 99.361 [45.000, 222.000], mean observation: 0.120 [0.000, 72.000], loss: 0.748203, mean_absolute_error: 0.470354, mean_q: 1.849466, mean_eps: 0.100000\n",
      "  28020/175000: episode: 794, duration: 0.917s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.368 [19.000, 197.000], mean observation: 0.218 [0.000, 76.000], loss: 0.318727, mean_absolute_error: 0.449134, mean_q: 1.822521, mean_eps: 0.100000\n",
      "  28068/175000: episode: 795, duration: 1.069s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 51.896 [15.000, 170.000], mean observation: 0.282 [0.000, 96.000], loss: 0.264682, mean_absolute_error: 0.412377, mean_q: 1.799206, mean_eps: 0.100000\n",
      "  28135/175000: episode: 796, duration: 1.491s, episode steps: 67, steps per second: 45, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 130.403 [6.000, 188.000], mean observation: 0.468 [0.000, 134.000], loss: 0.449101, mean_absolute_error: 0.368600, mean_q: 1.612078, mean_eps: 0.100000\n",
      "  28147/175000: episode: 797, duration: 0.255s, episode steps: 12, steps per second: 47, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 5.917 [4.000, 27.000], mean observation: 0.040 [0.000, 24.000], loss: 0.662992, mean_absolute_error: 0.396907, mean_q: 1.762872, mean_eps: 0.100000\n",
      "  28183/175000: episode: 798, duration: 0.697s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 80.444 [4.000, 219.000], mean observation: 0.226 [0.000, 72.000], loss: 0.390566, mean_absolute_error: 0.403066, mean_q: 1.792714, mean_eps: 0.100000\n",
      "  28211/175000: episode: 799, duration: 0.592s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.536 [4.000, 189.000], mean observation: 0.227 [0.000, 56.000], loss: 0.386202, mean_absolute_error: 0.437509, mean_q: 1.893496, mean_eps: 0.100000\n",
      "  28251/175000: episode: 800, duration: 0.889s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 77.300 [7.000, 170.000], mean observation: 0.445 [0.000, 80.000], loss: 0.450072, mean_absolute_error: 0.498322, mean_q: 2.099913, mean_eps: 0.100000\n",
      "  28283/175000: episode: 801, duration: 0.712s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.562 [11.000, 188.000], mean observation: 0.187 [0.000, 64.000], loss: 0.501060, mean_absolute_error: 0.526468, mean_q: 2.149803, mean_eps: 0.100000\n",
      "  28306/175000: episode: 802, duration: 0.500s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 117.130 [8.000, 212.000], mean observation: 0.110 [0.000, 46.000], loss: 0.981752, mean_absolute_error: 0.572767, mean_q: 2.065874, mean_eps: 0.100000\n",
      "  28354/175000: episode: 803, duration: 0.900s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 114.812 [7.000, 220.000], mean observation: 0.495 [0.000, 96.000], loss: 0.469752, mean_absolute_error: 0.557216, mean_q: 2.155614, mean_eps: 0.100000\n",
      "  28408/175000: episode: 804, duration: 1.061s, episode steps: 54, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 165.148 [33.000, 222.000], mean observation: 0.363 [0.000, 108.000], loss: 0.650304, mean_absolute_error: 0.489852, mean_q: 2.405640, mean_eps: 0.100000\n",
      "  28438/175000: episode: 805, duration: 0.658s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 144.233 [47.000, 207.000], mean observation: 0.114 [0.000, 60.000], loss: 0.480774, mean_absolute_error: 0.426328, mean_q: 2.439352, mean_eps: 0.100000\n",
      "  28475/175000: episode: 806, duration: 0.833s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.216 [30.000, 212.000], mean observation: 0.255 [0.000, 74.000], loss: 1.187161, mean_absolute_error: 0.422675, mean_q: 2.418532, mean_eps: 0.100000\n",
      "  28521/175000: episode: 807, duration: 0.971s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.717 [19.000, 222.000], mean observation: 0.284 [0.000, 92.000], loss: 0.750094, mean_absolute_error: 0.444340, mean_q: 2.511404, mean_eps: 0.100000\n",
      "  28565/175000: episode: 808, duration: 0.776s, episode steps: 44, steps per second: 57, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 167.000 [6.000, 212.000], mean observation: 0.182 [0.000, 88.000], loss: 0.388706, mean_absolute_error: 0.410654, mean_q: 2.384034, mean_eps: 0.100000\n",
      "  28599/175000: episode: 809, duration: 0.742s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.588 [19.000, 212.000], mean observation: 0.223 [0.000, 68.000], loss: 0.761800, mean_absolute_error: 0.407101, mean_q: 2.192364, mean_eps: 0.100000\n",
      "  28626/175000: episode: 810, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 134.296 [80.000, 218.000], mean observation: 0.163 [0.000, 54.000], loss: 0.387629, mean_absolute_error: 0.398535, mean_q: 2.121914, mean_eps: 0.100000\n",
      "  28649/175000: episode: 811, duration: 0.448s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 117.217 [19.000, 218.000], mean observation: 0.130 [0.000, 46.000], loss: 0.291618, mean_absolute_error: 0.449618, mean_q: 2.170792, mean_eps: 0.100000\n",
      "  28697/175000: episode: 812, duration: 0.884s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 40.938 [8.000, 207.000], mean observation: 0.433 [0.000, 96.000], loss: 0.287224, mean_absolute_error: 0.465197, mean_q: 2.053435, mean_eps: 0.100000\n",
      "  28742/175000: episode: 813, duration: 0.822s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 90.733 [8.000, 204.000], mean observation: 0.412 [0.000, 90.000], loss: 0.488517, mean_absolute_error: 0.444116, mean_q: 1.977262, mean_eps: 0.100000\n",
      "  28773/175000: episode: 814, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.387 [8.000, 169.000], mean observation: 0.078 [0.000, 62.000], loss: 0.429254, mean_absolute_error: 0.449238, mean_q: 1.953818, mean_eps: 0.100000\n",
      "  28801/175000: episode: 815, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 138.929 [9.000, 199.000], mean observation: 0.136 [0.000, 56.000], loss: 0.167234, mean_absolute_error: 0.446600, mean_q: 1.898533, mean_eps: 0.100000\n",
      "  28828/175000: episode: 816, duration: 0.641s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.926 [39.000, 152.000], mean observation: 0.073 [0.000, 54.000], loss: 0.453642, mean_absolute_error: 0.484660, mean_q: 1.872168, mean_eps: 0.100000\n",
      "  28871/175000: episode: 817, duration: 0.855s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.465 [36.000, 224.000], mean observation: 0.227 [0.000, 86.000], loss: 0.254591, mean_absolute_error: 0.462178, mean_q: 1.832652, mean_eps: 0.100000\n",
      "  28905/175000: episode: 818, duration: 0.707s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.353 [29.000, 198.000], mean observation: 0.157 [0.000, 68.000], loss: 0.640593, mean_absolute_error: 0.453194, mean_q: 1.824734, mean_eps: 0.100000\n",
      "  28957/175000: episode: 819, duration: 0.980s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 155.962 [19.000, 198.000], mean observation: 0.506 [0.000, 104.000], loss: 0.417925, mean_absolute_error: 0.414682, mean_q: 1.699468, mean_eps: 0.100000\n",
      "  29003/175000: episode: 820, duration: 0.919s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 158.304 [3.000, 220.000], mean observation: 0.460 [0.000, 92.000], loss: 0.271526, mean_absolute_error: 0.354528, mean_q: 1.621311, mean_eps: 0.100000\n",
      "  29036/175000: episode: 821, duration: 0.721s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 102.818 [61.000, 179.000], mean observation: 0.187 [0.000, 66.000], loss: 0.886952, mean_absolute_error: 0.346502, mean_q: 1.754606, mean_eps: 0.100000\n",
      "  29078/175000: episode: 822, duration: 0.828s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.595 [8.000, 207.000], mean observation: 0.212 [0.000, 84.000], loss: 0.221694, mean_absolute_error: 0.373555, mean_q: 1.755288, mean_eps: 0.100000\n",
      "  29119/175000: episode: 823, duration: 0.782s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.244 [6.000, 169.000], mean observation: 0.353 [0.000, 82.000], loss: 0.434672, mean_absolute_error: 0.402876, mean_q: 1.700926, mean_eps: 0.100000\n",
      "  29161/175000: episode: 824, duration: 0.772s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 78.643 [24.000, 187.000], mean observation: 0.280 [0.000, 84.000], loss: 0.231062, mean_absolute_error: 0.461530, mean_q: 1.805125, mean_eps: 0.100000\n",
      "  29187/175000: episode: 825, duration: 0.434s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 64.231 [2.000, 184.000], mean observation: 0.116 [0.000, 52.000], loss: 0.372260, mean_absolute_error: 0.479597, mean_q: 1.976014, mean_eps: 0.100000\n",
      "  29237/175000: episode: 826, duration: 0.914s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.340 [2.000, 201.000], mean observation: 0.422 [0.000, 100.000], loss: 0.414872, mean_absolute_error: 0.441598, mean_q: 1.968086, mean_eps: 0.100000\n",
      "  29272/175000: episode: 827, duration: 0.890s, episode steps: 35, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.200 [25.000, 192.000], mean observation: 0.289 [0.000, 70.000], loss: 0.560549, mean_absolute_error: 0.396662, mean_q: 1.813253, mean_eps: 0.100000\n",
      "  29310/175000: episode: 828, duration: 0.907s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.000 [51.000, 189.000], mean observation: 0.244 [0.000, 76.000], loss: 0.628288, mean_absolute_error: 0.393945, mean_q: 1.796562, mean_eps: 0.100000\n",
      "  29346/175000: episode: 829, duration: 0.780s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.306 [53.000, 173.000], mean observation: 0.234 [0.000, 72.000], loss: 0.574286, mean_absolute_error: 0.416612, mean_q: 1.890813, mean_eps: 0.100000\n",
      "  29380/175000: episode: 830, duration: 0.824s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.088 [9.000, 199.000], mean observation: 0.283 [0.000, 68.000], loss: 0.416839, mean_absolute_error: 0.392095, mean_q: 1.810023, mean_eps: 0.100000\n",
      "  29425/175000: episode: 831, duration: 1.124s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.222 [19.000, 221.000], mean observation: 0.337 [0.000, 90.000], loss: 0.343639, mean_absolute_error: 0.401181, mean_q: 1.905235, mean_eps: 0.100000\n",
      "  29447/175000: episode: 832, duration: 0.540s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 21.182 [15.000, 151.000], mean observation: 0.062 [0.000, 44.000], loss: 0.252740, mean_absolute_error: 0.431287, mean_q: 1.953534, mean_eps: 0.100000\n",
      "  29485/175000: episode: 833, duration: 0.958s, episode steps: 38, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.000 [15.000, 199.000], mean observation: 0.459 [0.000, 76.000], loss: 0.188409, mean_absolute_error: 0.422420, mean_q: 1.931111, mean_eps: 0.100000\n",
      "  29528/175000: episode: 834, duration: 1.036s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.465 [1.000, 140.000], mean observation: 0.360 [0.000, 86.000], loss: 0.470490, mean_absolute_error: 0.390171, mean_q: 1.758818, mean_eps: 0.100000\n",
      "  29554/175000: episode: 835, duration: 0.627s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 98.115 [0.000, 143.000], mean observation: 0.116 [0.000, 52.000], loss: 0.158224, mean_absolute_error: 0.363458, mean_q: 1.711728, mean_eps: 0.100000\n",
      "  29598/175000: episode: 836, duration: 0.800s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.773 [22.000, 208.000], mean observation: 0.420 [0.000, 88.000], loss: 0.232312, mean_absolute_error: 0.375450, mean_q: 1.814772, mean_eps: 0.100000\n",
      "  29619/175000: episode: 837, duration: 0.367s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 11.286 [5.000, 120.000], mean observation: 0.064 [0.000, 42.000], loss: 0.335043, mean_absolute_error: 0.369383, mean_q: 1.869389, mean_eps: 0.100000\n",
      "  29655/175000: episode: 838, duration: 0.756s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 61.611 [5.000, 184.000], mean observation: 0.175 [0.000, 72.000], loss: 0.286229, mean_absolute_error: 0.382262, mean_q: 1.930583, mean_eps: 0.100000\n",
      "  29681/175000: episode: 839, duration: 0.701s, episode steps: 26, steps per second: 37, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 65.731 [22.000, 156.000], mean observation: 0.197 [0.000, 52.000], loss: 0.192820, mean_absolute_error: 0.393808, mean_q: 2.018563, mean_eps: 0.100000\n",
      "  29714/175000: episode: 840, duration: 0.754s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 105.394 [60.000, 208.000], mean observation: 0.222 [0.000, 66.000], loss: 0.108029, mean_absolute_error: 0.346164, mean_q: 1.809319, mean_eps: 0.100000\n",
      "  29747/175000: episode: 841, duration: 0.796s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 39.848 [31.000, 160.000], mean observation: 0.084 [0.000, 66.000], loss: 0.252011, mean_absolute_error: 0.355713, mean_q: 1.863812, mean_eps: 0.100000\n",
      "  29807/175000: episode: 842, duration: 1.504s, episode steps: 60, steps per second: 40, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 83.450 [8.000, 194.000], mean observation: 0.739 [0.000, 120.000], loss: 0.360688, mean_absolute_error: 0.338218, mean_q: 1.826368, mean_eps: 0.100000\n",
      "  29864/175000: episode: 843, duration: 1.556s, episode steps: 57, steps per second: 37, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 32.386 [8.000, 191.000], mean observation: 0.297 [0.000, 114.000], loss: 0.155168, mean_absolute_error: 0.334869, mean_q: 1.867931, mean_eps: 0.100000\n",
      "  29893/175000: episode: 844, duration: 0.637s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 47.345 [8.000, 209.000], mean observation: 0.195 [0.000, 58.000], loss: 0.225078, mean_absolute_error: 0.330326, mean_q: 1.792578, mean_eps: 0.100000\n",
      "  29925/175000: episode: 845, duration: 0.611s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 90.781 [89.000, 146.000], mean observation: 0.090 [0.000, 64.000], loss: 1.167621, mean_absolute_error: 0.388750, mean_q: 2.057199, mean_eps: 0.100000\n",
      "  29956/175000: episode: 846, duration: 0.660s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.000 [49.000, 202.000], mean observation: 0.074 [0.000, 62.000], loss: 0.479115, mean_absolute_error: 0.380376, mean_q: 2.021892, mean_eps: 0.100000\n",
      "  29983/175000: episode: 847, duration: 0.592s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 165.926 [22.000, 202.000], mean observation: 0.088 [0.000, 54.000], loss: 0.244322, mean_absolute_error: 0.378302, mean_q: 2.001509, mean_eps: 0.100000\n",
      "  30019/175000: episode: 848, duration: 0.809s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 164.028 [21.000, 202.000], mean observation: 0.131 [0.000, 72.000], loss: 1.576836, mean_absolute_error: 0.390263, mean_q: 2.018594, mean_eps: 0.100000\n",
      "  30035/175000: episode: 849, duration: 0.311s, episode steps: 16, steps per second: 51, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 191.062 [109.000, 202.000], mean observation: 0.049 [0.000, 32.000], loss: 0.600934, mean_absolute_error: 0.356765, mean_q: 1.866988, mean_eps: 0.100000\n",
      "  30071/175000: episode: 850, duration: 0.849s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [51.000, 202.000], mean observation: 0.375 [0.000, 72.000], loss: 2.182937, mean_absolute_error: 0.431253, mean_q: 2.050601, mean_eps: 0.100000\n",
      "  30108/175000: episode: 851, duration: 0.877s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 129.351 [2.000, 195.000], mean observation: 0.169 [0.000, 74.000], loss: 0.209897, mean_absolute_error: 0.440005, mean_q: 2.136141, mean_eps: 0.100000\n",
      "  30153/175000: episode: 852, duration: 1.087s, episode steps: 45, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.800 [41.000, 211.000], mean observation: 0.373 [0.000, 90.000], loss: 1.730468, mean_absolute_error: 0.453648, mean_q: 2.145526, mean_eps: 0.100000\n",
      "  30182/175000: episode: 853, duration: 0.611s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.069 [15.000, 135.000], mean observation: 0.112 [0.000, 58.000], loss: 2.157681, mean_absolute_error: 0.477894, mean_q: 2.083644, mean_eps: 0.100000\n",
      "  30213/175000: episode: 854, duration: 0.670s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 40.484 [12.000, 193.000], mean observation: 0.183 [0.000, 62.000], loss: 1.900839, mean_absolute_error: 0.482201, mean_q: 2.119053, mean_eps: 0.100000\n",
      "  30241/175000: episode: 855, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.321 [12.000, 218.000], mean observation: 0.125 [0.000, 56.000], loss: 0.809498, mean_absolute_error: 0.468022, mean_q: 1.907527, mean_eps: 0.100000\n",
      "  30274/175000: episode: 856, duration: 0.562s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.303 [12.000, 178.000], mean observation: 0.231 [0.000, 66.000], loss: 1.106085, mean_absolute_error: 0.477987, mean_q: 2.001715, mean_eps: 0.100000\n",
      "  30320/175000: episode: 857, duration: 0.870s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 81.370 [28.000, 212.000], mean observation: 0.245 [0.000, 92.000], loss: 0.802924, mean_absolute_error: 0.435283, mean_q: 1.812546, mean_eps: 0.100000\n",
      "  30359/175000: episode: 858, duration: 0.696s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.385 [29.000, 212.000], mean observation: 0.325 [0.000, 78.000], loss: 3.076598, mean_absolute_error: 0.413779, mean_q: 1.812920, mean_eps: 0.100000\n",
      "  30373/175000: episode: 859, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 95.143 [31.000, 116.000], mean observation: 0.052 [0.000, 28.000], loss: 5.948199, mean_absolute_error: 0.477817, mean_q: 1.956259, mean_eps: 0.100000\n",
      "  30426/175000: episode: 860, duration: 0.982s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 126.660 [74.000, 203.000], mean observation: 0.468 [0.000, 106.000], loss: 1.172773, mean_absolute_error: 0.444885, mean_q: 1.845571, mean_eps: 0.100000\n",
      "  30467/175000: episode: 861, duration: 0.756s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.220 [14.000, 224.000], mean observation: 0.199 [0.000, 82.000], loss: 3.811341, mean_absolute_error: 0.441018, mean_q: 1.949279, mean_eps: 0.100000\n",
      "  30503/175000: episode: 862, duration: 0.713s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 169.167 [74.000, 204.000], mean observation: 0.134 [0.000, 72.000], loss: 1.812635, mean_absolute_error: 0.418887, mean_q: 1.917670, mean_eps: 0.100000\n",
      "  30535/175000: episode: 863, duration: 0.642s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 130.562 [15.000, 210.000], mean observation: 0.291 [0.000, 64.000], loss: 2.515732, mean_absolute_error: 0.492585, mean_q: 2.148654, mean_eps: 0.100000\n",
      "  30577/175000: episode: 864, duration: 0.883s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 132.714 [9.000, 210.000], mean observation: 0.374 [0.000, 84.000], loss: 1.301363, mean_absolute_error: 0.431463, mean_q: 1.986507, mean_eps: 0.100000\n",
      "  30594/175000: episode: 865, duration: 0.332s, episode steps: 17, steps per second: 51, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 148.235 [140.000, 160.000], mean observation: 0.052 [0.000, 34.000], loss: 6.804219, mean_absolute_error: 0.467716, mean_q: 2.087343, mean_eps: 0.100000\n",
      "  30629/175000: episode: 866, duration: 0.703s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.800 [5.000, 189.000], mean observation: 0.307 [0.000, 70.000], loss: 0.240950, mean_absolute_error: 0.368359, mean_q: 1.801442, mean_eps: 0.100000\n",
      "  30670/175000: episode: 867, duration: 0.718s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.634 [19.000, 176.000], mean observation: 0.277 [0.000, 82.000], loss: 2.947955, mean_absolute_error: 0.420367, mean_q: 1.703449, mean_eps: 0.100000\n",
      "  30712/175000: episode: 868, duration: 0.797s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.690 [19.000, 171.000], mean observation: 0.250 [0.000, 84.000], loss: 1.448645, mean_absolute_error: 0.416226, mean_q: 1.724058, mean_eps: 0.100000\n",
      "  30758/175000: episode: 869, duration: 0.843s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 84.978 [19.000, 186.000], mean observation: 0.346 [0.000, 92.000], loss: 0.178697, mean_absolute_error: 0.390799, mean_q: 1.659181, mean_eps: 0.100000\n",
      "  30785/175000: episode: 870, duration: 0.479s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 70.704 [19.000, 116.000], mean observation: 0.177 [0.000, 54.000], loss: 1.389605, mean_absolute_error: 0.427694, mean_q: 1.764334, mean_eps: 0.100000\n",
      "  30811/175000: episode: 871, duration: 0.461s, episode steps: 26, steps per second: 56, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 67.923 [10.000, 205.000], mean observation: 0.175 [0.000, 52.000], loss: 1.717069, mean_absolute_error: 0.455903, mean_q: 1.770401, mean_eps: 0.100000\n",
      "  30848/175000: episode: 872, duration: 0.710s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 72.000 [2.000, 154.000], mean observation: 0.124 [0.000, 74.000], loss: 1.171065, mean_absolute_error: 0.473381, mean_q: 1.729650, mean_eps: 0.100000\n",
      "  30863/175000: episode: 873, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 71.000 [71.000, 71.000], mean observation: 0.037 [0.000, 30.000], loss: 2.325951, mean_absolute_error: 0.489395, mean_q: 1.719098, mean_eps: 0.100000\n",
      "  30890/175000: episode: 874, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 77.667 [2.000, 188.000], mean observation: 0.195 [0.000, 54.000], loss: 2.742257, mean_absolute_error: 0.529342, mean_q: 1.838260, mean_eps: 0.100000\n",
      "  30926/175000: episode: 875, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 88.389 [2.000, 207.000], mean observation: 0.271 [0.000, 72.000], loss: 0.260968, mean_absolute_error: 0.437989, mean_q: 1.593314, mean_eps: 0.100000\n",
      "  30975/175000: episode: 876, duration: 0.884s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.041 [14.000, 185.000], mean observation: 0.494 [0.000, 98.000], loss: 0.734190, mean_absolute_error: 0.426745, mean_q: 1.632642, mean_eps: 0.100000\n",
      "  31003/175000: episode: 877, duration: 0.517s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 85.357 [19.000, 189.000], mean observation: 0.159 [0.000, 56.000], loss: 0.528442, mean_absolute_error: 0.428592, mean_q: 1.610212, mean_eps: 0.100000\n",
      "  31036/175000: episode: 878, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 42.545 [19.000, 198.000], mean observation: 0.149 [0.000, 66.000], loss: 3.334421, mean_absolute_error: 0.465811, mean_q: 1.639884, mean_eps: 0.100000\n",
      "  31051/175000: episode: 879, duration: 0.287s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.037 [0.000, 30.000], loss: 0.274105, mean_absolute_error: 0.460896, mean_q: 1.500430, mean_eps: 0.100000\n",
      "  31091/175000: episode: 880, duration: 0.740s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 34.950 [19.000, 196.000], mean observation: 0.254 [0.000, 80.000], loss: 0.281435, mean_absolute_error: 0.477334, mean_q: 1.510640, mean_eps: 0.100000\n",
      "  31125/175000: episode: 881, duration: 0.624s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 52.824 [19.000, 224.000], mean observation: 0.222 [0.000, 68.000], loss: 0.863587, mean_absolute_error: 0.467201, mean_q: 1.469580, mean_eps: 0.100000\n",
      "  31138/175000: episode: 882, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 19.000 [19.000, 19.000], mean observation: 0.033 [0.000, 26.000], loss: 3.843583, mean_absolute_error: 0.582358, mean_q: 1.945393, mean_eps: 0.100000\n",
      "  31181/175000: episode: 883, duration: 0.774s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 29.930 [19.000, 156.000], mean observation: 0.225 [0.000, 86.000], loss: 1.546305, mean_absolute_error: 0.456436, mean_q: 1.710745, mean_eps: 0.100000\n",
      "  31229/175000: episode: 884, duration: 0.885s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 40.646 [16.000, 223.000], mean observation: 0.430 [0.000, 96.000], loss: 1.398746, mean_absolute_error: 0.433456, mean_q: 1.795279, mean_eps: 0.100000\n",
      "  31282/175000: episode: 885, duration: 1.167s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 66.189 [3.000, 216.000], mean observation: 0.652 [0.000, 106.000], loss: 1.788293, mean_absolute_error: 0.418707, mean_q: 1.862907, mean_eps: 0.100000\n",
      "  31308/175000: episode: 886, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 121.346 [18.000, 205.000], mean observation: 0.255 [0.000, 52.000], loss: 2.502336, mean_absolute_error: 0.430601, mean_q: 1.852657, mean_eps: 0.100000\n",
      "  31336/175000: episode: 887, duration: 0.541s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.571 [31.000, 203.000], mean observation: 0.229 [0.000, 56.000], loss: 0.687443, mean_absolute_error: 0.405458, mean_q: 1.760626, mean_eps: 0.100000\n",
      "  31359/175000: episode: 888, duration: 0.553s, episode steps: 23, steps per second: 42, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.913 [7.000, 204.000], mean observation: 0.094 [0.000, 46.000], loss: 1.630010, mean_absolute_error: 0.390277, mean_q: 1.687604, mean_eps: 0.100000\n",
      "  31390/175000: episode: 889, duration: 0.666s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.774 [7.000, 216.000], mean observation: 0.175 [0.000, 62.000], loss: 1.510445, mean_absolute_error: 0.410700, mean_q: 1.845908, mean_eps: 0.100000\n",
      "  31441/175000: episode: 890, duration: 0.929s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 156.039 [1.000, 212.000], mean observation: 0.565 [0.000, 102.000], loss: 0.561377, mean_absolute_error: 0.366300, mean_q: 1.713715, mean_eps: 0.100000\n",
      "  31485/175000: episode: 891, duration: 0.797s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.136 [11.000, 198.000], mean observation: 0.357 [0.000, 88.000], loss: 0.673640, mean_absolute_error: 0.410277, mean_q: 1.969061, mean_eps: 0.100000\n",
      "  31514/175000: episode: 892, duration: 0.513s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 88.483 [29.000, 158.000], mean observation: 0.201 [0.000, 58.000], loss: 0.720205, mean_absolute_error: 0.380574, mean_q: 1.870453, mean_eps: 0.100000\n",
      "  31541/175000: episode: 893, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 68.370 [3.000, 207.000], mean observation: 0.179 [0.000, 54.000], loss: 0.580814, mean_absolute_error: 0.410364, mean_q: 1.904515, mean_eps: 0.100000\n",
      "  31570/175000: episode: 894, duration: 0.506s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 51.241 [19.000, 202.000], mean observation: 0.110 [0.000, 58.000], loss: 0.247488, mean_absolute_error: 0.380855, mean_q: 1.577983, mean_eps: 0.100000\n",
      "  31602/175000: episode: 895, duration: 0.564s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.250 [19.000, 216.000], mean observation: 0.212 [0.000, 64.000], loss: 0.829138, mean_absolute_error: 0.464740, mean_q: 1.858201, mean_eps: 0.100000\n",
      "  31627/175000: episode: 896, duration: 0.437s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 63.080 [7.000, 187.000], mean observation: 0.097 [0.000, 50.000], loss: 3.296956, mean_absolute_error: 0.477102, mean_q: 1.893784, mean_eps: 0.100000\n",
      "  31666/175000: episode: 897, duration: 0.759s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.821 [19.000, 187.000], mean observation: 0.272 [0.000, 78.000], loss: 1.681027, mean_absolute_error: 0.451883, mean_q: 1.888267, mean_eps: 0.100000\n",
      "  31697/175000: episode: 898, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 74.161 [12.000, 217.000], mean observation: 0.224 [0.000, 62.000], loss: 2.184688, mean_absolute_error: 0.415611, mean_q: 1.845437, mean_eps: 0.100000\n",
      "  31742/175000: episode: 899, duration: 0.903s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 48.867 [12.000, 158.000], mean observation: 0.191 [0.000, 90.000], loss: 1.378907, mean_absolute_error: 0.404081, mean_q: 1.876229, mean_eps: 0.100000\n",
      "  31794/175000: episode: 900, duration: 0.919s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 111.231 [12.000, 223.000], mean observation: 0.453 [0.000, 104.000], loss: 3.040295, mean_absolute_error: 0.433794, mean_q: 1.967214, mean_eps: 0.100000\n",
      "  31807/175000: episode: 901, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 50.769 [39.000, 192.000], mean observation: 0.035 [0.000, 26.000], loss: 0.282629, mean_absolute_error: 0.419968, mean_q: 1.872801, mean_eps: 0.100000\n",
      "  31827/175000: episode: 902, duration: 0.362s, episode steps: 20, steps per second: 55, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 51.850 [39.000, 207.000], mean observation: 0.060 [0.000, 40.000], loss: 1.373142, mean_absolute_error: 0.466934, mean_q: 2.241988, mean_eps: 0.100000\n",
      "  31869/175000: episode: 903, duration: 0.825s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.286 [39.000, 189.000], mean observation: 0.299 [0.000, 84.000], loss: 0.922163, mean_absolute_error: 0.444588, mean_q: 2.040029, mean_eps: 0.100000\n",
      "  31897/175000: episode: 904, duration: 0.488s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.857 [19.000, 158.000], mean observation: 0.288 [0.000, 56.000], loss: 2.583693, mean_absolute_error: 0.592023, mean_q: 2.763897, mean_eps: 0.100000\n",
      "  31944/175000: episode: 905, duration: 0.820s, episode steps: 47, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 66.787 [28.000, 177.000], mean observation: 0.369 [0.000, 94.000], loss: 0.187373, mean_absolute_error: 0.457651, mean_q: 2.154426, mean_eps: 0.100000\n",
      "  31993/175000: episode: 906, duration: 0.959s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 120.837 [13.000, 218.000], mean observation: 0.372 [0.000, 98.000], loss: 2.568782, mean_absolute_error: 0.517128, mean_q: 2.446536, mean_eps: 0.100000\n",
      "  32007/175000: episode: 907, duration: 0.246s, episode steps: 14, steps per second: 57, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 108.143 [76.000, 170.000], mean observation: 0.051 [0.000, 28.000], loss: 1.630324, mean_absolute_error: 0.468116, mean_q: 2.275265, mean_eps: 0.100000\n",
      "  32020/175000: episode: 908, duration: 0.271s, episode steps: 13, steps per second: 48, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 106.692 [76.000, 133.000], mean observation: 0.040 [0.000, 26.000], loss: 3.986799, mean_absolute_error: 0.605192, mean_q: 2.891181, mean_eps: 0.100000\n",
      "  32050/175000: episode: 909, duration: 0.581s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 129.533 [13.000, 205.000], mean observation: 0.267 [0.000, 60.000], loss: 0.890589, mean_absolute_error: 0.433316, mean_q: 2.180137, mean_eps: 0.100000\n",
      "  32081/175000: episode: 910, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 109.871 [13.000, 202.000], mean observation: 0.174 [0.000, 62.000], loss: 1.337349, mean_absolute_error: 0.462603, mean_q: 2.458629, mean_eps: 0.100000\n",
      "  32119/175000: episode: 911, duration: 0.697s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.000 [43.000, 202.000], mean observation: 0.289 [0.000, 76.000], loss: 0.673202, mean_absolute_error: 0.428848, mean_q: 2.165876, mean_eps: 0.100000\n",
      "  32154/175000: episode: 912, duration: 0.636s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.771 [6.000, 211.000], mean observation: 0.164 [0.000, 70.000], loss: 1.191694, mean_absolute_error: 0.429834, mean_q: 2.082751, mean_eps: 0.100000\n",
      "  32187/175000: episode: 913, duration: 0.560s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 186.879 [74.000, 222.000], mean observation: 0.222 [0.000, 66.000], loss: 0.525276, mean_absolute_error: 0.403728, mean_q: 2.024873, mean_eps: 0.100000\n",
      "  32224/175000: episode: 914, duration: 0.698s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.135 [13.000, 211.000], mean observation: 0.241 [0.000, 74.000], loss: 0.358763, mean_absolute_error: 0.391642, mean_q: 2.036756, mean_eps: 0.100000\n",
      "  32272/175000: episode: 915, duration: 0.936s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 182.812 [13.000, 224.000], mean observation: 0.344 [0.000, 96.000], loss: 1.095095, mean_absolute_error: 0.379629, mean_q: 2.008066, mean_eps: 0.100000\n",
      "  32308/175000: episode: 916, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 174.778 [13.000, 219.000], mean observation: 0.112 [0.000, 72.000], loss: 2.427423, mean_absolute_error: 0.395827, mean_q: 2.122543, mean_eps: 0.100000\n",
      "  32334/175000: episode: 917, duration: 0.494s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.962 [13.000, 216.000], mean observation: 0.113 [0.000, 52.000], loss: 0.847572, mean_absolute_error: 0.376633, mean_q: 2.001126, mean_eps: 0.100000\n",
      "  32380/175000: episode: 918, duration: 0.872s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 195.087 [86.000, 216.000], mean observation: 0.461 [0.000, 92.000], loss: 3.438962, mean_absolute_error: 0.401960, mean_q: 2.151839, mean_eps: 0.100000\n",
      "  32427/175000: episode: 919, duration: 0.866s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 167.915 [3.000, 216.000], mean observation: 0.455 [0.000, 94.000], loss: 0.324996, mean_absolute_error: 0.382537, mean_q: 2.133870, mean_eps: 0.100000\n",
      "  32478/175000: episode: 920, duration: 1.040s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 132.549 [7.000, 223.000], mean observation: 0.419 [0.000, 102.000], loss: 1.965153, mean_absolute_error: 0.404441, mean_q: 2.218567, mean_eps: 0.100000\n",
      "  32502/175000: episode: 921, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 19.042 [7.000, 153.000], mean observation: 0.101 [0.000, 48.000], loss: 3.820039, mean_absolute_error: 0.474495, mean_q: 2.388373, mean_eps: 0.100000\n",
      "  32538/175000: episode: 922, duration: 0.696s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 96.694 [13.000, 209.000], mean observation: 0.246 [0.000, 72.000], loss: 1.522788, mean_absolute_error: 0.483255, mean_q: 2.421044, mean_eps: 0.100000\n",
      "  32573/175000: episode: 923, duration: 0.758s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.457 [13.000, 193.000], mean observation: 0.258 [0.000, 70.000], loss: 2.554104, mean_absolute_error: 0.462144, mean_q: 2.258373, mean_eps: 0.100000\n",
      "  32615/175000: episode: 924, duration: 0.935s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.238 [13.000, 209.000], mean observation: 0.264 [0.000, 84.000], loss: 4.079457, mean_absolute_error: 0.499526, mean_q: 2.392747, mean_eps: 0.100000\n",
      "  32659/175000: episode: 925, duration: 1.000s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.659 [13.000, 189.000], mean observation: 0.381 [0.000, 88.000], loss: 2.870590, mean_absolute_error: 0.470398, mean_q: 2.408092, mean_eps: 0.100000\n",
      "  32714/175000: episode: 926, duration: 1.170s, episode steps: 55, steps per second: 47, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 117.218 [13.000, 189.000], mean observation: 0.474 [0.000, 110.000], loss: 2.023482, mean_absolute_error: 0.430027, mean_q: 2.377982, mean_eps: 0.100000\n",
      "  32745/175000: episode: 927, duration: 0.726s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.548 [39.000, 223.000], mean observation: 0.146 [0.000, 62.000], loss: 1.455780, mean_absolute_error: 0.405952, mean_q: 2.189995, mean_eps: 0.100000\n",
      "  32767/175000: episode: 928, duration: 0.412s, episode steps: 22, steps per second: 53, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 129.682 [15.000, 223.000], mean observation: 0.139 [0.000, 44.000], loss: 1.072769, mean_absolute_error: 0.467517, mean_q: 2.396438, mean_eps: 0.100000\n",
      "  32798/175000: episode: 929, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.194 [1.000, 162.000], mean observation: 0.150 [0.000, 62.000], loss: 1.202317, mean_absolute_error: 0.463758, mean_q: 2.312006, mean_eps: 0.100000\n",
      "  32839/175000: episode: 930, duration: 0.759s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.610 [22.000, 219.000], mean observation: 0.442 [0.000, 82.000], loss: 1.846961, mean_absolute_error: 0.477603, mean_q: 2.373224, mean_eps: 0.100000\n",
      "  32867/175000: episode: 931, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 99.607 [54.000, 204.000], mean observation: 0.170 [0.000, 56.000], loss: 0.420587, mean_absolute_error: 0.432785, mean_q: 2.090994, mean_eps: 0.100000\n",
      "  32904/175000: episode: 932, duration: 0.754s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.514 [56.000, 214.000], mean observation: 0.238 [0.000, 74.000], loss: 0.992074, mean_absolute_error: 0.431066, mean_q: 1.965223, mean_eps: 0.100000\n",
      "  32941/175000: episode: 933, duration: 0.764s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.676 [56.000, 213.000], mean observation: 0.215 [0.000, 74.000], loss: 1.698473, mean_absolute_error: 0.427392, mean_q: 1.745288, mean_eps: 0.100000\n",
      "  32974/175000: episode: 934, duration: 0.633s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 61.424 [22.000, 140.000], mean observation: 0.134 [0.000, 66.000], loss: 3.304379, mean_absolute_error: 0.415428, mean_q: 1.817175, mean_eps: 0.100000\n",
      "  33025/175000: episode: 935, duration: 1.122s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.804 [22.000, 219.000], mean observation: 0.308 [0.000, 102.000], loss: 0.598485, mean_absolute_error: 0.395026, mean_q: 1.916214, mean_eps: 0.100000\n",
      "  33061/175000: episode: 936, duration: 0.706s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 116.028 [2.000, 197.000], mean observation: 0.271 [0.000, 72.000], loss: 0.264061, mean_absolute_error: 0.346188, mean_q: 1.634527, mean_eps: 0.100000\n",
      "  33091/175000: episode: 937, duration: 0.570s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 164.300 [138.000, 195.000], mean observation: 0.084 [0.000, 60.000], loss: 0.699315, mean_absolute_error: 0.389704, mean_q: 1.822081, mean_eps: 0.100000\n",
      "  33110/175000: episode: 938, duration: 0.374s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 125.789 [74.000, 198.000], mean observation: 0.102 [0.000, 38.000], loss: 1.041271, mean_absolute_error: 0.396829, mean_q: 1.720405, mean_eps: 0.100000\n",
      "  33128/175000: episode: 939, duration: 0.381s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 134.444 [13.000, 190.000], mean observation: 0.112 [0.000, 36.000], loss: 3.214460, mean_absolute_error: 0.483182, mean_q: 2.135416, mean_eps: 0.100000\n",
      "  33176/175000: episode: 940, duration: 1.063s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 140.333 [13.000, 222.000], mean observation: 0.601 [0.000, 96.000], loss: 2.046392, mean_absolute_error: 0.429328, mean_q: 1.974891, mean_eps: 0.100000\n",
      "  33212/175000: episode: 941, duration: 0.776s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.472 [15.000, 118.000], mean observation: 0.148 [0.000, 72.000], loss: 1.819939, mean_absolute_error: 0.448636, mean_q: 2.325254, mean_eps: 0.100000\n",
      "  33257/175000: episode: 942, duration: 1.012s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.422 [23.000, 213.000], mean observation: 0.241 [0.000, 90.000], loss: 2.171712, mean_absolute_error: 0.423240, mean_q: 2.156290, mean_eps: 0.100000\n",
      "  33306/175000: episode: 943, duration: 0.923s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 131.531 [66.000, 198.000], mean observation: 0.326 [0.000, 98.000], loss: 0.898385, mean_absolute_error: 0.374689, mean_q: 1.891218, mean_eps: 0.100000\n",
      "  33347/175000: episode: 944, duration: 0.778s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.634 [27.000, 198.000], mean observation: 0.291 [0.000, 82.000], loss: 1.530292, mean_absolute_error: 0.397354, mean_q: 1.922460, mean_eps: 0.100000\n",
      "  33394/175000: episode: 945, duration: 0.984s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 14.809 [9.000, 210.000], mean observation: 0.144 [0.000, 94.000], loss: 1.992885, mean_absolute_error: 0.397914, mean_q: 2.021141, mean_eps: 0.100000\n",
      "  33414/175000: episode: 946, duration: 0.394s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 30.150 [9.000, 222.000], mean observation: 0.073 [0.000, 40.000], loss: 0.101832, mean_absolute_error: 0.366812, mean_q: 1.845405, mean_eps: 0.100000\n",
      "  33448/175000: episode: 947, duration: 0.722s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 89.676 [8.000, 164.000], mean observation: 0.241 [0.000, 68.000], loss: 0.967819, mean_absolute_error: 0.380155, mean_q: 1.852926, mean_eps: 0.100000\n",
      "  33473/175000: episode: 948, duration: 0.539s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 115.560 [56.000, 204.000], mean observation: 0.159 [0.000, 50.000], loss: 1.915869, mean_absolute_error: 0.387348, mean_q: 2.023424, mean_eps: 0.100000\n",
      "  33513/175000: episode: 949, duration: 0.765s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 85.225 [17.000, 191.000], mean observation: 0.306 [0.000, 80.000], loss: 3.559324, mean_absolute_error: 0.415256, mean_q: 2.154258, mean_eps: 0.100000\n",
      "  33555/175000: episode: 950, duration: 0.840s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.357 [18.000, 213.000], mean observation: 0.138 [0.000, 84.000], loss: 0.557298, mean_absolute_error: 0.316932, mean_q: 1.758578, mean_eps: 0.100000\n",
      "  33586/175000: episode: 951, duration: 0.757s, episode steps: 31, steps per second: 41, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 79.032 [0.000, 187.000], mean observation: 0.138 [0.000, 62.000], loss: 1.375399, mean_absolute_error: 0.386175, mean_q: 2.099490, mean_eps: 0.100000\n",
      "  33595/175000: episode: 952, duration: 0.215s, episode steps: 9, steps per second: 42, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 143.444 [75.000, 152.000], mean observation: 0.037 [0.000, 18.000], loss: 1.379974, mean_absolute_error: 0.391751, mean_q: 2.140618, mean_eps: 0.100000\n",
      "  33643/175000: episode: 953, duration: 1.067s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.146 [16.000, 177.000], mean observation: 0.225 [0.000, 96.000], loss: 2.949286, mean_absolute_error: 0.419061, mean_q: 2.164119, mean_eps: 0.100000\n",
      "  33670/175000: episode: 954, duration: 0.650s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 55.407 [24.000, 203.000], mean observation: 0.097 [0.000, 54.000], loss: 0.744269, mean_absolute_error: 0.382285, mean_q: 1.845433, mean_eps: 0.100000\n",
      "  33716/175000: episode: 955, duration: 1.110s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.565 [0.000, 189.000], mean observation: 0.336 [0.000, 92.000], loss: 0.890053, mean_absolute_error: 0.409628, mean_q: 2.025324, mean_eps: 0.100000\n",
      "  33761/175000: episode: 956, duration: 1.180s, episode steps: 45, steps per second: 38, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 95.333 [25.000, 208.000], mean observation: 0.271 [0.000, 90.000], loss: 0.105288, mean_absolute_error: 0.335992, mean_q: 1.775765, mean_eps: 0.100000\n",
      "  33797/175000: episode: 957, duration: 0.859s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 86.500 [10.000, 209.000], mean observation: 0.214 [0.000, 72.000], loss: 4.423489, mean_absolute_error: 0.390258, mean_q: 1.975487, mean_eps: 0.100000\n",
      "  33824/175000: episode: 958, duration: 0.753s, episode steps: 27, steps per second: 36, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.444 [3.000, 209.000], mean observation: 0.115 [0.000, 54.000], loss: 0.740320, mean_absolute_error: 0.363813, mean_q: 1.920976, mean_eps: 0.100000\n",
      "  33866/175000: episode: 959, duration: 1.078s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 78.286 [28.000, 208.000], mean observation: 0.224 [0.000, 84.000], loss: 1.301539, mean_absolute_error: 0.418026, mean_q: 2.035448, mean_eps: 0.100000\n",
      "  33917/175000: episode: 960, duration: 1.087s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.235 [8.000, 212.000], mean observation: 0.478 [0.000, 102.000], loss: 3.130201, mean_absolute_error: 0.521646, mean_q: 2.248283, mean_eps: 0.100000\n",
      "  33956/175000: episode: 961, duration: 0.787s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.231 [10.000, 145.000], mean observation: 0.240 [0.000, 78.000], loss: 2.632580, mean_absolute_error: 0.484820, mean_q: 2.377787, mean_eps: 0.100000\n",
      "  33989/175000: episode: 962, duration: 0.755s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.030 [50.000, 141.000], mean observation: 0.287 [0.000, 66.000], loss: 0.959975, mean_absolute_error: 0.427502, mean_q: 2.369527, mean_eps: 0.100000\n",
      "  34013/175000: episode: 963, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 107.125 [25.000, 125.000], mean observation: 0.146 [0.000, 48.000], loss: 0.488703, mean_absolute_error: 0.413659, mean_q: 2.335108, mean_eps: 0.100000\n",
      "  34046/175000: episode: 964, duration: 0.647s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.848 [43.000, 161.000], mean observation: 0.241 [0.000, 66.000], loss: 0.342007, mean_absolute_error: 0.436470, mean_q: 2.267998, mean_eps: 0.100000\n",
      "  34076/175000: episode: 965, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 106.233 [67.000, 222.000], mean observation: 0.243 [0.000, 60.000], loss: 0.865251, mean_absolute_error: 0.459048, mean_q: 2.295910, mean_eps: 0.100000\n",
      "  34124/175000: episode: 966, duration: 0.975s, episode steps: 48, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.979 [38.000, 209.000], mean observation: 0.265 [0.000, 96.000], loss: 0.492157, mean_absolute_error: 0.435148, mean_q: 2.133774, mean_eps: 0.100000\n",
      "  34160/175000: episode: 967, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 166.750 [55.000, 215.000], mean observation: 0.146 [0.000, 72.000], loss: 1.428366, mean_absolute_error: 0.443884, mean_q: 2.194524, mean_eps: 0.100000\n",
      "  34205/175000: episode: 968, duration: 0.884s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.956 [34.000, 204.000], mean observation: 0.412 [0.000, 90.000], loss: 0.524132, mean_absolute_error: 0.401585, mean_q: 1.973255, mean_eps: 0.100000\n",
      "  34267/175000: episode: 969, duration: 1.361s, episode steps: 62, steps per second: 46, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 138.790 [21.000, 221.000], mean observation: 0.786 [0.000, 124.000], loss: 0.625028, mean_absolute_error: 0.433933, mean_q: 2.220147, mean_eps: 0.100000\n",
      "  34305/175000: episode: 970, duration: 0.838s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 155.474 [15.000, 208.000], mean observation: 0.327 [0.000, 76.000], loss: 1.595600, mean_absolute_error: 0.451680, mean_q: 2.108759, mean_eps: 0.100000\n",
      "  34355/175000: episode: 971, duration: 0.967s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.700 [38.000, 214.000], mean observation: 0.392 [0.000, 100.000], loss: 0.358624, mean_absolute_error: 0.477170, mean_q: 2.270680, mean_eps: 0.100000\n",
      "  34396/175000: episode: 972, duration: 0.853s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 177.171 [48.000, 218.000], mean observation: 0.354 [0.000, 82.000], loss: 1.578184, mean_absolute_error: 0.436054, mean_q: 2.098200, mean_eps: 0.100000\n",
      "  34409/175000: episode: 973, duration: 0.283s, episode steps: 13, steps per second: 46, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 102.692 [63.000, 183.000], mean observation: 0.061 [0.000, 26.000], loss: 0.298775, mean_absolute_error: 0.354696, mean_q: 1.725728, mean_eps: 0.100000\n",
      "  34440/175000: episode: 974, duration: 0.679s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 143.806 [30.000, 186.000], mean observation: 0.216 [0.000, 62.000], loss: 0.612547, mean_absolute_error: 0.441715, mean_q: 2.234944, mean_eps: 0.100000\n",
      "  34465/175000: episode: 975, duration: 0.532s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 152.480 [19.000, 186.000], mean observation: 0.155 [0.000, 50.000], loss: 1.402055, mean_absolute_error: 0.374287, mean_q: 2.039159, mean_eps: 0.100000\n",
      "  34498/175000: episode: 976, duration: 0.685s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.091 [19.000, 209.000], mean observation: 0.275 [0.000, 66.000], loss: 0.585193, mean_absolute_error: 0.346413, mean_q: 2.005877, mean_eps: 0.100000\n",
      "  34537/175000: episode: 977, duration: 0.836s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.154 [19.000, 178.000], mean observation: 0.345 [0.000, 78.000], loss: 1.163560, mean_absolute_error: 0.351620, mean_q: 2.094034, mean_eps: 0.100000\n",
      "  34563/175000: episode: 978, duration: 0.623s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 130.692 [22.000, 179.000], mean observation: 0.192 [0.000, 52.000], loss: 0.625978, mean_absolute_error: 0.361461, mean_q: 2.079082, mean_eps: 0.100000\n",
      "  34599/175000: episode: 979, duration: 0.880s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 112.917 [26.000, 189.000], mean observation: 0.260 [0.000, 72.000], loss: 0.167984, mean_absolute_error: 0.339203, mean_q: 1.908597, mean_eps: 0.100000\n",
      "  34628/175000: episode: 980, duration: 0.628s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 95.103 [16.000, 175.000], mean observation: 0.260 [0.000, 58.000], loss: 1.016656, mean_absolute_error: 0.401911, mean_q: 2.197012, mean_eps: 0.100000\n",
      "  34658/175000: episode: 981, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 155.700 [19.000, 201.000], mean observation: 0.238 [0.000, 60.000], loss: 0.513021, mean_absolute_error: 0.372898, mean_q: 2.067816, mean_eps: 0.100000\n",
      "  34717/175000: episode: 982, duration: 1.194s, episode steps: 59, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.153 [19.000, 214.000], mean observation: 0.560 [0.000, 118.000], loss: 0.771898, mean_absolute_error: 0.351961, mean_q: 2.152541, mean_eps: 0.100000\n",
      "  34759/175000: episode: 983, duration: 0.913s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.857 [52.000, 179.000], mean observation: 0.346 [0.000, 84.000], loss: 1.134739, mean_absolute_error: 0.384117, mean_q: 2.351948, mean_eps: 0.100000\n",
      "  34800/175000: episode: 984, duration: 0.962s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.805 [42.000, 220.000], mean observation: 0.447 [0.000, 82.000], loss: 1.762310, mean_absolute_error: 0.388779, mean_q: 2.363724, mean_eps: 0.100000\n",
      "  34838/175000: episode: 985, duration: 0.863s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.395 [19.000, 184.000], mean observation: 0.360 [0.000, 76.000], loss: 0.396329, mean_absolute_error: 0.354448, mean_q: 1.875746, mean_eps: 0.100000\n",
      "  34862/175000: episode: 986, duration: 0.518s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 160.042 [16.000, 202.000], mean observation: 0.144 [0.000, 48.000], loss: 1.532184, mean_absolute_error: 0.357838, mean_q: 1.847799, mean_eps: 0.100000\n",
      "  34884/175000: episode: 987, duration: 0.479s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 120.773 [35.000, 200.000], mean observation: 0.093 [0.000, 44.000], loss: 0.860185, mean_absolute_error: 0.361303, mean_q: 1.914819, mean_eps: 0.100000\n",
      "  34930/175000: episode: 988, duration: 1.010s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.217 [30.000, 180.000], mean observation: 0.162 [0.000, 92.000], loss: 2.234085, mean_absolute_error: 0.397807, mean_q: 2.239561, mean_eps: 0.100000\n",
      "  34958/175000: episode: 989, duration: 0.675s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.786 [62.000, 150.000], mean observation: 0.135 [0.000, 56.000], loss: 0.976727, mean_absolute_error: 0.395214, mean_q: 2.065568, mean_eps: 0.100000\n",
      "  34999/175000: episode: 990, duration: 0.828s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.829 [8.000, 197.000], mean observation: 0.483 [0.000, 82.000], loss: 0.413731, mean_absolute_error: 0.402991, mean_q: 1.921232, mean_eps: 0.100000\n",
      "  35035/175000: episode: 991, duration: 0.743s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.028 [23.000, 218.000], mean observation: 0.263 [0.000, 72.000], loss: 0.177740, mean_absolute_error: 0.402736, mean_q: 1.737930, mean_eps: 0.100000\n",
      "  35065/175000: episode: 992, duration: 0.721s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 100.533 [12.000, 218.000], mean observation: 0.237 [0.000, 60.000], loss: 0.195019, mean_absolute_error: 0.371996, mean_q: 1.633163, mean_eps: 0.100000\n",
      "  35094/175000: episode: 993, duration: 0.599s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 89.172 [7.000, 119.000], mean observation: 0.238 [0.000, 58.000], loss: 0.081360, mean_absolute_error: 0.351968, mean_q: 1.582228, mean_eps: 0.100000\n",
      "  35121/175000: episode: 994, duration: 0.573s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.037 [36.000, 197.000], mean observation: 0.227 [0.000, 54.000], loss: 0.621498, mean_absolute_error: 0.378186, mean_q: 1.696476, mean_eps: 0.100000\n",
      "  35179/175000: episode: 995, duration: 1.270s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 108.810 [28.000, 197.000], mean observation: 0.673 [0.000, 116.000], loss: 0.534732, mean_absolute_error: 0.351964, mean_q: 1.601677, mean_eps: 0.100000\n",
      "  35205/175000: episode: 996, duration: 0.563s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.269 [49.000, 140.000], mean observation: 0.121 [0.000, 52.000], loss: 3.210087, mean_absolute_error: 0.435782, mean_q: 1.936990, mean_eps: 0.100000\n",
      "  35238/175000: episode: 997, duration: 0.681s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.394 [19.000, 221.000], mean observation: 0.179 [0.000, 66.000], loss: 0.415744, mean_absolute_error: 0.417165, mean_q: 1.683759, mean_eps: 0.100000\n",
      "  35267/175000: episode: 998, duration: 0.653s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 143.448 [19.000, 184.000], mean observation: 0.142 [0.000, 58.000], loss: 1.150902, mean_absolute_error: 0.439039, mean_q: 1.707491, mean_eps: 0.100000\n",
      "  35300/175000: episode: 999, duration: 0.792s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.667 [49.000, 158.000], mean observation: 0.128 [0.000, 66.000], loss: 0.758791, mean_absolute_error: 0.431799, mean_q: 1.500425, mean_eps: 0.100000\n",
      "  35345/175000: episode: 1000, duration: 1.139s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 136.267 [1.000, 220.000], mean observation: 0.537 [0.000, 90.000], loss: 1.528182, mean_absolute_error: 0.469986, mean_q: 1.651931, mean_eps: 0.100000\n",
      "  35361/175000: episode: 1001, duration: 0.294s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 136.500 [58.000, 161.000], mean observation: 0.096 [0.000, 32.000], loss: 0.684301, mean_absolute_error: 0.391601, mean_q: 1.444103, mean_eps: 0.100000\n",
      "  35386/175000: episode: 1002, duration: 0.503s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 217.560 [155.000, 224.000], mean observation: 0.084 [0.000, 50.000], loss: 0.470023, mean_absolute_error: 0.394704, mean_q: 1.607464, mean_eps: 0.100000\n",
      "  35414/175000: episode: 1003, duration: 0.597s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 169.607 [12.000, 220.000], mean observation: 0.093 [0.000, 56.000], loss: 3.867312, mean_absolute_error: 0.465507, mean_q: 1.891151, mean_eps: 0.100000\n",
      "  35459/175000: episode: 1004, duration: 1.097s, episode steps: 45, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.911 [10.000, 212.000], mean observation: 0.395 [0.000, 90.000], loss: 3.975795, mean_absolute_error: 0.460414, mean_q: 1.850776, mean_eps: 0.100000\n",
      "  35510/175000: episode: 1005, duration: 1.310s, episode steps: 51, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 138.098 [16.000, 212.000], mean observation: 0.430 [0.000, 102.000], loss: 1.842816, mean_absolute_error: 0.420222, mean_q: 1.942713, mean_eps: 0.100000\n",
      "  35538/175000: episode: 1006, duration: 0.557s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 146.786 [16.000, 204.000], mean observation: 0.158 [0.000, 56.000], loss: 3.293489, mean_absolute_error: 0.537351, mean_q: 2.479485, mean_eps: 0.100000\n",
      "  35575/175000: episode: 1007, duration: 0.704s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 202.541 [98.000, 208.000], mean observation: 0.099 [0.000, 74.000], loss: 1.231824, mean_absolute_error: 0.437317, mean_q: 1.995783, mean_eps: 0.100000\n",
      "  35618/175000: episode: 1008, duration: 0.848s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 196.674 [16.000, 219.000], mean observation: 0.366 [0.000, 86.000], loss: 0.587183, mean_absolute_error: 0.378611, mean_q: 1.999849, mean_eps: 0.100000\n",
      "  35652/175000: episode: 1009, duration: 0.692s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.529 [16.000, 208.000], mean observation: 0.360 [0.000, 68.000], loss: 0.998633, mean_absolute_error: 0.401900, mean_q: 2.033626, mean_eps: 0.100000\n",
      "  35695/175000: episode: 1010, duration: 0.963s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 79.698 [0.000, 188.000], mean observation: 0.385 [0.000, 86.000], loss: 0.919840, mean_absolute_error: 0.402739, mean_q: 2.117695, mean_eps: 0.100000\n",
      "  35727/175000: episode: 1011, duration: 0.673s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 83.344 [10.000, 220.000], mean observation: 0.161 [0.000, 64.000], loss: 0.664951, mean_absolute_error: 0.397525, mean_q: 2.050117, mean_eps: 0.100000\n",
      "  35772/175000: episode: 1012, duration: 0.886s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 155.022 [12.000, 208.000], mean observation: 0.354 [0.000, 90.000], loss: 4.439710, mean_absolute_error: 0.501513, mean_q: 2.343568, mean_eps: 0.100000\n",
      "  35803/175000: episode: 1013, duration: 0.636s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 82.226 [42.000, 216.000], mean observation: 0.139 [0.000, 62.000], loss: 0.795488, mean_absolute_error: 0.408855, mean_q: 2.002181, mean_eps: 0.100000\n",
      "  35827/175000: episode: 1014, duration: 0.524s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 69.958 [32.000, 213.000], mean observation: 0.071 [0.000, 48.000], loss: 0.539637, mean_absolute_error: 0.431491, mean_q: 2.145693, mean_eps: 0.100000\n",
      "  35857/175000: episode: 1015, duration: 0.668s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.633 [43.000, 219.000], mean observation: 0.200 [0.000, 60.000], loss: 0.430246, mean_absolute_error: 0.408433, mean_q: 2.094347, mean_eps: 0.100000\n",
      "  35898/175000: episode: 1016, duration: 0.799s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.780 [14.000, 191.000], mean observation: 0.398 [0.000, 82.000], loss: 0.499904, mean_absolute_error: 0.393329, mean_q: 1.995600, mean_eps: 0.100000\n",
      "  35922/175000: episode: 1017, duration: 0.459s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 114.542 [42.000, 163.000], mean observation: 0.133 [0.000, 48.000], loss: 0.632761, mean_absolute_error: 0.412490, mean_q: 2.072085, mean_eps: 0.100000\n",
      "  35967/175000: episode: 1018, duration: 0.879s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 58.556 [3.000, 206.000], mean observation: 0.376 [0.000, 90.000], loss: 0.803012, mean_absolute_error: 0.450754, mean_q: 2.090617, mean_eps: 0.100000\n",
      "  36005/175000: episode: 1019, duration: 0.800s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.947 [15.000, 209.000], mean observation: 0.341 [0.000, 76.000], loss: 0.253741, mean_absolute_error: 0.396244, mean_q: 1.695365, mean_eps: 0.100000\n",
      "  36039/175000: episode: 1020, duration: 0.685s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.794 [19.000, 142.000], mean observation: 0.233 [0.000, 68.000], loss: 0.805728, mean_absolute_error: 0.371179, mean_q: 1.701428, mean_eps: 0.100000\n",
      "  36075/175000: episode: 1021, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 63.639 [19.000, 218.000], mean observation: 0.162 [0.000, 72.000], loss: 1.254946, mean_absolute_error: 0.383451, mean_q: 1.838298, mean_eps: 0.100000\n",
      "  36123/175000: episode: 1022, duration: 0.907s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 46.375 [7.000, 197.000], mean observation: 0.365 [0.000, 96.000], loss: 1.569841, mean_absolute_error: 0.373314, mean_q: 1.877793, mean_eps: 0.100000\n",
      "  36156/175000: episode: 1023, duration: 0.665s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.061 [19.000, 183.000], mean observation: 0.137 [0.000, 66.000], loss: 1.277883, mean_absolute_error: 0.415079, mean_q: 1.989740, mean_eps: 0.100000\n",
      "  36181/175000: episode: 1024, duration: 0.547s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 95.920 [6.000, 153.000], mean observation: 0.178 [0.000, 50.000], loss: 1.653713, mean_absolute_error: 0.443082, mean_q: 1.961375, mean_eps: 0.100000\n",
      "  36221/175000: episode: 1025, duration: 0.858s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 102.650 [6.000, 151.000], mean observation: 0.230 [0.000, 80.000], loss: 1.989866, mean_absolute_error: 0.405707, mean_q: 1.943424, mean_eps: 0.100000\n",
      "  36260/175000: episode: 1026, duration: 0.793s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.205 [21.000, 175.000], mean observation: 0.243 [0.000, 78.000], loss: 1.142394, mean_absolute_error: 0.395862, mean_q: 1.940066, mean_eps: 0.100000\n",
      "  36298/175000: episode: 1027, duration: 0.821s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.526 [19.000, 222.000], mean observation: 0.277 [0.000, 76.000], loss: 1.146935, mean_absolute_error: 0.465947, mean_q: 2.203907, mean_eps: 0.100000\n",
      "  36332/175000: episode: 1028, duration: 0.737s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 116.676 [98.000, 222.000], mean observation: 0.165 [0.000, 68.000], loss: 0.844429, mean_absolute_error: 0.518575, mean_q: 2.531006, mean_eps: 0.100000\n",
      "  36357/175000: episode: 1029, duration: 0.529s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 157.040 [108.000, 203.000], mean observation: 0.093 [0.000, 50.000], loss: 2.712969, mean_absolute_error: 0.490183, mean_q: 2.224808, mean_eps: 0.100000\n",
      "  36378/175000: episode: 1030, duration: 0.471s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 134.190 [66.000, 213.000], mean observation: 0.135 [0.000, 42.000], loss: 3.855714, mean_absolute_error: 0.488624, mean_q: 2.198656, mean_eps: 0.100000\n",
      "  36424/175000: episode: 1031, duration: 1.016s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.891 [66.000, 191.000], mean observation: 0.306 [0.000, 92.000], loss: 0.246908, mean_absolute_error: 0.394355, mean_q: 1.614681, mean_eps: 0.100000\n",
      "  36451/175000: episode: 1032, duration: 0.574s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.222 [9.000, 180.000], mean observation: 0.144 [0.000, 54.000], loss: 2.308978, mean_absolute_error: 0.456342, mean_q: 1.954949, mean_eps: 0.100000\n",
      "  36494/175000: episode: 1033, duration: 0.955s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 146.605 [3.000, 223.000], mean observation: 0.405 [0.000, 86.000], loss: 1.167455, mean_absolute_error: 0.423130, mean_q: 1.730864, mean_eps: 0.100000\n",
      "  36540/175000: episode: 1034, duration: 0.956s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 144.413 [20.000, 213.000], mean observation: 0.395 [0.000, 92.000], loss: 2.653200, mean_absolute_error: 0.474460, mean_q: 1.970847, mean_eps: 0.100000\n",
      "  36582/175000: episode: 1035, duration: 0.884s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.190 [115.000, 191.000], mean observation: 0.280 [0.000, 84.000], loss: 0.184413, mean_absolute_error: 0.405855, mean_q: 1.849759, mean_eps: 0.100000\n",
      "  36621/175000: episode: 1036, duration: 0.794s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 129.179 [49.000, 217.000], mean observation: 0.301 [0.000, 78.000], loss: 0.355859, mean_absolute_error: 0.398853, mean_q: 1.832207, mean_eps: 0.100000\n",
      "  36666/175000: episode: 1037, duration: 0.888s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 120.178 [19.000, 199.000], mean observation: 0.273 [0.000, 90.000], loss: 0.333741, mean_absolute_error: 0.375047, mean_q: 1.660595, mean_eps: 0.100000\n",
      "  36698/175000: episode: 1038, duration: 0.655s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 114.406 [19.000, 193.000], mean observation: 0.203 [0.000, 64.000], loss: 0.694983, mean_absolute_error: 0.386946, mean_q: 1.702139, mean_eps: 0.100000\n",
      "  36728/175000: episode: 1039, duration: 0.695s, episode steps: 30, steps per second: 43, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.267 [2.000, 173.000], mean observation: 0.163 [0.000, 60.000], loss: 2.117112, mean_absolute_error: 0.503039, mean_q: 2.153629, mean_eps: 0.100000\n",
      "  36758/175000: episode: 1040, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 143.233 [1.000, 214.000], mean observation: 0.330 [0.000, 60.000], loss: 0.200798, mean_absolute_error: 0.418815, mean_q: 1.859711, mean_eps: 0.100000\n",
      "  36787/175000: episode: 1041, duration: 0.552s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 36.931 [16.000, 174.000], mean observation: 0.114 [0.000, 58.000], loss: 1.392774, mean_absolute_error: 0.408944, mean_q: 1.859578, mean_eps: 0.100000\n",
      "  36830/175000: episode: 1042, duration: 0.879s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.233 [0.000, 179.000], mean observation: 0.267 [0.000, 86.000], loss: 2.496085, mean_absolute_error: 0.421245, mean_q: 2.069841, mean_eps: 0.100000\n",
      "  36855/175000: episode: 1043, duration: 0.482s, episode steps: 25, steps per second: 52, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 93.560 [0.000, 215.000], mean observation: 0.177 [0.000, 50.000], loss: 2.770767, mean_absolute_error: 0.477780, mean_q: 2.264618, mean_eps: 0.100000\n",
      "  36889/175000: episode: 1044, duration: 0.772s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.471 [15.000, 190.000], mean observation: 0.196 [0.000, 68.000], loss: 0.261165, mean_absolute_error: 0.381675, mean_q: 1.848793, mean_eps: 0.100000\n",
      "  36911/175000: episode: 1045, duration: 0.433s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 71.182 [13.000, 140.000], mean observation: 0.105 [0.000, 44.000], loss: 0.807369, mean_absolute_error: 0.410905, mean_q: 1.895268, mean_eps: 0.100000\n",
      "  36944/175000: episode: 1046, duration: 0.744s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 86.636 [13.000, 213.000], mean observation: 0.154 [0.000, 66.000], loss: 1.371806, mean_absolute_error: 0.443163, mean_q: 2.005339, mean_eps: 0.100000\n",
      "  37010/175000: episode: 1047, duration: 1.324s, episode steps: 66, steps per second: 50, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 126.894 [13.000, 218.000], mean observation: 0.849 [0.000, 132.000], loss: 1.186167, mean_absolute_error: 0.436103, mean_q: 2.022280, mean_eps: 0.100000\n",
      "  37067/175000: episode: 1048, duration: 1.105s, episode steps: 57, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 146.316 [16.000, 215.000], mean observation: 0.606 [0.000, 114.000], loss: 0.403155, mean_absolute_error: 0.406909, mean_q: 1.986429, mean_eps: 0.100000\n",
      "  37109/175000: episode: 1049, duration: 0.854s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 130.000 [1.000, 209.000], mean observation: 0.283 [0.000, 84.000], loss: 2.798690, mean_absolute_error: 0.464554, mean_q: 2.284588, mean_eps: 0.100000\n",
      "  37159/175000: episode: 1050, duration: 0.992s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 53.240 [1.000, 203.000], mean observation: 0.364 [0.000, 100.000], loss: 1.037445, mean_absolute_error: 0.421814, mean_q: 2.192749, mean_eps: 0.100000\n",
      "  37191/175000: episode: 1051, duration: 0.607s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 81.312 [5.000, 221.000], mean observation: 0.326 [0.000, 64.000], loss: 3.748017, mean_absolute_error: 0.477117, mean_q: 2.446203, mean_eps: 0.100000\n",
      "  37227/175000: episode: 1052, duration: 0.782s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 30.417 [6.000, 217.000], mean observation: 0.181 [0.000, 72.000], loss: 1.227682, mean_absolute_error: 0.456896, mean_q: 2.364789, mean_eps: 0.100000\n",
      "  37263/175000: episode: 1053, duration: 0.683s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.750 [6.000, 177.000], mean observation: 0.206 [0.000, 72.000], loss: 1.098259, mean_absolute_error: 0.392620, mean_q: 1.982067, mean_eps: 0.100000\n",
      "  37296/175000: episode: 1054, duration: 0.727s, episode steps: 33, steps per second: 45, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.303 [6.000, 177.000], mean observation: 0.189 [0.000, 66.000], loss: 1.824322, mean_absolute_error: 0.426174, mean_q: 2.263823, mean_eps: 0.100000\n",
      "  37326/175000: episode: 1055, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 82.067 [6.000, 177.000], mean observation: 0.215 [0.000, 60.000], loss: 1.801567, mean_absolute_error: 0.436510, mean_q: 2.254470, mean_eps: 0.100000\n",
      "  37355/175000: episode: 1056, duration: 0.592s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 99.690 [6.000, 217.000], mean observation: 0.156 [0.000, 58.000], loss: 0.175095, mean_absolute_error: 0.322187, mean_q: 1.774232, mean_eps: 0.100000\n",
      "  37381/175000: episode: 1057, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 34.885 [6.000, 177.000], mean observation: 0.072 [0.000, 52.000], loss: 0.828539, mean_absolute_error: 0.390436, mean_q: 2.117138, mean_eps: 0.100000\n",
      "  37418/175000: episode: 1058, duration: 0.774s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 114.622 [1.000, 202.000], mean observation: 0.272 [0.000, 74.000], loss: 0.459594, mean_absolute_error: 0.375202, mean_q: 1.972408, mean_eps: 0.100000\n",
      "  37456/175000: episode: 1059, duration: 0.759s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.079 [6.000, 208.000], mean observation: 0.243 [0.000, 76.000], loss: 1.218352, mean_absolute_error: 0.402302, mean_q: 2.274423, mean_eps: 0.100000\n",
      "  37484/175000: episode: 1060, duration: 0.709s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 157.571 [53.000, 202.000], mean observation: 0.194 [0.000, 56.000], loss: 2.413677, mean_absolute_error: 0.420996, mean_q: 2.149356, mean_eps: 0.100000\n",
      "  37529/175000: episode: 1061, duration: 0.982s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 138.222 [29.000, 202.000], mean observation: 0.360 [0.000, 90.000], loss: 2.065917, mean_absolute_error: 0.401783, mean_q: 1.788256, mean_eps: 0.100000\n",
      "  37562/175000: episode: 1062, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 161.727 [6.000, 224.000], mean observation: 0.372 [0.000, 66.000], loss: 0.678488, mean_absolute_error: 0.426710, mean_q: 1.936707, mean_eps: 0.100000\n",
      "  37608/175000: episode: 1063, duration: 0.959s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 172.957 [14.000, 224.000], mean observation: 0.477 [0.000, 92.000], loss: 1.335684, mean_absolute_error: 0.439704, mean_q: 1.825878, mean_eps: 0.100000\n",
      "  37638/175000: episode: 1064, duration: 0.665s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 61.300 [0.000, 220.000], mean observation: 0.167 [0.000, 60.000], loss: 0.960993, mean_absolute_error: 0.474020, mean_q: 1.942782, mean_eps: 0.100000\n",
      "  37665/175000: episode: 1065, duration: 0.647s, episode steps: 27, steps per second: 42, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 116.037 [21.000, 200.000], mean observation: 0.125 [0.000, 54.000], loss: 0.225028, mean_absolute_error: 0.421059, mean_q: 1.759087, mean_eps: 0.100000\n",
      "  37698/175000: episode: 1066, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 152.364 [2.000, 200.000], mean observation: 0.241 [0.000, 66.000], loss: 0.266691, mean_absolute_error: 0.421033, mean_q: 1.870338, mean_eps: 0.100000\n",
      "  37732/175000: episode: 1067, duration: 0.703s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.794 [60.000, 161.000], mean observation: 0.160 [0.000, 68.000], loss: 0.157975, mean_absolute_error: 0.377501, mean_q: 1.856318, mean_eps: 0.100000\n",
      "  37768/175000: episode: 1068, duration: 0.775s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.778 [7.000, 161.000], mean observation: 0.336 [0.000, 72.000], loss: 0.363976, mean_absolute_error: 0.380213, mean_q: 1.853026, mean_eps: 0.100000\n",
      "  37790/175000: episode: 1069, duration: 0.535s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 26.636 [21.000, 145.000], mean observation: 0.064 [0.000, 44.000], loss: 1.740038, mean_absolute_error: 0.452368, mean_q: 2.131953, mean_eps: 0.100000\n",
      "  37831/175000: episode: 1070, duration: 0.771s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 76.024 [21.000, 198.000], mean observation: 0.221 [0.000, 82.000], loss: 0.994209, mean_absolute_error: 0.511397, mean_q: 2.363125, mean_eps: 0.100000\n",
      "  37866/175000: episode: 1071, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.886 [1.000, 198.000], mean observation: 0.346 [0.000, 70.000], loss: 2.496412, mean_absolute_error: 0.563658, mean_q: 2.458278, mean_eps: 0.100000\n",
      "  37897/175000: episode: 1072, duration: 0.582s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 75.065 [1.000, 191.000], mean observation: 0.253 [0.000, 62.000], loss: 3.216961, mean_absolute_error: 0.562061, mean_q: 2.507462, mean_eps: 0.100000\n",
      "  37927/175000: episode: 1073, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 14.867 [14.000, 15.000], mean observation: 0.106 [0.000, 60.000], loss: 0.636348, mean_absolute_error: 0.468841, mean_q: 2.178030, mean_eps: 0.100000\n",
      "  37963/175000: episode: 1074, duration: 0.693s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 51.222 [2.000, 215.000], mean observation: 0.233 [0.000, 72.000], loss: 0.462289, mean_absolute_error: 0.483507, mean_q: 2.241560, mean_eps: 0.100000\n",
      "  37987/175000: episode: 1075, duration: 0.488s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 40.000 [40.000, 40.000], mean observation: 0.057 [0.000, 48.000], loss: 3.294738, mean_absolute_error: 0.477053, mean_q: 2.221840, mean_eps: 0.100000\n",
      "  38026/175000: episode: 1076, duration: 0.804s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 70.026 [22.000, 160.000], mean observation: 0.262 [0.000, 78.000], loss: 2.187293, mean_absolute_error: 0.456201, mean_q: 2.183293, mean_eps: 0.100000\n",
      "  38076/175000: episode: 1077, duration: 1.011s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 96.920 [1.000, 187.000], mean observation: 0.576 [0.000, 100.000], loss: 0.363156, mean_absolute_error: 0.403927, mean_q: 1.886736, mean_eps: 0.100000\n",
      "  38098/175000: episode: 1078, duration: 0.502s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 70.273 [19.000, 153.000], mean observation: 0.095 [0.000, 44.000], loss: 0.204136, mean_absolute_error: 0.371202, mean_q: 1.693286, mean_eps: 0.100000\n",
      "  38130/175000: episode: 1079, duration: 0.704s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 143.656 [15.000, 219.000], mean observation: 0.225 [0.000, 64.000], loss: 0.486624, mean_absolute_error: 0.426765, mean_q: 2.056154, mean_eps: 0.100000\n",
      "  38158/175000: episode: 1080, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 149.357 [9.000, 159.000], mean observation: 0.144 [0.000, 56.000], loss: 0.555721, mean_absolute_error: 0.412368, mean_q: 1.887002, mean_eps: 0.100000\n",
      "  38182/175000: episode: 1081, duration: 0.606s, episode steps: 24, steps per second: 40, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 109.917 [19.000, 209.000], mean observation: 0.130 [0.000, 48.000], loss: 0.108574, mean_absolute_error: 0.375107, mean_q: 1.653414, mean_eps: 0.100000\n",
      "  38216/175000: episode: 1082, duration: 0.870s, episode steps: 34, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 124.618 [51.000, 196.000], mean observation: 0.257 [0.000, 68.000], loss: 2.000323, mean_absolute_error: 0.436163, mean_q: 1.743665, mean_eps: 0.100000\n",
      "  38243/175000: episode: 1083, duration: 0.577s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 192.370 [0.000, 208.000], mean observation: 0.080 [0.000, 54.000], loss: 0.373491, mean_absolute_error: 0.434837, mean_q: 2.084011, mean_eps: 0.100000\n",
      "  38273/175000: episode: 1084, duration: 0.711s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 160.300 [28.000, 213.000], mean observation: 0.248 [0.000, 60.000], loss: 0.440009, mean_absolute_error: 0.389508, mean_q: 1.844108, mean_eps: 0.100000\n",
      "  38315/175000: episode: 1085, duration: 0.903s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 148.262 [5.000, 215.000], mean observation: 0.275 [0.000, 84.000], loss: 1.021350, mean_absolute_error: 0.388253, mean_q: 1.804690, mean_eps: 0.100000\n",
      "  38326/175000: episode: 1086, duration: 0.295s, episode steps: 11, steps per second: 37, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 150.636 [67.000, 159.000], mean observation: 0.029 [0.000, 22.000], loss: 1.499034, mean_absolute_error: 0.449347, mean_q: 2.013661, mean_eps: 0.100000\n",
      "  38346/175000: episode: 1087, duration: 0.579s, episode steps: 20, steps per second: 35, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 169.300 [67.000, 215.000], mean observation: 0.082 [0.000, 40.000], loss: 0.593802, mean_absolute_error: 0.476388, mean_q: 2.182410, mean_eps: 0.100000\n",
      "  38370/175000: episode: 1088, duration: 0.523s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 179.125 [7.000, 215.000], mean observation: 0.122 [0.000, 48.000], loss: 2.737619, mean_absolute_error: 0.398420, mean_q: 1.792970, mean_eps: 0.100000\n",
      "  38408/175000: episode: 1089, duration: 0.947s, episode steps: 38, steps per second: 40, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 155.711 [30.000, 219.000], mean observation: 0.289 [0.000, 76.000], loss: 0.281801, mean_absolute_error: 0.365925, mean_q: 1.738689, mean_eps: 0.100000\n",
      "  38445/175000: episode: 1090, duration: 0.835s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 147.378 [36.000, 219.000], mean observation: 0.311 [0.000, 74.000], loss: 4.085842, mean_absolute_error: 0.498658, mean_q: 2.263127, mean_eps: 0.100000\n",
      "  38464/175000: episode: 1091, duration: 0.356s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 102.842 [16.000, 219.000], mean observation: 0.130 [0.000, 38.000], loss: 0.739665, mean_absolute_error: 0.386067, mean_q: 1.859219, mean_eps: 0.100000\n",
      "  38495/175000: episode: 1092, duration: 0.629s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 111.548 [14.000, 197.000], mean observation: 0.199 [0.000, 62.000], loss: 1.523062, mean_absolute_error: 0.424906, mean_q: 2.073175, mean_eps: 0.100000\n",
      "  38545/175000: episode: 1093, duration: 0.952s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.140 [66.000, 219.000], mean observation: 0.419 [0.000, 100.000], loss: 0.963998, mean_absolute_error: 0.391870, mean_q: 1.877410, mean_eps: 0.100000\n",
      "  38584/175000: episode: 1094, duration: 0.811s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.769 [7.000, 185.000], mean observation: 0.307 [0.000, 78.000], loss: 0.305312, mean_absolute_error: 0.408575, mean_q: 1.717593, mean_eps: 0.100000\n",
      "  38621/175000: episode: 1095, duration: 0.944s, episode steps: 37, steps per second: 39, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.541 [14.000, 196.000], mean observation: 0.243 [0.000, 74.000], loss: 0.794022, mean_absolute_error: 0.524538, mean_q: 1.994553, mean_eps: 0.100000\n",
      "  38643/175000: episode: 1096, duration: 0.387s, episode steps: 22, steps per second: 57, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.091 [6.000, 198.000], mean observation: 0.161 [0.000, 44.000], loss: 5.863587, mean_absolute_error: 0.619860, mean_q: 2.745254, mean_eps: 0.100000\n",
      "  38667/175000: episode: 1097, duration: 0.520s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 101.667 [14.000, 137.000], mean observation: 0.143 [0.000, 48.000], loss: 1.980538, mean_absolute_error: 0.531577, mean_q: 2.206663, mean_eps: 0.100000\n",
      "  38707/175000: episode: 1098, duration: 1.041s, episode steps: 40, steps per second: 38, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.200 [14.000, 212.000], mean observation: 0.457 [0.000, 80.000], loss: 0.706321, mean_absolute_error: 0.449034, mean_q: 2.081301, mean_eps: 0.100000\n",
      "  38758/175000: episode: 1099, duration: 1.083s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 47.020 [2.000, 116.000], mean observation: 0.407 [0.000, 102.000], loss: 0.513703, mean_absolute_error: 0.383984, mean_q: 1.880576, mean_eps: 0.100000\n",
      "  38781/175000: episode: 1100, duration: 0.455s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 66.957 [25.000, 191.000], mean observation: 0.124 [0.000, 46.000], loss: 0.181921, mean_absolute_error: 0.340953, mean_q: 1.672332, mean_eps: 0.100000\n",
      "  38818/175000: episode: 1101, duration: 0.728s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 94.892 [42.000, 200.000], mean observation: 0.340 [0.000, 74.000], loss: 2.625719, mean_absolute_error: 0.427894, mean_q: 1.957959, mean_eps: 0.100000\n",
      "  38841/175000: episode: 1102, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 60.130 [25.000, 116.000], mean observation: 0.165 [0.000, 46.000], loss: 3.293710, mean_absolute_error: 0.534380, mean_q: 2.316172, mean_eps: 0.100000\n",
      "  38875/175000: episode: 1103, duration: 0.648s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 86.971 [23.000, 169.000], mean observation: 0.127 [0.000, 68.000], loss: 0.305549, mean_absolute_error: 0.438465, mean_q: 1.723641, mean_eps: 0.100000\n",
      "  38914/175000: episode: 1104, duration: 0.934s, episode steps: 39, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 67.564 [27.000, 189.000], mean observation: 0.195 [0.000, 78.000], loss: 0.398827, mean_absolute_error: 0.491564, mean_q: 1.980822, mean_eps: 0.100000\n",
      "  38949/175000: episode: 1105, duration: 0.695s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 54.857 [25.000, 207.000], mean observation: 0.299 [0.000, 70.000], loss: 0.489693, mean_absolute_error: 0.431876, mean_q: 1.749560, mean_eps: 0.100000\n",
      "  38982/175000: episode: 1106, duration: 0.645s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.515 [27.000, 217.000], mean observation: 0.195 [0.000, 66.000], loss: 2.601106, mean_absolute_error: 0.416521, mean_q: 1.801690, mean_eps: 0.100000\n",
      "  39006/175000: episode: 1107, duration: 0.491s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 166.792 [27.000, 217.000], mean observation: 0.073 [0.000, 48.000], loss: 0.664172, mean_absolute_error: 0.400841, mean_q: 1.973041, mean_eps: 0.100000\n",
      "  39035/175000: episode: 1108, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.483 [26.000, 217.000], mean observation: 0.150 [0.000, 58.000], loss: 2.689514, mean_absolute_error: 0.435286, mean_q: 2.188006, mean_eps: 0.100000\n",
      "  39063/175000: episode: 1109, duration: 0.691s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 144.714 [17.000, 217.000], mean observation: 0.216 [0.000, 56.000], loss: 5.254273, mean_absolute_error: 0.494587, mean_q: 2.482879, mean_eps: 0.100000\n",
      "  39097/175000: episode: 1110, duration: 0.733s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 194.912 [76.000, 217.000], mean observation: 0.301 [0.000, 68.000], loss: 0.625689, mean_absolute_error: 0.413659, mean_q: 2.211203, mean_eps: 0.100000\n",
      "  39117/175000: episode: 1111, duration: 0.438s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 195.300 [23.000, 217.000], mean observation: 0.106 [0.000, 40.000], loss: 4.077189, mean_absolute_error: 0.453287, mean_q: 2.196670, mean_eps: 0.100000\n",
      "  39162/175000: episode: 1112, duration: 0.885s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 177.089 [128.000, 217.000], mean observation: 0.312 [0.000, 90.000], loss: 0.896372, mean_absolute_error: 0.444347, mean_q: 2.110367, mean_eps: 0.100000\n",
      "  39209/175000: episode: 1113, duration: 0.989s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 161.638 [90.000, 224.000], mean observation: 0.443 [0.000, 94.000], loss: 2.402519, mean_absolute_error: 0.474683, mean_q: 2.204709, mean_eps: 0.100000\n",
      "  39246/175000: episode: 1114, duration: 0.774s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 77.541 [8.000, 201.000], mean observation: 0.313 [0.000, 74.000], loss: 3.662541, mean_absolute_error: 0.419675, mean_q: 1.834208, mean_eps: 0.100000\n",
      "  39284/175000: episode: 1115, duration: 0.833s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 86.158 [55.000, 224.000], mean observation: 0.343 [0.000, 76.000], loss: 0.340679, mean_absolute_error: 0.391323, mean_q: 1.849779, mean_eps: 0.100000\n",
      "  39335/175000: episode: 1116, duration: 0.985s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.137 [4.000, 188.000], mean observation: 0.591 [0.000, 102.000], loss: 0.553275, mean_absolute_error: 0.387326, mean_q: 1.889567, mean_eps: 0.100000\n",
      "  39390/175000: episode: 1117, duration: 1.100s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 161.927 [24.000, 188.000], mean observation: 0.286 [0.000, 110.000], loss: 0.152610, mean_absolute_error: 0.311895, mean_q: 1.544845, mean_eps: 0.100000\n",
      "  39436/175000: episode: 1118, duration: 0.982s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 139.587 [2.000, 181.000], mean observation: 0.188 [0.000, 92.000], loss: 0.444408, mean_absolute_error: 0.453040, mean_q: 2.275676, mean_eps: 0.100000\n",
      "  39472/175000: episode: 1119, duration: 0.760s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 132.750 [14.000, 211.000], mean observation: 0.321 [0.000, 72.000], loss: 1.903203, mean_absolute_error: 0.533468, mean_q: 2.634401, mean_eps: 0.100000\n",
      "  39514/175000: episode: 1120, duration: 0.938s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.690 [46.000, 188.000], mean observation: 0.209 [0.000, 84.000], loss: 1.421685, mean_absolute_error: 0.457858, mean_q: 2.331563, mean_eps: 0.100000\n",
      "  39542/175000: episode: 1121, duration: 0.636s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.857 [19.000, 188.000], mean observation: 0.245 [0.000, 56.000], loss: 0.711529, mean_absolute_error: 0.486538, mean_q: 2.481628, mean_eps: 0.100000\n",
      "  39575/175000: episode: 1122, duration: 0.675s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.727 [46.000, 188.000], mean observation: 0.261 [0.000, 66.000], loss: 2.874992, mean_absolute_error: 0.514994, mean_q: 2.509152, mean_eps: 0.100000\n",
      "  39616/175000: episode: 1123, duration: 0.901s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 132.024 [5.000, 202.000], mean observation: 0.396 [0.000, 82.000], loss: 1.573981, mean_absolute_error: 0.492395, mean_q: 2.353119, mean_eps: 0.100000\n",
      "  39663/175000: episode: 1124, duration: 0.944s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 179.064 [8.000, 202.000], mean observation: 0.372 [0.000, 94.000], loss: 0.602813, mean_absolute_error: 0.480385, mean_q: 2.457567, mean_eps: 0.100000\n",
      "  39697/175000: episode: 1125, duration: 0.785s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 188.500 [101.000, 202.000], mean observation: 0.172 [0.000, 68.000], loss: 0.335186, mean_absolute_error: 0.465078, mean_q: 2.408453, mean_eps: 0.100000\n",
      "  39753/175000: episode: 1126, duration: 1.061s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 93.196 [22.000, 196.000], mean observation: 0.531 [0.000, 112.000], loss: 2.148138, mean_absolute_error: 0.512449, mean_q: 2.429006, mean_eps: 0.100000\n",
      "  39786/175000: episode: 1127, duration: 0.684s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.212 [40.000, 222.000], mean observation: 0.250 [0.000, 66.000], loss: 0.475237, mean_absolute_error: 0.425192, mean_q: 1.915751, mean_eps: 0.100000\n",
      "  39812/175000: episode: 1128, duration: 0.544s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 159.885 [74.000, 208.000], mean observation: 0.152 [0.000, 52.000], loss: 0.281384, mean_absolute_error: 0.497431, mean_q: 2.489579, mean_eps: 0.100000\n",
      "  39847/175000: episode: 1129, duration: 0.818s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.000 [22.000, 222.000], mean observation: 0.213 [0.000, 70.000], loss: 0.398712, mean_absolute_error: 0.482997, mean_q: 2.459255, mean_eps: 0.100000\n",
      "  39882/175000: episode: 1130, duration: 0.744s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 136.314 [1.000, 224.000], mean observation: 0.326 [0.000, 70.000], loss: 0.447694, mean_absolute_error: 0.452479, mean_q: 2.326213, mean_eps: 0.100000\n",
      "  39919/175000: episode: 1131, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 156.649 [92.000, 184.000], mean observation: 0.226 [0.000, 74.000], loss: 1.041056, mean_absolute_error: 0.462224, mean_q: 2.297958, mean_eps: 0.100000\n",
      "  39968/175000: episode: 1132, duration: 1.126s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 156.857 [30.000, 222.000], mean observation: 0.577 [0.000, 98.000], loss: 1.421821, mean_absolute_error: 0.424614, mean_q: 1.959880, mean_eps: 0.100000\n",
      "  40007/175000: episode: 1133, duration: 0.839s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 62.718 [30.000, 222.000], mean observation: 0.256 [0.000, 78.000], loss: 0.543910, mean_absolute_error: 0.433730, mean_q: 2.137681, mean_eps: 0.100000\n",
      "  40052/175000: episode: 1134, duration: 1.162s, episode steps: 45, steps per second: 39, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.622 [17.000, 222.000], mean observation: 0.465 [0.000, 90.000], loss: 1.283066, mean_absolute_error: 0.383228, mean_q: 1.751626, mean_eps: 0.100000\n",
      "  40111/175000: episode: 1135, duration: 1.287s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.288 [1.000, 216.000], mean observation: 0.579 [0.000, 118.000], loss: 2.913383, mean_absolute_error: 0.474174, mean_q: 2.098608, mean_eps: 0.100000\n",
      "  40144/175000: episode: 1136, duration: 0.879s, episode steps: 33, steps per second: 38, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 122.333 [104.000, 140.000], mean observation: 0.117 [0.000, 66.000], loss: 0.320468, mean_absolute_error: 0.389704, mean_q: 1.747658, mean_eps: 0.100000\n",
      "  40174/175000: episode: 1137, duration: 0.802s, episode steps: 30, steps per second: 37, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 109.833 [30.000, 207.000], mean observation: 0.163 [0.000, 60.000], loss: 2.146541, mean_absolute_error: 0.384389, mean_q: 1.876046, mean_eps: 0.100000\n",
      "  40228/175000: episode: 1138, duration: 1.187s, episode steps: 54, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.130 [7.000, 214.000], mean observation: 0.746 [0.000, 108.000], loss: 5.144331, mean_absolute_error: 0.423846, mean_q: 2.059471, mean_eps: 0.100000\n",
      "  40246/175000: episode: 1139, duration: 0.432s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 207.556 [98.000, 214.000], mean observation: 0.044 [0.000, 36.000], loss: 4.057935, mean_absolute_error: 0.409110, mean_q: 2.026552, mean_eps: 0.100000\n",
      "  40275/175000: episode: 1140, duration: 0.785s, episode steps: 29, steps per second: 37, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 180.172 [2.000, 218.000], mean observation: 0.084 [0.000, 58.000], loss: 4.874667, mean_absolute_error: 0.424251, mean_q: 2.018797, mean_eps: 0.100000\n",
      "  40305/175000: episode: 1141, duration: 0.771s, episode steps: 30, steps per second: 39, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 131.333 [2.000, 214.000], mean observation: 0.170 [0.000, 60.000], loss: 1.221432, mean_absolute_error: 0.417955, mean_q: 2.109011, mean_eps: 0.100000\n",
      "  40336/175000: episode: 1142, duration: 0.860s, episode steps: 31, steps per second: 36, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 162.710 [2.000, 214.000], mean observation: 0.132 [0.000, 62.000], loss: 0.466011, mean_absolute_error: 0.368579, mean_q: 1.954690, mean_eps: 0.100000\n",
      "  40373/175000: episode: 1143, duration: 0.982s, episode steps: 37, steps per second: 38, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 109.595 [2.000, 128.000], mean observation: 0.163 [0.000, 74.000], loss: 1.053861, mean_absolute_error: 0.365291, mean_q: 2.043710, mean_eps: 0.100000\n",
      "  40416/175000: episode: 1144, duration: 1.025s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.256 [2.000, 202.000], mean observation: 0.582 [0.000, 86.000], loss: 5.564116, mean_absolute_error: 0.419378, mean_q: 2.147796, mean_eps: 0.100000\n",
      "  40438/175000: episode: 1145, duration: 0.577s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.818 [12.000, 152.000], mean observation: 0.103 [0.000, 44.000], loss: 4.797705, mean_absolute_error: 0.413852, mean_q: 2.074689, mean_eps: 0.100000\n",
      "  40472/175000: episode: 1146, duration: 0.918s, episode steps: 34, steps per second: 37, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 60.059 [12.000, 210.000], mean observation: 0.171 [0.000, 68.000], loss: 2.408540, mean_absolute_error: 0.376178, mean_q: 1.812653, mean_eps: 0.100000\n",
      "  40525/175000: episode: 1147, duration: 1.163s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 99.868 [12.000, 221.000], mean observation: 0.406 [0.000, 106.000], loss: 4.018116, mean_absolute_error: 0.375763, mean_q: 1.799172, mean_eps: 0.100000\n",
      "  40567/175000: episode: 1148, duration: 0.873s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 72.024 [12.000, 167.000], mean observation: 0.176 [0.000, 84.000], loss: 6.125892, mean_absolute_error: 0.463485, mean_q: 2.188241, mean_eps: 0.100000\n",
      "  40608/175000: episode: 1149, duration: 0.991s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 64.854 [0.000, 217.000], mean observation: 0.230 [0.000, 82.000], loss: 8.082304, mean_absolute_error: 0.469823, mean_q: 2.068748, mean_eps: 0.100000\n",
      "  40647/175000: episode: 1150, duration: 0.938s, episode steps: 39, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 175.128 [21.000, 221.000], mean observation: 0.366 [0.000, 78.000], loss: 3.605019, mean_absolute_error: 0.406569, mean_q: 1.925922, mean_eps: 0.100000\n",
      "  40683/175000: episode: 1151, duration: 0.788s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 171.444 [25.000, 221.000], mean observation: 0.288 [0.000, 72.000], loss: 0.221041, mean_absolute_error: 0.343694, mean_q: 1.652480, mean_eps: 0.100000\n",
      "  40715/175000: episode: 1152, duration: 0.627s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 125.375 [25.000, 221.000], mean observation: 0.200 [0.000, 64.000], loss: 7.561351, mean_absolute_error: 0.508872, mean_q: 2.233221, mean_eps: 0.100000\n",
      "  40742/175000: episode: 1153, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 79.667 [25.000, 211.000], mean observation: 0.179 [0.000, 54.000], loss: 3.045541, mean_absolute_error: 0.545051, mean_q: 2.510636, mean_eps: 0.100000\n",
      "  40785/175000: episode: 1154, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.395 [25.000, 216.000], mean observation: 0.369 [0.000, 86.000], loss: 2.473823, mean_absolute_error: 0.467942, mean_q: 2.086476, mean_eps: 0.100000\n",
      "  40819/175000: episode: 1155, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.118 [13.000, 222.000], mean observation: 0.287 [0.000, 68.000], loss: 3.230809, mean_absolute_error: 0.519287, mean_q: 2.266144, mean_eps: 0.100000\n",
      "  40841/175000: episode: 1156, duration: 0.467s, episode steps: 22, steps per second: 47, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 58.727 [2.000, 140.000], mean observation: 0.158 [0.000, 44.000], loss: 2.563908, mean_absolute_error: 0.486754, mean_q: 2.210629, mean_eps: 0.100000\n",
      "  40882/175000: episode: 1157, duration: 0.929s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 28.951 [2.000, 164.000], mean observation: 0.316 [0.000, 82.000], loss: 5.270693, mean_absolute_error: 0.505480, mean_q: 2.287762, mean_eps: 0.100000\n",
      "  40925/175000: episode: 1158, duration: 1.029s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 67.209 [2.000, 220.000], mean observation: 0.429 [0.000, 86.000], loss: 4.141234, mean_absolute_error: 0.490537, mean_q: 2.268334, mean_eps: 0.100000\n",
      "  40965/175000: episode: 1159, duration: 0.808s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 186.850 [25.000, 222.000], mean observation: 0.164 [0.000, 80.000], loss: 3.082929, mean_absolute_error: 0.458091, mean_q: 2.126931, mean_eps: 0.100000\n",
      "  41023/175000: episode: 1160, duration: 1.107s, episode steps: 58, steps per second: 52, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 46.448 [1.000, 222.000], mean observation: 0.608 [0.000, 116.000], loss: 3.359857, mean_absolute_error: 0.433456, mean_q: 2.030130, mean_eps: 0.100000\n",
      "  41051/175000: episode: 1161, duration: 0.550s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 142.071 [74.000, 222.000], mean observation: 0.264 [0.000, 56.000], loss: 0.177408, mean_absolute_error: 0.313465, mean_q: 1.610644, mean_eps: 0.100000\n",
      "  41088/175000: episode: 1162, duration: 0.841s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 102.432 [90.000, 103.000], mean observation: 0.120 [0.000, 74.000], loss: 0.167641, mean_absolute_error: 0.340289, mean_q: 1.579546, mean_eps: 0.100000\n",
      "  41130/175000: episode: 1163, duration: 0.879s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.500 [34.000, 140.000], mean observation: 0.240 [0.000, 84.000], loss: 1.632545, mean_absolute_error: 0.402801, mean_q: 1.780929, mean_eps: 0.100000\n",
      "  41177/175000: episode: 1164, duration: 0.942s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 118.319 [16.000, 190.000], mean observation: 0.507 [0.000, 94.000], loss: 1.422751, mean_absolute_error: 0.413743, mean_q: 1.889969, mean_eps: 0.100000\n",
      "  41221/175000: episode: 1165, duration: 0.881s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.477 [21.000, 222.000], mean observation: 0.357 [0.000, 88.000], loss: 1.165675, mean_absolute_error: 0.384890, mean_q: 1.821150, mean_eps: 0.100000\n",
      "  41265/175000: episode: 1166, duration: 0.927s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.114 [46.000, 222.000], mean observation: 0.553 [0.000, 88.000], loss: 13.545392, mean_absolute_error: 0.597798, mean_q: 2.623497, mean_eps: 0.100000\n",
      "  41298/175000: episode: 1167, duration: 0.673s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 90.758 [32.000, 196.000], mean observation: 0.174 [0.000, 66.000], loss: 0.420701, mean_absolute_error: 0.427529, mean_q: 2.074564, mean_eps: 0.100000\n",
      "  41325/175000: episode: 1168, duration: 0.544s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 84.778 [14.000, 224.000], mean observation: 0.160 [0.000, 54.000], loss: 0.217933, mean_absolute_error: 0.377032, mean_q: 1.820366, mean_eps: 0.100000\n",
      "  41354/175000: episode: 1169, duration: 0.583s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.172 [4.000, 190.000], mean observation: 0.219 [0.000, 58.000], loss: 2.201852, mean_absolute_error: 0.453440, mean_q: 2.184471, mean_eps: 0.100000\n",
      "  41397/175000: episode: 1170, duration: 0.884s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.721 [25.000, 219.000], mean observation: 0.424 [0.000, 86.000], loss: 2.513341, mean_absolute_error: 0.374910, mean_q: 1.775444, mean_eps: 0.100000\n",
      "  41436/175000: episode: 1171, duration: 0.831s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 135.667 [30.000, 190.000], mean observation: 0.243 [0.000, 78.000], loss: 0.102134, mean_absolute_error: 0.316083, mean_q: 1.608620, mean_eps: 0.100000\n",
      "  41487/175000: episode: 1172, duration: 1.023s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.157 [14.000, 218.000], mean observation: 0.280 [0.000, 102.000], loss: 3.503754, mean_absolute_error: 0.338787, mean_q: 1.638244, mean_eps: 0.100000\n",
      "  41512/175000: episode: 1173, duration: 0.562s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 132.000 [54.000, 210.000], mean observation: 0.186 [0.000, 50.000], loss: 3.096036, mean_absolute_error: 0.443432, mean_q: 2.035592, mean_eps: 0.100000\n",
      "  41555/175000: episode: 1174, duration: 0.965s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 75.395 [17.000, 172.000], mean observation: 0.431 [0.000, 86.000], loss: 5.093032, mean_absolute_error: 0.490149, mean_q: 2.139858, mean_eps: 0.100000\n",
      "  41592/175000: episode: 1175, duration: 0.795s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 41.027 [20.000, 202.000], mean observation: 0.311 [0.000, 74.000], loss: 4.153787, mean_absolute_error: 0.409944, mean_q: 1.885994, mean_eps: 0.100000\n",
      "  41633/175000: episode: 1176, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 59.171 [20.000, 193.000], mean observation: 0.429 [0.000, 82.000], loss: 3.601349, mean_absolute_error: 0.443416, mean_q: 2.086829, mean_eps: 0.100000\n",
      "  41684/175000: episode: 1177, duration: 1.045s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 51.980 [20.000, 223.000], mean observation: 0.369 [0.000, 102.000], loss: 1.033950, mean_absolute_error: 0.380825, mean_q: 1.884913, mean_eps: 0.100000\n",
      "  41708/175000: episode: 1178, duration: 0.669s, episode steps: 24, steps per second: 36, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 83.250 [54.000, 207.000], mean observation: 0.178 [0.000, 48.000], loss: 1.762019, mean_absolute_error: 0.422497, mean_q: 2.099072, mean_eps: 0.100000\n",
      "  41756/175000: episode: 1179, duration: 1.155s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 182.938 [54.000, 223.000], mean observation: 0.431 [0.000, 96.000], loss: 4.359509, mean_absolute_error: 0.440931, mean_q: 1.929239, mean_eps: 0.100000\n",
      "  41803/175000: episode: 1180, duration: 0.962s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 155.957 [5.000, 202.000], mean observation: 0.499 [0.000, 94.000], loss: 0.141775, mean_absolute_error: 0.328379, mean_q: 1.504038, mean_eps: 0.100000\n",
      "  41836/175000: episode: 1181, duration: 0.716s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.182 [33.000, 214.000], mean observation: 0.268 [0.000, 66.000], loss: 0.246419, mean_absolute_error: 0.351999, mean_q: 1.677075, mean_eps: 0.100000\n",
      "  41872/175000: episode: 1182, duration: 0.813s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 147.750 [40.000, 185.000], mean observation: 0.310 [0.000, 72.000], loss: 9.646737, mean_absolute_error: 0.498464, mean_q: 2.183691, mean_eps: 0.100000\n",
      "  41909/175000: episode: 1183, duration: 0.755s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.378 [10.000, 194.000], mean observation: 0.332 [0.000, 74.000], loss: 2.676664, mean_absolute_error: 0.428879, mean_q: 1.984334, mean_eps: 0.100000\n",
      "  41943/175000: episode: 1184, duration: 0.801s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.088 [50.000, 208.000], mean observation: 0.346 [0.000, 68.000], loss: 8.066359, mean_absolute_error: 0.529394, mean_q: 2.360398, mean_eps: 0.100000\n",
      "  41971/175000: episode: 1185, duration: 0.568s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 132.607 [2.000, 159.000], mean observation: 0.176 [0.000, 56.000], loss: 2.205248, mean_absolute_error: 0.405455, mean_q: 1.990331, mean_eps: 0.100000\n",
      "  42006/175000: episode: 1186, duration: 0.805s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 177.257 [61.000, 216.000], mean observation: 0.251 [0.000, 70.000], loss: 4.419576, mean_absolute_error: 0.459257, mean_q: 2.126104, mean_eps: 0.100000\n",
      "  42037/175000: episode: 1187, duration: 0.627s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 152.032 [20.000, 211.000], mean observation: 0.276 [0.000, 62.000], loss: 3.000350, mean_absolute_error: 0.446769, mean_q: 2.042874, mean_eps: 0.100000\n",
      "  42061/175000: episode: 1188, duration: 0.525s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.542 [86.000, 140.000], mean observation: 0.186 [0.000, 48.000], loss: 4.042609, mean_absolute_error: 0.450241, mean_q: 2.173163, mean_eps: 0.100000\n",
      "  42105/175000: episode: 1189, duration: 0.916s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 119.091 [61.000, 213.000], mean observation: 0.302 [0.000, 88.000], loss: 2.883256, mean_absolute_error: 0.438203, mean_q: 2.108170, mean_eps: 0.100000\n",
      "  42134/175000: episode: 1190, duration: 0.600s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 51.552 [27.000, 162.000], mean observation: 0.180 [0.000, 58.000], loss: 3.217082, mean_absolute_error: 0.478066, mean_q: 2.194699, mean_eps: 0.100000\n",
      "  42178/175000: episode: 1191, duration: 0.930s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 36.659 [27.000, 109.000], mean observation: 0.312 [0.000, 88.000], loss: 3.052137, mean_absolute_error: 0.527853, mean_q: 2.380878, mean_eps: 0.100000\n",
      "  42218/175000: episode: 1192, duration: 0.864s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 56.575 [18.000, 210.000], mean observation: 0.286 [0.000, 80.000], loss: 6.305719, mean_absolute_error: 0.529504, mean_q: 2.399448, mean_eps: 0.100000\n",
      "  42259/175000: episode: 1193, duration: 0.783s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 57.878 [24.000, 213.000], mean observation: 0.390 [0.000, 82.000], loss: 1.516139, mean_absolute_error: 0.408639, mean_q: 2.022669, mean_eps: 0.100000\n",
      "  42275/175000: episode: 1194, duration: 0.328s, episode steps: 16, steps per second: 49, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 135.125 [24.000, 189.000], mean observation: 0.062 [0.000, 32.000], loss: 0.230983, mean_absolute_error: 0.321879, mean_q: 1.727226, mean_eps: 0.100000\n",
      "  42305/175000: episode: 1195, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 153.100 [24.000, 219.000], mean observation: 0.170 [0.000, 60.000], loss: 7.484634, mean_absolute_error: 0.411218, mean_q: 1.995621, mean_eps: 0.100000\n",
      "  42330/175000: episode: 1196, duration: 0.461s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 161.920 [93.000, 209.000], mean observation: 0.100 [0.000, 50.000], loss: 5.791261, mean_absolute_error: 0.606525, mean_q: 2.871103, mean_eps: 0.100000\n",
      "  42376/175000: episode: 1197, duration: 1.081s, episode steps: 46, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 166.565 [32.000, 196.000], mean observation: 0.318 [0.000, 92.000], loss: 0.142679, mean_absolute_error: 0.333878, mean_q: 1.803605, mean_eps: 0.100000\n",
      "  42422/175000: episode: 1198, duration: 0.992s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 68.543 [24.000, 157.000], mean observation: 0.267 [0.000, 92.000], loss: 3.453154, mean_absolute_error: 0.543568, mean_q: 2.551800, mean_eps: 0.100000\n",
      "  42460/175000: episode: 1199, duration: 0.828s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.526 [28.000, 217.000], mean observation: 0.272 [0.000, 76.000], loss: 0.125101, mean_absolute_error: 0.321970, mean_q: 1.566316, mean_eps: 0.100000\n",
      "  42491/175000: episode: 1200, duration: 0.644s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 124.774 [6.000, 187.000], mean observation: 0.290 [0.000, 62.000], loss: 4.908935, mean_absolute_error: 0.449570, mean_q: 2.069656, mean_eps: 0.100000\n",
      "  42545/175000: episode: 1201, duration: 1.180s, episode steps: 54, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 88.963 [35.000, 187.000], mean observation: 0.325 [0.000, 108.000], loss: 4.014646, mean_absolute_error: 0.451869, mean_q: 2.159744, mean_eps: 0.100000\n",
      "  42584/175000: episode: 1202, duration: 0.839s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 80.641 [4.000, 200.000], mean observation: 0.387 [0.000, 78.000], loss: 0.577158, mean_absolute_error: 0.395831, mean_q: 2.047513, mean_eps: 0.100000\n",
      "  42623/175000: episode: 1203, duration: 0.788s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.795 [5.000, 222.000], mean observation: 0.271 [0.000, 78.000], loss: 3.471261, mean_absolute_error: 0.566201, mean_q: 2.666537, mean_eps: 0.100000\n",
      "  42654/175000: episode: 1204, duration: 0.673s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 44.355 [5.000, 200.000], mean observation: 0.165 [0.000, 62.000], loss: 4.026572, mean_absolute_error: 0.536797, mean_q: 2.584600, mean_eps: 0.100000\n",
      "  42692/175000: episode: 1205, duration: 0.804s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 48.842 [11.000, 60.000], mean observation: 0.141 [0.000, 76.000], loss: 4.052021, mean_absolute_error: 0.527013, mean_q: 2.437695, mean_eps: 0.100000\n",
      "  42725/175000: episode: 1206, duration: 0.773s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 157.091 [60.000, 221.000], mean observation: 0.249 [0.000, 66.000], loss: 2.395375, mean_absolute_error: 0.561429, mean_q: 2.636087, mean_eps: 0.100000\n",
      "  42771/175000: episode: 1207, duration: 0.983s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 76.326 [19.000, 200.000], mean observation: 0.218 [0.000, 92.000], loss: 1.394422, mean_absolute_error: 0.453290, mean_q: 2.211818, mean_eps: 0.100000\n",
      "  42801/175000: episode: 1208, duration: 0.618s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.067 [7.000, 140.000], mean observation: 0.329 [0.000, 60.000], loss: 2.619155, mean_absolute_error: 0.546570, mean_q: 2.695083, mean_eps: 0.100000\n",
      "  42819/175000: episode: 1209, duration: 0.409s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 39.167 [22.000, 140.000], mean observation: 0.113 [0.000, 36.000], loss: 2.871495, mean_absolute_error: 0.495109, mean_q: 2.447755, mean_eps: 0.100000\n",
      "  42849/175000: episode: 1210, duration: 0.728s, episode steps: 30, steps per second: 41, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 28.400 [3.000, 159.000], mean observation: 0.268 [0.000, 60.000], loss: 9.779677, mean_absolute_error: 0.548226, mean_q: 2.477645, mean_eps: 0.100000\n",
      "  42856/175000: episode: 1211, duration: 0.202s, episode steps: 7, steps per second: 35, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 135.143 [58.000, 148.000], mean observation: 0.027 [0.000, 14.000], loss: 0.113461, mean_absolute_error: 0.342706, mean_q: 1.710265, mean_eps: 0.100000\n",
      "  42896/175000: episode: 1212, duration: 0.939s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 120.200 [28.000, 222.000], mean observation: 0.337 [0.000, 80.000], loss: 0.890440, mean_absolute_error: 0.420308, mean_q: 2.084811, mean_eps: 0.100000\n",
      "  42917/175000: episode: 1213, duration: 0.524s, episode steps: 21, steps per second: 40, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 80.667 [6.000, 140.000], mean observation: 0.136 [0.000, 42.000], loss: 5.672188, mean_absolute_error: 0.458731, mean_q: 2.238208, mean_eps: 0.100000\n",
      "  42960/175000: episode: 1214, duration: 0.916s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 53.209 [28.000, 172.000], mean observation: 0.400 [0.000, 86.000], loss: 4.767280, mean_absolute_error: 0.424113, mean_q: 2.121513, mean_eps: 0.100000\n",
      "  42996/175000: episode: 1215, duration: 0.787s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 93.472 [16.000, 176.000], mean observation: 0.182 [0.000, 72.000], loss: 4.822450, mean_absolute_error: 0.445573, mean_q: 2.254131, mean_eps: 0.100000\n",
      "  43029/175000: episode: 1216, duration: 0.752s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 130.152 [16.000, 176.000], mean observation: 0.208 [0.000, 66.000], loss: 2.083253, mean_absolute_error: 0.432492, mean_q: 2.399431, mean_eps: 0.100000\n",
      "  43088/175000: episode: 1217, duration: 1.262s, episode steps: 59, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 129.797 [2.000, 189.000], mean observation: 0.409 [0.000, 118.000], loss: 5.482405, mean_absolute_error: 0.475011, mean_q: 2.439195, mean_eps: 0.100000\n",
      "  43124/175000: episode: 1218, duration: 0.819s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.778 [2.000, 30.000], mean observation: 0.084 [0.000, 72.000], loss: 8.687422, mean_absolute_error: 0.538059, mean_q: 2.766378, mean_eps: 0.100000\n",
      "  43164/175000: episode: 1219, duration: 0.977s, episode steps: 40, steps per second: 41, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 76.750 [2.000, 183.000], mean observation: 0.206 [0.000, 80.000], loss: 1.423102, mean_absolute_error: 0.354884, mean_q: 2.075368, mean_eps: 0.100000\n",
      "  43209/175000: episode: 1220, duration: 0.989s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.733 [5.000, 152.000], mean observation: 0.324 [0.000, 90.000], loss: 3.283796, mean_absolute_error: 0.480153, mean_q: 2.549921, mean_eps: 0.100000\n",
      "  43256/175000: episode: 1221, duration: 0.923s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.745 [2.000, 219.000], mean observation: 0.324 [0.000, 94.000], loss: 5.476370, mean_absolute_error: 0.469419, mean_q: 2.496622, mean_eps: 0.100000\n",
      "  43306/175000: episode: 1222, duration: 1.161s, episode steps: 50, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 126.360 [21.000, 162.000], mean observation: 0.481 [0.000, 100.000], loss: 0.472405, mean_absolute_error: 0.464268, mean_q: 2.459166, mean_eps: 0.100000\n",
      "  43338/175000: episode: 1223, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 135.812 [46.000, 182.000], mean observation: 0.278 [0.000, 64.000], loss: 1.991958, mean_absolute_error: 0.461151, mean_q: 2.455903, mean_eps: 0.100000\n",
      "  43370/175000: episode: 1224, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.938 [3.000, 186.000], mean observation: 0.208 [0.000, 64.000], loss: 4.494615, mean_absolute_error: 0.594464, mean_q: 3.013190, mean_eps: 0.100000\n",
      "  43398/175000: episode: 1225, duration: 0.572s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 54.857 [3.000, 105.000], mean observation: 0.160 [0.000, 56.000], loss: 8.290328, mean_absolute_error: 0.734080, mean_q: 3.636357, mean_eps: 0.100000\n",
      "  43414/175000: episode: 1226, duration: 0.343s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 109.750 [3.000, 224.000], mean observation: 0.114 [0.000, 32.000], loss: 0.368867, mean_absolute_error: 0.418799, mean_q: 2.422562, mean_eps: 0.100000\n",
      "  43450/175000: episode: 1227, duration: 0.703s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.056 [2.000, 224.000], mean observation: 0.210 [0.000, 72.000], loss: 3.088450, mean_absolute_error: 0.570328, mean_q: 3.079220, mean_eps: 0.100000\n",
      "  43477/175000: episode: 1228, duration: 0.570s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 157.741 [2.000, 224.000], mean observation: 0.176 [0.000, 54.000], loss: 4.004288, mean_absolute_error: 0.600717, mean_q: 3.221952, mean_eps: 0.100000\n",
      "  43509/175000: episode: 1229, duration: 0.693s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.469 [47.000, 222.000], mean observation: 0.191 [0.000, 64.000], loss: 2.058065, mean_absolute_error: 0.518184, mean_q: 2.909089, mean_eps: 0.100000\n",
      "  43546/175000: episode: 1230, duration: 0.738s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 95.243 [53.000, 222.000], mean observation: 0.166 [0.000, 74.000], loss: 1.950913, mean_absolute_error: 0.521967, mean_q: 2.896326, mean_eps: 0.100000\n",
      "  43578/175000: episode: 1231, duration: 0.611s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 156.188 [53.000, 222.000], mean observation: 0.185 [0.000, 64.000], loss: 3.149549, mean_absolute_error: 0.500601, mean_q: 2.774648, mean_eps: 0.100000\n",
      "  43621/175000: episode: 1232, duration: 0.903s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 163.791 [39.000, 222.000], mean observation: 0.208 [0.000, 86.000], loss: 0.760605, mean_absolute_error: 0.387615, mean_q: 2.374838, mean_eps: 0.100000\n",
      "  43642/175000: episode: 1233, duration: 0.475s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 63.190 [53.000, 204.000], mean observation: 0.069 [0.000, 42.000], loss: 0.660919, mean_absolute_error: 0.400335, mean_q: 2.496142, mean_eps: 0.100000\n",
      "  43681/175000: episode: 1234, duration: 0.883s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.538 [53.000, 219.000], mean observation: 0.247 [0.000, 78.000], loss: 4.657092, mean_absolute_error: 0.655178, mean_q: 3.600782, mean_eps: 0.100000\n",
      "  43720/175000: episode: 1235, duration: 0.905s, episode steps: 39, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.615 [5.000, 200.000], mean observation: 0.366 [0.000, 78.000], loss: 3.873622, mean_absolute_error: 0.423090, mean_q: 2.555205, mean_eps: 0.100000\n",
      "  43767/175000: episode: 1236, duration: 0.964s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 114.894 [53.000, 206.000], mean observation: 0.353 [0.000, 94.000], loss: 2.540675, mean_absolute_error: 0.529901, mean_q: 3.156990, mean_eps: 0.100000\n",
      "  43825/175000: episode: 1237, duration: 1.254s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 147.397 [28.000, 218.000], mean observation: 0.419 [0.000, 116.000], loss: 8.265475, mean_absolute_error: 0.616753, mean_q: 3.562024, mean_eps: 0.100000\n",
      "  43854/175000: episode: 1238, duration: 0.598s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.310 [53.000, 183.000], mean observation: 0.151 [0.000, 58.000], loss: 0.199475, mean_absolute_error: 0.396416, mean_q: 2.823287, mean_eps: 0.100000\n",
      "  43876/175000: episode: 1239, duration: 0.456s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 86.455 [83.000, 154.000], mean observation: 0.086 [0.000, 44.000], loss: 6.791997, mean_absolute_error: 0.535395, mean_q: 3.344936, mean_eps: 0.100000\n",
      "  43922/175000: episode: 1240, duration: 1.017s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 65.826 [28.000, 224.000], mean observation: 0.244 [0.000, 92.000], loss: 2.322396, mean_absolute_error: 0.573456, mean_q: 3.406792, mean_eps: 0.100000\n",
      "  43944/175000: episode: 1241, duration: 0.520s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 57.182 [21.000, 140.000], mean observation: 0.161 [0.000, 44.000], loss: 0.270003, mean_absolute_error: 0.497685, mean_q: 3.008490, mean_eps: 0.100000\n",
      "  43973/175000: episode: 1242, duration: 0.679s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 130.034 [14.000, 199.000], mean observation: 0.088 [0.000, 58.000], loss: 1.823331, mean_absolute_error: 0.424153, mean_q: 2.706211, mean_eps: 0.100000\n",
      "  44007/175000: episode: 1243, duration: 0.621s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 181.500 [3.000, 215.000], mean observation: 0.248 [0.000, 68.000], loss: 0.546248, mean_absolute_error: 0.403148, mean_q: 2.635035, mean_eps: 0.100000\n",
      "  44045/175000: episode: 1244, duration: 0.795s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.132 [24.000, 217.000], mean observation: 0.241 [0.000, 76.000], loss: 3.948570, mean_absolute_error: 0.422447, mean_q: 2.578354, mean_eps: 0.100000\n",
      "  44076/175000: episode: 1245, duration: 0.647s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 145.097 [6.000, 211.000], mean observation: 0.203 [0.000, 62.000], loss: 2.320244, mean_absolute_error: 0.463895, mean_q: 2.860930, mean_eps: 0.100000\n",
      "  44117/175000: episode: 1246, duration: 0.896s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.829 [69.000, 199.000], mean observation: 0.273 [0.000, 82.000], loss: 1.569967, mean_absolute_error: 0.457306, mean_q: 2.714450, mean_eps: 0.100000\n",
      "  44158/175000: episode: 1247, duration: 0.801s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.146 [2.000, 215.000], mean observation: 0.316 [0.000, 82.000], loss: 1.634096, mean_absolute_error: 0.405119, mean_q: 2.476941, mean_eps: 0.100000\n",
      "  44198/175000: episode: 1248, duration: 0.842s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 182.000 [15.000, 215.000], mean observation: 0.387 [0.000, 80.000], loss: 1.778603, mean_absolute_error: 0.530112, mean_q: 3.138502, mean_eps: 0.100000\n",
      "  44228/175000: episode: 1249, duration: 0.681s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.267 [1.000, 218.000], mean observation: 0.270 [0.000, 60.000], loss: 2.282068, mean_absolute_error: 0.528808, mean_q: 2.834562, mean_eps: 0.100000\n",
      "  44272/175000: episode: 1250, duration: 0.944s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.295 [9.000, 219.000], mean observation: 0.469 [0.000, 88.000], loss: 0.502266, mean_absolute_error: 0.470561, mean_q: 2.607858, mean_eps: 0.100000\n",
      "  44316/175000: episode: 1251, duration: 0.992s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 119.795 [17.000, 199.000], mean observation: 0.346 [0.000, 88.000], loss: 3.114926, mean_absolute_error: 0.487217, mean_q: 2.755363, mean_eps: 0.100000\n",
      "  44356/175000: episode: 1252, duration: 0.855s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 142.250 [0.000, 223.000], mean observation: 0.249 [0.000, 80.000], loss: 3.989068, mean_absolute_error: 0.486181, mean_q: 2.727052, mean_eps: 0.100000\n",
      "  44399/175000: episode: 1253, duration: 0.820s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 123.907 [32.000, 183.000], mean observation: 0.170 [0.000, 86.000], loss: 1.756224, mean_absolute_error: 0.459958, mean_q: 2.570846, mean_eps: 0.100000\n",
      "  44433/175000: episode: 1254, duration: 0.720s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 72.000 [49.000, 186.000], mean observation: 0.178 [0.000, 68.000], loss: 3.333979, mean_absolute_error: 0.533105, mean_q: 3.029151, mean_eps: 0.100000\n",
      "  44467/175000: episode: 1255, duration: 0.700s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 53.235 [49.000, 193.000], mean observation: 0.083 [0.000, 68.000], loss: 1.834539, mean_absolute_error: 0.420290, mean_q: 2.448847, mean_eps: 0.100000\n",
      "  44493/175000: episode: 1256, duration: 0.534s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 68.577 [27.000, 211.000], mean observation: 0.116 [0.000, 52.000], loss: 1.968554, mean_absolute_error: 0.459706, mean_q: 2.710154, mean_eps: 0.100000\n",
      "  44528/175000: episode: 1257, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 51.600 [3.000, 136.000], mean observation: 0.099 [0.000, 70.000], loss: 1.946780, mean_absolute_error: 0.495660, mean_q: 2.932773, mean_eps: 0.100000\n",
      "  44565/175000: episode: 1258, duration: 0.796s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.622 [11.000, 183.000], mean observation: 0.190 [0.000, 74.000], loss: 0.176445, mean_absolute_error: 0.351328, mean_q: 2.080100, mean_eps: 0.100000\n",
      "  44603/175000: episode: 1259, duration: 0.720s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.974 [11.000, 222.000], mean observation: 0.107 [0.000, 76.000], loss: 0.192029, mean_absolute_error: 0.351711, mean_q: 2.126264, mean_eps: 0.100000\n",
      "  44639/175000: episode: 1260, duration: 0.703s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 19.611 [11.000, 141.000], mean observation: 0.200 [0.000, 72.000], loss: 0.794498, mean_absolute_error: 0.404592, mean_q: 2.257928, mean_eps: 0.100000\n",
      "  44675/175000: episode: 1261, duration: 0.702s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 113.528 [11.000, 204.000], mean observation: 0.305 [0.000, 72.000], loss: 3.916768, mean_absolute_error: 0.580850, mean_q: 3.194670, mean_eps: 0.100000\n",
      "  44702/175000: episode: 1262, duration: 0.547s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 51.037 [11.000, 219.000], mean observation: 0.178 [0.000, 54.000], loss: 6.077535, mean_absolute_error: 0.575780, mean_q: 3.155228, mean_eps: 0.100000\n",
      "  44733/175000: episode: 1263, duration: 0.623s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 37.290 [11.000, 210.000], mean observation: 0.230 [0.000, 62.000], loss: 0.212999, mean_absolute_error: 0.463012, mean_q: 2.796545, mean_eps: 0.100000\n",
      "  44756/175000: episode: 1264, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 35.522 [11.000, 188.000], mean observation: 0.132 [0.000, 46.000], loss: 2.278330, mean_absolute_error: 0.472425, mean_q: 2.943165, mean_eps: 0.100000\n",
      "  44781/175000: episode: 1265, duration: 0.525s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 13.840 [11.000, 37.000], mean observation: 0.097 [0.000, 50.000], loss: 4.056911, mean_absolute_error: 0.539592, mean_q: 3.139714, mean_eps: 0.100000\n",
      "  44821/175000: episode: 1266, duration: 0.837s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 26.375 [11.000, 219.000], mean observation: 0.183 [0.000, 80.000], loss: 8.710400, mean_absolute_error: 0.566399, mean_q: 3.193938, mean_eps: 0.100000\n",
      "  44850/175000: episode: 1267, duration: 0.581s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 48.483 [11.000, 221.000], mean observation: 0.130 [0.000, 58.000], loss: 2.398418, mean_absolute_error: 0.545498, mean_q: 3.156063, mean_eps: 0.100000\n",
      "  44878/175000: episode: 1268, duration: 0.582s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.321 [11.000, 161.000], mean observation: 0.124 [0.000, 56.000], loss: 5.745887, mean_absolute_error: 0.582717, mean_q: 3.291665, mean_eps: 0.100000\n",
      "  44913/175000: episode: 1269, duration: 0.775s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.800 [11.000, 81.000], mean observation: 0.141 [0.000, 70.000], loss: 6.112890, mean_absolute_error: 0.467690, mean_q: 2.699285, mean_eps: 0.100000\n",
      "  44940/175000: episode: 1270, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.741 [53.000, 211.000], mean observation: 0.165 [0.000, 54.000], loss: 3.581352, mean_absolute_error: 0.464552, mean_q: 2.692848, mean_eps: 0.100000\n",
      "  44982/175000: episode: 1271, duration: 0.831s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.571 [49.000, 211.000], mean observation: 0.292 [0.000, 84.000], loss: 8.425577, mean_absolute_error: 0.537372, mean_q: 3.048779, mean_eps: 0.100000\n",
      "  45025/175000: episode: 1272, duration: 0.818s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 146.581 [49.000, 216.000], mean observation: 0.149 [0.000, 86.000], loss: 1.945188, mean_absolute_error: 0.409282, mean_q: 2.419596, mean_eps: 0.100000\n",
      "  45056/175000: episode: 1273, duration: 0.665s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 194.935 [43.000, 216.000], mean observation: 0.142 [0.000, 62.000], loss: 4.839852, mean_absolute_error: 0.540482, mean_q: 3.095657, mean_eps: 0.100000\n",
      "  45103/175000: episode: 1274, duration: 0.982s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 95.979 [43.000, 216.000], mean observation: 0.426 [0.000, 94.000], loss: 0.922804, mean_absolute_error: 0.474387, mean_q: 2.788541, mean_eps: 0.100000\n",
      "  45146/175000: episode: 1275, duration: 0.917s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.419 [16.000, 216.000], mean observation: 0.242 [0.000, 86.000], loss: 1.395662, mean_absolute_error: 0.490199, mean_q: 2.896722, mean_eps: 0.100000\n",
      "  45177/175000: episode: 1276, duration: 0.675s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 132.452 [0.000, 216.000], mean observation: 0.128 [0.000, 62.000], loss: 6.617824, mean_absolute_error: 0.577823, mean_q: 3.220451, mean_eps: 0.100000\n",
      "  45203/175000: episode: 1277, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 179.115 [107.000, 182.000], mean observation: 0.063 [0.000, 52.000], loss: 0.263658, mean_absolute_error: 0.413763, mean_q: 2.425485, mean_eps: 0.100000\n",
      "  45234/175000: episode: 1278, duration: 0.663s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 169.935 [35.000, 182.000], mean observation: 0.152 [0.000, 62.000], loss: 4.415204, mean_absolute_error: 0.517676, mean_q: 2.929167, mean_eps: 0.100000\n",
      "  45266/175000: episode: 1279, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 98.688 [2.000, 218.000], mean observation: 0.207 [0.000, 64.000], loss: 2.935054, mean_absolute_error: 0.527206, mean_q: 2.903256, mean_eps: 0.100000\n",
      "  45295/175000: episode: 1280, duration: 0.603s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 173.897 [41.000, 182.000], mean observation: 0.138 [0.000, 58.000], loss: 5.078295, mean_absolute_error: 0.645647, mean_q: 3.456616, mean_eps: 0.100000\n",
      "  45348/175000: episode: 1281, duration: 1.200s, episode steps: 53, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 144.075 [49.000, 189.000], mean observation: 0.241 [0.000, 106.000], loss: 6.978615, mean_absolute_error: 0.599133, mean_q: 3.061131, mean_eps: 0.100000\n",
      "  45376/175000: episode: 1282, duration: 0.658s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 169.964 [13.000, 182.000], mean observation: 0.082 [0.000, 56.000], loss: 1.095484, mean_absolute_error: 0.521090, mean_q: 2.661598, mean_eps: 0.100000\n",
      "  45417/175000: episode: 1283, duration: 0.976s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 170.439 [37.000, 182.000], mean observation: 0.110 [0.000, 82.000], loss: 1.946255, mean_absolute_error: 0.547197, mean_q: 2.778563, mean_eps: 0.100000\n",
      "  45455/175000: episode: 1284, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.737 [3.000, 191.000], mean observation: 0.173 [0.000, 76.000], loss: 0.309175, mean_absolute_error: 0.457116, mean_q: 2.261442, mean_eps: 0.100000\n",
      "  45491/175000: episode: 1285, duration: 0.790s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.083 [11.000, 212.000], mean observation: 0.256 [0.000, 72.000], loss: 0.953197, mean_absolute_error: 0.534486, mean_q: 2.750731, mean_eps: 0.100000\n",
      "  45511/175000: episode: 1286, duration: 0.412s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.250 [11.000, 191.000], mean observation: 0.113 [0.000, 40.000], loss: 3.151748, mean_absolute_error: 0.624947, mean_q: 3.210395, mean_eps: 0.100000\n",
      "  45535/175000: episode: 1287, duration: 0.498s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 172.167 [87.000, 182.000], mean observation: 0.072 [0.000, 48.000], loss: 0.343609, mean_absolute_error: 0.532791, mean_q: 2.744599, mean_eps: 0.100000\n",
      "  45568/175000: episode: 1288, duration: 0.784s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 127.364 [5.000, 182.000], mean observation: 0.202 [0.000, 66.000], loss: 0.610852, mean_absolute_error: 0.531226, mean_q: 2.783368, mean_eps: 0.100000\n",
      "  45602/175000: episode: 1289, duration: 0.779s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 165.853 [41.000, 182.000], mean observation: 0.108 [0.000, 68.000], loss: 2.397456, mean_absolute_error: 0.521017, mean_q: 2.697017, mean_eps: 0.100000\n",
      "  45628/175000: episode: 1290, duration: 0.587s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.269 [80.000, 135.000], mean observation: 0.093 [0.000, 52.000], loss: 0.424348, mean_absolute_error: 0.516902, mean_q: 2.785217, mean_eps: 0.100000\n",
      "  45652/175000: episode: 1291, duration: 0.601s, episode steps: 24, steps per second: 40, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 82.708 [80.000, 138.000], mean observation: 0.102 [0.000, 48.000], loss: 0.170596, mean_absolute_error: 0.469628, mean_q: 2.622819, mean_eps: 0.100000\n",
      "  45699/175000: episode: 1292, duration: 1.170s, episode steps: 47, steps per second: 40, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 69.191 [19.000, 170.000], mean observation: 0.332 [0.000, 94.000], loss: 4.646150, mean_absolute_error: 0.527588, mean_q: 2.856086, mean_eps: 0.100000\n",
      "  45743/175000: episode: 1293, duration: 1.022s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.136 [16.000, 192.000], mean observation: 0.269 [0.000, 88.000], loss: 7.619244, mean_absolute_error: 0.510177, mean_q: 2.715968, mean_eps: 0.100000\n",
      "  45784/175000: episode: 1294, duration: 1.037s, episode steps: 41, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 61.122 [28.000, 215.000], mean observation: 0.242 [0.000, 82.000], loss: 0.527177, mean_absolute_error: 0.434533, mean_q: 2.521634, mean_eps: 0.100000\n",
      "  45811/175000: episode: 1295, duration: 0.594s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 129.185 [11.000, 156.000], mean observation: 0.160 [0.000, 54.000], loss: 2.092845, mean_absolute_error: 0.427458, mean_q: 2.461393, mean_eps: 0.100000\n",
      "  45849/175000: episode: 1296, duration: 0.997s, episode steps: 38, steps per second: 38, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.605 [11.000, 156.000], mean observation: 0.392 [0.000, 76.000], loss: 4.921760, mean_absolute_error: 0.511120, mean_q: 2.812434, mean_eps: 0.100000\n",
      "  45891/175000: episode: 1297, duration: 0.936s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 96.238 [11.000, 193.000], mean observation: 0.306 [0.000, 84.000], loss: 5.406665, mean_absolute_error: 0.564420, mean_q: 3.110890, mean_eps: 0.100000\n",
      "  45933/175000: episode: 1298, duration: 0.985s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.595 [11.000, 223.000], mean observation: 0.634 [0.000, 84.000], loss: 4.216367, mean_absolute_error: 0.510073, mean_q: 2.897108, mean_eps: 0.100000\n",
      "  45969/175000: episode: 1299, duration: 0.957s, episode steps: 36, steps per second: 38, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 93.250 [7.000, 208.000], mean observation: 0.421 [0.000, 72.000], loss: 8.940468, mean_absolute_error: 0.516944, mean_q: 2.835510, mean_eps: 0.100000\n",
      "  46005/175000: episode: 1300, duration: 0.800s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 121.889 [11.000, 222.000], mean observation: 0.330 [0.000, 72.000], loss: 0.329564, mean_absolute_error: 0.443677, mean_q: 2.625647, mean_eps: 0.100000\n",
      "  46040/175000: episode: 1301, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.800 [16.000, 163.000], mean observation: 0.211 [0.000, 70.000], loss: 0.424804, mean_absolute_error: 0.457763, mean_q: 2.689158, mean_eps: 0.100000\n",
      "  46084/175000: episode: 1302, duration: 0.983s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.000 [0.000, 212.000], mean observation: 0.262 [0.000, 88.000], loss: 1.024603, mean_absolute_error: 0.482219, mean_q: 2.868059, mean_eps: 0.100000\n",
      "  46130/175000: episode: 1303, duration: 0.948s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.109 [20.000, 158.000], mean observation: 0.276 [0.000, 92.000], loss: 1.420863, mean_absolute_error: 0.442311, mean_q: 2.700012, mean_eps: 0.100000\n",
      "  46171/175000: episode: 1304, duration: 0.895s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.415 [18.000, 151.000], mean observation: 0.228 [0.000, 82.000], loss: 0.463002, mean_absolute_error: 0.388197, mean_q: 2.492075, mean_eps: 0.100000\n",
      "  46212/175000: episode: 1305, duration: 0.971s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.098 [18.000, 221.000], mean observation: 0.253 [0.000, 82.000], loss: 0.815895, mean_absolute_error: 0.449280, mean_q: 2.668922, mean_eps: 0.100000\n",
      "  46234/175000: episode: 1306, duration: 0.516s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 122.182 [18.000, 221.000], mean observation: 0.129 [0.000, 44.000], loss: 2.573150, mean_absolute_error: 0.625566, mean_q: 3.502190, mean_eps: 0.100000\n",
      "  46257/175000: episode: 1307, duration: 0.550s, episode steps: 23, steps per second: 42, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 95.391 [34.000, 224.000], mean observation: 0.111 [0.000, 46.000], loss: 3.460170, mean_absolute_error: 0.576612, mean_q: 3.256637, mean_eps: 0.100000\n",
      "  46283/175000: episode: 1308, duration: 0.544s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 85.885 [0.000, 214.000], mean observation: 0.157 [0.000, 52.000], loss: 1.521285, mean_absolute_error: 0.435787, mean_q: 2.579296, mean_eps: 0.100000\n",
      "  46331/175000: episode: 1309, duration: 1.115s, episode steps: 48, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.812 [0.000, 212.000], mean observation: 0.412 [0.000, 96.000], loss: 1.269192, mean_absolute_error: 0.504660, mean_q: 2.930367, mean_eps: 0.100000\n",
      "  46379/175000: episode: 1310, duration: 0.964s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 87.042 [0.000, 188.000], mean observation: 0.414 [0.000, 96.000], loss: 4.856408, mean_absolute_error: 0.684927, mean_q: 3.739371, mean_eps: 0.100000\n",
      "  46417/175000: episode: 1311, duration: 0.863s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 64.816 [0.000, 223.000], mean observation: 0.210 [0.000, 76.000], loss: 2.925447, mean_absolute_error: 0.565538, mean_q: 3.294419, mean_eps: 0.100000\n",
      "  46459/175000: episode: 1312, duration: 0.852s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.786 [0.000, 205.000], mean observation: 0.418 [0.000, 84.000], loss: 0.377337, mean_absolute_error: 0.598062, mean_q: 3.511036, mean_eps: 0.100000\n",
      "  46496/175000: episode: 1313, duration: 0.839s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 97.676 [47.000, 221.000], mean observation: 0.226 [0.000, 74.000], loss: 11.676919, mean_absolute_error: 0.833735, mean_q: 4.410857, mean_eps: 0.100000\n",
      "  46537/175000: episode: 1314, duration: 0.937s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.780 [11.000, 218.000], mean observation: 0.327 [0.000, 82.000], loss: 3.188343, mean_absolute_error: 0.550278, mean_q: 3.406333, mean_eps: 0.100000\n",
      "  46574/175000: episode: 1315, duration: 0.760s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.459 [1.000, 97.000], mean observation: 0.211 [0.000, 74.000], loss: 4.857095, mean_absolute_error: 0.706508, mean_q: 4.101905, mean_eps: 0.100000\n",
      "  46608/175000: episode: 1316, duration: 0.781s, episode steps: 34, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.471 [1.000, 199.000], mean observation: 0.268 [0.000, 68.000], loss: 1.753264, mean_absolute_error: 0.490931, mean_q: 3.222777, mean_eps: 0.100000\n",
      "  46653/175000: episode: 1317, duration: 1.064s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.844 [1.000, 213.000], mean observation: 0.341 [0.000, 90.000], loss: 2.232551, mean_absolute_error: 0.482467, mean_q: 3.085295, mean_eps: 0.100000\n",
      "  46675/175000: episode: 1318, duration: 0.422s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 87.273 [1.000, 214.000], mean observation: 0.155 [0.000, 44.000], loss: 0.174658, mean_absolute_error: 0.398859, mean_q: 2.727041, mean_eps: 0.100000\n",
      "  46687/175000: episode: 1319, duration: 0.235s, episode steps: 12, steps per second: 51, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 210.583 [206.000, 213.000], mean observation: 0.046 [0.000, 24.000], loss: 5.307523, mean_absolute_error: 0.661880, mean_q: 3.685313, mean_eps: 0.100000\n",
      "  46738/175000: episode: 1320, duration: 1.164s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 181.529 [29.000, 213.000], mean observation: 0.368 [0.000, 102.000], loss: 0.679656, mean_absolute_error: 0.368000, mean_q: 2.540163, mean_eps: 0.100000\n",
      "  46780/175000: episode: 1321, duration: 1.045s, episode steps: 42, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.429 [44.000, 214.000], mean observation: 0.335 [0.000, 84.000], loss: 1.046857, mean_absolute_error: 0.429306, mean_q: 2.799351, mean_eps: 0.100000\n",
      "  46813/175000: episode: 1322, duration: 0.849s, episode steps: 33, steps per second: 39, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.061 [1.000, 162.000], mean observation: 0.189 [0.000, 66.000], loss: 0.690635, mean_absolute_error: 0.444727, mean_q: 2.905831, mean_eps: 0.100000\n",
      "  46857/175000: episode: 1323, duration: 0.983s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.273 [40.000, 135.000], mean observation: 0.247 [0.000, 88.000], loss: 3.361691, mean_absolute_error: 0.484555, mean_q: 3.106783, mean_eps: 0.100000\n",
      "  46895/175000: episode: 1324, duration: 0.834s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.816 [25.000, 218.000], mean observation: 0.305 [0.000, 76.000], loss: 0.238055, mean_absolute_error: 0.375230, mean_q: 2.485620, mean_eps: 0.100000\n",
      "  46923/175000: episode: 1325, duration: 0.632s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.714 [39.000, 204.000], mean observation: 0.142 [0.000, 56.000], loss: 0.194212, mean_absolute_error: 0.394188, mean_q: 2.614317, mean_eps: 0.100000\n",
      "  46956/175000: episode: 1326, duration: 0.803s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.394 [26.000, 151.000], mean observation: 0.294 [0.000, 66.000], loss: 11.195768, mean_absolute_error: 0.681972, mean_q: 3.735626, mean_eps: 0.100000\n",
      "  46998/175000: episode: 1327, duration: 1.002s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 72.976 [4.000, 139.000], mean observation: 0.167 [0.000, 84.000], loss: 3.398360, mean_absolute_error: 0.539636, mean_q: 3.212929, mean_eps: 0.100000\n",
      "  47039/175000: episode: 1328, duration: 0.953s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.537 [3.000, 221.000], mean observation: 0.307 [0.000, 82.000], loss: 1.418336, mean_absolute_error: 0.452562, mean_q: 2.841273, mean_eps: 0.100000\n",
      "  47057/175000: episode: 1329, duration: 0.403s, episode steps: 18, steps per second: 45, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 106.500 [32.000, 211.000], mean observation: 0.144 [0.000, 36.000], loss: 6.976594, mean_absolute_error: 0.801844, mean_q: 4.460484, mean_eps: 0.100000\n",
      "  47089/175000: episode: 1330, duration: 0.742s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 75.656 [9.000, 203.000], mean observation: 0.282 [0.000, 64.000], loss: 5.132276, mean_absolute_error: 0.638696, mean_q: 3.716978, mean_eps: 0.100000\n",
      "  47128/175000: episode: 1331, duration: 0.846s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 69.462 [9.000, 197.000], mean observation: 0.219 [0.000, 78.000], loss: 1.279539, mean_absolute_error: 0.577551, mean_q: 3.538867, mean_eps: 0.100000\n",
      "  47186/175000: episode: 1332, duration: 1.369s, episode steps: 58, steps per second: 42, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 57.569 [11.000, 196.000], mean observation: 0.343 [0.000, 116.000], loss: 2.603953, mean_absolute_error: 0.513073, mean_q: 3.197390, mean_eps: 0.100000\n",
      "  47238/175000: episode: 1333, duration: 1.151s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.538 [50.000, 214.000], mean observation: 0.233 [0.000, 104.000], loss: 6.235244, mean_absolute_error: 0.593729, mean_q: 3.522736, mean_eps: 0.100000\n",
      "  47267/175000: episode: 1334, duration: 0.637s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.379 [25.000, 209.000], mean observation: 0.123 [0.000, 58.000], loss: 4.362929, mean_absolute_error: 0.463936, mean_q: 2.944398, mean_eps: 0.100000\n",
      "  47288/175000: episode: 1335, duration: 0.514s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 179.190 [67.000, 209.000], mean observation: 0.077 [0.000, 42.000], loss: 0.199241, mean_absolute_error: 0.488334, mean_q: 3.175151, mean_eps: 0.100000\n",
      "  47329/175000: episode: 1336, duration: 0.973s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.195 [38.000, 209.000], mean observation: 0.202 [0.000, 82.000], loss: 4.342822, mean_absolute_error: 0.611689, mean_q: 3.755630, mean_eps: 0.100000\n",
      "  47358/175000: episode: 1337, duration: 0.630s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.414 [3.000, 209.000], mean observation: 0.104 [0.000, 58.000], loss: 1.013974, mean_absolute_error: 0.507070, mean_q: 3.163554, mean_eps: 0.100000\n",
      "  47397/175000: episode: 1338, duration: 0.882s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 145.538 [15.000, 209.000], mean observation: 0.265 [0.000, 78.000], loss: 0.338862, mean_absolute_error: 0.445960, mean_q: 2.924083, mean_eps: 0.100000\n",
      "  47445/175000: episode: 1339, duration: 1.131s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.729 [9.000, 217.000], mean observation: 0.515 [0.000, 96.000], loss: 1.765265, mean_absolute_error: 0.589128, mean_q: 3.607549, mean_eps: 0.100000\n",
      "  47488/175000: episode: 1340, duration: 0.961s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.628 [15.000, 201.000], mean observation: 0.252 [0.000, 86.000], loss: 11.191080, mean_absolute_error: 0.591195, mean_q: 3.296627, mean_eps: 0.100000\n",
      "  47547/175000: episode: 1341, duration: 1.288s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 87.475 [15.000, 152.000], mean observation: 0.516 [0.000, 118.000], loss: 2.749710, mean_absolute_error: 0.554546, mean_q: 3.305110, mean_eps: 0.100000\n",
      "  47595/175000: episode: 1342, duration: 1.127s, episode steps: 48, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 136.312 [27.000, 163.000], mean observation: 0.243 [0.000, 96.000], loss: 3.465869, mean_absolute_error: 0.466724, mean_q: 2.896026, mean_eps: 0.100000\n",
      "  47635/175000: episode: 1343, duration: 0.883s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 82.600 [15.000, 208.000], mean observation: 0.345 [0.000, 80.000], loss: 1.012712, mean_absolute_error: 0.496001, mean_q: 3.043369, mean_eps: 0.100000\n",
      "  47663/175000: episode: 1344, duration: 0.620s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 145.786 [56.000, 211.000], mean observation: 0.129 [0.000, 56.000], loss: 3.330934, mean_absolute_error: 0.604684, mean_q: 3.447396, mean_eps: 0.100000\n",
      "  47702/175000: episode: 1345, duration: 0.844s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 150.513 [69.000, 167.000], mean observation: 0.164 [0.000, 78.000], loss: 2.379299, mean_absolute_error: 0.451481, mean_q: 2.675409, mean_eps: 0.100000\n",
      "  47740/175000: episode: 1346, duration: 0.907s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 131.526 [69.000, 177.000], mean observation: 0.158 [0.000, 76.000], loss: 1.131413, mean_absolute_error: 0.492580, mean_q: 2.936021, mean_eps: 0.100000\n",
      "  47782/175000: episode: 1347, duration: 0.996s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.905 [52.000, 211.000], mean observation: 0.273 [0.000, 84.000], loss: 3.372980, mean_absolute_error: 0.632922, mean_q: 3.454270, mean_eps: 0.100000\n",
      "  47807/175000: episode: 1348, duration: 0.518s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 109.400 [65.000, 152.000], mean observation: 0.126 [0.000, 50.000], loss: 0.936654, mean_absolute_error: 0.441098, mean_q: 2.640427, mean_eps: 0.100000\n",
      "  47847/175000: episode: 1349, duration: 0.885s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 116.175 [7.000, 163.000], mean observation: 0.378 [0.000, 80.000], loss: 6.565986, mean_absolute_error: 0.627090, mean_q: 3.387035, mean_eps: 0.100000\n",
      "  47886/175000: episode: 1350, duration: 0.812s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 108.667 [2.000, 170.000], mean observation: 0.208 [0.000, 78.000], loss: 1.891782, mean_absolute_error: 0.618006, mean_q: 3.482416, mean_eps: 0.100000\n",
      "  47911/175000: episode: 1351, duration: 0.499s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 114.720 [45.000, 190.000], mean observation: 0.158 [0.000, 50.000], loss: 0.985854, mean_absolute_error: 0.596674, mean_q: 3.489968, mean_eps: 0.100000\n",
      "  47951/175000: episode: 1352, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.125 [14.000, 202.000], mean observation: 0.209 [0.000, 80.000], loss: 3.311729, mean_absolute_error: 0.583524, mean_q: 3.489506, mean_eps: 0.100000\n",
      "  48002/175000: episode: 1353, duration: 1.115s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.627 [14.000, 192.000], mean observation: 0.306 [0.000, 102.000], loss: 2.803753, mean_absolute_error: 0.578449, mean_q: 3.507605, mean_eps: 0.100000\n",
      "  48038/175000: episode: 1354, duration: 0.884s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.972 [12.000, 158.000], mean observation: 0.108 [0.000, 72.000], loss: 0.566469, mean_absolute_error: 0.486370, mean_q: 3.125243, mean_eps: 0.100000\n",
      "  48075/175000: episode: 1355, duration: 0.808s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.703 [11.000, 107.000], mean observation: 0.162 [0.000, 74.000], loss: 0.453430, mean_absolute_error: 0.557462, mean_q: 3.402035, mean_eps: 0.100000\n",
      "  48122/175000: episode: 1356, duration: 1.035s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 74.532 [11.000, 204.000], mean observation: 0.290 [0.000, 94.000], loss: 0.737003, mean_absolute_error: 0.493852, mean_q: 3.173508, mean_eps: 0.100000\n",
      "  48160/175000: episode: 1357, duration: 0.841s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.289 [3.000, 174.000], mean observation: 0.266 [0.000, 76.000], loss: 8.462336, mean_absolute_error: 0.647488, mean_q: 3.692753, mean_eps: 0.100000\n",
      "  48191/175000: episode: 1358, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 56.032 [11.000, 111.000], mean observation: 0.153 [0.000, 62.000], loss: 2.409754, mean_absolute_error: 0.579192, mean_q: 3.476103, mean_eps: 0.100000\n",
      "  48226/175000: episode: 1359, duration: 0.773s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 57.914 [11.000, 173.000], mean observation: 0.227 [0.000, 70.000], loss: 3.262756, mean_absolute_error: 0.584861, mean_q: 3.499800, mean_eps: 0.100000\n",
      "  48274/175000: episode: 1360, duration: 1.050s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 104.771 [7.000, 216.000], mean observation: 0.505 [0.000, 96.000], loss: 2.920008, mean_absolute_error: 0.537299, mean_q: 3.297148, mean_eps: 0.100000\n",
      "  48327/175000: episode: 1361, duration: 1.234s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 127.736 [15.000, 177.000], mean observation: 0.185 [0.000, 106.000], loss: 0.367258, mean_absolute_error: 0.422552, mean_q: 2.830739, mean_eps: 0.100000\n",
      "  48351/175000: episode: 1362, duration: 0.573s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 136.375 [7.000, 142.000], mean observation: 0.071 [0.000, 48.000], loss: 0.761032, mean_absolute_error: 0.492132, mean_q: 3.195595, mean_eps: 0.100000\n",
      "  48398/175000: episode: 1363, duration: 1.026s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.021 [67.000, 224.000], mean observation: 0.175 [0.000, 94.000], loss: 0.783751, mean_absolute_error: 0.557010, mean_q: 3.561195, mean_eps: 0.100000\n",
      "  48445/175000: episode: 1364, duration: 0.979s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 142.936 [56.000, 218.000], mean observation: 0.229 [0.000, 94.000], loss: 12.697129, mean_absolute_error: 0.714843, mean_q: 4.072659, mean_eps: 0.100000\n",
      "  48479/175000: episode: 1365, duration: 0.730s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.353 [1.000, 222.000], mean observation: 0.124 [0.000, 68.000], loss: 1.359800, mean_absolute_error: 0.596463, mean_q: 3.603343, mean_eps: 0.100000\n",
      "  48517/175000: episode: 1366, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 150.921 [38.000, 222.000], mean observation: 0.202 [0.000, 76.000], loss: 1.797494, mean_absolute_error: 0.589552, mean_q: 3.591598, mean_eps: 0.100000\n",
      "  48558/175000: episode: 1367, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.659 [11.000, 222.000], mean observation: 0.417 [0.000, 82.000], loss: 0.277906, mean_absolute_error: 0.414327, mean_q: 2.816772, mean_eps: 0.100000\n",
      "  48590/175000: episode: 1368, duration: 0.711s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 168.375 [48.000, 222.000], mean observation: 0.190 [0.000, 64.000], loss: 4.663086, mean_absolute_error: 0.688053, mean_q: 4.057435, mean_eps: 0.100000\n",
      "  48624/175000: episode: 1369, duration: 0.798s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 143.265 [9.000, 222.000], mean observation: 0.208 [0.000, 68.000], loss: 0.336955, mean_absolute_error: 0.471004, mean_q: 3.078533, mean_eps: 0.100000\n",
      "  48666/175000: episode: 1370, duration: 0.970s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 191.690 [53.000, 222.000], mean observation: 0.152 [0.000, 84.000], loss: 2.696778, mean_absolute_error: 0.551961, mean_q: 3.367146, mean_eps: 0.100000\n",
      "  48698/175000: episode: 1371, duration: 0.682s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 169.312 [11.000, 222.000], mean observation: 0.124 [0.000, 64.000], loss: 1.220486, mean_absolute_error: 0.485206, mean_q: 2.998784, mean_eps: 0.100000\n",
      "  48740/175000: episode: 1372, duration: 0.990s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 117.452 [16.000, 222.000], mean observation: 0.403 [0.000, 84.000], loss: 4.734957, mean_absolute_error: 0.621077, mean_q: 3.535571, mean_eps: 0.100000\n",
      "  48770/175000: episode: 1373, duration: 0.669s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 137.633 [47.000, 204.000], mean observation: 0.110 [0.000, 60.000], loss: 4.805708, mean_absolute_error: 0.666726, mean_q: 3.758099, mean_eps: 0.100000\n",
      "  48805/175000: episode: 1374, duration: 0.804s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.971 [74.000, 224.000], mean observation: 0.136 [0.000, 70.000], loss: 2.393486, mean_absolute_error: 0.575007, mean_q: 3.402181, mean_eps: 0.100000\n",
      "  48861/175000: episode: 1375, duration: 1.270s, episode steps: 56, steps per second: 44, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 81.107 [3.000, 201.000], mean observation: 0.303 [0.000, 112.000], loss: 3.779722, mean_absolute_error: 0.531438, mean_q: 3.094923, mean_eps: 0.100000\n",
      "  48886/175000: episode: 1376, duration: 0.619s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 110.560 [21.000, 192.000], mean observation: 0.072 [0.000, 50.000], loss: 1.851297, mean_absolute_error: 0.663544, mean_q: 3.821379, mean_eps: 0.100000\n",
      "  48921/175000: episode: 1377, duration: 0.865s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.486 [21.000, 181.000], mean observation: 0.132 [0.000, 70.000], loss: 3.295253, mean_absolute_error: 0.698919, mean_q: 3.863283, mean_eps: 0.100000\n",
      "  48965/175000: episode: 1378, duration: 0.970s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 115.818 [11.000, 218.000], mean observation: 0.359 [0.000, 88.000], loss: 1.944403, mean_absolute_error: 0.587513, mean_q: 3.489508, mean_eps: 0.100000\n",
      "  48971/175000: episode: 1379, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: -1.000, mean reward: -0.167 [-1.000, 0.000], mean action: 107.833 [83.000, 141.000], mean observation: 0.027 [0.000, 12.000], loss: 0.153134, mean_absolute_error: 0.373544, mean_q: 2.529405, mean_eps: 0.100000\n",
      "  49010/175000: episode: 1380, duration: 0.874s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.179 [11.000, 181.000], mean observation: 0.339 [0.000, 78.000], loss: 4.128558, mean_absolute_error: 0.565253, mean_q: 3.463062, mean_eps: 0.100000\n",
      "  49044/175000: episode: 1381, duration: 0.822s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.059 [9.000, 175.000], mean observation: 0.302 [0.000, 68.000], loss: 0.453251, mean_absolute_error: 0.454715, mean_q: 3.168888, mean_eps: 0.100000\n",
      "  49073/175000: episode: 1382, duration: 0.605s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 127.966 [11.000, 211.000], mean observation: 0.207 [0.000, 58.000], loss: 0.806276, mean_absolute_error: 0.479887, mean_q: 3.278107, mean_eps: 0.100000\n",
      "  49089/175000: episode: 1383, duration: 0.311s, episode steps: 16, steps per second: 51, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 94.812 [8.000, 141.000], mean observation: 0.104 [0.000, 32.000], loss: 0.232820, mean_absolute_error: 0.399527, mean_q: 3.011750, mean_eps: 0.100000\n",
      "  49127/175000: episode: 1384, duration: 0.790s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.421 [11.000, 211.000], mean observation: 0.293 [0.000, 76.000], loss: 0.180105, mean_absolute_error: 0.383174, mean_q: 2.802491, mean_eps: 0.100000\n",
      "  49142/175000: episode: 1385, duration: 0.374s, episode steps: 15, steps per second: 40, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 118.533 [11.000, 181.000], mean observation: 0.076 [0.000, 30.000], loss: 0.271376, mean_absolute_error: 0.423998, mean_q: 2.895520, mean_eps: 0.100000\n",
      "  49174/175000: episode: 1386, duration: 0.715s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 103.906 [12.000, 222.000], mean observation: 0.236 [0.000, 64.000], loss: 1.119892, mean_absolute_error: 0.519998, mean_q: 3.318883, mean_eps: 0.100000\n",
      "  49197/175000: episode: 1387, duration: 0.566s, episode steps: 23, steps per second: 41, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 107.522 [11.000, 181.000], mean observation: 0.180 [0.000, 46.000], loss: 3.128134, mean_absolute_error: 0.580399, mean_q: 3.543434, mean_eps: 0.100000\n",
      "  49241/175000: episode: 1388, duration: 0.987s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.523 [26.000, 223.000], mean observation: 0.261 [0.000, 88.000], loss: 1.958570, mean_absolute_error: 0.616530, mean_q: 3.726809, mean_eps: 0.100000\n",
      "  49275/175000: episode: 1389, duration: 0.707s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.882 [9.000, 181.000], mean observation: 0.292 [0.000, 68.000], loss: 0.261725, mean_absolute_error: 0.502897, mean_q: 3.340910, mean_eps: 0.100000\n",
      "  49295/175000: episode: 1390, duration: 0.443s, episode steps: 20, steps per second: 45, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 153.100 [141.000, 181.000], mean observation: 0.101 [0.000, 40.000], loss: 0.172388, mean_absolute_error: 0.445235, mean_q: 3.040928, mean_eps: 0.100000\n",
      "  49323/175000: episode: 1391, duration: 0.636s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 60.679 [10.000, 141.000], mean observation: 0.202 [0.000, 56.000], loss: 2.328072, mean_absolute_error: 0.587189, mean_q: 3.833028, mean_eps: 0.100000\n",
      "  49360/175000: episode: 1392, duration: 0.951s, episode steps: 37, steps per second: 39, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 92.270 [24.000, 223.000], mean observation: 0.230 [0.000, 74.000], loss: 1.635115, mean_absolute_error: 0.537501, mean_q: 3.650488, mean_eps: 0.100000\n",
      "  49396/175000: episode: 1393, duration: 0.868s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.083 [24.000, 193.000], mean observation: 0.232 [0.000, 72.000], loss: 2.268496, mean_absolute_error: 0.478471, mean_q: 3.420859, mean_eps: 0.100000\n",
      "  49445/175000: episode: 1394, duration: 1.124s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 108.061 [20.000, 223.000], mean observation: 0.346 [0.000, 98.000], loss: 0.767728, mean_absolute_error: 0.486190, mean_q: 3.427422, mean_eps: 0.100000\n",
      "  49467/175000: episode: 1395, duration: 0.440s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 35.955 [24.000, 128.000], mean observation: 0.066 [0.000, 44.000], loss: 3.761217, mean_absolute_error: 0.691532, mean_q: 4.213485, mean_eps: 0.100000\n",
      "  49513/175000: episode: 1396, duration: 1.016s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.000 [24.000, 192.000], mean observation: 0.294 [0.000, 92.000], loss: 2.329607, mean_absolute_error: 0.517189, mean_q: 3.453793, mean_eps: 0.100000\n",
      "  49558/175000: episode: 1397, duration: 1.023s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.733 [24.000, 193.000], mean observation: 0.298 [0.000, 90.000], loss: 0.641137, mean_absolute_error: 0.515277, mean_q: 3.542118, mean_eps: 0.100000\n",
      "  49608/175000: episode: 1398, duration: 1.087s, episode steps: 50, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 39.060 [2.000, 148.000], mean observation: 0.170 [0.000, 100.000], loss: 0.306081, mean_absolute_error: 0.470099, mean_q: 3.280974, mean_eps: 0.100000\n",
      "  49653/175000: episode: 1399, duration: 0.926s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 27.178 [24.000, 167.000], mean observation: 0.118 [0.000, 90.000], loss: 0.827941, mean_absolute_error: 0.431409, mean_q: 3.137305, mean_eps: 0.100000\n",
      "  49684/175000: episode: 1400, duration: 0.788s, episode steps: 31, steps per second: 39, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.581 [24.000, 141.000], mean observation: 0.100 [0.000, 62.000], loss: 8.102303, mean_absolute_error: 0.719983, mean_q: 4.293749, mean_eps: 0.100000\n",
      "  49719/175000: episode: 1401, duration: 0.804s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.543 [24.000, 181.000], mean observation: 0.347 [0.000, 70.000], loss: 2.350683, mean_absolute_error: 0.534307, mean_q: 3.618840, mean_eps: 0.100000\n",
      "  49755/175000: episode: 1402, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 113.472 [6.000, 184.000], mean observation: 0.379 [0.000, 72.000], loss: 0.363370, mean_absolute_error: 0.468338, mean_q: 3.379471, mean_eps: 0.100000\n",
      "  49793/175000: episode: 1403, duration: 0.880s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.289 [6.000, 221.000], mean observation: 0.432 [0.000, 76.000], loss: 1.810488, mean_absolute_error: 0.602886, mean_q: 3.891772, mean_eps: 0.100000\n",
      "  49820/175000: episode: 1404, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 136.074 [61.000, 221.000], mean observation: 0.269 [0.000, 54.000], loss: 0.352256, mean_absolute_error: 0.485641, mean_q: 3.396765, mean_eps: 0.100000\n",
      "  49871/175000: episode: 1405, duration: 1.017s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.020 [13.000, 185.000], mean observation: 0.666 [0.000, 102.000], loss: 0.893881, mean_absolute_error: 0.530263, mean_q: 3.630663, mean_eps: 0.100000\n",
      "  49902/175000: episode: 1406, duration: 0.713s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 61.935 [2.000, 169.000], mean observation: 0.161 [0.000, 62.000], loss: 1.485621, mean_absolute_error: 0.548570, mean_q: 3.730303, mean_eps: 0.100000\n",
      "  49941/175000: episode: 1407, duration: 0.795s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.410 [35.000, 221.000], mean observation: 0.243 [0.000, 78.000], loss: 0.677156, mean_absolute_error: 0.455901, mean_q: 3.202958, mean_eps: 0.100000\n",
      "  49992/175000: episode: 1408, duration: 1.101s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.804 [25.000, 221.000], mean observation: 0.496 [0.000, 102.000], loss: 2.841810, mean_absolute_error: 0.507995, mean_q: 3.424939, mean_eps: 0.100000\n",
      "  50019/175000: episode: 1409, duration: 0.620s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 60.148 [53.000, 209.000], mean observation: 0.104 [0.000, 54.000], loss: 0.697853, mean_absolute_error: 0.635581, mean_q: 4.022233, mean_eps: 0.100000\n",
      "  50080/175000: episode: 1410, duration: 1.322s, episode steps: 61, steps per second: 46, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 91.328 [11.000, 193.000], mean observation: 0.226 [0.000, 122.000], loss: 25.369234, mean_absolute_error: 0.827342, mean_q: 4.508752, mean_eps: 0.100000\n",
      "  50129/175000: episode: 1411, duration: 1.099s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.571 [4.000, 221.000], mean observation: 0.241 [0.000, 98.000], loss: 12.724407, mean_absolute_error: 0.541195, mean_q: 3.411422, mean_eps: 0.100000\n",
      "  50169/175000: episode: 1412, duration: 0.870s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.275 [17.000, 216.000], mean observation: 0.134 [0.000, 80.000], loss: 12.006957, mean_absolute_error: 0.586382, mean_q: 3.679672, mean_eps: 0.100000\n",
      "  50195/175000: episode: 1413, duration: 0.534s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.231 [28.000, 100.000], mean observation: 0.073 [0.000, 52.000], loss: 4.190702, mean_absolute_error: 0.515692, mean_q: 3.524902, mean_eps: 0.100000\n",
      "  50232/175000: episode: 1414, duration: 0.861s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.541 [2.000, 218.000], mean observation: 0.206 [0.000, 74.000], loss: 7.265421, mean_absolute_error: 0.545209, mean_q: 3.558122, mean_eps: 0.100000\n",
      "  50277/175000: episode: 1415, duration: 0.942s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.000 [96.000, 216.000], mean observation: 0.182 [0.000, 90.000], loss: 7.312521, mean_absolute_error: 0.512896, mean_q: 3.348810, mean_eps: 0.100000\n",
      "  50323/175000: episode: 1416, duration: 0.974s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.174 [16.000, 204.000], mean observation: 0.281 [0.000, 92.000], loss: 5.286148, mean_absolute_error: 0.540826, mean_q: 3.555848, mean_eps: 0.100000\n",
      "  50351/175000: episode: 1417, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 112.500 [60.000, 180.000], mean observation: 0.166 [0.000, 56.000], loss: 5.830413, mean_absolute_error: 0.548537, mean_q: 3.680075, mean_eps: 0.100000\n",
      "  50388/175000: episode: 1418, duration: 0.770s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 115.243 [42.000, 212.000], mean observation: 0.221 [0.000, 74.000], loss: 0.311096, mean_absolute_error: 0.428863, mean_q: 3.211696, mean_eps: 0.100000\n",
      "  50451/175000: episode: 1419, duration: 1.457s, episode steps: 63, steps per second: 43, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 115.206 [28.000, 199.000], mean observation: 0.441 [0.000, 126.000], loss: 13.759096, mean_absolute_error: 0.701574, mean_q: 4.100510, mean_eps: 0.100000\n",
      "  50489/175000: episode: 1420, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.763 [28.000, 167.000], mean observation: 0.144 [0.000, 76.000], loss: 8.945423, mean_absolute_error: 0.596173, mean_q: 3.712201, mean_eps: 0.100000\n",
      "  50533/175000: episode: 1421, duration: 0.984s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.318 [28.000, 173.000], mean observation: 0.298 [0.000, 88.000], loss: 7.665238, mean_absolute_error: 0.528818, mean_q: 3.451375, mean_eps: 0.100000\n",
      "  50561/175000: episode: 1422, duration: 0.649s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 110.750 [28.000, 210.000], mean observation: 0.128 [0.000, 56.000], loss: 9.253344, mean_absolute_error: 0.592394, mean_q: 3.686323, mean_eps: 0.100000\n",
      "  50583/175000: episode: 1423, duration: 0.408s, episode steps: 22, steps per second: 54, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 210.000 [210.000, 210.000], mean observation: 0.053 [0.000, 44.000], loss: 9.042503, mean_absolute_error: 0.768389, mean_q: 4.615783, mean_eps: 0.100000\n",
      "  50618/175000: episode: 1424, duration: 0.773s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.514 [16.000, 223.000], mean observation: 0.231 [0.000, 70.000], loss: 11.690769, mean_absolute_error: 0.675684, mean_q: 4.157070, mean_eps: 0.100000\n",
      "  50668/175000: episode: 1425, duration: 1.186s, episode steps: 50, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 150.560 [17.000, 210.000], mean observation: 0.325 [0.000, 100.000], loss: 1.307423, mean_absolute_error: 0.432688, mean_q: 3.378243, mean_eps: 0.100000\n",
      "  50711/175000: episode: 1426, duration: 0.916s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 172.047 [1.000, 210.000], mean observation: 0.212 [0.000, 86.000], loss: 16.026788, mean_absolute_error: 0.671467, mean_q: 4.125902, mean_eps: 0.100000\n",
      "  50730/175000: episode: 1427, duration: 0.454s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 210.000 [210.000, 210.000], mean observation: 0.046 [0.000, 38.000], loss: 0.130312, mean_absolute_error: 0.358748, mean_q: 2.960749, mean_eps: 0.100000\n",
      "  50756/175000: episode: 1428, duration: 0.548s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 182.269 [0.000, 210.000], mean observation: 0.073 [0.000, 52.000], loss: 3.654425, mean_absolute_error: 0.654168, mean_q: 4.162488, mean_eps: 0.100000\n",
      "  50812/175000: episode: 1429, duration: 1.358s, episode steps: 56, steps per second: 41, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 174.429 [0.000, 210.000], mean observation: 0.194 [0.000, 112.000], loss: 5.192043, mean_absolute_error: 0.569910, mean_q: 3.799836, mean_eps: 0.100000\n",
      "  50839/175000: episode: 1430, duration: 0.614s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 142.704 [0.000, 210.000], mean observation: 0.165 [0.000, 54.000], loss: 4.237853, mean_absolute_error: 0.505531, mean_q: 3.528954, mean_eps: 0.100000\n",
      "  50860/175000: episode: 1431, duration: 0.473s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 192.048 [17.000, 210.000], mean observation: 0.098 [0.000, 42.000], loss: 8.378397, mean_absolute_error: 0.589038, mean_q: 3.818135, mean_eps: 0.100000\n",
      "  50889/175000: episode: 1432, duration: 0.640s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 75.138 [22.000, 210.000], mean observation: 0.127 [0.000, 58.000], loss: 1.143141, mean_absolute_error: 0.561908, mean_q: 3.920753, mean_eps: 0.100000\n",
      "  50923/175000: episode: 1433, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.971 [18.000, 223.000], mean observation: 0.333 [0.000, 68.000], loss: 27.717953, mean_absolute_error: 0.911276, mean_q: 4.932851, mean_eps: 0.100000\n",
      "  50958/175000: episode: 1434, duration: 0.853s, episode steps: 35, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.086 [0.000, 210.000], mean observation: 0.246 [0.000, 70.000], loss: 29.865350, mean_absolute_error: 0.990387, mean_q: 5.258156, mean_eps: 0.100000\n",
      "  50979/175000: episode: 1435, duration: 0.433s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 154.714 [150.000, 160.000], mean observation: 0.070 [0.000, 42.000], loss: 18.995407, mean_absolute_error: 0.846212, mean_q: 4.805124, mean_eps: 0.100000\n",
      "  51014/175000: episode: 1436, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 159.629 [54.000, 210.000], mean observation: 0.215 [0.000, 70.000], loss: 20.971460, mean_absolute_error: 0.907813, mean_q: 5.115514, mean_eps: 0.100000\n",
      "  51039/175000: episode: 1437, duration: 0.497s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 153.440 [150.000, 215.000], mean observation: 0.061 [0.000, 50.000], loss: 4.721346, mean_absolute_error: 0.678587, mean_q: 4.578498, mean_eps: 0.100000\n",
      "  51068/175000: episode: 1438, duration: 0.629s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 152.517 [54.000, 194.000], mean observation: 0.132 [0.000, 58.000], loss: 2.963250, mean_absolute_error: 0.725140, mean_q: 4.889092, mean_eps: 0.100000\n",
      "  51111/175000: episode: 1439, duration: 1.006s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.256 [6.000, 205.000], mean observation: 0.434 [0.000, 86.000], loss: 8.868018, mean_absolute_error: 0.667040, mean_q: 4.534442, mean_eps: 0.100000\n",
      "  51155/175000: episode: 1440, duration: 1.063s, episode steps: 44, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 155.159 [22.000, 214.000], mean observation: 0.398 [0.000, 88.000], loss: 2.876038, mean_absolute_error: 0.538236, mean_q: 4.024938, mean_eps: 0.100000\n",
      "  51168/175000: episode: 1441, duration: 0.372s, episode steps: 13, steps per second: 35, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 128.308 [22.000, 205.000], mean observation: 0.077 [0.000, 26.000], loss: 4.261552, mean_absolute_error: 0.668213, mean_q: 4.529806, mean_eps: 0.100000\n",
      "  51217/175000: episode: 1442, duration: 1.186s, episode steps: 49, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 152.122 [31.000, 218.000], mean observation: 0.564 [0.000, 98.000], loss: 15.656995, mean_absolute_error: 0.831381, mean_q: 5.252382, mean_eps: 0.100000\n",
      "  51266/175000: episode: 1443, duration: 1.004s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 150.796 [11.000, 218.000], mean observation: 0.377 [0.000, 98.000], loss: 9.683254, mean_absolute_error: 0.697684, mean_q: 4.867325, mean_eps: 0.100000\n",
      "  51308/175000: episode: 1444, duration: 0.925s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 169.810 [10.000, 219.000], mean observation: 0.357 [0.000, 84.000], loss: 5.990058, mean_absolute_error: 0.594547, mean_q: 4.501570, mean_eps: 0.100000\n",
      "  51346/175000: episode: 1445, duration: 0.879s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 161.158 [11.000, 218.000], mean observation: 0.260 [0.000, 76.000], loss: 22.706436, mean_absolute_error: 0.728095, mean_q: 4.766508, mean_eps: 0.100000\n",
      "  51371/175000: episode: 1446, duration: 0.533s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 143.560 [11.000, 218.000], mean observation: 0.128 [0.000, 50.000], loss: 2.277086, mean_absolute_error: 0.845254, mean_q: 5.773095, mean_eps: 0.100000\n",
      "  51408/175000: episode: 1447, duration: 0.872s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 125.189 [11.000, 210.000], mean observation: 0.372 [0.000, 74.000], loss: 15.161803, mean_absolute_error: 0.652365, mean_q: 4.613285, mean_eps: 0.100000\n",
      "  51432/175000: episode: 1448, duration: 0.543s, episode steps: 24, steps per second: 44, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 134.625 [8.000, 205.000], mean observation: 0.183 [0.000, 48.000], loss: 7.560375, mean_absolute_error: 0.689963, mean_q: 4.960673, mean_eps: 0.100000\n",
      "  51469/175000: episode: 1449, duration: 0.799s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.730 [58.000, 204.000], mean observation: 0.297 [0.000, 74.000], loss: 4.759404, mean_absolute_error: 0.622589, mean_q: 4.621041, mean_eps: 0.100000\n",
      "  51516/175000: episode: 1450, duration: 0.969s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 139.277 [43.000, 182.000], mean observation: 0.400 [0.000, 94.000], loss: 5.595720, mean_absolute_error: 0.658125, mean_q: 4.763998, mean_eps: 0.100000\n",
      "  51564/175000: episode: 1451, duration: 1.046s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.125 [60.000, 203.000], mean observation: 0.371 [0.000, 96.000], loss: 0.261949, mean_absolute_error: 0.440015, mean_q: 3.831345, mean_eps: 0.100000\n",
      "  51585/175000: episode: 1452, duration: 0.515s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 166.333 [101.000, 182.000], mean observation: 0.117 [0.000, 42.000], loss: 10.803931, mean_absolute_error: 0.546136, mean_q: 3.998606, mean_eps: 0.100000\n",
      "  51633/175000: episode: 1453, duration: 1.051s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 150.562 [2.000, 200.000], mean observation: 0.422 [0.000, 96.000], loss: 10.056943, mean_absolute_error: 0.676382, mean_q: 4.710404, mean_eps: 0.100000\n",
      "  51657/175000: episode: 1454, duration: 0.576s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 156.542 [110.000, 207.000], mean observation: 0.122 [0.000, 48.000], loss: 0.619366, mean_absolute_error: 0.536283, mean_q: 4.339050, mean_eps: 0.100000\n",
      "  51703/175000: episode: 1455, duration: 0.912s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.304 [22.000, 182.000], mean observation: 0.416 [0.000, 92.000], loss: 9.808875, mean_absolute_error: 0.626430, mean_q: 4.509296, mean_eps: 0.100000\n",
      "  51744/175000: episode: 1456, duration: 0.911s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.854 [8.000, 159.000], mean observation: 0.499 [0.000, 82.000], loss: 12.617119, mean_absolute_error: 0.597118, mean_q: 4.349414, mean_eps: 0.100000\n",
      "  51785/175000: episode: 1457, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 157.976 [1.000, 221.000], mean observation: 0.280 [0.000, 82.000], loss: 1.407247, mean_absolute_error: 0.548218, mean_q: 4.296673, mean_eps: 0.100000\n",
      "  51809/175000: episode: 1458, duration: 0.533s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 197.542 [33.000, 213.000], mean observation: 0.140 [0.000, 48.000], loss: 0.572801, mean_absolute_error: 0.532052, mean_q: 4.252767, mean_eps: 0.100000\n",
      "  51853/175000: episode: 1459, duration: 0.933s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 203.136 [61.000, 213.000], mean observation: 0.272 [0.000, 88.000], loss: 7.965817, mean_absolute_error: 0.618186, mean_q: 4.476963, mean_eps: 0.100000\n",
      "  51886/175000: episode: 1460, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 196.727 [67.000, 213.000], mean observation: 0.175 [0.000, 66.000], loss: 9.629847, mean_absolute_error: 0.567635, mean_q: 4.160142, mean_eps: 0.100000\n",
      "  51914/175000: episode: 1461, duration: 0.564s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 196.036 [48.000, 213.000], mean observation: 0.125 [0.000, 56.000], loss: 3.897262, mean_absolute_error: 0.518223, mean_q: 3.987534, mean_eps: 0.100000\n",
      "  51946/175000: episode: 1462, duration: 0.608s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 202.219 [39.000, 213.000], mean observation: 0.088 [0.000, 64.000], loss: 14.189194, mean_absolute_error: 0.714589, mean_q: 4.881717, mean_eps: 0.100000\n",
      "  51977/175000: episode: 1463, duration: 0.639s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.774 [29.000, 213.000], mean observation: 0.213 [0.000, 62.000], loss: 9.108847, mean_absolute_error: 0.526471, mean_q: 3.966371, mean_eps: 0.100000\n",
      "  52015/175000: episode: 1464, duration: 0.761s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 202.421 [43.000, 213.000], mean observation: 0.127 [0.000, 76.000], loss: 5.569040, mean_absolute_error: 0.520163, mean_q: 4.091415, mean_eps: 0.100000\n",
      "  52052/175000: episode: 1465, duration: 0.788s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 191.865 [45.000, 213.000], mean observation: 0.194 [0.000, 74.000], loss: 9.736114, mean_absolute_error: 0.617639, mean_q: 4.465214, mean_eps: 0.100000\n",
      "  52079/175000: episode: 1466, duration: 0.563s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 213.000 [213.000, 213.000], mean observation: 0.064 [0.000, 54.000], loss: 18.963775, mean_absolute_error: 0.700482, mean_q: 4.596093, mean_eps: 0.100000\n",
      "  52098/175000: episode: 1467, duration: 0.448s, episode steps: 19, steps per second: 42, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 203.211 [148.000, 213.000], mean observation: 0.050 [0.000, 38.000], loss: 0.280108, mean_absolute_error: 0.449302, mean_q: 3.766561, mean_eps: 0.100000\n",
      "  52143/175000: episode: 1468, duration: 0.931s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 205.689 [24.000, 222.000], mean observation: 0.184 [0.000, 90.000], loss: 14.604291, mean_absolute_error: 0.639011, mean_q: 4.473388, mean_eps: 0.100000\n",
      "  52182/175000: episode: 1469, duration: 0.788s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.462 [16.000, 213.000], mean observation: 0.328 [0.000, 78.000], loss: 0.858170, mean_absolute_error: 0.490495, mean_q: 3.985049, mean_eps: 0.100000\n",
      "  52224/175000: episode: 1470, duration: 0.920s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.857 [89.000, 213.000], mean observation: 0.188 [0.000, 84.000], loss: 8.047802, mean_absolute_error: 0.626748, mean_q: 4.601269, mean_eps: 0.100000\n",
      "  52269/175000: episode: 1471, duration: 0.942s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 149.733 [7.000, 213.000], mean observation: 0.498 [0.000, 90.000], loss: 0.242107, mean_absolute_error: 0.530295, mean_q: 4.380727, mean_eps: 0.100000\n",
      "  52303/175000: episode: 1472, duration: 0.632s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 170.147 [66.000, 213.000], mean observation: 0.148 [0.000, 68.000], loss: 2.137868, mean_absolute_error: 0.633468, mean_q: 4.774744, mean_eps: 0.100000\n",
      "  52332/175000: episode: 1473, duration: 0.670s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.586 [28.000, 205.000], mean observation: 0.212 [0.000, 58.000], loss: 0.576667, mean_absolute_error: 0.625434, mean_q: 4.716885, mean_eps: 0.100000\n",
      "  52354/175000: episode: 1474, duration: 0.516s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 132.364 [23.000, 213.000], mean observation: 0.091 [0.000, 44.000], loss: 18.089309, mean_absolute_error: 0.717892, mean_q: 4.813962, mean_eps: 0.100000\n",
      "  52396/175000: episode: 1475, duration: 0.981s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 165.524 [75.000, 213.000], mean observation: 0.259 [0.000, 84.000], loss: 1.348326, mean_absolute_error: 0.515340, mean_q: 4.235298, mean_eps: 0.100000\n",
      "  52455/175000: episode: 1476, duration: 1.233s, episode steps: 59, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 133.373 [91.000, 214.000], mean observation: 0.671 [0.000, 118.000], loss: 0.695012, mean_absolute_error: 0.544350, mean_q: 4.399865, mean_eps: 0.100000\n",
      "  52486/175000: episode: 1477, duration: 0.698s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 87.419 [5.000, 214.000], mean observation: 0.245 [0.000, 62.000], loss: 4.374273, mean_absolute_error: 0.554934, mean_q: 4.330021, mean_eps: 0.100000\n",
      "  52522/175000: episode: 1478, duration: 0.784s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 205.111 [14.000, 215.000], mean observation: 0.121 [0.000, 72.000], loss: 16.442439, mean_absolute_error: 0.645986, mean_q: 4.516903, mean_eps: 0.100000\n",
      "  52578/175000: episode: 1479, duration: 1.231s, episode steps: 56, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 174.143 [5.000, 220.000], mean observation: 0.246 [0.000, 112.000], loss: 9.713419, mean_absolute_error: 0.685356, mean_q: 4.903910, mean_eps: 0.100000\n",
      "  52610/175000: episode: 1480, duration: 0.666s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 187.156 [12.000, 218.000], mean observation: 0.222 [0.000, 64.000], loss: 6.191670, mean_absolute_error: 0.594588, mean_q: 4.456434, mean_eps: 0.100000\n",
      "  52646/175000: episode: 1481, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 153.222 [105.000, 215.000], mean observation: 0.245 [0.000, 72.000], loss: 10.056928, mean_absolute_error: 0.693710, mean_q: 4.871799, mean_eps: 0.100000\n",
      "  52706/175000: episode: 1482, duration: 1.247s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 149.533 [36.000, 215.000], mean observation: 0.524 [0.000, 120.000], loss: 6.559228, mean_absolute_error: 0.553730, mean_q: 4.265946, mean_eps: 0.100000\n",
      "  52744/175000: episode: 1483, duration: 0.883s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 146.868 [37.000, 220.000], mean observation: 0.234 [0.000, 76.000], loss: 12.351929, mean_absolute_error: 0.609284, mean_q: 4.346174, mean_eps: 0.100000\n",
      "  52785/175000: episode: 1484, duration: 0.915s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.390 [14.000, 213.000], mean observation: 0.438 [0.000, 82.000], loss: 5.744723, mean_absolute_error: 0.505051, mean_q: 4.010641, mean_eps: 0.100000\n",
      "  52825/175000: episode: 1485, duration: 0.828s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 73.525 [4.000, 201.000], mean observation: 0.255 [0.000, 80.000], loss: 13.990340, mean_absolute_error: 0.752208, mean_q: 5.086269, mean_eps: 0.100000\n",
      "  52864/175000: episode: 1486, duration: 0.876s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 29.846 [7.000, 166.000], mean observation: 0.214 [0.000, 78.000], loss: 4.289713, mean_absolute_error: 0.555854, mean_q: 4.362439, mean_eps: 0.100000\n",
      "  52913/175000: episode: 1487, duration: 1.100s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 56.816 [20.000, 189.000], mean observation: 0.371 [0.000, 98.000], loss: 2.032108, mean_absolute_error: 0.484137, mean_q: 4.070392, mean_eps: 0.100000\n",
      "  52945/175000: episode: 1488, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.562 [20.000, 165.000], mean observation: 0.196 [0.000, 64.000], loss: 0.256578, mean_absolute_error: 0.447139, mean_q: 3.929540, mean_eps: 0.100000\n",
      "  52988/175000: episode: 1489, duration: 0.907s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 128.698 [81.000, 190.000], mean observation: 0.138 [0.000, 86.000], loss: 6.517298, mean_absolute_error: 0.617175, mean_q: 4.676004, mean_eps: 0.100000\n",
      "  53007/175000: episode: 1490, duration: 0.369s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 103.526 [31.000, 149.000], mean observation: 0.119 [0.000, 38.000], loss: 0.267028, mean_absolute_error: 0.447421, mean_q: 3.955770, mean_eps: 0.100000\n",
      "  53053/175000: episode: 1491, duration: 0.978s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 98.413 [31.000, 197.000], mean observation: 0.391 [0.000, 92.000], loss: 14.333003, mean_absolute_error: 0.627978, mean_q: 4.487075, mean_eps: 0.100000\n",
      "  53091/175000: episode: 1492, duration: 0.761s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.289 [24.000, 167.000], mean observation: 0.177 [0.000, 76.000], loss: 5.258950, mean_absolute_error: 0.539339, mean_q: 4.246764, mean_eps: 0.100000\n",
      "  53127/175000: episode: 1493, duration: 0.819s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.639 [7.000, 220.000], mean observation: 0.225 [0.000, 72.000], loss: 8.150340, mean_absolute_error: 0.625154, mean_q: 4.581124, mean_eps: 0.100000\n",
      "  53167/175000: episode: 1494, duration: 0.805s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 160.550 [75.000, 167.000], mean observation: 0.152 [0.000, 80.000], loss: 3.453491, mean_absolute_error: 0.559042, mean_q: 4.353645, mean_eps: 0.100000\n",
      "  53181/175000: episode: 1495, duration: 0.331s, episode steps: 14, steps per second: 42, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 140.286 [8.000, 174.000], mean observation: 0.069 [0.000, 28.000], loss: 11.379117, mean_absolute_error: 0.637193, mean_q: 4.456205, mean_eps: 0.100000\n",
      "  53234/175000: episode: 1496, duration: 1.100s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 131.660 [74.000, 167.000], mean observation: 0.315 [0.000, 106.000], loss: 5.391470, mean_absolute_error: 0.759303, mean_q: 5.279871, mean_eps: 0.100000\n",
      "  53272/175000: episode: 1497, duration: 0.797s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 129.237 [49.000, 167.000], mean observation: 0.191 [0.000, 76.000], loss: 5.231686, mean_absolute_error: 0.626787, mean_q: 4.452097, mean_eps: 0.100000\n",
      "  53310/175000: episode: 1498, duration: 0.815s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 148.526 [42.000, 217.000], mean observation: 0.208 [0.000, 76.000], loss: 9.161141, mean_absolute_error: 0.704933, mean_q: 4.870490, mean_eps: 0.100000\n",
      "  53339/175000: episode: 1499, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 57.000 [57.000, 57.000], mean observation: 0.068 [0.000, 58.000], loss: 20.583160, mean_absolute_error: 0.868975, mean_q: 5.659205, mean_eps: 0.100000\n",
      "  53374/175000: episode: 1500, duration: 0.786s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.771 [8.000, 224.000], mean observation: 0.212 [0.000, 70.000], loss: 1.434391, mean_absolute_error: 0.571107, mean_q: 4.406654, mean_eps: 0.100000\n",
      "  53422/175000: episode: 1501, duration: 1.013s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 58.417 [19.000, 107.000], mean observation: 0.138 [0.000, 96.000], loss: 25.885951, mean_absolute_error: 0.981597, mean_q: 6.326784, mean_eps: 0.100000\n",
      "  53454/175000: episode: 1502, duration: 0.705s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 63.969 [33.000, 213.000], mean observation: 0.187 [0.000, 64.000], loss: 9.780356, mean_absolute_error: 0.598904, mean_q: 4.496477, mean_eps: 0.100000\n",
      "  53496/175000: episode: 1503, duration: 0.890s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.667 [39.000, 211.000], mean observation: 0.238 [0.000, 84.000], loss: 8.157729, mean_absolute_error: 0.839536, mean_q: 6.020617, mean_eps: 0.100000\n",
      "  53525/175000: episode: 1504, duration: 0.631s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 55.517 [14.000, 57.000], mean observation: 0.085 [0.000, 58.000], loss: 1.377109, mean_absolute_error: 0.742109, mean_q: 5.582978, mean_eps: 0.100000\n",
      "  53579/175000: episode: 1505, duration: 1.148s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 62.926 [16.000, 180.000], mean observation: 0.442 [0.000, 108.000], loss: 9.646923, mean_absolute_error: 0.667673, mean_q: 4.839232, mean_eps: 0.100000\n",
      "  53624/175000: episode: 1506, duration: 1.066s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 68.022 [21.000, 209.000], mean observation: 0.195 [0.000, 90.000], loss: 0.603385, mean_absolute_error: 0.516239, mean_q: 4.064598, mean_eps: 0.100000\n",
      "  53657/175000: episode: 1507, duration: 0.762s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 64.485 [55.000, 203.000], mean observation: 0.134 [0.000, 66.000], loss: 3.889578, mean_absolute_error: 0.536414, mean_q: 4.002591, mean_eps: 0.100000\n",
      "  53715/175000: episode: 1508, duration: 1.273s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 64.517 [23.000, 185.000], mean observation: 0.380 [0.000, 116.000], loss: 7.907887, mean_absolute_error: 0.719269, mean_q: 5.045881, mean_eps: 0.100000\n",
      "  53746/175000: episode: 1509, duration: 0.700s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 82.065 [49.000, 205.000], mean observation: 0.153 [0.000, 62.000], loss: 4.888928, mean_absolute_error: 0.591765, mean_q: 4.329775, mean_eps: 0.100000\n",
      "  53782/175000: episode: 1510, duration: 0.809s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 73.778 [7.000, 210.000], mean observation: 0.233 [0.000, 72.000], loss: 2.054070, mean_absolute_error: 0.647358, mean_q: 4.737191, mean_eps: 0.100000\n",
      "  53803/175000: episode: 1511, duration: 0.453s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 68.762 [7.000, 118.000], mean observation: 0.128 [0.000, 42.000], loss: 0.616906, mean_absolute_error: 0.520970, mean_q: 3.964434, mean_eps: 0.100000\n",
      "  53836/175000: episode: 1512, duration: 0.768s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.152 [51.000, 211.000], mean observation: 0.106 [0.000, 66.000], loss: 8.678915, mean_absolute_error: 0.594174, mean_q: 4.152794, mean_eps: 0.100000\n",
      "  53878/175000: episode: 1513, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.095 [68.000, 193.000], mean observation: 0.144 [0.000, 84.000], loss: 4.251747, mean_absolute_error: 0.603425, mean_q: 4.335544, mean_eps: 0.100000\n",
      "  53906/175000: episode: 1514, duration: 0.664s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.893 [39.000, 88.000], mean observation: 0.100 [0.000, 56.000], loss: 13.620082, mean_absolute_error: 0.641824, mean_q: 4.238893, mean_eps: 0.100000\n",
      "  53932/175000: episode: 1515, duration: 0.556s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 75.462 [55.000, 79.000], mean observation: 0.081 [0.000, 52.000], loss: 35.950741, mean_absolute_error: 0.989408, mean_q: 5.816061, mean_eps: 0.100000\n",
      "  53958/175000: episode: 1516, duration: 0.604s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 49.000 [28.000, 79.000], mean observation: 0.113 [0.000, 52.000], loss: 2.813124, mean_absolute_error: 0.477968, mean_q: 3.412152, mean_eps: 0.100000\n",
      "  54007/175000: episode: 1517, duration: 1.032s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 80.265 [28.000, 168.000], mean observation: 0.321 [0.000, 98.000], loss: 0.335909, mean_absolute_error: 0.520964, mean_q: 3.688237, mean_eps: 0.100000\n",
      "  54032/175000: episode: 1518, duration: 0.618s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.059 [0.000, 50.000], loss: 0.236849, mean_absolute_error: 0.451984, mean_q: 3.224291, mean_eps: 0.100000\n",
      "  54068/175000: episode: 1519, duration: 0.799s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 118.778 [18.000, 170.000], mean observation: 0.339 [0.000, 72.000], loss: 6.473144, mean_absolute_error: 0.515873, mean_q: 3.428044, mean_eps: 0.100000\n",
      "  54089/175000: episode: 1520, duration: 0.499s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 65.810 [39.000, 222.000], mean observation: 0.091 [0.000, 42.000], loss: 0.363249, mean_absolute_error: 0.487019, mean_q: 3.418077, mean_eps: 0.100000\n",
      "  54131/175000: episode: 1521, duration: 0.885s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.333 [0.000, 208.000], mean observation: 0.276 [0.000, 84.000], loss: 15.823330, mean_absolute_error: 0.678927, mean_q: 4.209115, mean_eps: 0.100000\n",
      "  54172/175000: episode: 1522, duration: 0.905s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 37.902 [6.000, 97.000], mean observation: 0.159 [0.000, 82.000], loss: 4.952637, mean_absolute_error: 0.571963, mean_q: 3.955883, mean_eps: 0.100000\n",
      "  54218/175000: episode: 1523, duration: 0.997s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.413 [8.000, 214.000], mean observation: 0.404 [0.000, 92.000], loss: 1.494882, mean_absolute_error: 0.534575, mean_q: 3.841535, mean_eps: 0.100000\n",
      "  54248/175000: episode: 1524, duration: 0.749s, episode steps: 30, steps per second: 40, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 92.567 [8.000, 205.000], mean observation: 0.263 [0.000, 60.000], loss: 7.326492, mean_absolute_error: 0.578410, mean_q: 4.043291, mean_eps: 0.100000\n",
      "  54276/175000: episode: 1525, duration: 0.681s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 55.643 [20.000, 212.000], mean observation: 0.141 [0.000, 56.000], loss: 1.187516, mean_absolute_error: 0.670838, mean_q: 4.816987, mean_eps: 0.100000\n",
      "  54299/175000: episode: 1526, duration: 0.496s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 143.565 [43.000, 222.000], mean observation: 0.135 [0.000, 46.000], loss: 2.878043, mean_absolute_error: 0.547091, mean_q: 3.967980, mean_eps: 0.100000\n",
      "  54344/175000: episode: 1527, duration: 1.008s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 113.044 [2.000, 222.000], mean observation: 0.358 [0.000, 90.000], loss: 11.801882, mean_absolute_error: 0.727964, mean_q: 4.916427, mean_eps: 0.100000\n",
      "  54393/175000: episode: 1528, duration: 1.176s, episode steps: 49, steps per second: 42, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 179.245 [13.000, 222.000], mean observation: 0.395 [0.000, 98.000], loss: 13.110520, mean_absolute_error: 0.663743, mean_q: 4.409699, mean_eps: 0.100000\n",
      "  54439/175000: episode: 1529, duration: 1.007s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 184.739 [42.000, 222.000], mean observation: 0.312 [0.000, 92.000], loss: 9.058060, mean_absolute_error: 0.527570, mean_q: 3.812599, mean_eps: 0.100000\n",
      "  54483/175000: episode: 1530, duration: 0.955s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.955 [28.000, 208.000], mean observation: 0.385 [0.000, 88.000], loss: 2.984327, mean_absolute_error: 0.537064, mean_q: 4.104581, mean_eps: 0.100000\n",
      "  54517/175000: episode: 1531, duration: 0.721s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.529 [138.000, 192.000], mean observation: 0.141 [0.000, 68.000], loss: 8.105937, mean_absolute_error: 0.602828, mean_q: 4.228654, mean_eps: 0.100000\n",
      "  54575/175000: episode: 1532, duration: 1.301s, episode steps: 58, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 125.345 [23.000, 220.000], mean observation: 0.288 [0.000, 116.000], loss: 11.447982, mean_absolute_error: 0.665597, mean_q: 4.406071, mean_eps: 0.100000\n",
      "  54600/175000: episode: 1533, duration: 0.626s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 64.040 [26.000, 191.000], mean observation: 0.179 [0.000, 50.000], loss: 6.892288, mean_absolute_error: 0.568570, mean_q: 3.842962, mean_eps: 0.100000\n",
      "  54626/175000: episode: 1534, duration: 0.634s, episode steps: 26, steps per second: 41, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.154 [19.000, 199.000], mean observation: 0.226 [0.000, 52.000], loss: 8.344347, mean_absolute_error: 0.560527, mean_q: 3.832258, mean_eps: 0.100000\n",
      "  54651/175000: episode: 1535, duration: 0.600s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 67.200 [26.000, 182.000], mean observation: 0.159 [0.000, 50.000], loss: 1.195186, mean_absolute_error: 0.565497, mean_q: 4.168435, mean_eps: 0.100000\n",
      "  54680/175000: episode: 1536, duration: 0.724s, episode steps: 29, steps per second: 40, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 139.414 [26.000, 209.000], mean observation: 0.248 [0.000, 58.000], loss: 2.717662, mean_absolute_error: 0.616748, mean_q: 4.390754, mean_eps: 0.100000\n",
      "  54706/175000: episode: 1537, duration: 0.588s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 101.192 [22.000, 204.000], mean observation: 0.255 [0.000, 52.000], loss: 3.792678, mean_absolute_error: 0.537384, mean_q: 3.958266, mean_eps: 0.100000\n",
      "  54745/175000: episode: 1538, duration: 0.836s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 53.333 [9.000, 214.000], mean observation: 0.291 [0.000, 78.000], loss: 2.461739, mean_absolute_error: 0.709700, mean_q: 4.998109, mean_eps: 0.100000\n",
      "  54773/175000: episode: 1539, duration: 0.656s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 49.643 [26.000, 108.000], mean observation: 0.142 [0.000, 56.000], loss: 0.326468, mean_absolute_error: 0.422992, mean_q: 3.383011, mean_eps: 0.100000\n",
      "  54826/175000: episode: 1540, duration: 1.239s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 73.509 [5.000, 183.000], mean observation: 0.417 [0.000, 106.000], loss: 9.424042, mean_absolute_error: 0.639019, mean_q: 4.492000, mean_eps: 0.100000\n",
      "  54867/175000: episode: 1541, duration: 0.925s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.829 [23.000, 204.000], mean observation: 0.322 [0.000, 82.000], loss: 6.139265, mean_absolute_error: 0.598323, mean_q: 4.182908, mean_eps: 0.100000\n",
      "  54905/175000: episode: 1542, duration: 0.885s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 96.105 [24.000, 222.000], mean observation: 0.332 [0.000, 76.000], loss: 0.303819, mean_absolute_error: 0.458843, mean_q: 3.495080, mean_eps: 0.100000\n",
      "  54928/175000: episode: 1543, duration: 0.495s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 38.609 [24.000, 215.000], mean observation: 0.133 [0.000, 46.000], loss: 0.667924, mean_absolute_error: 0.444758, mean_q: 3.506197, mean_eps: 0.100000\n",
      "  54960/175000: episode: 1544, duration: 0.775s, episode steps: 32, steps per second: 41, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 32.375 [0.000, 215.000], mean observation: 0.162 [0.000, 64.000], loss: 0.219720, mean_absolute_error: 0.408715, mean_q: 3.328277, mean_eps: 0.100000\n",
      "  55022/175000: episode: 1545, duration: 1.385s, episode steps: 62, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 68.758 [24.000, 222.000], mean observation: 0.459 [0.000, 124.000], loss: 8.775417, mean_absolute_error: 0.570290, mean_q: 4.136487, mean_eps: 0.100000\n",
      "  55070/175000: episode: 1546, duration: 1.007s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 111.292 [19.000, 220.000], mean observation: 0.409 [0.000, 96.000], loss: 6.113498, mean_absolute_error: 0.573819, mean_q: 4.256839, mean_eps: 0.100000\n",
      "  55121/175000: episode: 1547, duration: 1.133s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.569 [4.000, 162.000], mean observation: 0.172 [0.000, 102.000], loss: 6.344545, mean_absolute_error: 0.557850, mean_q: 4.167610, mean_eps: 0.100000\n",
      "  55163/175000: episode: 1548, duration: 0.938s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 36.595 [24.000, 168.000], mean observation: 0.330 [0.000, 84.000], loss: 12.674917, mean_absolute_error: 0.671531, mean_q: 4.700743, mean_eps: 0.100000\n",
      "  55195/175000: episode: 1549, duration: 0.733s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 30.406 [24.000, 145.000], mean observation: 0.092 [0.000, 64.000], loss: 11.063596, mean_absolute_error: 0.729585, mean_q: 5.203867, mean_eps: 0.100000\n",
      "  55234/175000: episode: 1550, duration: 0.859s, episode steps: 39, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.487 [12.000, 196.000], mean observation: 0.214 [0.000, 78.000], loss: 2.512905, mean_absolute_error: 0.486292, mean_q: 3.934921, mean_eps: 0.100000\n",
      "  55275/175000: episode: 1551, duration: 0.927s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.146 [55.000, 210.000], mean observation: 0.270 [0.000, 82.000], loss: 11.062264, mean_absolute_error: 0.816237, mean_q: 5.809162, mean_eps: 0.100000\n",
      "  55316/175000: episode: 1552, duration: 0.938s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.439 [9.000, 222.000], mean observation: 0.402 [0.000, 82.000], loss: 0.535547, mean_absolute_error: 0.396319, mean_q: 3.441228, mean_eps: 0.100000\n",
      "  55341/175000: episode: 1553, duration: 0.585s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 128.440 [32.000, 162.000], mean observation: 0.094 [0.000, 50.000], loss: 7.480511, mean_absolute_error: 0.646929, mean_q: 4.865459, mean_eps: 0.100000\n",
      "  55388/175000: episode: 1554, duration: 1.049s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 156.660 [42.000, 221.000], mean observation: 0.512 [0.000, 94.000], loss: 0.665279, mean_absolute_error: 0.497496, mean_q: 4.055415, mean_eps: 0.100000\n",
      "  55412/175000: episode: 1555, duration: 0.623s, episode steps: 24, steps per second: 39, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 77.958 [33.000, 214.000], mean observation: 0.138 [0.000, 48.000], loss: 0.207849, mean_absolute_error: 0.390134, mean_q: 3.286407, mean_eps: 0.100000\n",
      "  55456/175000: episode: 1556, duration: 1.030s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 55.205 [28.000, 142.000], mean observation: 0.146 [0.000, 88.000], loss: 13.879715, mean_absolute_error: 0.693319, mean_q: 4.877512, mean_eps: 0.100000\n",
      "  55487/175000: episode: 1557, duration: 0.658s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 29.935 [28.000, 88.000], mean observation: 0.073 [0.000, 62.000], loss: 0.874900, mean_absolute_error: 0.467364, mean_q: 3.882614, mean_eps: 0.100000\n",
      "  55505/175000: episode: 1558, duration: 0.452s, episode steps: 18, steps per second: 40, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 38.667 [11.000, 132.000], mean observation: 0.073 [0.000, 36.000], loss: 4.072525, mean_absolute_error: 0.412269, mean_q: 3.375680, mean_eps: 0.100000\n",
      "  55545/175000: episode: 1559, duration: 0.895s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.900 [28.000, 217.000], mean observation: 0.376 [0.000, 80.000], loss: 2.209227, mean_absolute_error: 0.432059, mean_q: 3.703155, mean_eps: 0.100000\n",
      "  55569/175000: episode: 1560, duration: 0.576s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 138.708 [28.000, 207.000], mean observation: 0.131 [0.000, 48.000], loss: 6.378321, mean_absolute_error: 0.695558, mean_q: 5.156571, mean_eps: 0.100000\n",
      "  55607/175000: episode: 1561, duration: 0.812s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 30.289 [28.000, 104.000], mean observation: 0.117 [0.000, 76.000], loss: 12.130128, mean_absolute_error: 0.758073, mean_q: 5.433328, mean_eps: 0.100000\n",
      "  55638/175000: episode: 1562, duration: 0.673s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 39.000 [7.000, 212.000], mean observation: 0.151 [0.000, 62.000], loss: 6.444292, mean_absolute_error: 0.653553, mean_q: 4.965588, mean_eps: 0.100000\n",
      "  55665/175000: episode: 1563, duration: 0.598s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.667 [2.000, 64.000], mean observation: 0.124 [0.000, 54.000], loss: 1.005852, mean_absolute_error: 0.607452, mean_q: 4.786840, mean_eps: 0.100000\n",
      "  55697/175000: episode: 1564, duration: 0.654s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 59.938 [28.000, 104.000], mean observation: 0.147 [0.000, 64.000], loss: 11.964987, mean_absolute_error: 0.652459, mean_q: 4.803770, mean_eps: 0.100000\n",
      "  55725/175000: episode: 1565, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 82.000 [64.000, 163.000], mean observation: 0.084 [0.000, 56.000], loss: 5.636026, mean_absolute_error: 0.744393, mean_q: 5.616975, mean_eps: 0.100000\n",
      "  55760/175000: episode: 1566, duration: 0.748s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 80.229 [57.000, 207.000], mean observation: 0.185 [0.000, 70.000], loss: 9.702895, mean_absolute_error: 0.672390, mean_q: 5.131438, mean_eps: 0.100000\n",
      "  55799/175000: episode: 1567, duration: 0.794s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.513 [8.000, 217.000], mean observation: 0.406 [0.000, 78.000], loss: 10.673881, mean_absolute_error: 0.618890, mean_q: 4.739794, mean_eps: 0.100000\n",
      "  55824/175000: episode: 1568, duration: 0.537s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 118.000 [26.000, 207.000], mean observation: 0.127 [0.000, 50.000], loss: 9.589331, mean_absolute_error: 0.691326, mean_q: 5.157554, mean_eps: 0.100000\n",
      "  55841/175000: episode: 1569, duration: 0.431s, episode steps: 17, steps per second: 39, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 86.824 [28.000, 110.000], mean observation: 0.064 [0.000, 34.000], loss: 0.382052, mean_absolute_error: 0.409155, mean_q: 3.624578, mean_eps: 0.100000\n",
      "  55887/175000: episode: 1570, duration: 1.025s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.304 [21.000, 169.000], mean observation: 0.386 [0.000, 92.000], loss: 10.507962, mean_absolute_error: 0.790014, mean_q: 5.771977, mean_eps: 0.100000\n",
      "  55927/175000: episode: 1571, duration: 0.848s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 94.625 [7.000, 210.000], mean observation: 0.149 [0.000, 80.000], loss: 5.146607, mean_absolute_error: 0.503092, mean_q: 4.092715, mean_eps: 0.100000\n",
      "  55953/175000: episode: 1572, duration: 0.593s, episode steps: 26, steps per second: 44, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 69.462 [7.000, 176.000], mean observation: 0.080 [0.000, 52.000], loss: 8.279879, mean_absolute_error: 0.563740, mean_q: 4.286078, mean_eps: 0.100000\n",
      "  55981/175000: episode: 1573, duration: 0.567s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 93.536 [7.000, 217.000], mean observation: 0.144 [0.000, 56.000], loss: 1.054633, mean_absolute_error: 0.579867, mean_q: 4.697052, mean_eps: 0.100000\n",
      "  56005/175000: episode: 1574, duration: 0.497s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 65.833 [7.000, 176.000], mean observation: 0.124 [0.000, 48.000], loss: 13.724016, mean_absolute_error: 0.710499, mean_q: 5.125188, mean_eps: 0.100000\n",
      "  56035/175000: episode: 1575, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 58.633 [7.000, 176.000], mean observation: 0.137 [0.000, 60.000], loss: 5.196870, mean_absolute_error: 0.667902, mean_q: 5.106653, mean_eps: 0.100000\n",
      "  56069/175000: episode: 1576, duration: 0.757s, episode steps: 34, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.794 [3.000, 221.000], mean observation: 0.261 [0.000, 68.000], loss: 3.249656, mean_absolute_error: 0.609855, mean_q: 4.783056, mean_eps: 0.100000\n",
      "  56109/175000: episode: 1577, duration: 0.806s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 114.350 [23.000, 198.000], mean observation: 0.302 [0.000, 80.000], loss: 5.363272, mean_absolute_error: 0.577549, mean_q: 4.632449, mean_eps: 0.100000\n",
      "  56159/175000: episode: 1578, duration: 1.052s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.900 [8.000, 194.000], mean observation: 0.358 [0.000, 100.000], loss: 4.941119, mean_absolute_error: 0.475610, mean_q: 3.974075, mean_eps: 0.100000\n",
      "  56185/175000: episode: 1579, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 49.846 [28.000, 162.000], mean observation: 0.112 [0.000, 52.000], loss: 15.032805, mean_absolute_error: 0.869012, mean_q: 6.113208, mean_eps: 0.100000\n",
      "  56213/175000: episode: 1580, duration: 0.524s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 41.536 [28.000, 195.000], mean observation: 0.092 [0.000, 56.000], loss: 1.417425, mean_absolute_error: 0.435622, mean_q: 3.913975, mean_eps: 0.100000\n",
      "  56262/175000: episode: 1581, duration: 1.011s, episode steps: 49, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.816 [21.000, 212.000], mean observation: 0.179 [0.000, 98.000], loss: 19.309645, mean_absolute_error: 0.728159, mean_q: 5.127856, mean_eps: 0.100000\n",
      "  56287/175000: episode: 1582, duration: 0.526s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 42.560 [28.000, 209.000], mean observation: 0.071 [0.000, 50.000], loss: 0.373427, mean_absolute_error: 0.411185, mean_q: 3.683105, mean_eps: 0.100000\n",
      "  56317/175000: episode: 1583, duration: 0.622s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 36.433 [28.000, 170.000], mean observation: 0.155 [0.000, 60.000], loss: 4.618830, mean_absolute_error: 0.649028, mean_q: 5.106291, mean_eps: 0.100000\n",
      "  56327/175000: episode: 1584, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 61.300 [3.000, 212.000], mean observation: 0.053 [0.000, 20.000], loss: 3.283117, mean_absolute_error: 0.747959, mean_q: 5.913825, mean_eps: 0.100000\n",
      "  56368/175000: episode: 1585, duration: 0.892s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.756 [28.000, 221.000], mean observation: 0.219 [0.000, 82.000], loss: 4.147097, mean_absolute_error: 0.586357, mean_q: 4.817129, mean_eps: 0.100000\n",
      "  56405/175000: episode: 1586, duration: 0.806s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 48.730 [28.000, 167.000], mean observation: 0.221 [0.000, 74.000], loss: 16.593582, mean_absolute_error: 0.663096, mean_q: 5.084386, mean_eps: 0.100000\n",
      "  56443/175000: episode: 1587, duration: 0.794s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 35.974 [28.000, 201.000], mean observation: 0.159 [0.000, 76.000], loss: 5.021916, mean_absolute_error: 0.549995, mean_q: 4.790676, mean_eps: 0.100000\n",
      "  56477/175000: episode: 1588, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.794 [22.000, 215.000], mean observation: 0.240 [0.000, 68.000], loss: 2.040301, mean_absolute_error: 0.675046, mean_q: 5.583856, mean_eps: 0.100000\n",
      "  56515/175000: episode: 1589, duration: 0.747s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 31.921 [27.000, 115.000], mean observation: 0.127 [0.000, 76.000], loss: 5.300569, mean_absolute_error: 0.618379, mean_q: 5.199807, mean_eps: 0.100000\n",
      "  56550/175000: episode: 1590, duration: 0.800s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 27.400 [7.000, 28.000], mean observation: 0.088 [0.000, 70.000], loss: 9.187183, mean_absolute_error: 0.531800, mean_q: 4.545769, mean_eps: 0.100000\n",
      "  56586/175000: episode: 1591, duration: 0.751s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 32.306 [0.000, 211.000], mean observation: 0.101 [0.000, 72.000], loss: 0.235004, mean_absolute_error: 0.544912, mean_q: 4.796157, mean_eps: 0.100000\n",
      "  56627/175000: episode: 1592, duration: 0.960s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 47.756 [8.000, 220.000], mean observation: 0.214 [0.000, 82.000], loss: 0.856427, mean_absolute_error: 0.503390, mean_q: 4.514312, mean_eps: 0.100000\n",
      "  56675/175000: episode: 1593, duration: 1.013s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 74.188 [28.000, 219.000], mean observation: 0.236 [0.000, 96.000], loss: 5.140057, mean_absolute_error: 0.556735, mean_q: 4.732043, mean_eps: 0.100000\n",
      "  56721/175000: episode: 1594, duration: 1.012s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 85.630 [28.000, 207.000], mean observation: 0.337 [0.000, 92.000], loss: 12.661700, mean_absolute_error: 0.626874, mean_q: 4.966370, mean_eps: 0.100000\n",
      "  56745/175000: episode: 1595, duration: 0.534s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 175.125 [7.000, 221.000], mean observation: 0.092 [0.000, 48.000], loss: 0.092949, mean_absolute_error: 0.393062, mean_q: 3.815544, mean_eps: 0.100000\n",
      "  56767/175000: episode: 1596, duration: 0.501s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 167.682 [7.000, 221.000], mean observation: 0.100 [0.000, 44.000], loss: 9.839968, mean_absolute_error: 0.726164, mean_q: 5.623345, mean_eps: 0.100000\n",
      "  56795/175000: episode: 1597, duration: 0.625s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.714 [6.000, 217.000], mean observation: 0.174 [0.000, 56.000], loss: 6.375897, mean_absolute_error: 0.613440, mean_q: 5.000583, mean_eps: 0.100000\n",
      "  56839/175000: episode: 1598, duration: 0.962s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 155.545 [29.000, 215.000], mean observation: 0.236 [0.000, 88.000], loss: 4.969499, mean_absolute_error: 0.564798, mean_q: 4.759268, mean_eps: 0.100000\n",
      "  56888/175000: episode: 1599, duration: 1.079s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 62.796 [2.000, 215.000], mean observation: 0.275 [0.000, 98.000], loss: 0.882367, mean_absolute_error: 0.457589, mean_q: 4.301828, mean_eps: 0.100000\n",
      "  56933/175000: episode: 1600, duration: 0.928s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 82.511 [25.000, 182.000], mean observation: 0.289 [0.000, 90.000], loss: 7.000874, mean_absolute_error: 0.566131, mean_q: 4.722329, mean_eps: 0.100000\n",
      "  56962/175000: episode: 1601, duration: 0.689s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 121.724 [5.000, 177.000], mean observation: 0.188 [0.000, 58.000], loss: 19.791453, mean_absolute_error: 0.797536, mean_q: 5.667100, mean_eps: 0.100000\n",
      "  56979/175000: episode: 1602, duration: 0.340s, episode steps: 17, steps per second: 50, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 116.647 [58.000, 177.000], mean observation: 0.060 [0.000, 34.000], loss: 17.652748, mean_absolute_error: 0.934397, mean_q: 6.662557, mean_eps: 0.100000\n",
      "  57006/175000: episode: 1603, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.185 [58.000, 221.000], mean observation: 0.120 [0.000, 54.000], loss: 4.510584, mean_absolute_error: 0.596008, mean_q: 4.857464, mean_eps: 0.100000\n",
      "  57038/175000: episode: 1604, duration: 0.669s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.469 [28.000, 192.000], mean observation: 0.226 [0.000, 64.000], loss: 4.579614, mean_absolute_error: 0.619541, mean_q: 5.076294, mean_eps: 0.100000\n",
      "  57065/175000: episode: 1605, duration: 0.617s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 114.148 [58.000, 177.000], mean observation: 0.103 [0.000, 54.000], loss: 1.956947, mean_absolute_error: 0.601705, mean_q: 5.006450, mean_eps: 0.100000\n",
      "  57091/175000: episode: 1606, duration: 0.515s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 152.500 [33.000, 177.000], mean observation: 0.134 [0.000, 52.000], loss: 10.353190, mean_absolute_error: 0.558041, mean_q: 4.501030, mean_eps: 0.100000\n",
      "  57118/175000: episode: 1607, duration: 0.619s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 151.148 [4.000, 215.000], mean observation: 0.209 [0.000, 54.000], loss: 1.552299, mean_absolute_error: 0.641275, mean_q: 5.368451, mean_eps: 0.100000\n",
      "  57144/175000: episode: 1608, duration: 0.601s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 125.538 [58.000, 177.000], mean observation: 0.185 [0.000, 52.000], loss: 8.542819, mean_absolute_error: 0.686895, mean_q: 5.316065, mean_eps: 0.100000\n",
      "  57181/175000: episode: 1609, duration: 0.817s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 105.568 [7.000, 215.000], mean observation: 0.276 [0.000, 74.000], loss: 12.264800, mean_absolute_error: 0.738879, mean_q: 5.578620, mean_eps: 0.100000\n",
      "  57224/175000: episode: 1610, duration: 0.922s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.326 [18.000, 178.000], mean observation: 0.359 [0.000, 86.000], loss: 17.266279, mean_absolute_error: 0.804132, mean_q: 5.929602, mean_eps: 0.100000\n",
      "  57244/175000: episode: 1611, duration: 0.532s, episode steps: 20, steps per second: 38, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 103.150 [26.000, 193.000], mean observation: 0.113 [0.000, 40.000], loss: 8.190167, mean_absolute_error: 0.639594, mean_q: 5.288075, mean_eps: 0.100000\n",
      "  57269/175000: episode: 1612, duration: 0.554s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 93.520 [26.000, 223.000], mean observation: 0.117 [0.000, 50.000], loss: 0.294013, mean_absolute_error: 0.382242, mean_q: 3.876955, mean_eps: 0.100000\n",
      "  57286/175000: episode: 1613, duration: 0.364s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 91.941 [26.000, 177.000], mean observation: 0.061 [0.000, 34.000], loss: 8.531670, mean_absolute_error: 0.604376, mean_q: 5.017163, mean_eps: 0.100000\n",
      "  57314/175000: episode: 1614, duration: 0.576s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.036 [1.000, 182.000], mean observation: 0.256 [0.000, 56.000], loss: 8.357963, mean_absolute_error: 0.559643, mean_q: 4.710922, mean_eps: 0.100000\n",
      "  57345/175000: episode: 1615, duration: 0.693s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 77.226 [24.000, 177.000], mean observation: 0.168 [0.000, 62.000], loss: 0.368908, mean_absolute_error: 0.432136, mean_q: 4.221892, mean_eps: 0.100000\n",
      "  57383/175000: episode: 1616, duration: 0.840s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 87.316 [7.000, 223.000], mean observation: 0.335 [0.000, 76.000], loss: 10.912548, mean_absolute_error: 0.724448, mean_q: 5.685862, mean_eps: 0.100000\n",
      "  57421/175000: episode: 1617, duration: 0.846s, episode steps: 38, steps per second: 45, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.105 [12.000, 219.000], mean observation: 0.357 [0.000, 76.000], loss: 0.239307, mean_absolute_error: 0.523676, mean_q: 4.748284, mean_eps: 0.100000\n",
      "  57453/175000: episode: 1618, duration: 0.656s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.438 [28.000, 192.000], mean observation: 0.229 [0.000, 64.000], loss: 13.760105, mean_absolute_error: 0.870284, mean_q: 6.511605, mean_eps: 0.100000\n",
      "  57489/175000: episode: 1619, duration: 0.739s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 44.083 [28.000, 220.000], mean observation: 0.099 [0.000, 72.000], loss: 2.190101, mean_absolute_error: 0.596687, mean_q: 5.124616, mean_eps: 0.100000\n",
      "  57528/175000: episode: 1620, duration: 0.882s, episode steps: 39, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 56.538 [2.000, 206.000], mean observation: 0.238 [0.000, 78.000], loss: 5.620002, mean_absolute_error: 0.666030, mean_q: 5.505485, mean_eps: 0.100000\n",
      "  57544/175000: episode: 1621, duration: 0.402s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 34.188 [28.000, 127.000], mean observation: 0.050 [0.000, 32.000], loss: 11.666658, mean_absolute_error: 0.709251, mean_q: 5.536005, mean_eps: 0.100000\n",
      "  57566/175000: episode: 1622, duration: 0.479s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 30.818 [28.000, 90.000], mean observation: 0.079 [0.000, 44.000], loss: 2.406646, mean_absolute_error: 0.619291, mean_q: 5.269806, mean_eps: 0.100000\n",
      "  57595/175000: episode: 1623, duration: 0.604s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 46.690 [28.000, 223.000], mean observation: 0.083 [0.000, 58.000], loss: 0.229980, mean_absolute_error: 0.434230, mean_q: 4.283807, mean_eps: 0.100000\n",
      "  57636/175000: episode: 1624, duration: 0.875s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.244 [0.000, 215.000], mean observation: 0.323 [0.000, 82.000], loss: 0.350864, mean_absolute_error: 0.465566, mean_q: 4.310979, mean_eps: 0.100000\n",
      "  57674/175000: episode: 1625, duration: 0.793s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.500 [17.000, 172.000], mean observation: 0.284 [0.000, 76.000], loss: 18.007057, mean_absolute_error: 0.772127, mean_q: 5.720407, mean_eps: 0.100000\n",
      "  57700/175000: episode: 1626, duration: 0.616s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 143.962 [34.000, 179.000], mean observation: 0.153 [0.000, 52.000], loss: 0.168800, mean_absolute_error: 0.416424, mean_q: 4.058294, mean_eps: 0.100000\n",
      "  57762/175000: episode: 1627, duration: 1.364s, episode steps: 62, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.613 [51.000, 202.000], mean observation: 0.468 [0.000, 124.000], loss: 3.849147, mean_absolute_error: 0.490027, mean_q: 4.489534, mean_eps: 0.100000\n",
      "  57798/175000: episode: 1628, duration: 0.772s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 143.861 [44.000, 154.000], mean observation: 0.110 [0.000, 72.000], loss: 6.678929, mean_absolute_error: 0.604463, mean_q: 5.015589, mean_eps: 0.100000\n",
      "  57836/175000: episode: 1629, duration: 0.833s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 138.553 [36.000, 180.000], mean observation: 0.161 [0.000, 76.000], loss: 10.190105, mean_absolute_error: 0.692936, mean_q: 5.477002, mean_eps: 0.100000\n",
      "  57873/175000: episode: 1630, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 140.351 [76.000, 180.000], mean observation: 0.127 [0.000, 74.000], loss: 11.444116, mean_absolute_error: 0.640070, mean_q: 5.043344, mean_eps: 0.100000\n",
      "  57890/175000: episode: 1631, duration: 0.301s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 125.000 [125.000, 125.000], mean observation: 0.041 [0.000, 34.000], loss: 0.166441, mean_absolute_error: 0.380258, mean_q: 3.877445, mean_eps: 0.100000\n",
      "  57917/175000: episode: 1632, duration: 0.518s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 122.370 [0.000, 189.000], mean observation: 0.085 [0.000, 54.000], loss: 3.504898, mean_absolute_error: 0.628133, mean_q: 5.302195, mean_eps: 0.100000\n",
      "  57945/175000: episode: 1633, duration: 0.613s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 121.536 [5.000, 177.000], mean observation: 0.126 [0.000, 56.000], loss: 2.118299, mean_absolute_error: 0.616763, mean_q: 5.282612, mean_eps: 0.100000\n",
      "  57987/175000: episode: 1634, duration: 0.871s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.524 [68.000, 177.000], mean observation: 0.198 [0.000, 84.000], loss: 6.992834, mean_absolute_error: 0.521549, mean_q: 4.547003, mean_eps: 0.100000\n",
      "  58031/175000: episode: 1635, duration: 0.896s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.250 [4.000, 154.000], mean observation: 0.181 [0.000, 88.000], loss: 8.301491, mean_absolute_error: 0.664245, mean_q: 5.465325, mean_eps: 0.100000\n",
      "  58074/175000: episode: 1636, duration: 0.910s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 46.907 [28.000, 194.000], mean observation: 0.124 [0.000, 86.000], loss: 3.960728, mean_absolute_error: 0.473192, mean_q: 4.408805, mean_eps: 0.100000\n",
      "  58124/175000: episode: 1637, duration: 1.067s, episode steps: 50, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 84.140 [28.000, 215.000], mean observation: 0.277 [0.000, 100.000], loss: 11.569916, mean_absolute_error: 0.635121, mean_q: 5.198634, mean_eps: 0.100000\n",
      "  58150/175000: episode: 1638, duration: 0.644s, episode steps: 26, steps per second: 40, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 48.885 [48.000, 71.000], mean observation: 0.094 [0.000, 52.000], loss: 0.218375, mean_absolute_error: 0.426555, mean_q: 4.258832, mean_eps: 0.100000\n",
      "  58195/175000: episode: 1639, duration: 1.018s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.000 [48.000, 222.000], mean observation: 0.237 [0.000, 90.000], loss: 0.151001, mean_absolute_error: 0.457187, mean_q: 4.533607, mean_eps: 0.100000\n",
      "  58245/175000: episode: 1640, duration: 1.109s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.920 [1.000, 162.000], mean observation: 0.345 [0.000, 100.000], loss: 2.690514, mean_absolute_error: 0.513767, mean_q: 4.720468, mean_eps: 0.100000\n",
      "  58286/175000: episode: 1641, duration: 0.811s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.976 [1.000, 217.000], mean observation: 0.267 [0.000, 82.000], loss: 7.501027, mean_absolute_error: 0.696066, mean_q: 5.648218, mean_eps: 0.100000\n",
      "  58318/175000: episode: 1642, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 116.188 [32.000, 154.000], mean observation: 0.217 [0.000, 64.000], loss: 14.276983, mean_absolute_error: 0.676690, mean_q: 5.407278, mean_eps: 0.100000\n",
      "  58346/175000: episode: 1643, duration: 0.573s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.714 [3.000, 204.000], mean observation: 0.131 [0.000, 56.000], loss: 3.834811, mean_absolute_error: 0.616698, mean_q: 5.322791, mean_eps: 0.100000\n",
      "  58392/175000: episode: 1644, duration: 1.069s, episode steps: 46, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 86.304 [1.000, 193.000], mean observation: 0.422 [0.000, 92.000], loss: 12.128870, mean_absolute_error: 0.681688, mean_q: 5.548238, mean_eps: 0.100000\n",
      "  58434/175000: episode: 1645, duration: 0.905s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 67.310 [20.000, 221.000], mean observation: 0.140 [0.000, 84.000], loss: 0.257677, mean_absolute_error: 0.420062, mean_q: 4.201980, mean_eps: 0.100000\n",
      "  58469/175000: episode: 1646, duration: 0.759s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.143 [28.000, 189.000], mean observation: 0.140 [0.000, 70.000], loss: 2.326031, mean_absolute_error: 0.499853, mean_q: 4.651866, mean_eps: 0.100000\n",
      "  58527/175000: episode: 1647, duration: 1.204s, episode steps: 58, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 127.948 [11.000, 204.000], mean observation: 0.490 [0.000, 116.000], loss: 0.297361, mean_absolute_error: 0.476317, mean_q: 4.631590, mean_eps: 0.100000\n",
      "  58567/175000: episode: 1648, duration: 0.784s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 67.325 [28.000, 172.000], mean observation: 0.223 [0.000, 80.000], loss: 9.476713, mean_absolute_error: 0.619391, mean_q: 5.171517, mean_eps: 0.100000\n",
      "  58609/175000: episode: 1649, duration: 0.989s, episode steps: 42, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.643 [28.000, 184.000], mean observation: 0.311 [0.000, 84.000], loss: 8.873924, mean_absolute_error: 0.690948, mean_q: 5.650604, mean_eps: 0.100000\n",
      "  58650/175000: episode: 1650, duration: 0.841s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.220 [10.000, 224.000], mean observation: 0.259 [0.000, 82.000], loss: 6.864399, mean_absolute_error: 0.573007, mean_q: 5.007933, mean_eps: 0.100000\n",
      "  58709/175000: episode: 1651, duration: 1.292s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 67.525 [32.000, 201.000], mean observation: 0.453 [0.000, 118.000], loss: 16.480970, mean_absolute_error: 0.627335, mean_q: 5.053859, mean_eps: 0.100000\n",
      "  58752/175000: episode: 1652, duration: 0.946s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.070 [47.000, 176.000], mean observation: 0.410 [0.000, 86.000], loss: 0.545972, mean_absolute_error: 0.544291, mean_q: 5.030826, mean_eps: 0.100000\n",
      "  58799/175000: episode: 1653, duration: 0.987s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.447 [48.000, 176.000], mean observation: 0.390 [0.000, 94.000], loss: 3.268312, mean_absolute_error: 0.589408, mean_q: 5.226615, mean_eps: 0.100000\n",
      "  58835/175000: episode: 1654, duration: 0.765s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 64.111 [37.000, 222.000], mean observation: 0.216 [0.000, 72.000], loss: 13.315586, mean_absolute_error: 0.824189, mean_q: 6.274832, mean_eps: 0.100000\n",
      "  58857/175000: episode: 1655, duration: 0.558s, episode steps: 22, steps per second: 39, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 78.273 [38.000, 149.000], mean observation: 0.123 [0.000, 44.000], loss: 10.381297, mean_absolute_error: 0.719902, mean_q: 5.769134, mean_eps: 0.100000\n",
      "  58884/175000: episode: 1656, duration: 0.601s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 73.259 [37.000, 125.000], mean observation: 0.094 [0.000, 54.000], loss: 0.492751, mean_absolute_error: 0.535414, mean_q: 4.979130, mean_eps: 0.100000\n",
      "  58937/175000: episode: 1657, duration: 1.125s, episode steps: 53, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 94.472 [26.000, 217.000], mean observation: 0.374 [0.000, 106.000], loss: 5.110913, mean_absolute_error: 0.610356, mean_q: 5.264334, mean_eps: 0.100000\n",
      "  58966/175000: episode: 1658, duration: 0.594s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 111.690 [28.000, 144.000], mean observation: 0.150 [0.000, 58.000], loss: 0.180560, mean_absolute_error: 0.447482, mean_q: 4.376171, mean_eps: 0.100000\n",
      "  59001/175000: episode: 1659, duration: 0.747s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.571 [48.000, 218.000], mean observation: 0.206 [0.000, 70.000], loss: 2.177029, mean_absolute_error: 0.597639, mean_q: 5.296827, mean_eps: 0.100000\n",
      "  59022/175000: episode: 1660, duration: 0.512s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 101.286 [47.000, 172.000], mean observation: 0.166 [0.000, 42.000], loss: 13.708733, mean_absolute_error: 0.794661, mean_q: 6.084163, mean_eps: 0.100000\n",
      "  59046/175000: episode: 1661, duration: 0.541s, episode steps: 24, steps per second: 44, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 163.208 [26.000, 203.000], mean observation: 0.117 [0.000, 48.000], loss: 15.572655, mean_absolute_error: 0.648730, mean_q: 5.291874, mean_eps: 0.100000\n",
      "  59072/175000: episode: 1662, duration: 0.607s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 132.692 [32.000, 177.000], mean observation: 0.125 [0.000, 52.000], loss: 8.599587, mean_absolute_error: 0.806192, mean_q: 6.428931, mean_eps: 0.100000\n",
      "  59131/175000: episode: 1663, duration: 1.315s, episode steps: 59, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 99.949 [32.000, 205.000], mean observation: 0.401 [0.000, 118.000], loss: 2.126438, mean_absolute_error: 0.488404, mean_q: 4.702550, mean_eps: 0.100000\n",
      "  59162/175000: episode: 1664, duration: 0.697s, episode steps: 31, steps per second: 44, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.129 [32.000, 204.000], mean observation: 0.169 [0.000, 62.000], loss: 4.004638, mean_absolute_error: 0.610449, mean_q: 5.351303, mean_eps: 0.100000\n",
      "  59194/175000: episode: 1665, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 41.844 [32.000, 177.000], mean observation: 0.145 [0.000, 64.000], loss: 6.524590, mean_absolute_error: 0.680653, mean_q: 5.724111, mean_eps: 0.100000\n",
      "  59227/175000: episode: 1666, duration: 0.702s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 72.939 [16.000, 224.000], mean observation: 0.221 [0.000, 66.000], loss: 2.528947, mean_absolute_error: 0.584009, mean_q: 5.354770, mean_eps: 0.100000\n",
      "  59267/175000: episode: 1667, duration: 0.854s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 110.075 [8.000, 223.000], mean observation: 0.322 [0.000, 80.000], loss: 0.431663, mean_absolute_error: 0.453684, mean_q: 4.673921, mean_eps: 0.100000\n",
      "  59291/175000: episode: 1668, duration: 0.525s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 149.792 [32.000, 223.000], mean observation: 0.130 [0.000, 48.000], loss: 12.975854, mean_absolute_error: 0.744274, mean_q: 5.989864, mean_eps: 0.100000\n",
      "  59337/175000: episode: 1669, duration: 0.996s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 104.283 [26.000, 223.000], mean observation: 0.364 [0.000, 92.000], loss: 10.267079, mean_absolute_error: 0.676877, mean_q: 5.825233, mean_eps: 0.100000\n",
      "  59378/175000: episode: 1670, duration: 0.871s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 151.171 [14.000, 223.000], mean observation: 0.323 [0.000, 82.000], loss: 9.285623, mean_absolute_error: 0.675009, mean_q: 5.737013, mean_eps: 0.100000\n",
      "  59432/175000: episode: 1671, duration: 1.145s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 140.926 [16.000, 223.000], mean observation: 0.488 [0.000, 108.000], loss: 18.205689, mean_absolute_error: 0.774839, mean_q: 6.172124, mean_eps: 0.100000\n",
      "  59470/175000: episode: 1672, duration: 0.810s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.974 [39.000, 167.000], mean observation: 0.217 [0.000, 76.000], loss: 1.623706, mean_absolute_error: 0.588658, mean_q: 5.511117, mean_eps: 0.100000\n",
      "  59514/175000: episode: 1673, duration: 0.946s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 66.614 [32.000, 198.000], mean observation: 0.306 [0.000, 88.000], loss: 7.028875, mean_absolute_error: 0.612400, mean_q: 5.426429, mean_eps: 0.100000\n",
      "  59535/175000: episode: 1674, duration: 0.449s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 80.571 [39.000, 181.000], mean observation: 0.135 [0.000, 42.000], loss: 13.246789, mean_absolute_error: 0.811987, mean_q: 6.520494, mean_eps: 0.100000\n",
      "  59553/175000: episode: 1675, duration: 0.408s, episode steps: 18, steps per second: 44, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 97.722 [36.000, 164.000], mean observation: 0.088 [0.000, 36.000], loss: 0.239921, mean_absolute_error: 0.590922, mean_q: 5.622192, mean_eps: 0.100000\n",
      "  59597/175000: episode: 1676, duration: 0.934s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.386 [39.000, 210.000], mean observation: 0.389 [0.000, 88.000], loss: 1.815249, mean_absolute_error: 0.486961, mean_q: 4.832787, mean_eps: 0.100000\n",
      "  59630/175000: episode: 1677, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 134.152 [67.000, 222.000], mean observation: 0.267 [0.000, 66.000], loss: 0.360858, mean_absolute_error: 0.521004, mean_q: 5.038550, mean_eps: 0.100000\n",
      "  59681/175000: episode: 1678, duration: 1.121s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.196 [27.000, 209.000], mean observation: 0.429 [0.000, 102.000], loss: 12.906561, mean_absolute_error: 0.818569, mean_q: 6.502695, mean_eps: 0.100000\n",
      "  59726/175000: episode: 1679, duration: 1.020s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 48.756 [15.000, 153.000], mean observation: 0.235 [0.000, 90.000], loss: 5.897121, mean_absolute_error: 0.504108, mean_q: 4.785029, mean_eps: 0.100000\n",
      "  59764/175000: episode: 1680, duration: 0.872s, episode steps: 38, steps per second: 44, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 49.105 [4.000, 193.000], mean observation: 0.159 [0.000, 76.000], loss: 0.180607, mean_absolute_error: 0.422883, mean_q: 4.606428, mean_eps: 0.100000\n",
      "  59800/175000: episode: 1681, duration: 0.881s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 92.056 [29.000, 204.000], mean observation: 0.227 [0.000, 72.000], loss: 0.170827, mean_absolute_error: 0.402119, mean_q: 4.268855, mean_eps: 0.100000\n",
      "  59822/175000: episode: 1682, duration: 0.520s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 52.182 [39.000, 97.000], mean observation: 0.055 [0.000, 44.000], loss: 0.295703, mean_absolute_error: 0.478814, mean_q: 4.623590, mean_eps: 0.100000\n",
      "  59866/175000: episode: 1683, duration: 0.921s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.659 [39.000, 204.000], mean observation: 0.490 [0.000, 88.000], loss: 5.617218, mean_absolute_error: 0.518135, mean_q: 4.773291, mean_eps: 0.100000\n",
      "  59908/175000: episode: 1684, duration: 0.907s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.262 [27.000, 216.000], mean observation: 0.299 [0.000, 84.000], loss: 6.408302, mean_absolute_error: 0.513388, mean_q: 4.714973, mean_eps: 0.100000\n",
      "  59955/175000: episode: 1685, duration: 1.008s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.702 [15.000, 209.000], mean observation: 0.466 [0.000, 94.000], loss: 0.131246, mean_absolute_error: 0.431856, mean_q: 4.557708, mean_eps: 0.100000\n",
      "  59983/175000: episode: 1686, duration: 0.601s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 40.607 [5.000, 224.000], mean observation: 0.097 [0.000, 56.000], loss: 8.369215, mean_absolute_error: 0.554745, mean_q: 5.023637, mean_eps: 0.100000\n",
      "  60032/175000: episode: 1687, duration: 1.073s, episode steps: 49, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.429 [1.000, 167.000], mean observation: 0.203 [0.000, 98.000], loss: 24.702770, mean_absolute_error: 0.723602, mean_q: 5.660991, mean_eps: 0.100000\n",
      "  60072/175000: episode: 1688, duration: 0.978s, episode steps: 40, steps per second: 41, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 31.800 [28.000, 180.000], mean observation: 0.102 [0.000, 80.000], loss: 19.246681, mean_absolute_error: 0.696676, mean_q: 5.751792, mean_eps: 0.100000\n",
      "  60117/175000: episode: 1689, duration: 0.971s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.622 [10.000, 176.000], mean observation: 0.255 [0.000, 90.000], loss: 21.558830, mean_absolute_error: 0.766214, mean_q: 6.233960, mean_eps: 0.100000\n",
      "  60168/175000: episode: 1690, duration: 1.128s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.294 [14.000, 138.000], mean observation: 0.305 [0.000, 102.000], loss: 11.244192, mean_absolute_error: 0.753446, mean_q: 6.455717, mean_eps: 0.100000\n",
      "  60195/175000: episode: 1691, duration: 0.593s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.667 [27.000, 148.000], mean observation: 0.151 [0.000, 54.000], loss: 16.955656, mean_absolute_error: 0.717211, mean_q: 6.082788, mean_eps: 0.100000\n",
      "  60205/175000: episode: 1692, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 79.300 [58.000, 196.000], mean observation: 0.050 [0.000, 20.000], loss: 0.133522, mean_absolute_error: 0.434735, mean_q: 4.894471, mean_eps: 0.100000\n",
      "  60252/175000: episode: 1693, duration: 1.076s, episode steps: 47, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 68.340 [26.000, 188.000], mean observation: 0.262 [0.000, 94.000], loss: 0.524582, mean_absolute_error: 0.425977, mean_q: 4.728412, mean_eps: 0.100000\n",
      "  60298/175000: episode: 1694, duration: 1.027s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 61.522 [58.000, 220.000], mean observation: 0.129 [0.000, 92.000], loss: 0.213825, mean_absolute_error: 0.498025, mean_q: 5.029126, mean_eps: 0.100000\n",
      "  60330/175000: episode: 1695, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 95.406 [6.000, 179.000], mean observation: 0.243 [0.000, 64.000], loss: 0.146887, mean_absolute_error: 0.435275, mean_q: 4.661815, mean_eps: 0.100000\n",
      "  60391/175000: episode: 1696, duration: 1.310s, episode steps: 61, steps per second: 47, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 83.705 [28.000, 201.000], mean observation: 0.488 [0.000, 122.000], loss: 31.466210, mean_absolute_error: 0.725658, mean_q: 5.668744, mean_eps: 0.100000\n",
      "  60428/175000: episode: 1697, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 60.162 [32.000, 67.000], mean observation: 0.146 [0.000, 74.000], loss: 12.800471, mean_absolute_error: 0.651876, mean_q: 5.715330, mean_eps: 0.100000\n",
      "  60494/175000: episode: 1698, duration: 1.379s, episode steps: 66, steps per second: 48, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 87.894 [5.000, 221.000], mean observation: 0.536 [0.000, 132.000], loss: 23.821400, mean_absolute_error: 0.761200, mean_q: 6.204952, mean_eps: 0.100000\n",
      "  60516/175000: episode: 1699, duration: 0.504s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 164.682 [39.000, 216.000], mean observation: 0.132 [0.000, 44.000], loss: 12.322367, mean_absolute_error: 0.641532, mean_q: 5.797558, mean_eps: 0.100000\n",
      "  60569/175000: episode: 1700, duration: 1.161s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 96.887 [33.000, 190.000], mean observation: 0.312 [0.000, 106.000], loss: 18.897382, mean_absolute_error: 0.707968, mean_q: 6.073641, mean_eps: 0.100000\n",
      "  60591/175000: episode: 1701, duration: 0.442s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 124.636 [62.000, 190.000], mean observation: 0.091 [0.000, 44.000], loss: 0.136660, mean_absolute_error: 0.502232, mean_q: 5.300583, mean_eps: 0.100000\n",
      "  60628/175000: episode: 1702, duration: 0.836s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 137.027 [48.000, 190.000], mean observation: 0.237 [0.000, 74.000], loss: 0.117478, mean_absolute_error: 0.430598, mean_q: 4.735228, mean_eps: 0.100000\n",
      "  60671/175000: episode: 1703, duration: 1.004s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.279 [13.000, 216.000], mean observation: 0.197 [0.000, 86.000], loss: 32.238664, mean_absolute_error: 0.910507, mean_q: 6.935763, mean_eps: 0.100000\n",
      "  60712/175000: episode: 1704, duration: 0.923s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 129.683 [19.000, 135.000], mean observation: 0.147 [0.000, 82.000], loss: 0.297739, mean_absolute_error: 0.501785, mean_q: 5.123521, mean_eps: 0.100000\n",
      "  60747/175000: episode: 1705, duration: 0.758s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 133.000 [58.000, 142.000], mean observation: 0.099 [0.000, 70.000], loss: 0.135228, mean_absolute_error: 0.452913, mean_q: 4.787676, mean_eps: 0.100000\n",
      "  60774/175000: episode: 1706, duration: 0.602s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 130.333 [97.000, 214.000], mean observation: 0.178 [0.000, 54.000], loss: 0.418899, mean_absolute_error: 0.453571, mean_q: 4.739082, mean_eps: 0.100000\n",
      "  60815/175000: episode: 1707, duration: 0.825s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.732 [44.000, 219.000], mean observation: 0.171 [0.000, 82.000], loss: 30.110801, mean_absolute_error: 0.909413, mean_q: 6.864361, mean_eps: 0.100000\n",
      "  60835/175000: episode: 1708, duration: 0.397s, episode steps: 20, steps per second: 50, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 135.000 [135.000, 135.000], mean observation: 0.048 [0.000, 40.000], loss: 0.389230, mean_absolute_error: 0.517185, mean_q: 5.173217, mean_eps: 0.100000\n",
      "  60888/175000: episode: 1709, duration: 1.247s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 126.792 [37.000, 216.000], mean observation: 0.526 [0.000, 106.000], loss: 4.414619, mean_absolute_error: 0.619890, mean_q: 5.802641, mean_eps: 0.100000\n",
      "  60926/175000: episode: 1710, duration: 0.834s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 118.711 [58.000, 219.000], mean observation: 0.242 [0.000, 76.000], loss: 27.503890, mean_absolute_error: 0.745985, mean_q: 5.899248, mean_eps: 0.100000\n",
      "  60982/175000: episode: 1711, duration: 1.216s, episode steps: 56, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 142.339 [37.000, 188.000], mean observation: 0.301 [0.000, 112.000], loss: 12.636260, mean_absolute_error: 0.639187, mean_q: 5.618490, mean_eps: 0.100000\n",
      "  61018/175000: episode: 1712, duration: 0.762s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 152.639 [15.000, 183.000], mean observation: 0.196 [0.000, 72.000], loss: 11.772203, mean_absolute_error: 0.688546, mean_q: 5.945489, mean_eps: 0.100000\n",
      "  61043/175000: episode: 1713, duration: 0.524s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 150.760 [58.000, 176.000], mean observation: 0.097 [0.000, 50.000], loss: 0.308449, mean_absolute_error: 0.445803, mean_q: 4.791879, mean_eps: 0.100000\n",
      "  61091/175000: episode: 1714, duration: 1.037s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 137.375 [21.000, 176.000], mean observation: 0.261 [0.000, 96.000], loss: 5.837027, mean_absolute_error: 0.613812, mean_q: 5.727990, mean_eps: 0.100000\n",
      "  61126/175000: episode: 1715, duration: 0.769s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.514 [4.000, 223.000], mean observation: 0.375 [0.000, 70.000], loss: 3.058018, mean_absolute_error: 0.554817, mean_q: 5.560183, mean_eps: 0.100000\n",
      "  61174/175000: episode: 1716, duration: 1.066s, episode steps: 48, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.729 [22.000, 212.000], mean observation: 0.404 [0.000, 96.000], loss: 27.013983, mean_absolute_error: 0.703434, mean_q: 5.823912, mean_eps: 0.100000\n",
      "  61197/175000: episode: 1717, duration: 0.527s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 115.870 [76.000, 176.000], mean observation: 0.104 [0.000, 46.000], loss: 18.909102, mean_absolute_error: 0.715227, mean_q: 6.155952, mean_eps: 0.100000\n",
      "  61209/175000: episode: 1718, duration: 0.265s, episode steps: 12, steps per second: 45, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 78.083 [1.000, 176.000], mean observation: 0.039 [0.000, 24.000], loss: 52.298720, mean_absolute_error: 0.935109, mean_q: 6.359159, mean_eps: 0.100000\n",
      "  61221/175000: episode: 1719, duration: 0.260s, episode steps: 12, steps per second: 46, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 84.333 [76.000, 176.000], mean observation: 0.038 [0.000, 24.000], loss: 0.123059, mean_absolute_error: 0.400986, mean_q: 4.492319, mean_eps: 0.100000\n",
      "  61246/175000: episode: 1720, duration: 0.494s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 104.640 [29.000, 176.000], mean observation: 0.106 [0.000, 50.000], loss: 0.246805, mean_absolute_error: 0.519297, mean_q: 5.252873, mean_eps: 0.100000\n",
      "  61291/175000: episode: 1721, duration: 0.941s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.267 [5.000, 182.000], mean observation: 0.297 [0.000, 90.000], loss: 10.786651, mean_absolute_error: 0.633956, mean_q: 5.689885, mean_eps: 0.100000\n",
      "  61322/175000: episode: 1722, duration: 0.716s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 120.774 [21.000, 211.000], mean observation: 0.228 [0.000, 62.000], loss: 5.119672, mean_absolute_error: 0.657606, mean_q: 5.995652, mean_eps: 0.100000\n",
      "  61364/175000: episode: 1723, duration: 0.916s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.452 [24.000, 191.000], mean observation: 0.311 [0.000, 84.000], loss: 11.601989, mean_absolute_error: 0.692856, mean_q: 6.022731, mean_eps: 0.100000\n",
      "  61407/175000: episode: 1724, duration: 0.964s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.907 [28.000, 176.000], mean observation: 0.267 [0.000, 86.000], loss: 3.411199, mean_absolute_error: 0.641752, mean_q: 5.899288, mean_eps: 0.100000\n",
      "  61466/175000: episode: 1725, duration: 1.295s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 147.136 [8.000, 218.000], mean observation: 0.445 [0.000, 118.000], loss: 0.287989, mean_absolute_error: 0.520759, mean_q: 5.194358, mean_eps: 0.100000\n",
      "  61503/175000: episode: 1726, duration: 0.764s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.108 [28.000, 216.000], mean observation: 0.189 [0.000, 74.000], loss: 17.903895, mean_absolute_error: 0.654819, mean_q: 5.536022, mean_eps: 0.100000\n",
      "  61539/175000: episode: 1727, duration: 0.695s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.111 [28.000, 223.000], mean observation: 0.442 [0.000, 72.000], loss: 0.205395, mean_absolute_error: 0.433026, mean_q: 4.490056, mean_eps: 0.100000\n",
      "  61595/175000: episode: 1728, duration: 1.165s, episode steps: 56, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 104.696 [13.000, 212.000], mean observation: 0.325 [0.000, 112.000], loss: 0.550871, mean_absolute_error: 0.521968, mean_q: 5.012722, mean_eps: 0.100000\n",
      "  61623/175000: episode: 1729, duration: 0.602s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.500 [8.000, 203.000], mean observation: 0.129 [0.000, 56.000], loss: 32.023632, mean_absolute_error: 0.967616, mean_q: 6.917364, mean_eps: 0.100000\n",
      "  61674/175000: episode: 1730, duration: 1.117s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 37.196 [25.000, 176.000], mean observation: 0.234 [0.000, 102.000], loss: 11.029651, mean_absolute_error: 0.716311, mean_q: 5.951287, mean_eps: 0.100000\n",
      "  61718/175000: episode: 1731, duration: 0.964s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 31.000 [28.000, 153.000], mean observation: 0.165 [0.000, 88.000], loss: 10.489231, mean_absolute_error: 0.793882, mean_q: 6.478064, mean_eps: 0.100000\n",
      "  61755/175000: episode: 1732, duration: 0.806s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 30.081 [26.000, 107.000], mean observation: 0.122 [0.000, 74.000], loss: 4.874324, mean_absolute_error: 0.799423, mean_q: 6.702920, mean_eps: 0.100000\n",
      "  61781/175000: episode: 1733, duration: 0.616s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.269 [28.000, 224.000], mean observation: 0.181 [0.000, 52.000], loss: 33.936865, mean_absolute_error: 0.716328, mean_q: 5.456864, mean_eps: 0.100000\n",
      "  61821/175000: episode: 1734, duration: 0.868s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.275 [28.000, 208.000], mean observation: 0.283 [0.000, 80.000], loss: 27.435671, mean_absolute_error: 0.855961, mean_q: 6.597758, mean_eps: 0.100000\n",
      "  61838/175000: episode: 1735, duration: 0.350s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 151.353 [83.000, 176.000], mean observation: 0.069 [0.000, 34.000], loss: 0.132712, mean_absolute_error: 0.433143, mean_q: 4.700998, mean_eps: 0.100000\n",
      "  61893/175000: episode: 1736, duration: 1.278s, episode steps: 55, steps per second: 43, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 166.618 [32.000, 212.000], mean observation: 0.495 [0.000, 110.000], loss: 9.815995, mean_absolute_error: 0.612409, mean_q: 5.630738, mean_eps: 0.100000\n",
      "  61947/175000: episode: 1737, duration: 1.104s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 158.444 [15.000, 221.000], mean observation: 0.641 [0.000, 108.000], loss: 24.413026, mean_absolute_error: 0.710657, mean_q: 6.009098, mean_eps: 0.100000\n",
      "  61981/175000: episode: 1738, duration: 0.797s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.324 [118.000, 135.000], mean observation: 0.118 [0.000, 68.000], loss: 0.196325, mean_absolute_error: 0.451179, mean_q: 5.050821, mean_eps: 0.100000\n",
      "  62000/175000: episode: 1739, duration: 0.464s, episode steps: 19, steps per second: 41, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 120.421 [50.000, 161.000], mean observation: 0.116 [0.000, 38.000], loss: 13.419942, mean_absolute_error: 0.729197, mean_q: 6.476435, mean_eps: 0.100000\n",
      "  62025/175000: episode: 1740, duration: 0.561s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 117.960 [7.000, 176.000], mean observation: 0.094 [0.000, 50.000], loss: 1.290111, mean_absolute_error: 0.414101, mean_q: 4.677970, mean_eps: 0.100000\n",
      "  62053/175000: episode: 1741, duration: 0.626s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 74.143 [11.000, 122.000], mean observation: 0.150 [0.000, 56.000], loss: 9.281042, mean_absolute_error: 0.630053, mean_q: 5.857211, mean_eps: 0.100000\n",
      "  62096/175000: episode: 1742, duration: 0.924s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.419 [20.000, 204.000], mean observation: 0.263 [0.000, 86.000], loss: 8.083178, mean_absolute_error: 0.756992, mean_q: 6.749409, mean_eps: 0.100000\n",
      "  62143/175000: episode: 1743, duration: 0.975s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.681 [36.000, 224.000], mean observation: 0.374 [0.000, 94.000], loss: 6.032317, mean_absolute_error: 0.621658, mean_q: 5.920766, mean_eps: 0.100000\n",
      "  62199/175000: episode: 1744, duration: 1.189s, episode steps: 56, steps per second: 47, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 102.143 [14.000, 170.000], mean observation: 0.539 [0.000, 112.000], loss: 0.745892, mean_absolute_error: 0.512484, mean_q: 5.316025, mean_eps: 0.100000\n",
      "  62228/175000: episode: 1745, duration: 0.655s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 116.586 [28.000, 174.000], mean observation: 0.182 [0.000, 58.000], loss: 5.566424, mean_absolute_error: 0.631826, mean_q: 5.931098, mean_eps: 0.100000\n",
      "  62260/175000: episode: 1746, duration: 0.722s, episode steps: 32, steps per second: 44, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 113.719 [29.000, 192.000], mean observation: 0.237 [0.000, 64.000], loss: 18.799284, mean_absolute_error: 0.852866, mean_q: 6.907928, mean_eps: 0.100000\n",
      "  62298/175000: episode: 1747, duration: 0.819s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.605 [9.000, 171.000], mean observation: 0.273 [0.000, 76.000], loss: 35.364278, mean_absolute_error: 1.109412, mean_q: 8.169170, mean_eps: 0.100000\n",
      "  62330/175000: episode: 1748, duration: 0.706s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.031 [43.000, 214.000], mean observation: 0.241 [0.000, 64.000], loss: 60.078225, mean_absolute_error: 0.999375, mean_q: 6.659664, mean_eps: 0.100000\n",
      "  62363/175000: episode: 1749, duration: 0.701s, episode steps: 33, steps per second: 47, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 102.242 [36.000, 176.000], mean observation: 0.280 [0.000, 66.000], loss: 13.558572, mean_absolute_error: 0.674751, mean_q: 5.839313, mean_eps: 0.100000\n",
      "  62407/175000: episode: 1750, duration: 0.997s, episode steps: 44, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.682 [10.000, 212.000], mean observation: 0.430 [0.000, 88.000], loss: 4.533536, mean_absolute_error: 0.551094, mean_q: 5.325033, mean_eps: 0.100000\n",
      "  62438/175000: episode: 1751, duration: 0.696s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 116.452 [35.000, 221.000], mean observation: 0.173 [0.000, 62.000], loss: 9.353217, mean_absolute_error: 0.623481, mean_q: 5.640208, mean_eps: 0.100000\n",
      "  62470/175000: episode: 1752, duration: 0.699s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 89.375 [23.000, 122.000], mean observation: 0.138 [0.000, 64.000], loss: 0.273228, mean_absolute_error: 0.455710, mean_q: 4.785941, mean_eps: 0.100000\n",
      "  62509/175000: episode: 1753, duration: 0.845s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 90.615 [0.000, 194.000], mean observation: 0.213 [0.000, 78.000], loss: 0.217131, mean_absolute_error: 0.457700, mean_q: 4.761196, mean_eps: 0.100000\n",
      "  62547/175000: episode: 1754, duration: 0.797s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.500 [57.000, 122.000], mean observation: 0.192 [0.000, 76.000], loss: 0.283006, mean_absolute_error: 0.474397, mean_q: 4.850465, mean_eps: 0.100000\n",
      "  62580/175000: episode: 1755, duration: 0.758s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 107.000 [37.000, 190.000], mean observation: 0.171 [0.000, 66.000], loss: 18.194761, mean_absolute_error: 0.598322, mean_q: 4.973907, mean_eps: 0.100000\n",
      "  62618/175000: episode: 1756, duration: 0.892s, episode steps: 38, steps per second: 43, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.026 [37.000, 202.000], mean observation: 0.303 [0.000, 76.000], loss: 3.074280, mean_absolute_error: 0.440080, mean_q: 4.356324, mean_eps: 0.100000\n",
      "  62667/175000: episode: 1757, duration: 1.121s, episode steps: 49, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.776 [7.000, 195.000], mean observation: 0.337 [0.000, 98.000], loss: 7.304338, mean_absolute_error: 0.486231, mean_q: 4.610131, mean_eps: 0.100000\n",
      "  62694/175000: episode: 1758, duration: 0.578s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 123.556 [29.000, 203.000], mean observation: 0.161 [0.000, 54.000], loss: 2.698652, mean_absolute_error: 0.431758, mean_q: 4.264270, mean_eps: 0.100000\n",
      "  62730/175000: episode: 1759, duration: 0.778s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 140.778 [40.000, 205.000], mean observation: 0.197 [0.000, 72.000], loss: 29.743113, mean_absolute_error: 0.849093, mean_q: 6.151817, mean_eps: 0.100000\n",
      "  62772/175000: episode: 1760, duration: 0.961s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.190 [10.000, 190.000], mean observation: 0.257 [0.000, 84.000], loss: 14.251815, mean_absolute_error: 0.694046, mean_q: 5.605181, mean_eps: 0.100000\n",
      "  62808/175000: episode: 1761, duration: 0.825s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 123.500 [7.000, 190.000], mean observation: 0.269 [0.000, 72.000], loss: 16.771960, mean_absolute_error: 0.589940, mean_q: 4.843120, mean_eps: 0.100000\n",
      "  62853/175000: episode: 1762, duration: 1.013s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 129.911 [48.000, 190.000], mean observation: 0.236 [0.000, 90.000], loss: 12.304883, mean_absolute_error: 0.557624, mean_q: 4.824182, mean_eps: 0.100000\n",
      "  62897/175000: episode: 1763, duration: 0.987s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 121.205 [38.000, 218.000], mean observation: 0.331 [0.000, 88.000], loss: 4.226644, mean_absolute_error: 0.615331, mean_q: 5.431546, mean_eps: 0.100000\n",
      "  62943/175000: episode: 1764, duration: 0.946s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.304 [10.000, 200.000], mean observation: 0.402 [0.000, 92.000], loss: 2.330231, mean_absolute_error: 0.453984, mean_q: 4.378090, mean_eps: 0.100000\n",
      "  62987/175000: episode: 1765, duration: 0.930s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.545 [10.000, 200.000], mean observation: 0.432 [0.000, 88.000], loss: 17.247672, mean_absolute_error: 0.690169, mean_q: 5.468667, mean_eps: 0.100000\n",
      "  63023/175000: episode: 1766, duration: 0.824s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.889 [10.000, 200.000], mean observation: 0.351 [0.000, 72.000], loss: 0.357015, mean_absolute_error: 0.436614, mean_q: 4.199301, mean_eps: 0.100000\n",
      "  63084/175000: episode: 1767, duration: 1.478s, episode steps: 61, steps per second: 41, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 104.262 [7.000, 218.000], mean observation: 0.622 [0.000, 122.000], loss: 19.258680, mean_absolute_error: 0.645154, mean_q: 5.178587, mean_eps: 0.100000\n",
      "  63099/175000: episode: 1768, duration: 0.350s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 72.400 [28.000, 118.000], mean observation: 0.065 [0.000, 30.000], loss: 0.798480, mean_absolute_error: 0.416874, mean_q: 4.051844, mean_eps: 0.100000\n",
      "  63129/175000: episode: 1769, duration: 0.796s, episode steps: 30, steps per second: 38, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 89.933 [13.000, 215.000], mean observation: 0.192 [0.000, 60.000], loss: 0.344334, mean_absolute_error: 0.453434, mean_q: 4.581277, mean_eps: 0.100000\n",
      "  63166/175000: episode: 1770, duration: 0.856s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.865 [74.000, 208.000], mean observation: 0.115 [0.000, 74.000], loss: 26.926913, mean_absolute_error: 0.675172, mean_q: 5.187296, mean_eps: 0.100000\n",
      "  63218/175000: episode: 1771, duration: 1.164s, episode steps: 52, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 82.404 [29.000, 209.000], mean observation: 0.289 [0.000, 104.000], loss: 19.104153, mean_absolute_error: 0.585908, mean_q: 4.806631, mean_eps: 0.100000\n",
      "  63243/175000: episode: 1772, duration: 0.538s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 88.360 [33.000, 144.000], mean observation: 0.179 [0.000, 50.000], loss: 4.937593, mean_absolute_error: 0.587860, mean_q: 5.241570, mean_eps: 0.100000\n",
      "  63271/175000: episode: 1773, duration: 0.648s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.500 [65.000, 165.000], mean observation: 0.169 [0.000, 56.000], loss: 39.103653, mean_absolute_error: 0.736663, mean_q: 5.206864, mean_eps: 0.100000\n",
      "  63311/175000: episode: 1774, duration: 0.905s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 108.100 [35.000, 220.000], mean observation: 0.280 [0.000, 80.000], loss: 27.927302, mean_absolute_error: 0.822677, mean_q: 6.148434, mean_eps: 0.100000\n",
      "  63356/175000: episode: 1775, duration: 1.122s, episode steps: 45, steps per second: 40, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 73.156 [15.000, 208.000], mean observation: 0.156 [0.000, 90.000], loss: 40.345204, mean_absolute_error: 0.759599, mean_q: 5.400432, mean_eps: 0.100000\n",
      "  63384/175000: episode: 1776, duration: 0.709s, episode steps: 28, steps per second: 40, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 78.714 [12.000, 209.000], mean observation: 0.153 [0.000, 56.000], loss: 34.679404, mean_absolute_error: 0.869005, mean_q: 6.454244, mean_eps: 0.100000\n",
      "  63406/175000: episode: 1777, duration: 0.493s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 105.545 [15.000, 211.000], mean observation: 0.109 [0.000, 44.000], loss: 1.314702, mean_absolute_error: 0.566403, mean_q: 5.392848, mean_eps: 0.100000\n",
      "  63440/175000: episode: 1778, duration: 0.787s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.500 [15.000, 190.000], mean observation: 0.256 [0.000, 68.000], loss: 0.183029, mean_absolute_error: 0.442544, mean_q: 4.585345, mean_eps: 0.100000\n",
      "  63481/175000: episode: 1779, duration: 0.954s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 81.829 [15.000, 190.000], mean observation: 0.268 [0.000, 82.000], loss: 13.900172, mean_absolute_error: 0.630817, mean_q: 5.386855, mean_eps: 0.100000\n",
      "  63530/175000: episode: 1780, duration: 1.130s, episode steps: 49, steps per second: 43, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.510 [5.000, 186.000], mean observation: 0.235 [0.000, 98.000], loss: 11.987877, mean_absolute_error: 0.735340, mean_q: 6.123842, mean_eps: 0.100000\n",
      "  63572/175000: episode: 1781, duration: 0.970s, episode steps: 42, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.381 [28.000, 203.000], mean observation: 0.210 [0.000, 84.000], loss: 12.907634, mean_absolute_error: 0.689595, mean_q: 5.849271, mean_eps: 0.100000\n",
      "  63600/175000: episode: 1782, duration: 0.620s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 108.643 [86.000, 191.000], mean observation: 0.125 [0.000, 56.000], loss: 9.706859, mean_absolute_error: 0.649981, mean_q: 5.663269, mean_eps: 0.100000\n",
      "  63645/175000: episode: 1783, duration: 1.004s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.711 [77.000, 219.000], mean observation: 0.253 [0.000, 90.000], loss: 1.192874, mean_absolute_error: 0.555698, mean_q: 5.272187, mean_eps: 0.100000\n",
      "  63688/175000: episode: 1784, duration: 0.899s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 105.953 [16.000, 218.000], mean observation: 0.261 [0.000, 86.000], loss: 0.184159, mean_absolute_error: 0.388973, mean_q: 4.198218, mean_eps: 0.100000\n",
      "  63723/175000: episode: 1785, duration: 0.761s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.486 [15.000, 213.000], mean observation: 0.212 [0.000, 70.000], loss: 22.362254, mean_absolute_error: 0.768730, mean_q: 6.044090, mean_eps: 0.100000\n",
      "  63754/175000: episode: 1786, duration: 0.742s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.323 [15.000, 210.000], mean observation: 0.221 [0.000, 62.000], loss: 0.175917, mean_absolute_error: 0.551053, mean_q: 5.266751, mean_eps: 0.100000\n",
      "  63788/175000: episode: 1787, duration: 0.690s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.912 [15.000, 182.000], mean observation: 0.266 [0.000, 68.000], loss: 2.075606, mean_absolute_error: 0.762781, mean_q: 6.544966, mean_eps: 0.100000\n",
      "  63810/175000: episode: 1788, duration: 0.581s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 175.136 [43.000, 182.000], mean observation: 0.086 [0.000, 44.000], loss: 16.594903, mean_absolute_error: 0.675966, mean_q: 5.586617, mean_eps: 0.100000\n",
      "  63857/175000: episode: 1789, duration: 1.053s, episode steps: 47, steps per second: 45, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 183.426 [28.000, 218.000], mean observation: 0.205 [0.000, 94.000], loss: 11.430303, mean_absolute_error: 0.661297, mean_q: 5.646856, mean_eps: 0.100000\n",
      "  63897/175000: episode: 1790, duration: 0.939s, episode steps: 40, steps per second: 43, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 170.150 [70.000, 218.000], mean observation: 0.153 [0.000, 80.000], loss: 0.273677, mean_absolute_error: 0.607301, mean_q: 5.564585, mean_eps: 0.100000\n",
      "  63942/175000: episode: 1791, duration: 0.953s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.022 [17.000, 218.000], mean observation: 0.314 [0.000, 90.000], loss: 4.409014, mean_absolute_error: 0.613074, mean_q: 5.602030, mean_eps: 0.100000\n",
      "  63979/175000: episode: 1792, duration: 0.736s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 120.568 [70.000, 218.000], mean observation: 0.138 [0.000, 74.000], loss: 15.471942, mean_absolute_error: 0.728351, mean_q: 6.014742, mean_eps: 0.100000\n",
      "  64011/175000: episode: 1793, duration: 0.718s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 111.688 [76.000, 221.000], mean observation: 0.083 [0.000, 64.000], loss: 11.231212, mean_absolute_error: 0.831945, mean_q: 6.739021, mean_eps: 0.100000\n",
      "  64057/175000: episode: 1794, duration: 1.027s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 183.000 [39.000, 221.000], mean observation: 0.241 [0.000, 92.000], loss: 43.687640, mean_absolute_error: 0.872710, mean_q: 6.090187, mean_eps: 0.100000\n",
      "  64087/175000: episode: 1795, duration: 0.646s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 193.000 [1.000, 221.000], mean observation: 0.132 [0.000, 60.000], loss: 31.387544, mean_absolute_error: 1.260505, mean_q: 9.033992, mean_eps: 0.100000\n",
      "  64131/175000: episode: 1796, duration: 1.012s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.659 [15.000, 221.000], mean observation: 0.211 [0.000, 88.000], loss: 2.946675, mean_absolute_error: 0.739620, mean_q: 6.506133, mean_eps: 0.100000\n",
      "  64160/175000: episode: 1797, duration: 0.706s, episode steps: 29, steps per second: 41, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 83.897 [3.000, 221.000], mean observation: 0.220 [0.000, 58.000], loss: 43.802495, mean_absolute_error: 0.893402, mean_q: 6.347217, mean_eps: 0.100000\n",
      "  64181/175000: episode: 1798, duration: 0.493s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 136.905 [12.000, 212.000], mean observation: 0.072 [0.000, 42.000], loss: 13.105470, mean_absolute_error: 0.782382, mean_q: 6.669906, mean_eps: 0.100000\n",
      "  64207/175000: episode: 1799, duration: 0.435s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 158.038 [87.000, 204.000], mean observation: 0.096 [0.000, 52.000], loss: 24.178375, mean_absolute_error: 0.895324, mean_q: 7.274692, mean_eps: 0.100000\n",
      "  64256/175000: episode: 1800, duration: 0.955s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.673 [3.000, 186.000], mean observation: 0.389 [0.000, 98.000], loss: 11.707150, mean_absolute_error: 0.658797, mean_q: 6.102135, mean_eps: 0.100000\n",
      "  64298/175000: episode: 1801, duration: 0.809s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.762 [3.000, 186.000], mean observation: 0.165 [0.000, 84.000], loss: 1.923204, mean_absolute_error: 0.522727, mean_q: 5.627380, mean_eps: 0.100000\n",
      "  64330/175000: episode: 1802, duration: 0.635s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 80.750 [1.000, 221.000], mean observation: 0.377 [0.000, 64.000], loss: 9.044634, mean_absolute_error: 0.598423, mean_q: 5.776255, mean_eps: 0.100000\n",
      "  64364/175000: episode: 1803, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.941 [27.000, 221.000], mean observation: 0.117 [0.000, 68.000], loss: 0.205932, mean_absolute_error: 0.548456, mean_q: 5.672620, mean_eps: 0.100000\n",
      "  64414/175000: episode: 1804, duration: 1.026s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 50.900 [27.000, 137.000], mean observation: 0.372 [0.000, 100.000], loss: 9.361634, mean_absolute_error: 0.721820, mean_q: 6.747935, mean_eps: 0.100000\n",
      "  64436/175000: episode: 1805, duration: 0.425s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 33.864 [27.000, 95.000], mean observation: 0.090 [0.000, 44.000], loss: 0.114913, mean_absolute_error: 0.403427, mean_q: 4.861033, mean_eps: 0.100000\n",
      "  64452/175000: episode: 1806, duration: 0.377s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 47.438 [27.000, 116.000], mean observation: 0.063 [0.000, 32.000], loss: 0.226946, mean_absolute_error: 0.390941, mean_q: 4.698382, mean_eps: 0.100000\n",
      "  64508/175000: episode: 1807, duration: 1.142s, episode steps: 56, steps per second: 49, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 57.893 [27.000, 218.000], mean observation: 0.188 [0.000, 112.000], loss: 4.131946, mean_absolute_error: 0.591153, mean_q: 5.943386, mean_eps: 0.100000\n",
      "  64544/175000: episode: 1808, duration: 0.761s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 49.250 [40.000, 180.000], mean observation: 0.120 [0.000, 72.000], loss: 0.236020, mean_absolute_error: 0.418410, mean_q: 5.015518, mean_eps: 0.100000\n",
      "  64590/175000: episode: 1809, duration: 0.911s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 59.326 [0.000, 162.000], mean observation: 0.332 [0.000, 92.000], loss: 3.039708, mean_absolute_error: 0.507219, mean_q: 5.392679, mean_eps: 0.100000\n",
      "  64643/175000: episode: 1810, duration: 1.164s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 109.189 [40.000, 176.000], mean observation: 0.379 [0.000, 106.000], loss: 0.499131, mean_absolute_error: 0.485500, mean_q: 5.266628, mean_eps: 0.100000\n",
      "  64674/175000: episode: 1811, duration: 0.749s, episode steps: 31, steps per second: 41, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 81.000 [10.000, 176.000], mean observation: 0.223 [0.000, 62.000], loss: 0.155399, mean_absolute_error: 0.402608, mean_q: 4.742933, mean_eps: 0.100000\n",
      "  64707/175000: episode: 1812, duration: 0.667s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 68.212 [27.000, 181.000], mean observation: 0.139 [0.000, 66.000], loss: 6.047505, mean_absolute_error: 0.671356, mean_q: 6.210323, mean_eps: 0.100000\n",
      "  64756/175000: episode: 1813, duration: 1.255s, episode steps: 49, steps per second: 39, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 144.980 [40.000, 207.000], mean observation: 0.274 [0.000, 98.000], loss: 11.626582, mean_absolute_error: 0.735953, mean_q: 6.498959, mean_eps: 0.100000\n",
      "  64794/175000: episode: 1814, duration: 0.919s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.474 [27.000, 172.000], mean observation: 0.320 [0.000, 76.000], loss: 11.944664, mean_absolute_error: 0.672939, mean_q: 6.224966, mean_eps: 0.100000\n",
      "  64828/175000: episode: 1815, duration: 0.807s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 143.853 [40.000, 207.000], mean observation: 0.215 [0.000, 68.000], loss: 13.628434, mean_absolute_error: 0.626585, mean_q: 5.809583, mean_eps: 0.100000\n",
      "  64847/175000: episode: 1816, duration: 0.443s, episode steps: 19, steps per second: 43, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 156.895 [99.000, 172.000], mean observation: 0.069 [0.000, 38.000], loss: 27.359208, mean_absolute_error: 0.707546, mean_q: 5.807482, mean_eps: 0.100000\n",
      "  64863/175000: episode: 1817, duration: 0.421s, episode steps: 16, steps per second: 38, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 129.688 [27.000, 162.000], mean observation: 0.061 [0.000, 32.000], loss: 0.068187, mean_absolute_error: 0.414672, mean_q: 4.794432, mean_eps: 0.100000\n",
      "  64890/175000: episode: 1818, duration: 0.682s, episode steps: 27, steps per second: 40, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.185 [27.000, 194.000], mean observation: 0.167 [0.000, 54.000], loss: 54.748811, mean_absolute_error: 1.069510, mean_q: 7.531328, mean_eps: 0.100000\n",
      "  64907/175000: episode: 1819, duration: 0.375s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 134.000 [27.000, 162.000], mean observation: 0.072 [0.000, 34.000], loss: 58.173192, mean_absolute_error: 0.934740, mean_q: 6.686133, mean_eps: 0.100000\n",
      "  64940/175000: episode: 1820, duration: 0.787s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.333 [13.000, 172.000], mean observation: 0.236 [0.000, 66.000], loss: 0.300013, mean_absolute_error: 0.406583, mean_q: 4.829399, mean_eps: 0.100000\n",
      "  64968/175000: episode: 1821, duration: 0.713s, episode steps: 28, steps per second: 39, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 115.893 [27.000, 218.000], mean observation: 0.246 [0.000, 56.000], loss: 0.089917, mean_absolute_error: 0.400520, mean_q: 4.906330, mean_eps: 0.100000\n",
      "  64992/175000: episode: 1822, duration: 0.569s, episode steps: 24, steps per second: 42, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 126.958 [22.000, 216.000], mean observation: 0.146 [0.000, 48.000], loss: 9.225678, mean_absolute_error: 0.645482, mean_q: 6.242297, mean_eps: 0.100000\n",
      "  65022/175000: episode: 1823, duration: 0.648s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 133.133 [9.000, 162.000], mean observation: 0.186 [0.000, 60.000], loss: 11.903264, mean_absolute_error: 0.592004, mean_q: 5.604559, mean_eps: 0.100000\n",
      "  65071/175000: episode: 1824, duration: 0.916s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 106.469 [39.000, 200.000], mean observation: 0.423 [0.000, 98.000], loss: 0.179484, mean_absolute_error: 0.407802, mean_q: 4.755869, mean_eps: 0.100000\n",
      "  65115/175000: episode: 1825, duration: 0.900s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.386 [41.000, 222.000], mean observation: 0.363 [0.000, 88.000], loss: 42.084188, mean_absolute_error: 0.881463, mean_q: 6.707853, mean_eps: 0.100000\n",
      "  65155/175000: episode: 1826, duration: 0.877s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 106.825 [3.000, 209.000], mean observation: 0.303 [0.000, 80.000], loss: 0.305657, mean_absolute_error: 0.530122, mean_q: 5.631816, mean_eps: 0.100000\n",
      "  65171/175000: episode: 1827, duration: 0.340s, episode steps: 16, steps per second: 47, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 115.312 [28.000, 162.000], mean observation: 0.070 [0.000, 32.000], loss: 0.171139, mean_absolute_error: 0.441197, mean_q: 5.006794, mean_eps: 0.100000\n",
      "  65210/175000: episode: 1828, duration: 0.798s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.949 [8.000, 144.000], mean observation: 0.240 [0.000, 78.000], loss: 25.745268, mean_absolute_error: 0.828178, mean_q: 6.881493, mean_eps: 0.100000\n",
      "  65253/175000: episode: 1829, duration: 0.836s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 140.651 [0.000, 216.000], mean observation: 0.240 [0.000, 86.000], loss: 16.875629, mean_absolute_error: 0.755807, mean_q: 6.732322, mean_eps: 0.100000\n",
      "  65296/175000: episode: 1830, duration: 0.815s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 157.279 [28.000, 207.000], mean observation: 0.285 [0.000, 86.000], loss: 0.136982, mean_absolute_error: 0.426032, mean_q: 4.965025, mean_eps: 0.100000\n",
      "  65336/175000: episode: 1831, duration: 0.809s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 138.250 [1.000, 207.000], mean observation: 0.325 [0.000, 80.000], loss: 18.587289, mean_absolute_error: 0.720498, mean_q: 6.457002, mean_eps: 0.100000\n",
      "  65371/175000: episode: 1832, duration: 0.702s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 137.514 [94.000, 207.000], mean observation: 0.148 [0.000, 70.000], loss: 24.821724, mean_absolute_error: 0.688831, mean_q: 6.068582, mean_eps: 0.100000\n",
      "  65395/175000: episode: 1833, duration: 0.460s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 87.833 [68.000, 173.000], mean observation: 0.132 [0.000, 48.000], loss: 28.022229, mean_absolute_error: 0.861470, mean_q: 7.171325, mean_eps: 0.100000\n",
      "  65410/175000: episode: 1834, duration: 0.330s, episode steps: 15, steps per second: 45, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 112.133 [107.000, 118.000], mean observation: 0.043 [0.000, 30.000], loss: 65.090411, mean_absolute_error: 1.196428, mean_q: 8.177466, mean_eps: 0.100000\n",
      "  65435/175000: episode: 1835, duration: 0.490s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 121.400 [6.000, 167.000], mean observation: 0.092 [0.000, 50.000], loss: 40.123982, mean_absolute_error: 0.767553, mean_q: 6.243529, mean_eps: 0.100000\n",
      "  65469/175000: episode: 1836, duration: 0.662s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.324 [70.000, 172.000], mean observation: 0.180 [0.000, 68.000], loss: 8.471733, mean_absolute_error: 0.623867, mean_q: 6.244227, mean_eps: 0.100000\n",
      "  65526/175000: episode: 1837, duration: 1.082s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 141.158 [14.000, 202.000], mean observation: 0.386 [0.000, 114.000], loss: 12.187156, mean_absolute_error: 0.596447, mean_q: 5.875235, mean_eps: 0.100000\n",
      "  65564/175000: episode: 1838, duration: 0.730s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 147.526 [52.000, 183.000], mean observation: 0.340 [0.000, 76.000], loss: 48.674703, mean_absolute_error: 1.030795, mean_q: 7.815511, mean_eps: 0.100000\n",
      "  65622/175000: episode: 1839, duration: 1.216s, episode steps: 58, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 61.638 [9.000, 172.000], mean observation: 0.272 [0.000, 116.000], loss: 12.269754, mean_absolute_error: 0.606628, mean_q: 6.092140, mean_eps: 0.100000\n",
      "  65665/175000: episode: 1840, duration: 1.041s, episode steps: 43, steps per second: 41, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.116 [9.000, 218.000], mean observation: 0.291 [0.000, 86.000], loss: 41.697173, mean_absolute_error: 1.058605, mean_q: 8.032584, mean_eps: 0.100000\n",
      "  65703/175000: episode: 1841, duration: 0.806s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.632 [52.000, 174.000], mean observation: 0.228 [0.000, 76.000], loss: 23.327541, mean_absolute_error: 0.719604, mean_q: 6.399108, mean_eps: 0.100000\n",
      "  65737/175000: episode: 1842, duration: 0.894s, episode steps: 34, steps per second: 38, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.559 [52.000, 198.000], mean observation: 0.221 [0.000, 68.000], loss: 1.511504, mean_absolute_error: 0.463966, mean_q: 5.358101, mean_eps: 0.100000\n",
      "  65768/175000: episode: 1843, duration: 0.804s, episode steps: 31, steps per second: 39, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.387 [51.000, 174.000], mean observation: 0.195 [0.000, 62.000], loss: 14.514331, mean_absolute_error: 0.627918, mean_q: 6.315702, mean_eps: 0.100000\n",
      "  65808/175000: episode: 1844, duration: 1.003s, episode steps: 40, steps per second: 40, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 136.075 [25.000, 207.000], mean observation: 0.369 [0.000, 80.000], loss: 19.483722, mean_absolute_error: 0.639184, mean_q: 6.147862, mean_eps: 0.100000\n",
      "  65869/175000: episode: 1845, duration: 1.582s, episode steps: 61, steps per second: 39, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 139.180 [13.000, 220.000], mean observation: 0.561 [0.000, 122.000], loss: 0.554991, mean_absolute_error: 0.538107, mean_q: 6.069471, mean_eps: 0.100000\n",
      "  65889/175000: episode: 1846, duration: 0.527s, episode steps: 20, steps per second: 38, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 54.750 [28.000, 162.000], mean observation: 0.053 [0.000, 40.000], loss: 56.237822, mean_absolute_error: 0.865908, mean_q: 6.634864, mean_eps: 0.100000\n",
      "  65907/175000: episode: 1847, duration: 0.416s, episode steps: 18, steps per second: 43, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.044 [0.000, 36.000], loss: 0.188293, mean_absolute_error: 0.465053, mean_q: 5.580339, mean_eps: 0.100000\n",
      "  65957/175000: episode: 1848, duration: 1.211s, episode steps: 50, steps per second: 41, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 80.800 [28.000, 177.000], mean observation: 0.268 [0.000, 100.000], loss: 9.387882, mean_absolute_error: 0.525484, mean_q: 5.719881, mean_eps: 0.100000\n",
      "  66005/175000: episode: 1849, duration: 1.101s, episode steps: 48, steps per second: 44, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.083 [10.000, 162.000], mean observation: 0.367 [0.000, 96.000], loss: 10.025339, mean_absolute_error: 0.560488, mean_q: 5.883733, mean_eps: 0.100000\n",
      "  66038/175000: episode: 1850, duration: 0.652s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 48.667 [28.000, 196.000], mean observation: 0.136 [0.000, 66.000], loss: 26.659511, mean_absolute_error: 0.681452, mean_q: 6.337864, mean_eps: 0.100000\n",
      "  66097/175000: episode: 1851, duration: 1.245s, episode steps: 59, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.424 [9.000, 218.000], mean observation: 0.426 [0.000, 118.000], loss: 2.498662, mean_absolute_error: 0.492606, mean_q: 5.701628, mean_eps: 0.100000\n",
      "  66125/175000: episode: 1852, duration: 0.685s, episode steps: 28, steps per second: 41, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.066 [0.000, 56.000], loss: 51.867539, mean_absolute_error: 0.977149, mean_q: 7.672759, mean_eps: 0.100000\n",
      "  66150/175000: episode: 1853, duration: 0.561s, episode steps: 25, steps per second: 45, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 81.680 [28.000, 202.000], mean observation: 0.121 [0.000, 50.000], loss: 33.729602, mean_absolute_error: 0.726530, mean_q: 6.463658, mean_eps: 0.100000\n",
      "  66190/175000: episode: 1854, duration: 0.854s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 30.275 [28.000, 119.000], mean observation: 0.116 [0.000, 80.000], loss: 41.672383, mean_absolute_error: 0.839719, mean_q: 7.089033, mean_eps: 0.100000\n",
      "  66217/175000: episode: 1855, duration: 0.634s, episode steps: 27, steps per second: 43, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.889 [28.000, 128.000], mean observation: 0.082 [0.000, 54.000], loss: 4.924550, mean_absolute_error: 0.590088, mean_q: 6.395655, mean_eps: 0.100000\n",
      "  66252/175000: episode: 1856, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.286 [28.000, 203.000], mean observation: 0.113 [0.000, 70.000], loss: 19.985986, mean_absolute_error: 0.701838, mean_q: 6.966030, mean_eps: 0.100000\n",
      "  66282/175000: episode: 1857, duration: 0.601s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 85.200 [28.000, 159.000], mean observation: 0.138 [0.000, 60.000], loss: 0.214326, mean_absolute_error: 0.430781, mean_q: 5.460288, mean_eps: 0.100000\n",
      "  66324/175000: episode: 1858, duration: 0.878s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.595 [0.000, 207.000], mean observation: 0.375 [0.000, 84.000], loss: 14.556595, mean_absolute_error: 0.796270, mean_q: 7.546836, mean_eps: 0.100000\n",
      "  66352/175000: episode: 1859, duration: 0.554s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 90.500 [18.000, 159.000], mean observation: 0.127 [0.000, 56.000], loss: 10.439262, mean_absolute_error: 0.813022, mean_q: 7.759048, mean_eps: 0.100000\n",
      "  66402/175000: episode: 1860, duration: 1.062s, episode steps: 50, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 59.780 [26.000, 159.000], mean observation: 0.255 [0.000, 100.000], loss: 34.308664, mean_absolute_error: 0.913422, mean_q: 7.790582, mean_eps: 0.100000\n",
      "  66424/175000: episode: 1861, duration: 0.491s, episode steps: 22, steps per second: 45, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 109.045 [28.000, 215.000], mean observation: 0.099 [0.000, 44.000], loss: 44.099279, mean_absolute_error: 0.868418, mean_q: 7.168876, mean_eps: 0.100000\n",
      "  66468/175000: episode: 1862, duration: 0.925s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.273 [15.000, 191.000], mean observation: 0.413 [0.000, 88.000], loss: 9.302595, mean_absolute_error: 0.650678, mean_q: 6.847604, mean_eps: 0.100000\n",
      "  66502/175000: episode: 1863, duration: 0.668s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 77.588 [28.000, 208.000], mean observation: 0.124 [0.000, 68.000], loss: 0.238790, mean_absolute_error: 0.488507, mean_q: 6.014602, mean_eps: 0.100000\n",
      "  66538/175000: episode: 1864, duration: 0.711s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 40.083 [8.000, 118.000], mean observation: 0.134 [0.000, 72.000], loss: 27.896081, mean_absolute_error: 0.937297, mean_q: 8.315169, mean_eps: 0.100000\n",
      "  66586/175000: episode: 1865, duration: 0.904s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.104 [28.000, 205.000], mean observation: 0.585 [0.000, 96.000], loss: 19.573184, mean_absolute_error: 0.871768, mean_q: 8.011470, mean_eps: 0.100000\n",
      "  66622/175000: episode: 1866, duration: 0.679s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.111 [28.000, 200.000], mean observation: 0.223 [0.000, 72.000], loss: 3.893359, mean_absolute_error: 0.616415, mean_q: 6.811553, mean_eps: 0.100000\n",
      "  66659/175000: episode: 1867, duration: 0.682s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 88.108 [4.000, 158.000], mean observation: 0.239 [0.000, 74.000], loss: 3.364942, mean_absolute_error: 0.576851, mean_q: 6.538236, mean_eps: 0.100000\n",
      "  66688/175000: episode: 1868, duration: 0.566s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 97.621 [4.000, 178.000], mean observation: 0.254 [0.000, 58.000], loss: 17.739083, mean_absolute_error: 0.690476, mean_q: 7.015155, mean_eps: 0.100000\n",
      "  66729/175000: episode: 1869, duration: 0.889s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 128.829 [1.000, 221.000], mean observation: 0.493 [0.000, 82.000], loss: 4.036756, mean_absolute_error: 0.576850, mean_q: 6.597752, mean_eps: 0.100000\n",
      "  66759/175000: episode: 1870, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 215.367 [52.000, 221.000], mean observation: 0.083 [0.000, 60.000], loss: 2.676035, mean_absolute_error: 0.733594, mean_q: 7.521869, mean_eps: 0.100000\n",
      "  66811/175000: episode: 1871, duration: 1.090s, episode steps: 52, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 166.923 [49.000, 221.000], mean observation: 0.240 [0.000, 104.000], loss: 18.288676, mean_absolute_error: 0.624051, mean_q: 6.334397, mean_eps: 0.100000\n",
      "  66854/175000: episode: 1872, duration: 0.826s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 152.605 [28.000, 221.000], mean observation: 0.339 [0.000, 86.000], loss: 31.949816, mean_absolute_error: 0.793620, mean_q: 7.333094, mean_eps: 0.100000\n",
      "  66897/175000: episode: 1873, duration: 0.766s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 78.628 [28.000, 221.000], mean observation: 0.205 [0.000, 86.000], loss: 7.533470, mean_absolute_error: 0.580334, mean_q: 6.434816, mean_eps: 0.100000\n",
      "  66916/175000: episode: 1874, duration: 0.352s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 39.316 [13.000, 223.000], mean observation: 0.100 [0.000, 38.000], loss: 0.130172, mean_absolute_error: 0.419223, mean_q: 5.400960, mean_eps: 0.100000\n",
      "  66935/175000: episode: 1875, duration: 0.358s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 31.105 [28.000, 87.000], mean observation: 0.059 [0.000, 38.000], loss: 5.027721, mean_absolute_error: 0.666898, mean_q: 7.156794, mean_eps: 0.100000\n",
      "  66973/175000: episode: 1876, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.763 [28.000, 197.000], mean observation: 0.224 [0.000, 76.000], loss: 1.790554, mean_absolute_error: 0.549523, mean_q: 6.391402, mean_eps: 0.100000\n",
      "  66988/175000: episode: 1877, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 45.667 [28.000, 177.000], mean observation: 0.049 [0.000, 30.000], loss: 86.826890, mean_absolute_error: 1.552031, mean_q: 10.916541, mean_eps: 0.100000\n",
      "  67015/175000: episode: 1878, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.064 [0.000, 54.000], loss: 18.049036, mean_absolute_error: 0.694017, mean_q: 7.059081, mean_eps: 0.100000\n",
      "  67050/175000: episode: 1879, duration: 0.729s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 35.343 [6.000, 199.000], mean observation: 0.193 [0.000, 70.000], loss: 4.456017, mean_absolute_error: 0.568580, mean_q: 6.454556, mean_eps: 0.100000\n",
      "  67078/175000: episode: 1880, duration: 0.646s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 31.286 [28.000, 101.000], mean observation: 0.111 [0.000, 56.000], loss: 17.648944, mean_absolute_error: 0.760986, mean_q: 7.521955, mean_eps: 0.100000\n",
      "  67107/175000: episode: 1881, duration: 0.511s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 29.552 [28.000, 57.000], mean observation: 0.114 [0.000, 58.000], loss: 74.134250, mean_absolute_error: 1.302634, mean_q: 9.425617, mean_eps: 0.100000\n",
      "  67134/175000: episode: 1882, duration: 0.512s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 30.741 [28.000, 102.000], mean observation: 0.065 [0.000, 54.000], loss: 0.270906, mean_absolute_error: 0.480243, mean_q: 5.958020, mean_eps: 0.100000\n",
      "  67171/175000: episode: 1883, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 36.135 [28.000, 184.000], mean observation: 0.125 [0.000, 74.000], loss: 1.298039, mean_absolute_error: 0.604359, mean_q: 6.928610, mean_eps: 0.100000\n",
      "  67219/175000: episode: 1884, duration: 0.856s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.146 [3.000, 181.000], mean observation: 0.272 [0.000, 96.000], loss: 0.356468, mean_absolute_error: 0.490615, mean_q: 6.234986, mean_eps: 0.100000\n",
      "  67245/175000: episode: 1885, duration: 0.505s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 52.538 [28.000, 187.000], mean observation: 0.110 [0.000, 52.000], loss: 13.518095, mean_absolute_error: 0.689468, mean_q: 6.948525, mean_eps: 0.100000\n",
      "  67282/175000: episode: 1886, duration: 0.670s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 50.351 [39.000, 168.000], mean observation: 0.126 [0.000, 74.000], loss: 0.129865, mean_absolute_error: 0.501697, mean_q: 6.014815, mean_eps: 0.100000\n",
      "  67317/175000: episode: 1887, duration: 0.685s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.771 [28.000, 221.000], mean observation: 0.199 [0.000, 70.000], loss: 15.083063, mean_absolute_error: 0.641696, mean_q: 6.722633, mean_eps: 0.100000\n",
      "  67369/175000: episode: 1888, duration: 0.980s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.538 [28.000, 221.000], mean observation: 0.718 [0.000, 104.000], loss: 37.567189, mean_absolute_error: 0.939722, mean_q: 8.037430, mean_eps: 0.100000\n",
      "  67408/175000: episode: 1889, duration: 0.776s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 165.256 [59.000, 221.000], mean observation: 0.241 [0.000, 78.000], loss: 19.053333, mean_absolute_error: 0.750433, mean_q: 7.269653, mean_eps: 0.100000\n",
      "  67434/175000: episode: 1890, duration: 0.507s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 173.269 [59.000, 221.000], mean observation: 0.123 [0.000, 52.000], loss: 0.149313, mean_absolute_error: 0.445798, mean_q: 5.886907, mean_eps: 0.100000\n",
      "  67469/175000: episode: 1891, duration: 0.697s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 165.771 [59.000, 221.000], mean observation: 0.345 [0.000, 70.000], loss: 13.510916, mean_absolute_error: 0.714852, mean_q: 7.178523, mean_eps: 0.100000\n",
      "  67510/175000: episode: 1892, duration: 0.740s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.829 [28.000, 221.000], mean observation: 0.232 [0.000, 82.000], loss: 4.679031, mean_absolute_error: 0.566078, mean_q: 6.608760, mean_eps: 0.100000\n",
      "  67539/175000: episode: 1893, duration: 0.532s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.897 [48.000, 221.000], mean observation: 0.136 [0.000, 58.000], loss: 0.251169, mean_absolute_error: 0.632142, mean_q: 7.326321, mean_eps: 0.100000\n",
      "  67576/175000: episode: 1894, duration: 0.711s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 138.189 [59.000, 191.000], mean observation: 0.188 [0.000, 74.000], loss: 0.555716, mean_absolute_error: 0.558641, mean_q: 6.713364, mean_eps: 0.100000\n",
      "  67609/175000: episode: 1895, duration: 0.671s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 34.758 [3.000, 184.000], mean observation: 0.143 [0.000, 66.000], loss: 24.138039, mean_absolute_error: 0.664705, mean_q: 6.813918, mean_eps: 0.100000\n",
      "  67636/175000: episode: 1896, duration: 0.502s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 49.370 [19.000, 203.000], mean observation: 0.232 [0.000, 54.000], loss: 8.662758, mean_absolute_error: 0.992083, mean_q: 9.421185, mean_eps: 0.100000\n",
      "  67655/175000: episode: 1897, duration: 0.387s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.046 [0.000, 38.000], loss: 10.049200, mean_absolute_error: 0.681008, mean_q: 6.992331, mean_eps: 0.100000\n",
      "  67669/175000: episode: 1898, duration: 0.294s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 27.286 [18.000, 28.000], mean observation: 0.036 [0.000, 28.000], loss: 28.851449, mean_absolute_error: 1.099934, mean_q: 9.354906, mean_eps: 0.100000\n",
      "  67706/175000: episode: 1899, duration: 0.890s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 33.135 [28.000, 218.000], mean observation: 0.087 [0.000, 74.000], loss: 33.271668, mean_absolute_error: 0.822412, mean_q: 7.428931, mean_eps: 0.100000\n",
      "  67750/175000: episode: 1900, duration: 0.836s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 40.818 [6.000, 213.000], mean observation: 0.208 [0.000, 88.000], loss: 22.003748, mean_absolute_error: 0.713330, mean_q: 7.007607, mean_eps: 0.100000\n",
      "  67788/175000: episode: 1901, duration: 0.916s, episode steps: 38, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 45.421 [28.000, 205.000], mean observation: 0.246 [0.000, 76.000], loss: 17.471564, mean_absolute_error: 0.901260, mean_q: 8.376204, mean_eps: 0.100000\n",
      "  67818/175000: episode: 1902, duration: 0.608s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 27.833 [23.000, 28.000], mean observation: 0.075 [0.000, 60.000], loss: 24.603057, mean_absolute_error: 1.009359, mean_q: 8.971950, mean_eps: 0.100000\n",
      "  67847/175000: episode: 1903, duration: 0.513s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 41.793 [4.000, 212.000], mean observation: 0.164 [0.000, 58.000], loss: 2.565078, mean_absolute_error: 0.697737, mean_q: 7.632745, mean_eps: 0.100000\n",
      "  67877/175000: episode: 1904, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 73.733 [28.000, 198.000], mean observation: 0.156 [0.000, 60.000], loss: 62.095998, mean_absolute_error: 1.078769, mean_q: 8.494900, mean_eps: 0.100000\n",
      "  67908/175000: episode: 1905, duration: 0.675s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 39.677 [28.000, 179.000], mean observation: 0.177 [0.000, 62.000], loss: 0.176998, mean_absolute_error: 0.618530, mean_q: 7.067373, mean_eps: 0.100000\n",
      "  67942/175000: episode: 1906, duration: 0.656s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 29.618 [24.000, 87.000], mean observation: 0.098 [0.000, 68.000], loss: 24.715383, mean_absolute_error: 0.793784, mean_q: 7.496963, mean_eps: 0.100000\n",
      "  67993/175000: episode: 1907, duration: 1.120s, episode steps: 51, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.627 [28.000, 191.000], mean observation: 0.206 [0.000, 102.000], loss: 7.036411, mean_absolute_error: 0.737053, mean_q: 7.653635, mean_eps: 0.100000\n",
      "  68026/175000: episode: 1908, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 33.909 [28.000, 137.000], mean observation: 0.102 [0.000, 66.000], loss: 43.450889, mean_absolute_error: 1.039777, mean_q: 8.588052, mean_eps: 0.100000\n",
      "  68051/175000: episode: 1909, duration: 0.470s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 34.320 [28.000, 119.000], mean observation: 0.083 [0.000, 50.000], loss: 9.254199, mean_absolute_error: 0.724712, mean_q: 7.717972, mean_eps: 0.100000\n",
      "  68097/175000: episode: 1910, duration: 1.020s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 37.043 [14.000, 185.000], mean observation: 0.248 [0.000, 92.000], loss: 2.801768, mean_absolute_error: 0.512120, mean_q: 6.316751, mean_eps: 0.100000\n",
      "  68147/175000: episode: 1911, duration: 0.875s, episode steps: 50, steps per second: 57, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 66.400 [10.000, 162.000], mean observation: 0.283 [0.000, 100.000], loss: 20.900746, mean_absolute_error: 0.839268, mean_q: 7.989657, mean_eps: 0.100000\n",
      "  68179/175000: episode: 1912, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.075 [0.000, 64.000], loss: 0.552173, mean_absolute_error: 0.606236, mean_q: 6.993151, mean_eps: 0.100000\n",
      "  68201/175000: episode: 1913, duration: 0.449s, episode steps: 22, steps per second: 49, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 49.955 [28.000, 156.000], mean observation: 0.068 [0.000, 44.000], loss: 0.311253, mean_absolute_error: 0.542627, mean_q: 6.757397, mean_eps: 0.100000\n",
      "  68242/175000: episode: 1914, duration: 0.920s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 44.293 [28.000, 219.000], mean observation: 0.173 [0.000, 82.000], loss: 7.072388, mean_absolute_error: 0.612668, mean_q: 6.991983, mean_eps: 0.100000\n",
      "  68273/175000: episode: 1915, duration: 0.558s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.806 [28.000, 221.000], mean observation: 0.339 [0.000, 62.000], loss: 23.600712, mean_absolute_error: 0.970241, mean_q: 8.862361, mean_eps: 0.100000\n",
      "  68314/175000: episode: 1916, duration: 0.933s, episode steps: 41, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.341 [16.000, 221.000], mean observation: 0.293 [0.000, 82.000], loss: 8.589952, mean_absolute_error: 0.615400, mean_q: 6.616575, mean_eps: 0.100000\n",
      "  68358/175000: episode: 1917, duration: 0.867s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.682 [16.000, 205.000], mean observation: 0.233 [0.000, 88.000], loss: 0.302839, mean_absolute_error: 0.482656, mean_q: 5.882338, mean_eps: 0.100000\n",
      "  68396/175000: episode: 1918, duration: 0.732s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 38.868 [28.000, 150.000], mean observation: 0.163 [0.000, 76.000], loss: 42.921102, mean_absolute_error: 0.792126, mean_q: 6.854482, mean_eps: 0.100000\n",
      "  68444/175000: episode: 1919, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 100.375 [3.000, 169.000], mean observation: 0.419 [0.000, 96.000], loss: 1.797355, mean_absolute_error: 0.513231, mean_q: 6.260532, mean_eps: 0.100000\n",
      "  68486/175000: episode: 1920, duration: 0.794s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.071 [4.000, 189.000], mean observation: 0.318 [0.000, 84.000], loss: 3.327609, mean_absolute_error: 0.573407, mean_q: 6.568008, mean_eps: 0.100000\n",
      "  68500/175000: episode: 1921, duration: 0.275s, episode steps: 14, steps per second: 51, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 139.500 [49.000, 207.000], mean observation: 0.058 [0.000, 28.000], loss: 43.852686, mean_absolute_error: 0.955673, mean_q: 8.010092, mean_eps: 0.100000\n",
      "  68521/175000: episode: 1922, duration: 0.486s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 142.286 [49.000, 162.000], mean observation: 0.115 [0.000, 42.000], loss: 13.718457, mean_absolute_error: 0.657702, mean_q: 6.843891, mean_eps: 0.100000\n",
      "  68549/175000: episode: 1923, duration: 0.525s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 130.607 [37.000, 190.000], mean observation: 0.179 [0.000, 56.000], loss: 0.136723, mean_absolute_error: 0.461942, mean_q: 5.969129, mean_eps: 0.100000\n",
      "  68585/175000: episode: 1924, duration: 0.661s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.083 [0.000, 222.000], mean observation: 0.231 [0.000, 72.000], loss: 5.736752, mean_absolute_error: 0.739198, mean_q: 7.724895, mean_eps: 0.100000\n",
      "  68618/175000: episode: 1925, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 155.000 [28.000, 218.000], mean observation: 0.192 [0.000, 66.000], loss: 0.271793, mean_absolute_error: 0.594327, mean_q: 6.839884, mean_eps: 0.100000\n",
      "  68651/175000: episode: 1926, duration: 0.560s, episode steps: 33, steps per second: 59, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 131.818 [28.000, 162.000], mean observation: 0.168 [0.000, 66.000], loss: 0.393567, mean_absolute_error: 0.597456, mean_q: 6.856568, mean_eps: 0.100000\n",
      "  68671/175000: episode: 1927, duration: 0.386s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 102.450 [3.000, 162.000], mean observation: 0.096 [0.000, 40.000], loss: 10.776922, mean_absolute_error: 0.748207, mean_q: 7.674607, mean_eps: 0.100000\n",
      "  68713/175000: episode: 1928, duration: 0.825s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.405 [17.000, 162.000], mean observation: 0.272 [0.000, 84.000], loss: 0.190120, mean_absolute_error: 0.431858, mean_q: 5.702335, mean_eps: 0.100000\n",
      "  68756/175000: episode: 1929, duration: 0.797s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 78.884 [27.000, 181.000], mean observation: 0.351 [0.000, 86.000], loss: 8.616553, mean_absolute_error: 0.817207, mean_q: 8.178877, mean_eps: 0.100000\n",
      "  68796/175000: episode: 1930, duration: 0.849s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 57.050 [27.000, 197.000], mean observation: 0.225 [0.000, 80.000], loss: 0.404349, mean_absolute_error: 0.562706, mean_q: 6.553903, mean_eps: 0.100000\n",
      "  68819/175000: episode: 1931, duration: 0.464s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 159.000 [159.000, 159.000], mean observation: 0.055 [0.000, 46.000], loss: 0.266152, mean_absolute_error: 0.442035, mean_q: 5.611116, mean_eps: 0.100000\n",
      "  68861/175000: episode: 1932, duration: 0.816s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.214 [12.000, 159.000], mean observation: 0.210 [0.000, 84.000], loss: 14.569751, mean_absolute_error: 0.754743, mean_q: 7.472973, mean_eps: 0.100000\n",
      "  68875/175000: episode: 1933, duration: 0.248s, episode steps: 14, steps per second: 56, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 140.286 [28.000, 159.000], mean observation: 0.040 [0.000, 28.000], loss: 3.224275, mean_absolute_error: 0.466789, mean_q: 6.073028, mean_eps: 0.100000\n",
      "  68905/175000: episode: 1934, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.533 [28.000, 162.000], mean observation: 0.106 [0.000, 60.000], loss: 21.340880, mean_absolute_error: 0.883254, mean_q: 8.042334, mean_eps: 0.100000\n",
      "  68944/175000: episode: 1935, duration: 0.697s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 78.923 [28.000, 159.000], mean observation: 0.189 [0.000, 78.000], loss: 16.394016, mean_absolute_error: 0.772829, mean_q: 7.479815, mean_eps: 0.100000\n",
      "  68982/175000: episode: 1936, duration: 0.717s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.395 [19.000, 221.000], mean observation: 0.248 [0.000, 76.000], loss: 1.015388, mean_absolute_error: 0.719007, mean_q: 7.721008, mean_eps: 0.100000\n",
      "  69016/175000: episode: 1937, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.000 [17.000, 216.000], mean observation: 0.275 [0.000, 68.000], loss: 1.160196, mean_absolute_error: 0.576301, mean_q: 6.572734, mean_eps: 0.100000\n",
      "  69053/175000: episode: 1938, duration: 0.725s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 32.459 [28.000, 193.000], mean observation: 0.090 [0.000, 74.000], loss: 46.998812, mean_absolute_error: 0.868708, mean_q: 7.294531, mean_eps: 0.100000\n",
      "  69074/175000: episode: 1939, duration: 0.369s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 33.476 [19.000, 129.000], mean observation: 0.069 [0.000, 42.000], loss: 2.716102, mean_absolute_error: 0.784520, mean_q: 8.164009, mean_eps: 0.100000\n",
      "  69128/175000: episode: 1940, duration: 1.089s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 40.167 [28.000, 218.000], mean observation: 0.308 [0.000, 108.000], loss: 31.891799, mean_absolute_error: 0.908008, mean_q: 8.053359, mean_eps: 0.100000\n",
      "  69173/175000: episode: 1941, duration: 1.035s, episode steps: 45, steps per second: 43, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 78.244 [28.000, 194.000], mean observation: 0.216 [0.000, 90.000], loss: 0.195603, mean_absolute_error: 0.532884, mean_q: 6.349533, mean_eps: 0.100000\n",
      "  69219/175000: episode: 1942, duration: 0.913s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 39.826 [25.000, 222.000], mean observation: 0.258 [0.000, 92.000], loss: 0.189822, mean_absolute_error: 0.436755, mean_q: 5.805037, mean_eps: 0.100000\n",
      "  69261/175000: episode: 1943, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 45.429 [28.000, 213.000], mean observation: 0.249 [0.000, 84.000], loss: 20.326131, mean_absolute_error: 0.879573, mean_q: 8.231557, mean_eps: 0.100000\n",
      "  69321/175000: episode: 1944, duration: 1.115s, episode steps: 60, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 95.183 [12.000, 223.000], mean observation: 0.313 [0.000, 120.000], loss: 15.066345, mean_absolute_error: 0.689148, mean_q: 7.068304, mean_eps: 0.100000\n",
      "  69354/175000: episode: 1945, duration: 0.620s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 32.212 [28.000, 167.000], mean observation: 0.101 [0.000, 66.000], loss: 0.195290, mean_absolute_error: 0.542307, mean_q: 6.360543, mean_eps: 0.100000\n",
      "  69383/175000: episode: 1946, duration: 0.576s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 50.966 [28.000, 99.000], mean observation: 0.150 [0.000, 58.000], loss: 41.417271, mean_absolute_error: 1.018701, mean_q: 8.487657, mean_eps: 0.100000\n",
      "  69412/175000: episode: 1947, duration: 0.615s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 41.414 [28.000, 209.000], mean observation: 0.121 [0.000, 58.000], loss: 0.160936, mean_absolute_error: 0.442292, mean_q: 5.687459, mean_eps: 0.100000\n",
      "  69445/175000: episode: 1948, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 46.364 [6.000, 124.000], mean observation: 0.155 [0.000, 66.000], loss: 0.222845, mean_absolute_error: 0.456355, mean_q: 5.778299, mean_eps: 0.100000\n",
      "  69468/175000: episode: 1949, duration: 0.475s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 37.087 [28.000, 164.000], mean observation: 0.062 [0.000, 46.000], loss: 0.200834, mean_absolute_error: 0.443159, mean_q: 5.669045, mean_eps: 0.100000\n",
      "  69519/175000: episode: 1950, duration: 0.977s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 72.216 [27.000, 197.000], mean observation: 0.255 [0.000, 102.000], loss: 0.158397, mean_absolute_error: 0.427802, mean_q: 5.607324, mean_eps: 0.100000\n",
      "  69558/175000: episode: 1951, duration: 0.705s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 43.026 [28.000, 221.000], mean observation: 0.099 [0.000, 78.000], loss: 15.341600, mean_absolute_error: 0.727131, mean_q: 7.325326, mean_eps: 0.100000\n",
      "  69599/175000: episode: 1952, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 37.146 [28.000, 201.000], mean observation: 0.287 [0.000, 82.000], loss: 37.591396, mean_absolute_error: 1.170558, mean_q: 9.695232, mean_eps: 0.100000\n",
      "  69628/175000: episode: 1953, duration: 0.681s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 42.034 [26.000, 214.000], mean observation: 0.161 [0.000, 58.000], loss: 1.787801, mean_absolute_error: 0.620106, mean_q: 6.849864, mean_eps: 0.100000\n",
      "  69655/175000: episode: 1954, duration: 0.577s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.148 [1.000, 140.000], mean observation: 0.130 [0.000, 54.000], loss: 0.470374, mean_absolute_error: 0.458221, mean_q: 5.844123, mean_eps: 0.100000\n",
      "  69691/175000: episode: 1955, duration: 0.780s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 33.222 [28.000, 216.000], mean observation: 0.118 [0.000, 72.000], loss: 0.162512, mean_absolute_error: 0.436598, mean_q: 5.693503, mean_eps: 0.100000\n",
      "  69721/175000: episode: 1956, duration: 0.613s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 29.200 [11.000, 83.000], mean observation: 0.120 [0.000, 60.000], loss: 24.401439, mean_absolute_error: 0.828659, mean_q: 7.629613, mean_eps: 0.100000\n",
      "  69773/175000: episode: 1957, duration: 0.913s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.154 [16.000, 207.000], mean observation: 0.284 [0.000, 104.000], loss: 3.292269, mean_absolute_error: 0.845887, mean_q: 8.443717, mean_eps: 0.100000\n",
      "  69799/175000: episode: 1958, duration: 0.456s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 106.769 [28.000, 171.000], mean observation: 0.155 [0.000, 52.000], loss: 14.431132, mean_absolute_error: 0.925549, mean_q: 8.786179, mean_eps: 0.100000\n",
      "  69814/175000: episode: 1959, duration: 0.294s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 68.533 [28.000, 182.000], mean observation: 0.093 [0.000, 30.000], loss: 6.981705, mean_absolute_error: 0.767856, mean_q: 8.080081, mean_eps: 0.100000\n",
      "  69857/175000: episode: 1960, duration: 0.839s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.186 [9.000, 203.000], mean observation: 0.382 [0.000, 86.000], loss: 12.161770, mean_absolute_error: 0.701393, mean_q: 7.293131, mean_eps: 0.100000\n",
      "  69879/175000: episode: 1961, duration: 0.382s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 57.227 [1.000, 179.000], mean observation: 0.139 [0.000, 44.000], loss: 43.941704, mean_absolute_error: 0.901881, mean_q: 7.890256, mean_eps: 0.100000\n",
      "  69906/175000: episode: 1962, duration: 0.530s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.148 [13.000, 159.000], mean observation: 0.174 [0.000, 54.000], loss: 0.162521, mean_absolute_error: 0.450044, mean_q: 6.107947, mean_eps: 0.100000\n",
      "  69932/175000: episode: 1963, duration: 0.523s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.423 [28.000, 159.000], mean observation: 0.221 [0.000, 52.000], loss: 0.220846, mean_absolute_error: 0.461159, mean_q: 6.112054, mean_eps: 0.100000\n",
      "  69945/175000: episode: 1964, duration: 0.289s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 116.923 [27.000, 222.000], mean observation: 0.077 [0.000, 26.000], loss: 0.186221, mean_absolute_error: 0.481419, mean_q: 6.409954, mean_eps: 0.100000\n",
      "  69977/175000: episode: 1965, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 115.031 [7.000, 206.000], mean observation: 0.297 [0.000, 64.000], loss: 37.961377, mean_absolute_error: 0.926731, mean_q: 8.362082, mean_eps: 0.100000\n",
      "  70003/175000: episode: 1966, duration: 0.485s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 51.962 [28.000, 156.000], mean observation: 0.169 [0.000, 52.000], loss: 15.742711, mean_absolute_error: 0.849136, mean_q: 8.388871, mean_eps: 0.100000\n",
      "  70054/175000: episode: 1967, duration: 0.956s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 96.804 [26.000, 217.000], mean observation: 0.606 [0.000, 102.000], loss: 0.496093, mean_absolute_error: 0.468042, mean_q: 6.351323, mean_eps: 0.100000\n",
      "  70083/175000: episode: 1968, duration: 0.519s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.138 [28.000, 216.000], mean observation: 0.144 [0.000, 58.000], loss: 24.148362, mean_absolute_error: 0.873238, mean_q: 8.402736, mean_eps: 0.100000\n",
      "  70126/175000: episode: 1969, duration: 0.881s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.884 [28.000, 216.000], mean observation: 0.241 [0.000, 86.000], loss: 10.637898, mean_absolute_error: 0.688719, mean_q: 7.496452, mean_eps: 0.100000\n",
      "  70165/175000: episode: 1970, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 39.051 [27.000, 220.000], mean observation: 0.162 [0.000, 78.000], loss: 5.051785, mean_absolute_error: 0.575026, mean_q: 6.928570, mean_eps: 0.100000\n",
      "  70214/175000: episode: 1971, duration: 0.940s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.857 [20.000, 205.000], mean observation: 0.346 [0.000, 98.000], loss: 18.405020, mean_absolute_error: 0.709957, mean_q: 7.452708, mean_eps: 0.100000\n",
      "  70247/175000: episode: 1972, duration: 0.613s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 107.333 [18.000, 186.000], mean observation: 0.380 [0.000, 66.000], loss: 4.479396, mean_absolute_error: 0.605742, mean_q: 7.213696, mean_eps: 0.100000\n",
      "  70285/175000: episode: 1973, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.816 [28.000, 222.000], mean observation: 0.145 [0.000, 76.000], loss: 23.613859, mean_absolute_error: 0.859884, mean_q: 8.296809, mean_eps: 0.100000\n",
      "  70321/175000: episode: 1974, duration: 0.753s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.889 [57.000, 174.000], mean observation: 0.260 [0.000, 72.000], loss: 26.812950, mean_absolute_error: 0.791376, mean_q: 7.776662, mean_eps: 0.100000\n",
      "  70354/175000: episode: 1975, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 142.303 [106.000, 213.000], mean observation: 0.244 [0.000, 66.000], loss: 15.388385, mean_absolute_error: 0.790894, mean_q: 8.187651, mean_eps: 0.100000\n",
      "  70391/175000: episode: 1976, duration: 0.684s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 136.595 [87.000, 174.000], mean observation: 0.154 [0.000, 74.000], loss: 48.067522, mean_absolute_error: 1.030046, mean_q: 9.043692, mean_eps: 0.100000\n",
      "  70429/175000: episode: 1977, duration: 0.766s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 133.474 [63.000, 223.000], mean observation: 0.351 [0.000, 76.000], loss: 0.185765, mean_absolute_error: 0.458520, mean_q: 6.422886, mean_eps: 0.100000\n",
      "  70456/175000: episode: 1978, duration: 0.571s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 147.074 [90.000, 162.000], mean observation: 0.136 [0.000, 54.000], loss: 0.715841, mean_absolute_error: 0.579084, mean_q: 7.193834, mean_eps: 0.100000\n",
      "  70523/175000: episode: 1979, duration: 1.383s, episode steps: 67, steps per second: 48, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 84.343 [5.000, 194.000], mean observation: 0.657 [0.000, 134.000], loss: 0.202777, mean_absolute_error: 0.514504, mean_q: 6.825099, mean_eps: 0.100000\n",
      "  70555/175000: episode: 1980, duration: 0.686s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 86.406 [24.000, 191.000], mean observation: 0.291 [0.000, 64.000], loss: 0.201859, mean_absolute_error: 0.475869, mean_q: 6.531876, mean_eps: 0.100000\n",
      "  70590/175000: episode: 1981, duration: 0.865s, episode steps: 35, steps per second: 40, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 45.114 [18.000, 192.000], mean observation: 0.181 [0.000, 70.000], loss: 20.128438, mean_absolute_error: 0.777186, mean_q: 7.996857, mean_eps: 0.100000\n",
      "  70626/175000: episode: 1982, duration: 0.692s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 26.333 [3.000, 27.000], mean observation: 0.118 [0.000, 72.000], loss: 0.322647, mean_absolute_error: 0.491164, mean_q: 6.746839, mean_eps: 0.100000\n",
      "  70653/175000: episode: 1983, duration: 0.517s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 65.037 [8.000, 156.000], mean observation: 0.255 [0.000, 54.000], loss: 0.412332, mean_absolute_error: 0.470689, mean_q: 6.741336, mean_eps: 0.100000\n",
      "  70685/175000: episode: 1984, duration: 0.615s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 87.031 [27.000, 208.000], mean observation: 0.176 [0.000, 64.000], loss: 60.672685, mean_absolute_error: 0.995984, mean_q: 8.547337, mean_eps: 0.100000\n",
      "  70713/175000: episode: 1985, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 168.679 [18.000, 215.000], mean observation: 0.131 [0.000, 56.000], loss: 2.491030, mean_absolute_error: 0.473131, mean_q: 6.557728, mean_eps: 0.100000\n",
      "  70760/175000: episode: 1986, duration: 0.944s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.617 [30.000, 224.000], mean observation: 0.362 [0.000, 94.000], loss: 67.359250, mean_absolute_error: 0.965163, mean_q: 7.937852, mean_eps: 0.100000\n",
      "  70800/175000: episode: 1987, duration: 0.830s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.625 [26.000, 219.000], mean observation: 0.363 [0.000, 80.000], loss: 7.906099, mean_absolute_error: 0.579543, mean_q: 7.129366, mean_eps: 0.100000\n",
      "  70831/175000: episode: 1988, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.000 [21.000, 186.000], mean observation: 0.213 [0.000, 62.000], loss: 68.943553, mean_absolute_error: 1.027774, mean_q: 8.477954, mean_eps: 0.100000\n",
      "  70865/175000: episode: 1989, duration: 0.807s, episode steps: 34, steps per second: 42, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.088 [17.000, 162.000], mean observation: 0.123 [0.000, 68.000], loss: 40.807526, mean_absolute_error: 1.028268, mean_q: 9.424409, mean_eps: 0.100000\n",
      "  70889/175000: episode: 1990, duration: 0.474s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 158.000 [150.000, 162.000], mean observation: 0.088 [0.000, 48.000], loss: 26.106470, mean_absolute_error: 0.961043, mean_q: 9.473010, mean_eps: 0.100000\n",
      "  70911/175000: episode: 1991, duration: 0.420s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 160.864 [156.000, 215.000], mean observation: 0.082 [0.000, 44.000], loss: 0.198607, mean_absolute_error: 0.449929, mean_q: 6.660967, mean_eps: 0.100000\n",
      "  70936/175000: episode: 1992, duration: 0.519s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 132.880 [8.000, 162.000], mean observation: 0.124 [0.000, 50.000], loss: 0.248109, mean_absolute_error: 0.454054, mean_q: 6.707075, mean_eps: 0.100000\n",
      "  70970/175000: episode: 1993, duration: 0.785s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 128.882 [12.000, 184.000], mean observation: 0.199 [0.000, 68.000], loss: 58.021243, mean_absolute_error: 0.956553, mean_q: 8.461681, mean_eps: 0.100000\n",
      "  71023/175000: episode: 1994, duration: 0.987s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.491 [32.000, 197.000], mean observation: 0.443 [0.000, 106.000], loss: 9.688795, mean_absolute_error: 0.572354, mean_q: 7.187525, mean_eps: 0.100000\n",
      "  71058/175000: episode: 1995, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.543 [9.000, 194.000], mean observation: 0.322 [0.000, 70.000], loss: 71.083899, mean_absolute_error: 1.270061, mean_q: 10.234366, mean_eps: 0.100000\n",
      "  71081/175000: episode: 1996, duration: 0.454s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 153.304 [46.000, 162.000], mean observation: 0.099 [0.000, 46.000], loss: 0.082831, mean_absolute_error: 0.427180, mean_q: 6.417583, mean_eps: 0.100000\n",
      "  71122/175000: episode: 1997, duration: 0.873s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.293 [28.000, 224.000], mean observation: 0.392 [0.000, 82.000], loss: 5.093147, mean_absolute_error: 0.563185, mean_q: 7.295025, mean_eps: 0.100000\n",
      "  71150/175000: episode: 1998, duration: 0.609s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.179 [48.000, 162.000], mean observation: 0.171 [0.000, 56.000], loss: 0.299434, mean_absolute_error: 0.449243, mean_q: 6.808192, mean_eps: 0.100000\n",
      "  71191/175000: episode: 1999, duration: 0.834s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.390 [28.000, 190.000], mean observation: 0.246 [0.000, 82.000], loss: 9.930498, mean_absolute_error: 0.581227, mean_q: 7.249050, mean_eps: 0.100000\n",
      "  71209/175000: episode: 2000, duration: 0.364s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 68.167 [42.000, 101.000], mean observation: 0.069 [0.000, 36.000], loss: 0.088960, mean_absolute_error: 0.428833, mean_q: 6.615179, mean_eps: 0.100000\n",
      "  71230/175000: episode: 2001, duration: 0.502s, episode steps: 21, steps per second: 42, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 74.429 [48.000, 162.000], mean observation: 0.065 [0.000, 42.000], loss: 6.220993, mean_absolute_error: 0.693858, mean_q: 8.425265, mean_eps: 0.100000\n",
      "  71274/175000: episode: 2002, duration: 1.112s, episode steps: 44, steps per second: 40, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 80.159 [18.000, 221.000], mean observation: 0.495 [0.000, 88.000], loss: 0.312669, mean_absolute_error: 0.457292, mean_q: 6.910391, mean_eps: 0.100000\n",
      "  71315/175000: episode: 2003, duration: 0.992s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.073 [52.000, 162.000], mean observation: 0.266 [0.000, 82.000], loss: 38.827936, mean_absolute_error: 0.949397, mean_q: 8.816987, mean_eps: 0.100000\n",
      "  71359/175000: episode: 2004, duration: 1.015s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.886 [3.000, 221.000], mean observation: 0.551 [0.000, 88.000], loss: 54.222634, mean_absolute_error: 0.816923, mean_q: 7.421530, mean_eps: 0.100000\n",
      "  71409/175000: episode: 2005, duration: 1.148s, episode steps: 50, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 57.840 [28.000, 156.000], mean observation: 0.228 [0.000, 100.000], loss: 17.726636, mean_absolute_error: 0.688141, mean_q: 7.741936, mean_eps: 0.100000\n",
      "  71459/175000: episode: 2006, duration: 0.939s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.600 [25.000, 221.000], mean observation: 0.560 [0.000, 100.000], loss: 21.345601, mean_absolute_error: 0.803786, mean_q: 8.523852, mean_eps: 0.100000\n",
      "  71511/175000: episode: 2007, duration: 0.967s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 84.404 [28.000, 224.000], mean observation: 0.360 [0.000, 104.000], loss: 24.182031, mean_absolute_error: 0.698756, mean_q: 7.583088, mean_eps: 0.100000\n",
      "  71545/175000: episode: 2008, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 105.559 [42.000, 164.000], mean observation: 0.284 [0.000, 68.000], loss: 50.186569, mean_absolute_error: 1.034043, mean_q: 9.170237, mean_eps: 0.100000\n",
      "  71576/175000: episode: 2009, duration: 0.676s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 128.871 [39.000, 188.000], mean observation: 0.148 [0.000, 62.000], loss: 0.108311, mean_absolute_error: 0.415444, mean_q: 6.292987, mean_eps: 0.100000\n",
      "  71604/175000: episode: 2010, duration: 0.660s, episode steps: 28, steps per second: 42, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 71.607 [28.000, 169.000], mean observation: 0.238 [0.000, 56.000], loss: 27.217320, mean_absolute_error: 0.890172, mean_q: 8.811085, mean_eps: 0.100000\n",
      "  71652/175000: episode: 2011, duration: 0.905s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 103.896 [1.000, 215.000], mean observation: 0.424 [0.000, 96.000], loss: 19.169100, mean_absolute_error: 0.711988, mean_q: 7.833720, mean_eps: 0.100000\n",
      "  71672/175000: episode: 2012, duration: 0.462s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 57.550 [15.000, 116.000], mean observation: 0.109 [0.000, 40.000], loss: 0.147691, mean_absolute_error: 0.403733, mean_q: 6.107743, mean_eps: 0.100000\n",
      "  71726/175000: episode: 2013, duration: 1.025s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 39.111 [28.000, 203.000], mean observation: 0.296 [0.000, 108.000], loss: 7.655484, mean_absolute_error: 0.555476, mean_q: 7.164510, mean_eps: 0.100000\n",
      "  71766/175000: episode: 2014, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 32.200 [28.000, 170.000], mean observation: 0.167 [0.000, 80.000], loss: 11.092187, mean_absolute_error: 0.594174, mean_q: 7.334002, mean_eps: 0.100000\n",
      "  71790/175000: episode: 2015, duration: 0.453s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 29.583 [28.000, 66.000], mean observation: 0.058 [0.000, 48.000], loss: 28.480382, mean_absolute_error: 0.742846, mean_q: 7.837541, mean_eps: 0.100000\n",
      "  71831/175000: episode: 2016, duration: 0.726s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.829 [15.000, 184.000], mean observation: 0.214 [0.000, 82.000], loss: 0.261590, mean_absolute_error: 0.408973, mean_q: 6.326092, mean_eps: 0.100000\n",
      "  71874/175000: episode: 2017, duration: 0.821s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.512 [10.000, 198.000], mean observation: 0.176 [0.000, 86.000], loss: 0.197684, mean_absolute_error: 0.419277, mean_q: 6.362025, mean_eps: 0.100000\n",
      "  71892/175000: episode: 2018, duration: 0.331s, episode steps: 18, steps per second: 54, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 71.667 [16.000, 179.000], mean observation: 0.085 [0.000, 36.000], loss: 0.177105, mean_absolute_error: 0.436178, mean_q: 6.420413, mean_eps: 0.100000\n",
      "  71926/175000: episode: 2019, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.941 [28.000, 172.000], mean observation: 0.163 [0.000, 68.000], loss: 46.710728, mean_absolute_error: 0.911557, mean_q: 8.250647, mean_eps: 0.100000\n",
      "  71952/175000: episode: 2020, duration: 0.479s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 136.538 [2.000, 184.000], mean observation: 0.128 [0.000, 52.000], loss: 0.262109, mean_absolute_error: 0.428044, mean_q: 6.137931, mean_eps: 0.100000\n",
      "  71990/175000: episode: 2021, duration: 0.795s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.526 [48.000, 197.000], mean observation: 0.335 [0.000, 76.000], loss: 2.249248, mean_absolute_error: 0.565084, mean_q: 7.100899, mean_eps: 0.100000\n",
      "  72010/175000: episode: 2022, duration: 0.380s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.950 [28.000, 160.000], mean observation: 0.131 [0.000, 40.000], loss: 0.242126, mean_absolute_error: 0.475458, mean_q: 6.626996, mean_eps: 0.100000\n",
      "  72029/175000: episode: 2023, duration: 0.353s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 138.632 [82.000, 202.000], mean observation: 0.084 [0.000, 38.000], loss: 0.436977, mean_absolute_error: 0.408453, mean_q: 6.049500, mean_eps: 0.100000\n",
      "  72081/175000: episode: 2024, duration: 0.950s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 122.038 [28.000, 195.000], mean observation: 0.593 [0.000, 104.000], loss: 3.344451, mean_absolute_error: 0.605340, mean_q: 7.467801, mean_eps: 0.100000\n",
      "  72103/175000: episode: 2025, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 113.727 [28.000, 221.000], mean observation: 0.107 [0.000, 44.000], loss: 58.174918, mean_absolute_error: 0.939103, mean_q: 8.204029, mean_eps: 0.100000\n",
      "  72118/175000: episode: 2026, duration: 0.286s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 80.133 [28.000, 159.000], mean observation: 0.103 [0.000, 30.000], loss: 0.128058, mean_absolute_error: 0.488101, mean_q: 6.747097, mean_eps: 0.100000\n",
      "  72161/175000: episode: 2027, duration: 0.811s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 91.860 [5.000, 221.000], mean observation: 0.347 [0.000, 86.000], loss: 28.641353, mean_absolute_error: 0.664141, mean_q: 6.977469, mean_eps: 0.100000\n",
      "  72182/175000: episode: 2028, duration: 0.362s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 30.143 [28.000, 73.000], mean observation: 0.081 [0.000, 42.000], loss: 17.059841, mean_absolute_error: 0.749483, mean_q: 7.935649, mean_eps: 0.100000\n",
      "  72233/175000: episode: 2029, duration: 1.006s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 74.078 [28.000, 142.000], mean observation: 0.255 [0.000, 102.000], loss: 26.540258, mean_absolute_error: 0.877562, mean_q: 8.626237, mean_eps: 0.100000\n",
      "  72264/175000: episode: 2030, duration: 0.553s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 31.710 [28.000, 87.000], mean observation: 0.083 [0.000, 62.000], loss: 0.419431, mean_absolute_error: 0.654969, mean_q: 7.855711, mean_eps: 0.100000\n",
      "  72292/175000: episode: 2031, duration: 0.554s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 57.607 [28.000, 156.000], mean observation: 0.094 [0.000, 56.000], loss: 0.193870, mean_absolute_error: 0.503504, mean_q: 6.986860, mean_eps: 0.100000\n",
      "  72310/175000: episode: 2032, duration: 0.386s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 40.611 [28.000, 221.000], mean observation: 0.059 [0.000, 36.000], loss: 31.663639, mean_absolute_error: 0.814930, mean_q: 8.063615, mean_eps: 0.100000\n",
      "  72355/175000: episode: 2033, duration: 0.811s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.756 [28.000, 156.000], mean observation: 0.225 [0.000, 90.000], loss: 0.281008, mean_absolute_error: 0.465506, mean_q: 6.581272, mean_eps: 0.100000\n",
      "  72376/175000: episode: 2034, duration: 0.470s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 92.095 [28.000, 156.000], mean observation: 0.147 [0.000, 42.000], loss: 0.596430, mean_absolute_error: 0.435522, mean_q: 6.380845, mean_eps: 0.100000\n",
      "  72428/175000: episode: 2035, duration: 1.014s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.135 [2.000, 198.000], mean observation: 0.523 [0.000, 104.000], loss: 0.955163, mean_absolute_error: 0.489629, mean_q: 6.636609, mean_eps: 0.100000\n",
      "  72461/175000: episode: 2036, duration: 0.689s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.818 [28.000, 195.000], mean observation: 0.370 [0.000, 66.000], loss: 28.274342, mean_absolute_error: 0.868250, mean_q: 8.431501, mean_eps: 0.100000\n",
      "  72482/175000: episode: 2037, duration: 0.401s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 78.952 [52.000, 195.000], mean observation: 0.096 [0.000, 42.000], loss: 4.058864, mean_absolute_error: 0.695522, mean_q: 7.986350, mean_eps: 0.100000\n",
      "  72511/175000: episode: 2038, duration: 0.551s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 92.345 [52.000, 169.000], mean observation: 0.087 [0.000, 58.000], loss: 24.107835, mean_absolute_error: 0.747545, mean_q: 7.911914, mean_eps: 0.100000\n",
      "  72547/175000: episode: 2039, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 77.000 [28.000, 198.000], mean observation: 0.147 [0.000, 72.000], loss: 59.006803, mean_absolute_error: 1.140728, mean_q: 9.569599, mean_eps: 0.100000\n",
      "  72595/175000: episode: 2040, duration: 0.912s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.667 [52.000, 218.000], mean observation: 0.225 [0.000, 96.000], loss: 0.223585, mean_absolute_error: 0.446091, mean_q: 6.395349, mean_eps: 0.100000\n",
      "  72621/175000: episode: 2041, duration: 0.554s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 52.308 [52.000, 60.000], mean observation: 0.078 [0.000, 52.000], loss: 38.377376, mean_absolute_error: 0.944583, mean_q: 8.743134, mean_eps: 0.100000\n",
      "  72640/175000: episode: 2042, duration: 0.373s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 58.737 [43.000, 179.000], mean observation: 0.062 [0.000, 38.000], loss: 0.210517, mean_absolute_error: 0.687841, mean_q: 7.944205, mean_eps: 0.100000\n",
      "  72691/175000: episode: 2043, duration: 1.036s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.353 [27.000, 220.000], mean observation: 0.474 [0.000, 102.000], loss: 5.490421, mean_absolute_error: 0.742543, mean_q: 8.320737, mean_eps: 0.100000\n",
      "  72710/175000: episode: 2044, duration: 0.362s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 90.684 [52.000, 148.000], mean observation: 0.075 [0.000, 38.000], loss: 0.204100, mean_absolute_error: 0.464885, mean_q: 6.625752, mean_eps: 0.100000\n",
      "  72739/175000: episode: 2045, duration: 0.607s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 101.655 [52.000, 210.000], mean observation: 0.169 [0.000, 58.000], loss: 0.341372, mean_absolute_error: 0.454625, mean_q: 6.364294, mean_eps: 0.100000\n",
      "  72756/175000: episode: 2046, duration: 0.353s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 52.000 [52.000, 52.000], mean observation: 0.041 [0.000, 34.000], loss: 0.132209, mean_absolute_error: 0.742731, mean_q: 8.444944, mean_eps: 0.100000\n",
      "  72790/175000: episode: 2047, duration: 0.687s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.618 [28.000, 202.000], mean observation: 0.211 [0.000, 68.000], loss: 0.146076, mean_absolute_error: 0.418029, mean_q: 5.962342, mean_eps: 0.100000\n",
      "  72818/175000: episode: 2048, duration: 0.607s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 105.929 [52.000, 172.000], mean observation: 0.120 [0.000, 56.000], loss: 0.560693, mean_absolute_error: 0.466193, mean_q: 6.565517, mean_eps: 0.100000\n",
      "  72856/175000: episode: 2049, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.421 [5.000, 215.000], mean observation: 0.176 [0.000, 76.000], loss: 19.448649, mean_absolute_error: 0.929703, mean_q: 9.075288, mean_eps: 0.100000\n",
      "  72873/175000: episode: 2050, duration: 0.357s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 30.824 [28.000, 76.000], mean observation: 0.043 [0.000, 34.000], loss: 20.203324, mean_absolute_error: 0.743455, mean_q: 7.590290, mean_eps: 0.100000\n",
      "  72921/175000: episode: 2051, duration: 1.140s, episode steps: 48, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.750 [28.000, 202.000], mean observation: 0.218 [0.000, 96.000], loss: 46.238581, mean_absolute_error: 0.839522, mean_q: 7.797550, mean_eps: 0.100000\n",
      "  72958/175000: episode: 2052, duration: 0.805s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 69.054 [28.000, 158.000], mean observation: 0.143 [0.000, 74.000], loss: 0.288312, mean_absolute_error: 0.547841, mean_q: 7.142111, mean_eps: 0.100000\n",
      "  72979/175000: episode: 2053, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 35.619 [10.000, 134.000], mean observation: 0.072 [0.000, 42.000], loss: 0.212969, mean_absolute_error: 0.418065, mean_q: 5.984916, mean_eps: 0.100000\n",
      "  73023/175000: episode: 2054, duration: 1.117s, episode steps: 44, steps per second: 39, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.432 [17.000, 211.000], mean observation: 0.348 [0.000, 88.000], loss: 0.521061, mean_absolute_error: 0.473381, mean_q: 6.584459, mean_eps: 0.100000\n",
      "  73060/175000: episode: 2055, duration: 0.782s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 63.459 [27.000, 127.000], mean observation: 0.179 [0.000, 74.000], loss: 15.063974, mean_absolute_error: 0.610542, mean_q: 6.958544, mean_eps: 0.100000\n",
      "  73098/175000: episode: 2056, duration: 0.765s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.447 [27.000, 202.000], mean observation: 0.175 [0.000, 76.000], loss: 0.207236, mean_absolute_error: 0.429518, mean_q: 6.127132, mean_eps: 0.100000\n",
      "  73132/175000: episode: 2057, duration: 0.683s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.912 [32.000, 162.000], mean observation: 0.205 [0.000, 68.000], loss: 38.144862, mean_absolute_error: 0.917679, mean_q: 8.279915, mean_eps: 0.100000\n",
      "  73187/175000: episode: 2058, duration: 1.264s, episode steps: 55, steps per second: 44, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 119.873 [5.000, 224.000], mean observation: 0.612 [0.000, 110.000], loss: 1.958109, mean_absolute_error: 0.525473, mean_q: 6.723226, mean_eps: 0.100000\n",
      "  73223/175000: episode: 2059, duration: 0.850s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 65.056 [6.000, 190.000], mean observation: 0.236 [0.000, 72.000], loss: 6.998519, mean_absolute_error: 0.724665, mean_q: 7.945472, mean_eps: 0.100000\n",
      "  73255/175000: episode: 2060, duration: 0.790s, episode steps: 32, steps per second: 40, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.688 [5.000, 200.000], mean observation: 0.252 [0.000, 64.000], loss: 4.017838, mean_absolute_error: 0.741579, mean_q: 8.052058, mean_eps: 0.100000\n",
      "  73289/175000: episode: 2061, duration: 0.825s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.559 [32.000, 127.000], mean observation: 0.120 [0.000, 68.000], loss: 10.073140, mean_absolute_error: 0.611022, mean_q: 7.018516, mean_eps: 0.100000\n",
      "  73337/175000: episode: 2062, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 76.646 [30.000, 197.000], mean observation: 0.258 [0.000, 96.000], loss: 15.039535, mean_absolute_error: 0.791658, mean_q: 8.214256, mean_eps: 0.100000\n",
      "  73375/175000: episode: 2063, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.895 [28.000, 183.000], mean observation: 0.346 [0.000, 76.000], loss: 20.454757, mean_absolute_error: 0.998246, mean_q: 9.710449, mean_eps: 0.100000\n",
      "  73411/175000: episode: 2064, duration: 0.852s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 105.583 [32.000, 183.000], mean observation: 0.357 [0.000, 72.000], loss: 0.840178, mean_absolute_error: 0.564770, mean_q: 7.096956, mean_eps: 0.100000\n",
      "  73473/175000: episode: 2065, duration: 1.442s, episode steps: 62, steps per second: 43, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 103.726 [15.000, 193.000], mean observation: 0.543 [0.000, 124.000], loss: 31.169109, mean_absolute_error: 0.788867, mean_q: 7.929210, mean_eps: 0.100000\n",
      "  73511/175000: episode: 2066, duration: 0.809s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.579 [28.000, 221.000], mean observation: 0.392 [0.000, 76.000], loss: 82.951673, mean_absolute_error: 1.234468, mean_q: 9.476587, mean_eps: 0.100000\n",
      "  73542/175000: episode: 2067, duration: 0.648s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 38.839 [28.000, 164.000], mean observation: 0.113 [0.000, 62.000], loss: 6.212882, mean_absolute_error: 0.949057, mean_q: 10.060990, mean_eps: 0.100000\n",
      "  73580/175000: episode: 2068, duration: 0.769s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.474 [23.000, 221.000], mean observation: 0.349 [0.000, 76.000], loss: 11.696059, mean_absolute_error: 0.802643, mean_q: 8.816039, mean_eps: 0.100000\n",
      "  73605/175000: episode: 2069, duration: 0.486s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 61.160 [28.000, 156.000], mean observation: 0.167 [0.000, 50.000], loss: 0.202799, mean_absolute_error: 0.419449, mean_q: 6.331637, mean_eps: 0.100000\n",
      "  73646/175000: episode: 2070, duration: 0.963s, episode steps: 41, steps per second: 43, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 64.439 [1.000, 168.000], mean observation: 0.371 [0.000, 82.000], loss: 0.225945, mean_absolute_error: 0.519600, mean_q: 7.229581, mean_eps: 0.100000\n",
      "  73692/175000: episode: 2071, duration: 0.891s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 73.761 [6.000, 193.000], mean observation: 0.401 [0.000, 92.000], loss: 3.365559, mean_absolute_error: 0.562560, mean_q: 7.239211, mean_eps: 0.100000\n",
      "  73737/175000: episode: 2072, duration: 0.910s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 84.089 [7.000, 210.000], mean observation: 0.509 [0.000, 90.000], loss: 10.567385, mean_absolute_error: 0.713563, mean_q: 8.207515, mean_eps: 0.100000\n",
      "  73769/175000: episode: 2073, duration: 0.571s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 37.312 [23.000, 112.000], mean observation: 0.160 [0.000, 64.000], loss: 14.475796, mean_absolute_error: 0.786973, mean_q: 8.606670, mean_eps: 0.100000\n",
      "  73805/175000: episode: 2074, duration: 0.694s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 52.472 [28.000, 191.000], mean observation: 0.152 [0.000, 72.000], loss: 49.390125, mean_absolute_error: 1.078755, mean_q: 9.902657, mean_eps: 0.100000\n",
      "  73851/175000: episode: 2075, duration: 0.854s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.891 [28.000, 223.000], mean observation: 0.369 [0.000, 92.000], loss: 14.386261, mean_absolute_error: 0.624939, mean_q: 7.671123, mean_eps: 0.100000\n",
      "  73892/175000: episode: 2076, duration: 0.907s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.927 [11.000, 207.000], mean observation: 0.253 [0.000, 82.000], loss: 11.481354, mean_absolute_error: 0.726398, mean_q: 8.550265, mean_eps: 0.100000\n",
      "  73934/175000: episode: 2077, duration: 1.029s, episode steps: 42, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.833 [28.000, 167.000], mean observation: 0.272 [0.000, 84.000], loss: 13.221429, mean_absolute_error: 0.614981, mean_q: 7.624047, mean_eps: 0.100000\n",
      "  73974/175000: episode: 2078, duration: 0.831s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 76.800 [13.000, 210.000], mean observation: 0.271 [0.000, 80.000], loss: 9.865552, mean_absolute_error: 0.750157, mean_q: 8.887114, mean_eps: 0.100000\n",
      "  74016/175000: episode: 2079, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.071 [24.000, 206.000], mean observation: 0.294 [0.000, 84.000], loss: 0.165477, mean_absolute_error: 0.507413, mean_q: 7.564474, mean_eps: 0.100000\n",
      "  74048/175000: episode: 2080, duration: 0.689s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 40.406 [28.000, 211.000], mean observation: 0.112 [0.000, 64.000], loss: 0.173138, mean_absolute_error: 0.464868, mean_q: 7.115989, mean_eps: 0.100000\n",
      "  74093/175000: episode: 2081, duration: 0.940s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.022 [28.000, 162.000], mean observation: 0.319 [0.000, 90.000], loss: 10.707790, mean_absolute_error: 0.788719, mean_q: 9.231096, mean_eps: 0.100000\n",
      "  74130/175000: episode: 2082, duration: 0.882s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 88.838 [27.000, 182.000], mean observation: 0.336 [0.000, 74.000], loss: 53.208896, mean_absolute_error: 0.955792, mean_q: 9.111463, mean_eps: 0.100000\n",
      "  74160/175000: episode: 2083, duration: 0.673s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 80.700 [24.000, 156.000], mean observation: 0.196 [0.000, 60.000], loss: 0.709134, mean_absolute_error: 0.510950, mean_q: 7.433999, mean_eps: 0.100000\n",
      "  74211/175000: episode: 2084, duration: 1.129s, episode steps: 51, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 75.824 [1.000, 178.000], mean observation: 0.465 [0.000, 102.000], loss: 1.050743, mean_absolute_error: 0.563426, mean_q: 7.872248, mean_eps: 0.100000\n",
      "  74262/175000: episode: 2085, duration: 1.158s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 83.275 [24.000, 188.000], mean observation: 0.456 [0.000, 102.000], loss: 0.183508, mean_absolute_error: 0.479838, mean_q: 7.331786, mean_eps: 0.100000\n",
      "  74309/175000: episode: 2086, duration: 0.897s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.574 [24.000, 184.000], mean observation: 0.328 [0.000, 94.000], loss: 22.010669, mean_absolute_error: 0.739687, mean_q: 8.463032, mean_eps: 0.100000\n",
      "  74336/175000: episode: 2087, duration: 0.571s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 53.444 [24.000, 220.000], mean observation: 0.142 [0.000, 54.000], loss: 0.218715, mean_absolute_error: 0.437498, mean_q: 6.965949, mean_eps: 0.100000\n",
      "  74357/175000: episode: 2088, duration: 0.480s, episode steps: 21, steps per second: 44, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 72.476 [24.000, 158.000], mean observation: 0.079 [0.000, 42.000], loss: 1.336567, mean_absolute_error: 0.652108, mean_q: 8.607960, mean_eps: 0.100000\n",
      "  74393/175000: episode: 2089, duration: 0.727s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 88.778 [24.000, 184.000], mean observation: 0.143 [0.000, 72.000], loss: 62.207638, mean_absolute_error: 0.983523, mean_q: 8.879408, mean_eps: 0.100000\n",
      "  74431/175000: episode: 2090, duration: 0.719s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.105 [10.000, 207.000], mean observation: 0.200 [0.000, 76.000], loss: 4.331852, mean_absolute_error: 0.612021, mean_q: 7.885777, mean_eps: 0.100000\n",
      "  74478/175000: episode: 2091, duration: 0.965s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 64.638 [1.000, 118.000], mean observation: 0.184 [0.000, 94.000], loss: 4.207999, mean_absolute_error: 0.564672, mean_q: 7.493568, mean_eps: 0.100000\n",
      "  74537/175000: episode: 2092, duration: 1.297s, episode steps: 59, steps per second: 45, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 77.203 [24.000, 216.000], mean observation: 0.270 [0.000, 118.000], loss: 0.238198, mean_absolute_error: 0.541534, mean_q: 7.518997, mean_eps: 0.100000\n",
      "  74577/175000: episode: 2093, duration: 0.901s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.475 [24.000, 168.000], mean observation: 0.157 [0.000, 80.000], loss: 14.203260, mean_absolute_error: 0.675987, mean_q: 7.957335, mean_eps: 0.100000\n",
      "  74629/175000: episode: 2094, duration: 1.000s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.615 [24.000, 216.000], mean observation: 0.476 [0.000, 104.000], loss: 8.371088, mean_absolute_error: 0.681222, mean_q: 8.259612, mean_eps: 0.100000\n",
      "  74667/175000: episode: 2095, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.158 [24.000, 153.000], mean observation: 0.263 [0.000, 76.000], loss: 30.419809, mean_absolute_error: 0.837920, mean_q: 8.686300, mean_eps: 0.100000\n",
      "  74704/175000: episode: 2096, duration: 0.759s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.378 [6.000, 214.000], mean observation: 0.279 [0.000, 74.000], loss: 22.592114, mean_absolute_error: 0.813910, mean_q: 8.873389, mean_eps: 0.100000\n",
      "  74737/175000: episode: 2097, duration: 0.632s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.091 [14.000, 210.000], mean observation: 0.263 [0.000, 66.000], loss: 17.791956, mean_absolute_error: 0.658025, mean_q: 7.742673, mean_eps: 0.100000\n",
      "  74770/175000: episode: 2098, duration: 0.608s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.667 [28.000, 101.000], mean observation: 0.157 [0.000, 66.000], loss: 42.796578, mean_absolute_error: 0.895255, mean_q: 8.873816, mean_eps: 0.100000\n",
      "  74797/175000: episode: 2099, duration: 0.575s, episode steps: 27, steps per second: 47, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 78.815 [28.000, 213.000], mean observation: 0.123 [0.000, 54.000], loss: 9.722585, mean_absolute_error: 0.636391, mean_q: 7.849809, mean_eps: 0.100000\n",
      "  74831/175000: episode: 2100, duration: 0.587s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 68.235 [0.000, 188.000], mean observation: 0.248 [0.000, 68.000], loss: 17.009051, mean_absolute_error: 0.639848, mean_q: 7.700007, mean_eps: 0.100000\n",
      "  74845/175000: episode: 2101, duration: 0.307s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 73.500 [27.000, 156.000], mean observation: 0.058 [0.000, 28.000], loss: 22.876418, mean_absolute_error: 1.126375, mean_q: 11.092598, mean_eps: 0.100000\n",
      "  74873/175000: episode: 2102, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 81.357 [27.000, 191.000], mean observation: 0.195 [0.000, 56.000], loss: 0.251250, mean_absolute_error: 0.464581, mean_q: 7.158204, mean_eps: 0.100000\n",
      "  74911/175000: episode: 2103, duration: 0.719s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 80.079 [9.000, 213.000], mean observation: 0.265 [0.000, 76.000], loss: 55.061305, mean_absolute_error: 0.883846, mean_q: 8.377126, mean_eps: 0.100000\n",
      "  74924/175000: episode: 2104, duration: 0.307s, episode steps: 13, steps per second: 42, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 112.615 [27.000, 211.000], mean observation: 0.073 [0.000, 26.000], loss: 76.378081, mean_absolute_error: 1.140056, mean_q: 9.599924, mean_eps: 0.100000\n",
      "  74944/175000: episode: 2105, duration: 0.423s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 106.500 [27.000, 216.000], mean observation: 0.120 [0.000, 40.000], loss: 0.243478, mean_absolute_error: 0.424941, mean_q: 6.908809, mean_eps: 0.100000\n",
      "  74987/175000: episode: 2106, duration: 0.819s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.674 [8.000, 219.000], mean observation: 0.457 [0.000, 86.000], loss: 28.778007, mean_absolute_error: 0.933060, mean_q: 9.856596, mean_eps: 0.100000\n",
      "  75027/175000: episode: 2107, duration: 0.704s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 130.450 [27.000, 221.000], mean observation: 0.348 [0.000, 80.000], loss: 20.528145, mean_absolute_error: 0.936835, mean_q: 10.143052, mean_eps: 0.100000\n",
      "  75076/175000: episode: 2108, duration: 0.956s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 165.592 [24.000, 223.000], mean observation: 0.483 [0.000, 98.000], loss: 0.198875, mean_absolute_error: 0.591158, mean_q: 8.233751, mean_eps: 0.100000\n",
      "  75117/175000: episode: 2109, duration: 0.774s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.537 [24.000, 200.000], mean observation: 0.401 [0.000, 82.000], loss: 0.202714, mean_absolute_error: 0.421458, mean_q: 7.062115, mean_eps: 0.100000\n",
      "  75158/175000: episode: 2110, duration: 0.771s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.902 [27.000, 184.000], mean observation: 0.243 [0.000, 82.000], loss: 36.237370, mean_absolute_error: 0.826992, mean_q: 8.908750, mean_eps: 0.100000\n",
      "  75201/175000: episode: 2111, duration: 0.844s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.186 [27.000, 221.000], mean observation: 0.288 [0.000, 86.000], loss: 0.134776, mean_absolute_error: 0.429249, mean_q: 7.042576, mean_eps: 0.100000\n",
      "  75257/175000: episode: 2112, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 162.071 [99.000, 192.000], mean observation: 0.494 [0.000, 112.000], loss: 39.054513, mean_absolute_error: 0.697134, mean_q: 7.752271, mean_eps: 0.100000\n",
      "  75290/175000: episode: 2113, duration: 0.584s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.606 [5.000, 206.000], mean observation: 0.210 [0.000, 66.000], loss: 22.518527, mean_absolute_error: 0.668935, mean_q: 8.050912, mean_eps: 0.100000\n",
      "  75328/175000: episode: 2114, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 84.132 [5.000, 204.000], mean observation: 0.256 [0.000, 76.000], loss: 38.471984, mean_absolute_error: 0.935315, mean_q: 9.705127, mean_eps: 0.100000\n",
      "  75350/175000: episode: 2115, duration: 0.420s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.053 [0.000, 44.000], loss: 1.556375, mean_absolute_error: 0.679649, mean_q: 8.692017, mean_eps: 0.100000\n",
      "  75394/175000: episode: 2116, duration: 0.820s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 126.318 [5.000, 198.000], mean observation: 0.439 [0.000, 88.000], loss: 0.180207, mean_absolute_error: 0.466022, mean_q: 7.430006, mean_eps: 0.100000\n",
      "  75414/175000: episode: 2117, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 137.450 [5.000, 192.000], mean observation: 0.131 [0.000, 40.000], loss: 20.941091, mean_absolute_error: 0.746671, mean_q: 8.732468, mean_eps: 0.100000\n",
      "  75461/175000: episode: 2118, duration: 0.880s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.787 [9.000, 209.000], mean observation: 0.552 [0.000, 94.000], loss: 12.712013, mean_absolute_error: 0.568101, mean_q: 7.566642, mean_eps: 0.100000\n",
      "  75497/175000: episode: 2119, duration: 0.624s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.528 [9.000, 224.000], mean observation: 0.303 [0.000, 72.000], loss: 0.181839, mean_absolute_error: 0.428035, mean_q: 6.994840, mean_eps: 0.100000\n",
      "  75521/175000: episode: 2120, duration: 0.503s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 140.667 [4.000, 216.000], mean observation: 0.153 [0.000, 48.000], loss: 21.054639, mean_absolute_error: 0.922953, mean_q: 9.880095, mean_eps: 0.100000\n",
      "  75565/175000: episode: 2121, duration: 0.882s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.136 [0.000, 198.000], mean observation: 0.385 [0.000, 88.000], loss: 46.219030, mean_absolute_error: 0.863714, mean_q: 8.443392, mean_eps: 0.100000\n",
      "  75587/175000: episode: 2122, duration: 0.380s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 150.545 [39.000, 192.000], mean observation: 0.092 [0.000, 44.000], loss: 0.199981, mean_absolute_error: 0.423729, mean_q: 6.673979, mean_eps: 0.100000\n",
      "  75629/175000: episode: 2123, duration: 0.826s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 80.333 [9.000, 193.000], mean observation: 0.298 [0.000, 84.000], loss: 35.244883, mean_absolute_error: 0.791815, mean_q: 8.328273, mean_eps: 0.100000\n",
      "  75657/175000: episode: 2124, duration: 0.493s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 16.214 [9.000, 211.000], mean observation: 0.067 [0.000, 56.000], loss: 17.014893, mean_absolute_error: 0.701870, mean_q: 8.214667, mean_eps: 0.100000\n",
      "  75708/175000: episode: 2125, duration: 0.997s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 125.765 [16.000, 192.000], mean observation: 0.571 [0.000, 102.000], loss: 0.177466, mean_absolute_error: 0.446255, mean_q: 6.805083, mean_eps: 0.100000\n",
      "  75741/175000: episode: 2126, duration: 0.726s, episode steps: 33, steps per second: 45, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 183.455 [86.000, 216.000], mean observation: 0.162 [0.000, 66.000], loss: 25.328467, mean_absolute_error: 0.676055, mean_q: 7.404281, mean_eps: 0.100000\n",
      "  75782/175000: episode: 2127, duration: 0.813s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 179.195 [30.000, 195.000], mean observation: 0.178 [0.000, 82.000], loss: 24.573752, mean_absolute_error: 0.798031, mean_q: 8.418841, mean_eps: 0.100000\n",
      "  75814/175000: episode: 2128, duration: 0.648s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.281 [7.000, 209.000], mean observation: 0.151 [0.000, 64.000], loss: 31.929518, mean_absolute_error: 0.907474, mean_q: 9.005359, mean_eps: 0.100000\n",
      "  75860/175000: episode: 2129, duration: 0.921s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.587 [7.000, 192.000], mean observation: 0.395 [0.000, 92.000], loss: 5.543322, mean_absolute_error: 0.594448, mean_q: 7.636609, mean_eps: 0.100000\n",
      "  75903/175000: episode: 2130, duration: 0.851s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 77.233 [3.000, 207.000], mean observation: 0.399 [0.000, 86.000], loss: 4.858162, mean_absolute_error: 0.567993, mean_q: 7.322278, mean_eps: 0.100000\n",
      "  75939/175000: episode: 2131, duration: 0.660s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 30.139 [6.000, 156.000], mean observation: 0.178 [0.000, 72.000], loss: 10.288419, mean_absolute_error: 0.661223, mean_q: 8.208113, mean_eps: 0.100000\n",
      "  75965/175000: episode: 2132, duration: 0.557s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.269 [1.000, 211.000], mean observation: 0.263 [0.000, 52.000], loss: 0.443100, mean_absolute_error: 0.469831, mean_q: 6.888450, mean_eps: 0.100000\n",
      "  76000/175000: episode: 2133, duration: 0.638s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.143 [5.000, 207.000], mean observation: 0.378 [0.000, 70.000], loss: 13.761317, mean_absolute_error: 0.658232, mean_q: 7.599225, mean_eps: 0.100000\n",
      "  76038/175000: episode: 2134, duration: 0.789s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.368 [5.000, 207.000], mean observation: 0.186 [0.000, 76.000], loss: 4.835594, mean_absolute_error: 0.604857, mean_q: 7.543602, mean_eps: 0.100000\n",
      "  76085/175000: episode: 2135, duration: 0.879s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 19.936 [5.000, 162.000], mean observation: 0.115 [0.000, 94.000], loss: 8.172147, mean_absolute_error: 0.586279, mean_q: 7.286747, mean_eps: 0.100000\n",
      "  76123/175000: episode: 2136, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.842 [5.000, 184.000], mean observation: 0.178 [0.000, 76.000], loss: 18.647166, mean_absolute_error: 0.649121, mean_q: 7.471844, mean_eps: 0.100000\n",
      "  76165/175000: episode: 2137, duration: 0.870s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 38.190 [5.000, 224.000], mean observation: 0.215 [0.000, 84.000], loss: 3.284527, mean_absolute_error: 0.576931, mean_q: 7.688480, mean_eps: 0.100000\n",
      "  76191/175000: episode: 2138, duration: 0.446s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 10.115 [5.000, 138.000], mean observation: 0.062 [0.000, 52.000], loss: 13.536180, mean_absolute_error: 0.722805, mean_q: 8.386852, mean_eps: 0.100000\n",
      "  76216/175000: episode: 2139, duration: 0.580s, episode steps: 25, steps per second: 43, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 98.400 [5.000, 223.000], mean observation: 0.113 [0.000, 50.000], loss: 0.252849, mean_absolute_error: 0.686350, mean_q: 8.995576, mean_eps: 0.100000\n",
      "  76237/175000: episode: 2140, duration: 0.425s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.571 [38.000, 209.000], mean observation: 0.076 [0.000, 42.000], loss: 39.186298, mean_absolute_error: 1.033189, mean_q: 10.053548, mean_eps: 0.100000\n",
      "  76290/175000: episode: 2141, duration: 1.091s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 135.038 [6.000, 207.000], mean observation: 0.329 [0.000, 106.000], loss: 0.202758, mean_absolute_error: 0.417748, mean_q: 6.654563, mean_eps: 0.100000\n",
      "  76330/175000: episode: 2142, duration: 0.900s, episode steps: 40, steps per second: 44, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.675 [6.000, 156.000], mean observation: 0.322 [0.000, 80.000], loss: 0.323174, mean_absolute_error: 0.429176, mean_q: 6.834097, mean_eps: 0.100000\n",
      "  76375/175000: episode: 2143, duration: 1.024s, episode steps: 45, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 71.356 [6.000, 194.000], mean observation: 0.317 [0.000, 90.000], loss: 14.685728, mean_absolute_error: 0.977567, mean_q: 10.315063, mean_eps: 0.100000\n",
      "  76408/175000: episode: 2144, duration: 0.634s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.667 [9.000, 149.000], mean observation: 0.120 [0.000, 66.000], loss: 0.510898, mean_absolute_error: 0.441856, mean_q: 6.675969, mean_eps: 0.100000\n",
      "  76452/175000: episode: 2145, duration: 1.047s, episode steps: 44, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 56.977 [28.000, 216.000], mean observation: 0.363 [0.000, 88.000], loss: 0.543929, mean_absolute_error: 0.444342, mean_q: 6.732817, mean_eps: 0.100000\n",
      "  76483/175000: episode: 2146, duration: 0.640s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.516 [0.000, 223.000], mean observation: 0.264 [0.000, 62.000], loss: 115.437568, mean_absolute_error: 1.416984, mean_q: 10.061588, mean_eps: 0.100000\n",
      "  76519/175000: episode: 2147, duration: 0.712s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 145.722 [28.000, 223.000], mean observation: 0.188 [0.000, 72.000], loss: 73.210287, mean_absolute_error: 1.057393, mean_q: 9.062605, mean_eps: 0.100000\n",
      "  76557/175000: episode: 2148, duration: 0.764s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 123.026 [20.000, 223.000], mean observation: 0.280 [0.000, 76.000], loss: 0.691993, mean_absolute_error: 0.589984, mean_q: 8.068216, mean_eps: 0.100000\n",
      "  76585/175000: episode: 2149, duration: 0.537s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 208.929 [30.000, 223.000], mean observation: 0.105 [0.000, 56.000], loss: 54.024885, mean_absolute_error: 1.031992, mean_q: 9.467903, mean_eps: 0.100000\n",
      "  76605/175000: episode: 2150, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 77.800 [27.000, 223.000], mean observation: 0.113 [0.000, 40.000], loss: 79.632421, mean_absolute_error: 1.023101, mean_q: 8.416183, mean_eps: 0.100000\n",
      "  76643/175000: episode: 2151, duration: 0.811s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.684 [27.000, 223.000], mean observation: 0.107 [0.000, 76.000], loss: 0.712083, mean_absolute_error: 0.551284, mean_q: 7.588836, mean_eps: 0.100000\n",
      "  76672/175000: episode: 2152, duration: 0.652s, episode steps: 29, steps per second: 44, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.103 [88.000, 220.000], mean observation: 0.096 [0.000, 58.000], loss: 7.786076, mean_absolute_error: 0.847034, mean_q: 9.629600, mean_eps: 0.100000\n",
      "  76711/175000: episode: 2153, duration: 0.832s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.615 [2.000, 220.000], mean observation: 0.171 [0.000, 78.000], loss: 32.257761, mean_absolute_error: 0.980149, mean_q: 9.849585, mean_eps: 0.100000\n",
      "  76742/175000: episode: 2154, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 98.323 [46.000, 150.000], mean observation: 0.153 [0.000, 62.000], loss: 10.060100, mean_absolute_error: 0.659364, mean_q: 8.237534, mean_eps: 0.100000\n",
      "  76792/175000: episode: 2155, duration: 0.999s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 116.820 [5.000, 220.000], mean observation: 0.299 [0.000, 100.000], loss: 0.274815, mean_absolute_error: 0.444674, mean_q: 6.834672, mean_eps: 0.100000\n",
      "  76825/175000: episode: 2156, duration: 0.667s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.545 [5.000, 150.000], mean observation: 0.188 [0.000, 66.000], loss: 0.216729, mean_absolute_error: 0.476397, mean_q: 7.083346, mean_eps: 0.100000\n",
      "  76851/175000: episode: 2157, duration: 0.468s, episode steps: 26, steps per second: 56, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 147.154 [82.000, 210.000], mean observation: 0.147 [0.000, 52.000], loss: 8.108506, mean_absolute_error: 0.718392, mean_q: 8.549023, mean_eps: 0.100000\n",
      "  76887/175000: episode: 2158, duration: 0.637s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 141.278 [32.000, 150.000], mean observation: 0.179 [0.000, 72.000], loss: 28.430931, mean_absolute_error: 0.968350, mean_q: 9.868832, mean_eps: 0.100000\n",
      "  76916/175000: episode: 2159, duration: 0.565s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 160.690 [88.000, 195.000], mean observation: 0.201 [0.000, 58.000], loss: 0.236208, mean_absolute_error: 0.456334, mean_q: 6.976318, mean_eps: 0.100000\n",
      "  76948/175000: episode: 2160, duration: 0.636s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 162.156 [141.000, 195.000], mean observation: 0.144 [0.000, 64.000], loss: 5.660075, mean_absolute_error: 0.548124, mean_q: 7.748206, mean_eps: 0.100000\n",
      "  76992/175000: episode: 2161, duration: 0.852s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.568 [16.000, 217.000], mean observation: 0.349 [0.000, 88.000], loss: 0.209744, mean_absolute_error: 0.445171, mean_q: 6.795978, mean_eps: 0.100000\n",
      "  77034/175000: episode: 2162, duration: 0.806s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.333 [23.000, 223.000], mean observation: 0.478 [0.000, 84.000], loss: 7.402396, mean_absolute_error: 0.620228, mean_q: 8.006081, mean_eps: 0.100000\n",
      "  77074/175000: episode: 2163, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 139.025 [28.000, 191.000], mean observation: 0.274 [0.000, 80.000], loss: 0.109844, mean_absolute_error: 0.446134, mean_q: 6.971526, mean_eps: 0.100000\n",
      "  77097/175000: episode: 2164, duration: 0.438s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 36.087 [27.000, 160.000], mean observation: 0.062 [0.000, 46.000], loss: 62.294985, mean_absolute_error: 0.950172, mean_q: 8.712871, mean_eps: 0.100000\n",
      "  77129/175000: episode: 2165, duration: 0.557s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 32.719 [25.000, 212.000], mean observation: 0.125 [0.000, 64.000], loss: 17.243136, mean_absolute_error: 0.858167, mean_q: 9.453856, mean_eps: 0.100000\n",
      "  77145/175000: episode: 2166, duration: 0.296s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 28.062 [27.000, 44.000], mean observation: 0.052 [0.000, 32.000], loss: 115.014564, mean_absolute_error: 1.261837, mean_q: 9.041242, mean_eps: 0.100000\n",
      "  77172/175000: episode: 2167, duration: 0.494s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 36.481 [27.000, 164.000], mean observation: 0.110 [0.000, 54.000], loss: 74.394396, mean_absolute_error: 1.018408, mean_q: 8.734990, mean_eps: 0.100000\n",
      "  77208/175000: episode: 2168, duration: 0.714s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 40.500 [27.000, 206.000], mean observation: 0.123 [0.000, 72.000], loss: 2.531445, mean_absolute_error: 0.473801, mean_q: 7.141344, mean_eps: 0.100000\n",
      "  77246/175000: episode: 2169, duration: 0.705s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 60.342 [8.000, 188.000], mean observation: 0.170 [0.000, 76.000], loss: 1.574884, mean_absolute_error: 0.493013, mean_q: 7.740530, mean_eps: 0.100000\n",
      "  77276/175000: episode: 2170, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 41.267 [14.000, 200.000], mean observation: 0.142 [0.000, 60.000], loss: 0.176147, mean_absolute_error: 0.452461, mean_q: 7.374690, mean_eps: 0.100000\n",
      "  77317/175000: episode: 2171, duration: 0.798s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 46.488 [1.000, 213.000], mean observation: 0.229 [0.000, 82.000], loss: 0.379213, mean_absolute_error: 0.431936, mean_q: 6.922673, mean_eps: 0.100000\n",
      "  77337/175000: episode: 2172, duration: 0.351s, episode steps: 20, steps per second: 57, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.048 [0.000, 40.000], loss: 63.273503, mean_absolute_error: 0.983350, mean_q: 9.067514, mean_eps: 0.100000\n",
      "  77380/175000: episode: 2173, duration: 0.799s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.209 [14.000, 204.000], mean observation: 0.308 [0.000, 86.000], loss: 11.176889, mean_absolute_error: 0.635595, mean_q: 7.914006, mean_eps: 0.100000\n",
      "  77445/175000: episode: 2174, duration: 1.203s, episode steps: 65, steps per second: 54, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 122.862 [16.000, 213.000], mean observation: 0.555 [0.000, 130.000], loss: 0.735650, mean_absolute_error: 0.509370, mean_q: 7.486817, mean_eps: 0.100000\n",
      "  77473/175000: episode: 2175, duration: 0.498s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 35.429 [28.000, 172.000], mean observation: 0.071 [0.000, 56.000], loss: 1.355174, mean_absolute_error: 0.465550, mean_q: 7.187298, mean_eps: 0.100000\n",
      "  77513/175000: episode: 2176, duration: 0.739s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.375 [3.000, 183.000], mean observation: 0.196 [0.000, 80.000], loss: 3.566906, mean_absolute_error: 0.611506, mean_q: 8.177759, mean_eps: 0.100000\n",
      "  77547/175000: episode: 2177, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 34.000 [28.000, 155.000], mean observation: 0.115 [0.000, 68.000], loss: 1.389392, mean_absolute_error: 0.454019, mean_q: 7.085069, mean_eps: 0.100000\n",
      "  77572/175000: episode: 2178, duration: 0.601s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 33.080 [28.000, 155.000], mean observation: 0.061 [0.000, 50.000], loss: 35.547517, mean_absolute_error: 0.825602, mean_q: 8.680229, mean_eps: 0.100000\n",
      "  77607/175000: episode: 2179, duration: 0.734s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.343 [27.000, 195.000], mean observation: 0.111 [0.000, 70.000], loss: 11.015883, mean_absolute_error: 0.625325, mean_q: 7.971013, mean_eps: 0.100000\n",
      "  77649/175000: episode: 2180, duration: 0.827s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.238 [6.000, 195.000], mean observation: 0.154 [0.000, 84.000], loss: 3.164064, mean_absolute_error: 0.584075, mean_q: 8.157425, mean_eps: 0.100000\n",
      "  77678/175000: episode: 2181, duration: 0.636s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 73.379 [24.000, 195.000], mean observation: 0.121 [0.000, 58.000], loss: 42.341274, mean_absolute_error: 0.849007, mean_q: 8.903147, mean_eps: 0.100000\n",
      "  77712/175000: episode: 2182, duration: 0.794s, episode steps: 34, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.559 [24.000, 223.000], mean observation: 0.300 [0.000, 68.000], loss: 34.273825, mean_absolute_error: 0.943134, mean_q: 9.549789, mean_eps: 0.100000\n",
      "  77736/175000: episode: 2183, duration: 0.624s, episode steps: 24, steps per second: 38, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 79.750 [22.000, 216.000], mean observation: 0.120 [0.000, 48.000], loss: 97.919363, mean_absolute_error: 1.684376, mean_q: 13.469561, mean_eps: 0.100000\n",
      "  77771/175000: episode: 2184, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.743 [28.000, 224.000], mean observation: 0.155 [0.000, 70.000], loss: 54.975084, mean_absolute_error: 0.964089, mean_q: 9.210756, mean_eps: 0.100000\n",
      "  77810/175000: episode: 2185, duration: 1.022s, episode steps: 39, steps per second: 38, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 38.231 [28.000, 204.000], mean observation: 0.164 [0.000, 78.000], loss: 0.138686, mean_absolute_error: 0.446661, mean_q: 7.325199, mean_eps: 0.100000\n",
      "  77845/175000: episode: 2186, duration: 0.721s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 38.343 [28.000, 129.000], mean observation: 0.128 [0.000, 70.000], loss: 11.593333, mean_absolute_error: 0.641663, mean_q: 8.346280, mean_eps: 0.100000\n",
      "  77894/175000: episode: 2187, duration: 1.051s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 36.000 [1.000, 200.000], mean observation: 0.249 [0.000, 98.000], loss: 0.176602, mean_absolute_error: 0.452020, mean_q: 7.328081, mean_eps: 0.100000\n",
      "  77932/175000: episode: 2188, duration: 0.826s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 36.316 [28.000, 184.000], mean observation: 0.118 [0.000, 76.000], loss: 52.836832, mean_absolute_error: 0.990418, mean_q: 9.539212, mean_eps: 0.100000\n",
      "  77969/175000: episode: 2189, duration: 0.868s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 55.081 [28.000, 156.000], mean observation: 0.160 [0.000, 74.000], loss: 0.117582, mean_absolute_error: 0.441994, mean_q: 7.125803, mean_eps: 0.100000\n",
      "  77995/175000: episode: 2190, duration: 0.524s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 28.962 [23.000, 58.000], mean observation: 0.093 [0.000, 52.000], loss: 0.105037, mean_absolute_error: 0.449165, mean_q: 7.076525, mean_eps: 0.100000\n",
      "  78033/175000: episode: 2191, duration: 0.915s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 33.237 [19.000, 208.000], mean observation: 0.194 [0.000, 76.000], loss: 16.916457, mean_absolute_error: 0.669497, mean_q: 8.408713, mean_eps: 0.100000\n",
      "  78068/175000: episode: 2192, duration: 0.807s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 55.914 [28.000, 199.000], mean observation: 0.165 [0.000, 70.000], loss: 13.215365, mean_absolute_error: 0.671666, mean_q: 8.263766, mean_eps: 0.100000\n",
      "  78123/175000: episode: 2193, duration: 1.211s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 68.127 [8.000, 174.000], mean observation: 0.434 [0.000, 110.000], loss: 18.926288, mean_absolute_error: 0.729053, mean_q: 8.880575, mean_eps: 0.100000\n",
      "  78167/175000: episode: 2194, duration: 0.923s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 45.614 [28.000, 210.000], mean observation: 0.149 [0.000, 88.000], loss: 0.216022, mean_absolute_error: 0.454904, mean_q: 7.592348, mean_eps: 0.100000\n",
      "  78200/175000: episode: 2195, duration: 0.662s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 52.061 [28.000, 221.000], mean observation: 0.119 [0.000, 66.000], loss: 82.017513, mean_absolute_error: 1.151015, mean_q: 9.756381, mean_eps: 0.100000\n",
      "  78243/175000: episode: 2196, duration: 1.033s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.860 [28.000, 182.000], mean observation: 0.363 [0.000, 86.000], loss: 23.958486, mean_absolute_error: 0.781681, mean_q: 9.128661, mean_eps: 0.100000\n",
      "  78276/175000: episode: 2197, duration: 0.816s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 39.788 [28.000, 210.000], mean observation: 0.143 [0.000, 66.000], loss: 31.026194, mean_absolute_error: 0.756215, mean_q: 8.665394, mean_eps: 0.100000\n",
      "  78314/175000: episode: 2198, duration: 0.801s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 56.026 [4.000, 190.000], mean observation: 0.211 [0.000, 76.000], loss: 47.205837, mean_absolute_error: 1.054040, mean_q: 10.296949, mean_eps: 0.100000\n",
      "  78348/175000: episode: 2199, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 39.971 [15.000, 223.000], mean observation: 0.171 [0.000, 68.000], loss: 0.395055, mean_absolute_error: 0.529247, mean_q: 8.138852, mean_eps: 0.100000\n",
      "  78387/175000: episode: 2200, duration: 0.830s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 139.385 [28.000, 156.000], mean observation: 0.158 [0.000, 78.000], loss: 0.190257, mean_absolute_error: 0.449384, mean_q: 7.559505, mean_eps: 0.100000\n",
      "  78412/175000: episode: 2201, duration: 0.636s, episode steps: 25, steps per second: 39, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 144.560 [28.000, 182.000], mean observation: 0.159 [0.000, 50.000], loss: 0.160526, mean_absolute_error: 0.461221, mean_q: 7.675017, mean_eps: 0.100000\n",
      "  78442/175000: episode: 2202, duration: 0.718s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.400 [28.000, 156.000], mean observation: 0.183 [0.000, 60.000], loss: 0.184858, mean_absolute_error: 0.439083, mean_q: 7.543046, mean_eps: 0.100000\n",
      "  78456/175000: episode: 2203, duration: 0.314s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 111.786 [5.000, 156.000], mean observation: 0.079 [0.000, 28.000], loss: 0.192802, mean_absolute_error: 0.423685, mean_q: 7.316387, mean_eps: 0.100000\n",
      "  78492/175000: episode: 2204, duration: 0.889s, episode steps: 36, steps per second: 41, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.000 [28.000, 221.000], mean observation: 0.258 [0.000, 72.000], loss: 0.224769, mean_absolute_error: 0.470106, mean_q: 7.687743, mean_eps: 0.100000\n",
      "  78546/175000: episode: 2205, duration: 1.141s, episode steps: 54, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 81.815 [16.000, 210.000], mean observation: 0.533 [0.000, 108.000], loss: 63.450094, mean_absolute_error: 1.007199, mean_q: 9.703346, mean_eps: 0.100000\n",
      "  78580/175000: episode: 2206, duration: 0.669s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.118 [20.000, 156.000], mean observation: 0.232 [0.000, 68.000], loss: 5.693702, mean_absolute_error: 0.556739, mean_q: 8.614956, mean_eps: 0.100000\n",
      "  78614/175000: episode: 2207, duration: 0.827s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.765 [28.000, 197.000], mean observation: 0.232 [0.000, 68.000], loss: 0.441715, mean_absolute_error: 0.471305, mean_q: 7.887000, mean_eps: 0.100000\n",
      "  78636/175000: episode: 2208, duration: 0.525s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 33.364 [18.000, 156.000], mean observation: 0.054 [0.000, 44.000], loss: 89.791389, mean_absolute_error: 1.375810, mean_q: 11.672538, mean_eps: 0.100000\n",
      "  78656/175000: episode: 2209, duration: 0.538s, episode steps: 20, steps per second: 37, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 80.150 [28.000, 213.000], mean observation: 0.070 [0.000, 40.000], loss: 0.233866, mean_absolute_error: 0.498832, mean_q: 8.290585, mean_eps: 0.100000\n",
      "  78709/175000: episode: 2210, duration: 1.227s, episode steps: 53, steps per second: 43, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.509 [4.000, 196.000], mean observation: 0.321 [0.000, 106.000], loss: 32.586582, mean_absolute_error: 0.712966, mean_q: 8.373075, mean_eps: 0.100000\n",
      "  78751/175000: episode: 2211, duration: 0.918s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 53.238 [28.000, 218.000], mean observation: 0.239 [0.000, 84.000], loss: 45.787474, mean_absolute_error: 0.798941, mean_q: 8.361064, mean_eps: 0.100000\n",
      "  78782/175000: episode: 2212, duration: 0.590s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 29.742 [6.000, 104.000], mean observation: 0.104 [0.000, 62.000], loss: 27.366097, mean_absolute_error: 0.924074, mean_q: 9.883640, mean_eps: 0.100000\n",
      "  78817/175000: episode: 2213, duration: 0.822s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 30.086 [5.000, 156.000], mean observation: 0.110 [0.000, 70.000], loss: 0.165632, mean_absolute_error: 0.487688, mean_q: 7.544779, mean_eps: 0.100000\n",
      "  78855/175000: episode: 2214, duration: 0.783s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 91.605 [5.000, 208.000], mean observation: 0.215 [0.000, 76.000], loss: 0.166075, mean_absolute_error: 0.454920, mean_q: 7.487658, mean_eps: 0.100000\n",
      "  78900/175000: episode: 2215, duration: 1.062s, episode steps: 45, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 145.556 [28.000, 208.000], mean observation: 0.363 [0.000, 90.000], loss: 29.943659, mean_absolute_error: 1.068066, mean_q: 10.990147, mean_eps: 0.100000\n",
      "  78953/175000: episode: 2216, duration: 1.151s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 122.736 [3.000, 208.000], mean observation: 0.385 [0.000, 106.000], loss: 11.012434, mean_absolute_error: 0.627889, mean_q: 8.486992, mean_eps: 0.100000\n",
      "  78978/175000: episode: 2217, duration: 0.494s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 81.560 [28.000, 199.000], mean observation: 0.169 [0.000, 50.000], loss: 0.351670, mean_absolute_error: 0.505625, mean_q: 7.767486, mean_eps: 0.100000\n",
      "  79034/175000: episode: 2218, duration: 1.257s, episode steps: 56, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 103.571 [28.000, 202.000], mean observation: 0.404 [0.000, 112.000], loss: 0.312094, mean_absolute_error: 0.466396, mean_q: 7.714925, mean_eps: 0.100000\n",
      "  79052/175000: episode: 2219, duration: 0.366s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 124.556 [50.000, 156.000], mean observation: 0.086 [0.000, 36.000], loss: 0.237135, mean_absolute_error: 0.470813, mean_q: 7.808732, mean_eps: 0.100000\n",
      "  79081/175000: episode: 2220, duration: 0.687s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 140.483 [80.000, 180.000], mean observation: 0.173 [0.000, 58.000], loss: 4.128229, mean_absolute_error: 0.648788, mean_q: 8.964659, mean_eps: 0.100000\n",
      "  79105/175000: episode: 2221, duration: 0.521s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 128.083 [21.000, 156.000], mean observation: 0.143 [0.000, 48.000], loss: 0.463751, mean_absolute_error: 0.455952, mean_q: 7.419898, mean_eps: 0.100000\n",
      "  79128/175000: episode: 2222, duration: 0.517s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 116.000 [89.000, 156.000], mean observation: 0.080 [0.000, 46.000], loss: 0.110446, mean_absolute_error: 0.455558, mean_q: 7.460796, mean_eps: 0.100000\n",
      "  79160/175000: episode: 2223, duration: 0.738s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 155.906 [49.000, 222.000], mean observation: 0.176 [0.000, 64.000], loss: 0.150432, mean_absolute_error: 0.532357, mean_q: 8.005572, mean_eps: 0.100000\n",
      "  79196/175000: episode: 2224, duration: 0.719s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 133.972 [86.000, 213.000], mean observation: 0.206 [0.000, 72.000], loss: 0.175303, mean_absolute_error: 0.505218, mean_q: 7.788980, mean_eps: 0.100000\n",
      "  79237/175000: episode: 2225, duration: 0.859s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 110.415 [1.000, 215.000], mean observation: 0.232 [0.000, 82.000], loss: 0.598613, mean_absolute_error: 0.515775, mean_q: 7.920213, mean_eps: 0.100000\n",
      "  79271/175000: episode: 2226, duration: 0.637s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 121.324 [0.000, 199.000], mean observation: 0.222 [0.000, 68.000], loss: 2.398636, mean_absolute_error: 0.491376, mean_q: 7.549493, mean_eps: 0.100000\n",
      "  79293/175000: episode: 2227, duration: 0.506s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 119.318 [79.000, 195.000], mean observation: 0.093 [0.000, 44.000], loss: 1.911274, mean_absolute_error: 0.502614, mean_q: 7.484316, mean_eps: 0.100000\n",
      "  79331/175000: episode: 2228, duration: 0.807s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.816 [1.000, 195.000], mean observation: 0.204 [0.000, 76.000], loss: 1.301912, mean_absolute_error: 0.548386, mean_q: 7.980261, mean_eps: 0.100000\n",
      "  79359/175000: episode: 2229, duration: 0.583s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 110.643 [79.000, 156.000], mean observation: 0.133 [0.000, 56.000], loss: 19.659858, mean_absolute_error: 0.613328, mean_q: 8.148609, mean_eps: 0.100000\n",
      "  79409/175000: episode: 2230, duration: 1.100s, episode steps: 50, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.280 [1.000, 223.000], mean observation: 0.453 [0.000, 100.000], loss: 2.192434, mean_absolute_error: 0.541264, mean_q: 8.073777, mean_eps: 0.100000\n",
      "  79456/175000: episode: 2231, duration: 1.019s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 117.404 [5.000, 223.000], mean observation: 0.399 [0.000, 94.000], loss: 1.172098, mean_absolute_error: 0.616564, mean_q: 8.441530, mean_eps: 0.100000\n",
      "  79489/175000: episode: 2232, duration: 0.845s, episode steps: 33, steps per second: 39, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 98.242 [20.000, 156.000], mean observation: 0.204 [0.000, 66.000], loss: 1.216089, mean_absolute_error: 0.520261, mean_q: 8.024208, mean_eps: 0.100000\n",
      "  79525/175000: episode: 2233, duration: 0.646s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 111.472 [56.000, 222.000], mean observation: 0.153 [0.000, 72.000], loss: 18.690110, mean_absolute_error: 0.705975, mean_q: 8.669885, mean_eps: 0.100000\n",
      "  79570/175000: episode: 2234, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.578 [72.000, 223.000], mean observation: 0.309 [0.000, 90.000], loss: 0.940149, mean_absolute_error: 0.514604, mean_q: 7.939281, mean_eps: 0.100000\n",
      "  79582/175000: episode: 2235, duration: 0.303s, episode steps: 12, steps per second: 40, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 83.500 [38.000, 133.000], mean observation: 0.051 [0.000, 24.000], loss: 0.317822, mean_absolute_error: 0.518857, mean_q: 7.918485, mean_eps: 0.100000\n",
      "  79625/175000: episode: 2236, duration: 0.999s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.488 [9.000, 220.000], mean observation: 0.211 [0.000, 86.000], loss: 31.933171, mean_absolute_error: 0.777915, mean_q: 8.860588, mean_eps: 0.100000\n",
      "  79647/175000: episode: 2237, duration: 0.408s, episode steps: 22, steps per second: 54, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 84.682 [9.000, 187.000], mean observation: 0.092 [0.000, 44.000], loss: 2.666606, mean_absolute_error: 0.519168, mean_q: 7.828578, mean_eps: 0.100000\n",
      "  79692/175000: episode: 2238, duration: 0.881s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 113.044 [43.000, 218.000], mean observation: 0.312 [0.000, 90.000], loss: 53.902602, mean_absolute_error: 0.874585, mean_q: 8.784642, mean_eps: 0.100000\n",
      "  79755/175000: episode: 2239, duration: 1.188s, episode steps: 63, steps per second: 53, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.175 [1.000, 208.000], mean observation: 0.580 [0.000, 126.000], loss: 28.682343, mean_absolute_error: 0.715294, mean_q: 8.308714, mean_eps: 0.100000\n",
      "  79783/175000: episode: 2240, duration: 0.487s, episode steps: 28, steps per second: 58, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.643 [15.000, 194.000], mean observation: 0.194 [0.000, 56.000], loss: 1.864996, mean_absolute_error: 0.524641, mean_q: 7.645294, mean_eps: 0.100000\n",
      "  79823/175000: episode: 2241, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 132.675 [40.000, 222.000], mean observation: 0.347 [0.000, 80.000], loss: 1.426082, mean_absolute_error: 0.542908, mean_q: 7.908337, mean_eps: 0.100000\n",
      "  79853/175000: episode: 2242, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 139.667 [79.000, 174.000], mean observation: 0.140 [0.000, 60.000], loss: 1.638730, mean_absolute_error: 0.542296, mean_q: 7.835054, mean_eps: 0.100000\n",
      "  79886/175000: episode: 2243, duration: 0.769s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 156.273 [133.000, 206.000], mean observation: 0.127 [0.000, 66.000], loss: 1.179507, mean_absolute_error: 0.563339, mean_q: 8.134776, mean_eps: 0.100000\n",
      "  79941/175000: episode: 2244, duration: 1.068s, episode steps: 55, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 81.800 [7.000, 200.000], mean observation: 0.504 [0.000, 110.000], loss: 1.186482, mean_absolute_error: 0.541680, mean_q: 7.998979, mean_eps: 0.100000\n",
      "  79978/175000: episode: 2245, duration: 0.785s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.595 [64.000, 215.000], mean observation: 0.261 [0.000, 74.000], loss: 20.035264, mean_absolute_error: 0.737103, mean_q: 8.658447, mean_eps: 0.100000\n",
      "  79994/175000: episode: 2246, duration: 0.403s, episode steps: 16, steps per second: 40, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 132.312 [52.000, 150.000], mean observation: 0.054 [0.000, 32.000], loss: 3.057430, mean_absolute_error: 0.559828, mean_q: 8.142532, mean_eps: 0.100000\n",
      "  80019/175000: episode: 2247, duration: 0.523s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 150.000 [150.000, 150.000], mean observation: 0.059 [0.000, 50.000], loss: 0.382101, mean_absolute_error: 0.526319, mean_q: 7.585968, mean_eps: 0.100000\n",
      "  80066/175000: episode: 2248, duration: 0.982s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.149 [64.000, 195.000], mean observation: 0.357 [0.000, 94.000], loss: 261.454629, mean_absolute_error: 2.125601, mean_q: 10.953390, mean_eps: 0.100000\n",
      "  80114/175000: episode: 2249, duration: 1.025s, episode steps: 48, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 62.375 [14.000, 182.000], mean observation: 0.285 [0.000, 96.000], loss: 1.129966, mean_absolute_error: 0.529637, mean_q: 7.566607, mean_eps: 0.100000\n",
      "  80154/175000: episode: 2250, duration: 0.762s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.925 [10.000, 168.000], mean observation: 0.357 [0.000, 80.000], loss: 0.999701, mean_absolute_error: 0.497608, mean_q: 7.605030, mean_eps: 0.100000\n",
      "  80185/175000: episode: 2251, duration: 0.609s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.871 [90.000, 165.000], mean observation: 0.122 [0.000, 62.000], loss: 31.467738, mean_absolute_error: 0.821418, mean_q: 8.855335, mean_eps: 0.100000\n",
      "  80227/175000: episode: 2252, duration: 0.752s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 147.429 [45.000, 176.000], mean observation: 0.227 [0.000, 84.000], loss: 109.680364, mean_absolute_error: 1.382328, mean_q: 10.441626, mean_eps: 0.100000\n",
      "  80273/175000: episode: 2253, duration: 0.880s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 148.891 [28.000, 215.000], mean observation: 0.369 [0.000, 92.000], loss: 0.661962, mean_absolute_error: 0.507988, mean_q: 7.383546, mean_eps: 0.100000\n",
      "  80321/175000: episode: 2254, duration: 0.863s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.583 [24.000, 218.000], mean observation: 0.302 [0.000, 96.000], loss: 1.076435, mean_absolute_error: 0.543955, mean_q: 7.636250, mean_eps: 0.100000\n",
      "  80339/175000: episode: 2255, duration: 0.417s, episode steps: 18, steps per second: 43, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 99.111 [28.000, 156.000], mean observation: 0.070 [0.000, 36.000], loss: 0.802519, mean_absolute_error: 0.529700, mean_q: 7.432351, mean_eps: 0.100000\n",
      "  80376/175000: episode: 2256, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 35.865 [28.000, 219.000], mean observation: 0.093 [0.000, 74.000], loss: 0.539173, mean_absolute_error: 0.497699, mean_q: 7.351336, mean_eps: 0.100000\n",
      "  80433/175000: episode: 2257, duration: 1.073s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 73.807 [0.000, 209.000], mean observation: 0.499 [0.000, 114.000], loss: 36.250597, mean_absolute_error: 0.837207, mean_q: 8.793617, mean_eps: 0.100000\n",
      "  80482/175000: episode: 2258, duration: 0.876s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 94.694 [2.000, 205.000], mean observation: 0.513 [0.000, 98.000], loss: 26.543971, mean_absolute_error: 0.725681, mean_q: 8.325041, mean_eps: 0.100000\n",
      "  80522/175000: episode: 2259, duration: 0.718s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 60.150 [8.000, 190.000], mean observation: 0.339 [0.000, 80.000], loss: 15.765971, mean_absolute_error: 0.708457, mean_q: 8.771997, mean_eps: 0.100000\n",
      "  80558/175000: episode: 2260, duration: 0.665s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.750 [28.000, 156.000], mean observation: 0.197 [0.000, 72.000], loss: 79.759160, mean_absolute_error: 0.998049, mean_q: 8.654291, mean_eps: 0.100000\n",
      "  80595/175000: episode: 2261, duration: 0.818s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.486 [15.000, 194.000], mean observation: 0.216 [0.000, 74.000], loss: 0.849509, mean_absolute_error: 0.508128, mean_q: 7.659420, mean_eps: 0.100000\n",
      "  80636/175000: episode: 2262, duration: 0.999s, episode steps: 41, steps per second: 41, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.268 [48.000, 197.000], mean observation: 0.209 [0.000, 82.000], loss: 94.810754, mean_absolute_error: 1.180119, mean_q: 9.591144, mean_eps: 0.100000\n",
      "  80649/175000: episode: 2263, duration: 0.328s, episode steps: 13, steps per second: 40, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 101.692 [45.000, 209.000], mean observation: 0.050 [0.000, 26.000], loss: 1.588889, mean_absolute_error: 0.488250, mean_q: 7.573715, mean_eps: 0.100000\n",
      "  80662/175000: episode: 2264, duration: 0.302s, episode steps: 13, steps per second: 43, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 96.769 [18.000, 206.000], mean observation: 0.072 [0.000, 26.000], loss: 1.176036, mean_absolute_error: 0.474814, mean_q: 7.516917, mean_eps: 0.100000\n",
      "  80692/175000: episode: 2265, duration: 0.770s, episode steps: 30, steps per second: 39, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.167 [9.000, 156.000], mean observation: 0.136 [0.000, 60.000], loss: 228.598221, mean_absolute_error: 1.890374, mean_q: 10.530818, mean_eps: 0.100000\n",
      "  80704/175000: episode: 2266, duration: 0.350s, episode steps: 12, steps per second: 34, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 99.333 [39.000, 162.000], mean observation: 0.062 [0.000, 24.000], loss: 321.302792, mean_absolute_error: 2.335838, mean_q: 10.804901, mean_eps: 0.100000\n",
      "  80730/175000: episode: 2267, duration: 0.682s, episode steps: 26, steps per second: 38, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.115 [34.000, 156.000], mean observation: 0.119 [0.000, 52.000], loss: 0.724404, mean_absolute_error: 0.495313, mean_q: 7.697802, mean_eps: 0.100000\n",
      "  80767/175000: episode: 2268, duration: 0.816s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.865 [37.000, 211.000], mean observation: 0.202 [0.000, 74.000], loss: 31.887927, mean_absolute_error: 0.650462, mean_q: 7.846342, mean_eps: 0.100000\n",
      "  80822/175000: episode: 2269, duration: 1.355s, episode steps: 55, steps per second: 41, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 69.345 [48.000, 191.000], mean observation: 0.247 [0.000, 110.000], loss: 1.948117, mean_absolute_error: 0.522037, mean_q: 8.084062, mean_eps: 0.100000\n",
      "  80879/175000: episode: 2270, duration: 1.476s, episode steps: 57, steps per second: 39, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 95.158 [24.000, 213.000], mean observation: 0.529 [0.000, 114.000], loss: 1.755997, mean_absolute_error: 0.542194, mean_q: 8.261411, mean_eps: 0.100000\n",
      "  80925/175000: episode: 2271, duration: 1.091s, episode steps: 46, steps per second: 42, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 79.739 [24.000, 171.000], mean observation: 0.345 [0.000, 92.000], loss: 134.943597, mean_absolute_error: 1.192280, mean_q: 8.494850, mean_eps: 0.100000\n",
      "  80951/175000: episode: 2272, duration: 0.563s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 87.077 [42.000, 212.000], mean observation: 0.128 [0.000, 52.000], loss: 31.327596, mean_absolute_error: 0.852400, mean_q: 9.339862, mean_eps: 0.100000\n",
      "  80966/175000: episode: 2273, duration: 0.364s, episode steps: 15, steps per second: 41, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 42.000 [42.000, 42.000], mean observation: 0.037 [0.000, 30.000], loss: 2.258407, mean_absolute_error: 0.533881, mean_q: 7.897177, mean_eps: 0.100000\n",
      "  81007/175000: episode: 2274, duration: 0.799s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 40.049 [24.000, 149.000], mean observation: 0.291 [0.000, 82.000], loss: 1.220303, mean_absolute_error: 0.505851, mean_q: 7.918692, mean_eps: 0.100000\n",
      "  81028/175000: episode: 2275, duration: 0.453s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 48.667 [44.000, 142.000], mean observation: 0.051 [0.000, 42.000], loss: 1.713806, mean_absolute_error: 0.477196, mean_q: 7.522077, mean_eps: 0.100000\n",
      "  81068/175000: episode: 2276, duration: 0.851s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 46.900 [16.000, 156.000], mean observation: 0.143 [0.000, 80.000], loss: 92.350814, mean_absolute_error: 1.024010, mean_q: 8.671646, mean_eps: 0.100000\n",
      "  81098/175000: episode: 2277, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 51.267 [11.000, 197.000], mean observation: 0.218 [0.000, 60.000], loss: 2.724673, mean_absolute_error: 0.513887, mean_q: 8.051216, mean_eps: 0.100000\n",
      "  81131/175000: episode: 2278, duration: 0.638s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.515 [28.000, 148.000], mean observation: 0.227 [0.000, 66.000], loss: 1.562858, mean_absolute_error: 0.517422, mean_q: 8.043687, mean_eps: 0.100000\n",
      "  81164/175000: episode: 2279, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 93.333 [27.000, 224.000], mean observation: 0.271 [0.000, 66.000], loss: 271.019101, mean_absolute_error: 2.205657, mean_q: 11.848255, mean_eps: 0.100000\n",
      "  81214/175000: episode: 2280, duration: 0.970s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 43.720 [1.000, 186.000], mean observation: 0.558 [0.000, 100.000], loss: 51.282329, mean_absolute_error: 0.946867, mean_q: 9.952374, mean_eps: 0.100000\n",
      "  81240/175000: episode: 2281, duration: 0.486s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 86.192 [24.000, 223.000], mean observation: 0.211 [0.000, 52.000], loss: 247.888628, mean_absolute_error: 1.808000, mean_q: 9.685314, mean_eps: 0.100000\n",
      "  81286/175000: episode: 2282, duration: 0.920s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 131.478 [27.000, 223.000], mean observation: 0.471 [0.000, 92.000], loss: 2.405603, mean_absolute_error: 0.634984, mean_q: 9.370491, mean_eps: 0.100000\n",
      "  81325/175000: episode: 2283, duration: 0.748s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.923 [27.000, 223.000], mean observation: 0.241 [0.000, 78.000], loss: 55.914715, mean_absolute_error: 1.026908, mean_q: 10.441189, mean_eps: 0.100000\n",
      "  81344/175000: episode: 2284, duration: 0.361s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 119.842 [27.000, 223.000], mean observation: 0.125 [0.000, 38.000], loss: 108.498397, mean_absolute_error: 1.334061, mean_q: 11.166432, mean_eps: 0.100000\n",
      "  81378/175000: episode: 2285, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 163.559 [27.000, 223.000], mean observation: 0.306 [0.000, 68.000], loss: 0.576087, mean_absolute_error: 0.528783, mean_q: 8.438046, mean_eps: 0.100000\n",
      "  81410/175000: episode: 2286, duration: 0.702s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 150.594 [27.000, 223.000], mean observation: 0.267 [0.000, 64.000], loss: 0.705052, mean_absolute_error: 0.506387, mean_q: 8.165869, mean_eps: 0.100000\n",
      "  81455/175000: episode: 2287, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 164.756 [27.000, 223.000], mean observation: 0.290 [0.000, 90.000], loss: 140.357413, mean_absolute_error: 1.392869, mean_q: 10.047480, mean_eps: 0.100000\n",
      "  81480/175000: episode: 2288, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 140.440 [59.000, 223.000], mean observation: 0.144 [0.000, 50.000], loss: 34.542221, mean_absolute_error: 0.884064, mean_q: 10.114592, mean_eps: 0.100000\n",
      "  81525/175000: episode: 2289, duration: 0.898s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 163.289 [28.000, 223.000], mean observation: 0.468 [0.000, 90.000], loss: 0.171881, mean_absolute_error: 0.511246, mean_q: 8.109496, mean_eps: 0.100000\n",
      "  81576/175000: episode: 2290, duration: 1.025s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 135.725 [2.000, 223.000], mean observation: 0.313 [0.000, 102.000], loss: 72.853015, mean_absolute_error: 1.075435, mean_q: 9.929765, mean_eps: 0.100000\n",
      "  81620/175000: episode: 2291, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.136 [29.000, 223.000], mean observation: 0.228 [0.000, 88.000], loss: 0.538010, mean_absolute_error: 0.536387, mean_q: 8.527173, mean_eps: 0.100000\n",
      "  81657/175000: episode: 2292, duration: 0.726s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 150.270 [3.000, 223.000], mean observation: 0.227 [0.000, 74.000], loss: 0.305670, mean_absolute_error: 0.509534, mean_q: 8.231634, mean_eps: 0.100000\n",
      "  81687/175000: episode: 2293, duration: 0.565s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 162.400 [25.000, 223.000], mean observation: 0.221 [0.000, 60.000], loss: 0.562382, mean_absolute_error: 0.511621, mean_q: 8.202224, mean_eps: 0.100000\n",
      "  81728/175000: episode: 2294, duration: 0.779s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 140.707 [6.000, 223.000], mean observation: 0.309 [0.000, 82.000], loss: 1.661126, mean_absolute_error: 0.556778, mean_q: 8.784200, mean_eps: 0.100000\n",
      "  81751/175000: episode: 2295, duration: 0.448s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 139.652 [27.000, 180.000], mean observation: 0.097 [0.000, 46.000], loss: 1.203378, mean_absolute_error: 0.499523, mean_q: 7.876977, mean_eps: 0.100000\n",
      "  81778/175000: episode: 2296, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 152.481 [27.000, 223.000], mean observation: 0.124 [0.000, 54.000], loss: 171.243228, mean_absolute_error: 1.672262, mean_q: 11.163108, mean_eps: 0.100000\n",
      "  81816/175000: episode: 2297, duration: 0.711s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 69.211 [27.000, 198.000], mean observation: 0.107 [0.000, 76.000], loss: 0.190280, mean_absolute_error: 0.514809, mean_q: 7.982120, mean_eps: 0.100000\n",
      "  81857/175000: episode: 2298, duration: 0.811s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.244 [11.000, 220.000], mean observation: 0.263 [0.000, 82.000], loss: 4.019058, mean_absolute_error: 0.570017, mean_q: 8.193092, mean_eps: 0.100000\n",
      "  81890/175000: episode: 2299, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 119.667 [6.000, 198.000], mean observation: 0.131 [0.000, 66.000], loss: 0.541779, mean_absolute_error: 0.526310, mean_q: 7.850963, mean_eps: 0.100000\n",
      "  81929/175000: episode: 2300, duration: 0.699s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.949 [6.000, 223.000], mean observation: 0.315 [0.000, 78.000], loss: 1.670649, mean_absolute_error: 0.532651, mean_q: 7.796836, mean_eps: 0.100000\n",
      "  81964/175000: episode: 2301, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.743 [59.000, 223.000], mean observation: 0.347 [0.000, 70.000], loss: 0.511451, mean_absolute_error: 0.558406, mean_q: 8.048564, mean_eps: 0.100000\n",
      "  82000/175000: episode: 2302, duration: 0.717s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 200.167 [41.000, 223.000], mean observation: 0.155 [0.000, 72.000], loss: 1.643429, mean_absolute_error: 0.531228, mean_q: 7.578003, mean_eps: 0.100000\n",
      "  82059/175000: episode: 2303, duration: 1.165s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 126.542 [1.000, 223.000], mean observation: 0.458 [0.000, 118.000], loss: 35.116144, mean_absolute_error: 0.765921, mean_q: 8.252524, mean_eps: 0.100000\n",
      "  82099/175000: episode: 2304, duration: 0.741s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 211.650 [65.000, 223.000], mean observation: 0.129 [0.000, 80.000], loss: 112.074974, mean_absolute_error: 1.181547, mean_q: 9.053073, mean_eps: 0.100000\n",
      "  82117/175000: episode: 2305, duration: 0.354s, episode steps: 18, steps per second: 51, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 179.778 [133.000, 223.000], mean observation: 0.066 [0.000, 36.000], loss: 1.593845, mean_absolute_error: 0.526190, mean_q: 7.726810, mean_eps: 0.100000\n",
      "  82149/175000: episode: 2306, duration: 0.570s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 223.000 [223.000, 223.000], mean observation: 0.075 [0.000, 64.000], loss: 1.234744, mean_absolute_error: 0.534654, mean_q: 7.714791, mean_eps: 0.100000\n",
      "  82178/175000: episode: 2307, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 110.241 [1.000, 223.000], mean observation: 0.183 [0.000, 58.000], loss: 1.053337, mean_absolute_error: 0.532726, mean_q: 7.641684, mean_eps: 0.100000\n",
      "  82209/175000: episode: 2308, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.871 [1.000, 223.000], mean observation: 0.241 [0.000, 62.000], loss: 1.131379, mean_absolute_error: 0.544431, mean_q: 7.885838, mean_eps: 0.100000\n",
      "  82249/175000: episode: 2309, duration: 0.834s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 151.300 [70.000, 215.000], mean observation: 0.309 [0.000, 80.000], loss: 48.953373, mean_absolute_error: 0.892838, mean_q: 8.943767, mean_eps: 0.100000\n",
      "  82286/175000: episode: 2310, duration: 0.646s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.811 [5.000, 169.000], mean observation: 0.201 [0.000, 74.000], loss: 2.534349, mean_absolute_error: 0.509758, mean_q: 7.326066, mean_eps: 0.100000\n",
      "  82315/175000: episode: 2311, duration: 0.511s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 149.655 [27.000, 172.000], mean observation: 0.129 [0.000, 58.000], loss: 195.689216, mean_absolute_error: 1.582989, mean_q: 9.177767, mean_eps: 0.100000\n",
      "  82346/175000: episode: 2312, duration: 0.546s, episode steps: 31, steps per second: 57, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 153.323 [11.000, 169.000], mean observation: 0.137 [0.000, 62.000], loss: 172.743365, mean_absolute_error: 1.579216, mean_q: 9.849679, mean_eps: 0.100000\n",
      "  82383/175000: episode: 2313, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 145.162 [19.000, 181.000], mean observation: 0.221 [0.000, 74.000], loss: 1.376037, mean_absolute_error: 0.530791, mean_q: 7.705863, mean_eps: 0.100000\n",
      "  82414/175000: episode: 2314, duration: 0.623s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.387 [92.000, 182.000], mean observation: 0.194 [0.000, 62.000], loss: 0.762293, mean_absolute_error: 0.546065, mean_q: 7.802207, mean_eps: 0.100000\n",
      "  82448/175000: episode: 2315, duration: 0.708s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.235 [27.000, 169.000], mean observation: 0.187 [0.000, 68.000], loss: 0.284908, mean_absolute_error: 0.543817, mean_q: 7.685466, mean_eps: 0.100000\n",
      "  82499/175000: episode: 2316, duration: 1.017s, episode steps: 51, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 135.373 [13.000, 222.000], mean observation: 0.387 [0.000, 102.000], loss: 10.664764, mean_absolute_error: 0.671133, mean_q: 8.327747, mean_eps: 0.100000\n",
      "  82538/175000: episode: 2317, duration: 0.773s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.744 [9.000, 204.000], mean observation: 0.351 [0.000, 78.000], loss: 90.913439, mean_absolute_error: 1.182654, mean_q: 9.615427, mean_eps: 0.100000\n",
      "  82573/175000: episode: 2318, duration: 0.677s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.371 [28.000, 218.000], mean observation: 0.262 [0.000, 70.000], loss: 0.584730, mean_absolute_error: 0.536489, mean_q: 7.856609, mean_eps: 0.100000\n",
      "  82587/175000: episode: 2319, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 47.500 [15.000, 219.000], mean observation: 0.070 [0.000, 28.000], loss: 0.334392, mean_absolute_error: 0.494889, mean_q: 7.413419, mean_eps: 0.100000\n",
      "  82617/175000: episode: 2320, duration: 0.582s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 108.367 [8.000, 217.000], mean observation: 0.214 [0.000, 60.000], loss: 2.786084, mean_absolute_error: 0.533704, mean_q: 7.808841, mean_eps: 0.100000\n",
      "  82641/175000: episode: 2321, duration: 0.463s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 69.833 [17.000, 163.000], mean observation: 0.061 [0.000, 48.000], loss: 132.464080, mean_absolute_error: 1.331848, mean_q: 9.493225, mean_eps: 0.100000\n",
      "  82675/175000: episode: 2322, duration: 0.575s, episode steps: 34, steps per second: 59, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.941 [68.000, 223.000], mean observation: 0.287 [0.000, 68.000], loss: 0.899178, mean_absolute_error: 0.561415, mean_q: 8.194729, mean_eps: 0.100000\n",
      "  82713/175000: episode: 2323, duration: 0.701s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.553 [68.000, 221.000], mean observation: 0.248 [0.000, 76.000], loss: 1.887676, mean_absolute_error: 0.532344, mean_q: 7.666017, mean_eps: 0.100000\n",
      "  82741/175000: episode: 2324, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.500 [68.000, 200.000], mean observation: 0.124 [0.000, 56.000], loss: 0.467168, mean_absolute_error: 0.551680, mean_q: 8.049722, mean_eps: 0.100000\n",
      "  82778/175000: episode: 2325, duration: 0.649s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.243 [15.000, 200.000], mean observation: 0.192 [0.000, 74.000], loss: 0.426242, mean_absolute_error: 0.495565, mean_q: 7.310665, mean_eps: 0.100000\n",
      "  82819/175000: episode: 2326, duration: 0.715s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 142.439 [15.000, 211.000], mean observation: 0.266 [0.000, 82.000], loss: 143.537064, mean_absolute_error: 1.305758, mean_q: 8.940401, mean_eps: 0.100000\n",
      "  82849/175000: episode: 2327, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 171.533 [37.000, 199.000], mean observation: 0.161 [0.000, 60.000], loss: 0.821319, mean_absolute_error: 0.506530, mean_q: 7.397968, mean_eps: 0.100000\n",
      "  82854/175000: episode: 2328, duration: 0.074s, episode steps: 5, steps per second: 67, episode reward: -1.000, mean reward: -0.200 [-1.000, 0.000], mean action: 182.000 [182.000, 182.000], mean observation: 0.015 [0.000, 10.000], loss: 460.187836, mean_absolute_error: 3.713093, mean_q: 16.965183, mean_eps: 0.100000\n",
      "  82910/175000: episode: 2329, duration: 1.084s, episode steps: 56, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 115.143 [15.000, 182.000], mean observation: 0.307 [0.000, 112.000], loss: 0.939602, mean_absolute_error: 0.508354, mean_q: 7.486415, mean_eps: 0.100000\n",
      "  82944/175000: episode: 2330, duration: 0.721s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.971 [15.000, 182.000], mean observation: 0.222 [0.000, 68.000], loss: 88.422049, mean_absolute_error: 1.058512, mean_q: 8.795320, mean_eps: 0.100000\n",
      "  82972/175000: episode: 2331, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.750 [33.000, 135.000], mean observation: 0.076 [0.000, 56.000], loss: 0.405171, mean_absolute_error: 0.487651, mean_q: 7.480550, mean_eps: 0.100000\n",
      "  82998/175000: episode: 2332, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 136.654 [32.000, 213.000], mean observation: 0.115 [0.000, 52.000], loss: 0.341096, mean_absolute_error: 0.495024, mean_q: 7.726760, mean_eps: 0.100000\n",
      "  83028/175000: episode: 2333, duration: 0.577s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.500 [60.000, 144.000], mean observation: 0.088 [0.000, 60.000], loss: 65.719908, mean_absolute_error: 0.995980, mean_q: 9.447239, mean_eps: 0.100000\n",
      "  83045/175000: episode: 2334, duration: 0.346s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 101.647 [15.000, 156.000], mean observation: 0.081 [0.000, 34.000], loss: 0.545043, mean_absolute_error: 0.495876, mean_q: 7.617129, mean_eps: 0.100000\n",
      "  83058/175000: episode: 2335, duration: 0.221s, episode steps: 13, steps per second: 59, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 81.846 [15.000, 156.000], mean observation: 0.054 [0.000, 26.000], loss: 0.235409, mean_absolute_error: 0.504330, mean_q: 8.040500, mean_eps: 0.100000\n",
      "  83098/175000: episode: 2336, duration: 0.743s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.500 [15.000, 211.000], mean observation: 0.337 [0.000, 80.000], loss: 0.397747, mean_absolute_error: 0.503111, mean_q: 7.495123, mean_eps: 0.100000\n",
      "  83149/175000: episode: 2337, duration: 0.936s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.706 [15.000, 195.000], mean observation: 0.553 [0.000, 102.000], loss: 1.177814, mean_absolute_error: 0.502065, mean_q: 7.480797, mean_eps: 0.100000\n",
      "  83174/175000: episode: 2338, duration: 0.437s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.059 [0.000, 50.000], loss: 2.127638, mean_absolute_error: 0.509353, mean_q: 7.460679, mean_eps: 0.100000\n",
      "  83214/175000: episode: 2339, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 34.875 [28.000, 197.000], mean observation: 0.150 [0.000, 80.000], loss: 109.231477, mean_absolute_error: 1.270067, mean_q: 9.574532, mean_eps: 0.100000\n",
      "  83245/175000: episode: 2340, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 54.355 [28.000, 200.000], mean observation: 0.174 [0.000, 62.000], loss: 61.101968, mean_absolute_error: 1.128536, mean_q: 10.164102, mean_eps: 0.100000\n",
      "  83293/175000: episode: 2341, duration: 1.008s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.833 [0.000, 223.000], mean observation: 0.451 [0.000, 96.000], loss: 0.235643, mean_absolute_error: 0.516577, mean_q: 7.508363, mean_eps: 0.100000\n",
      "  83320/175000: episode: 2342, duration: 0.537s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 75.074 [0.000, 223.000], mean observation: 0.096 [0.000, 54.000], loss: 1.696787, mean_absolute_error: 0.522945, mean_q: 7.402268, mean_eps: 0.100000\n",
      "  83364/175000: episode: 2343, duration: 0.864s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 145.886 [5.000, 223.000], mean observation: 0.332 [0.000, 88.000], loss: 228.634925, mean_absolute_error: 1.838291, mean_q: 9.947520, mean_eps: 0.100000\n",
      "  83411/175000: episode: 2344, duration: 0.965s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 128.638 [28.000, 223.000], mean observation: 0.154 [0.000, 94.000], loss: 23.432440, mean_absolute_error: 0.729799, mean_q: 8.471066, mean_eps: 0.100000\n",
      "  83450/175000: episode: 2345, duration: 0.804s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.949 [28.000, 223.000], mean observation: 0.135 [0.000, 78.000], loss: 102.576674, mean_absolute_error: 1.088993, mean_q: 8.963563, mean_eps: 0.100000\n",
      "  83478/175000: episode: 2346, duration: 0.589s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 129.071 [27.000, 223.000], mean observation: 0.105 [0.000, 56.000], loss: 5.696153, mean_absolute_error: 0.512692, mean_q: 7.737380, mean_eps: 0.100000\n",
      "  83524/175000: episode: 2347, duration: 0.924s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.587 [7.000, 223.000], mean observation: 0.213 [0.000, 92.000], loss: 83.336926, mean_absolute_error: 1.009851, mean_q: 9.058766, mean_eps: 0.100000\n",
      "  83539/175000: episode: 2348, duration: 0.352s, episode steps: 15, steps per second: 43, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 39.400 [28.000, 199.000], mean observation: 0.047 [0.000, 30.000], loss: 228.216620, mean_absolute_error: 1.859628, mean_q: 10.456377, mean_eps: 0.100000\n",
      "  83573/175000: episode: 2349, duration: 0.638s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 44.294 [28.000, 208.000], mean observation: 0.102 [0.000, 68.000], loss: 19.468740, mean_absolute_error: 0.758192, mean_q: 9.181904, mean_eps: 0.100000\n",
      "  83619/175000: episode: 2350, duration: 0.859s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 39.674 [28.000, 219.000], mean observation: 0.172 [0.000, 92.000], loss: 59.297549, mean_absolute_error: 1.051384, mean_q: 10.310574, mean_eps: 0.100000\n",
      "  83661/175000: episode: 2351, duration: 0.867s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.333 [28.000, 179.000], mean observation: 0.250 [0.000, 84.000], loss: 0.865862, mean_absolute_error: 0.546517, mean_q: 8.176727, mean_eps: 0.100000\n",
      "  83696/175000: episode: 2352, duration: 0.711s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.657 [5.000, 223.000], mean observation: 0.204 [0.000, 70.000], loss: 0.240497, mean_absolute_error: 0.528378, mean_q: 7.929050, mean_eps: 0.100000\n",
      "  83737/175000: episode: 2353, duration: 0.862s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 158.098 [28.000, 223.000], mean observation: 0.196 [0.000, 82.000], loss: 0.929256, mean_absolute_error: 0.554451, mean_q: 7.869608, mean_eps: 0.100000\n",
      "  83774/175000: episode: 2354, duration: 0.775s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 186.432 [27.000, 223.000], mean observation: 0.200 [0.000, 74.000], loss: 0.342663, mean_absolute_error: 0.581748, mean_q: 8.210847, mean_eps: 0.100000\n",
      "  83805/175000: episode: 2355, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 217.290 [46.000, 223.000], mean observation: 0.107 [0.000, 62.000], loss: 0.219888, mean_absolute_error: 0.508743, mean_q: 7.594464, mean_eps: 0.100000\n",
      "  83850/175000: episode: 2356, duration: 1.009s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 180.311 [68.000, 223.000], mean observation: 0.295 [0.000, 90.000], loss: 0.276066, mean_absolute_error: 0.643240, mean_q: 8.742993, mean_eps: 0.100000\n",
      "  83890/175000: episode: 2357, duration: 0.792s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 158.025 [24.000, 223.000], mean observation: 0.449 [0.000, 80.000], loss: 24.025770, mean_absolute_error: 0.760533, mean_q: 8.916865, mean_eps: 0.100000\n",
      "  83931/175000: episode: 2358, duration: 0.789s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 155.049 [8.000, 218.000], mean observation: 0.413 [0.000, 82.000], loss: 0.459577, mean_absolute_error: 0.534029, mean_q: 7.968855, mean_eps: 0.100000\n",
      "  83969/175000: episode: 2359, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.158 [68.000, 218.000], mean observation: 0.190 [0.000, 76.000], loss: 0.168151, mean_absolute_error: 0.500608, mean_q: 7.595205, mean_eps: 0.100000\n",
      "  84018/175000: episode: 2360, duration: 1.035s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 173.490 [27.000, 218.000], mean observation: 0.326 [0.000, 98.000], loss: 0.223335, mean_absolute_error: 0.515616, mean_q: 7.590151, mean_eps: 0.100000\n",
      "  84063/175000: episode: 2361, duration: 0.891s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 199.978 [84.000, 223.000], mean observation: 0.313 [0.000, 90.000], loss: 86.470508, mean_absolute_error: 0.985437, mean_q: 8.281868, mean_eps: 0.100000\n",
      "  84117/175000: episode: 2362, duration: 1.118s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 170.741 [3.000, 223.000], mean observation: 0.444 [0.000, 108.000], loss: 0.426277, mean_absolute_error: 0.494625, mean_q: 7.427762, mean_eps: 0.100000\n",
      "  84154/175000: episode: 2363, duration: 0.706s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 167.135 [44.000, 223.000], mean observation: 0.324 [0.000, 74.000], loss: 0.337741, mean_absolute_error: 0.496702, mean_q: 7.420149, mean_eps: 0.100000\n",
      "  84202/175000: episode: 2364, duration: 0.955s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 161.000 [1.000, 220.000], mean observation: 0.313 [0.000, 96.000], loss: 11.916905, mean_absolute_error: 0.807459, mean_q: 9.693801, mean_eps: 0.100000\n",
      "  84249/175000: episode: 2365, duration: 0.877s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 156.340 [38.000, 218.000], mean observation: 0.326 [0.000, 94.000], loss: 186.101196, mean_absolute_error: 1.659124, mean_q: 10.126737, mean_eps: 0.100000\n",
      "  84267/175000: episode: 2366, duration: 0.294s, episode steps: 18, steps per second: 61, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 210.556 [84.000, 218.000], mean observation: 0.050 [0.000, 36.000], loss: 0.388232, mean_absolute_error: 0.542229, mean_q: 8.059820, mean_eps: 0.100000\n",
      "  84301/175000: episode: 2367, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 197.559 [1.000, 223.000], mean observation: 0.165 [0.000, 68.000], loss: 270.351977, mean_absolute_error: 2.034372, mean_q: 10.132759, mean_eps: 0.100000\n",
      "  84328/175000: episode: 2368, duration: 0.616s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 194.074 [28.000, 218.000], mean observation: 0.155 [0.000, 54.000], loss: 0.168713, mean_absolute_error: 0.490623, mean_q: 7.434614, mean_eps: 0.100000\n",
      "  84369/175000: episode: 2369, duration: 0.798s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.951 [1.000, 220.000], mean observation: 0.184 [0.000, 82.000], loss: 83.896624, mean_absolute_error: 1.274865, mean_q: 10.901841, mean_eps: 0.100000\n",
      "  84412/175000: episode: 2370, duration: 0.831s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.256 [27.000, 218.000], mean observation: 0.243 [0.000, 86.000], loss: 1.244318, mean_absolute_error: 0.617157, mean_q: 8.617051, mean_eps: 0.100000\n",
      "  84443/175000: episode: 2371, duration: 0.638s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 187.581 [21.000, 223.000], mean observation: 0.213 [0.000, 62.000], loss: 25.786994, mean_absolute_error: 0.814673, mean_q: 9.413703, mean_eps: 0.100000\n",
      "  84459/175000: episode: 2372, duration: 0.334s, episode steps: 16, steps per second: 48, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 203.250 [83.000, 218.000], mean observation: 0.045 [0.000, 32.000], loss: 0.173598, mean_absolute_error: 0.527142, mean_q: 7.953900, mean_eps: 0.100000\n",
      "  84506/175000: episode: 2373, duration: 0.924s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.979 [0.000, 218.000], mean observation: 0.320 [0.000, 94.000], loss: 0.658444, mean_absolute_error: 0.488220, mean_q: 7.613235, mean_eps: 0.100000\n",
      "  84549/175000: episode: 2374, duration: 0.849s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 208.837 [75.000, 218.000], mean observation: 0.140 [0.000, 86.000], loss: 30.296594, mean_absolute_error: 0.877173, mean_q: 9.723721, mean_eps: 0.100000\n",
      "  84588/175000: episode: 2375, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 185.128 [101.000, 223.000], mean observation: 0.180 [0.000, 78.000], loss: 4.857646, mean_absolute_error: 0.482784, mean_q: 7.426526, mean_eps: 0.100000\n",
      "  84637/175000: episode: 2376, duration: 0.973s, episode steps: 49, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.449 [32.000, 218.000], mean observation: 0.289 [0.000, 98.000], loss: 3.738774, mean_absolute_error: 0.691816, mean_q: 9.086033, mean_eps: 0.100000\n",
      "  84672/175000: episode: 2377, duration: 0.673s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.343 [5.000, 223.000], mean observation: 0.394 [0.000, 70.000], loss: 0.426208, mean_absolute_error: 0.499956, mean_q: 7.714679, mean_eps: 0.100000\n",
      "  84702/175000: episode: 2378, duration: 0.593s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 167.967 [64.000, 223.000], mean observation: 0.189 [0.000, 60.000], loss: 0.161432, mean_absolute_error: 0.534716, mean_q: 7.904541, mean_eps: 0.100000\n",
      "  84735/175000: episode: 2379, duration: 0.589s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 170.909 [20.000, 224.000], mean observation: 0.200 [0.000, 66.000], loss: 69.041360, mean_absolute_error: 0.957280, mean_q: 9.103711, mean_eps: 0.100000\n",
      "  84782/175000: episode: 2380, duration: 1.010s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.277 [15.000, 223.000], mean observation: 0.469 [0.000, 94.000], loss: 135.799779, mean_absolute_error: 1.192082, mean_q: 8.475022, mean_eps: 0.100000\n",
      "  84824/175000: episode: 2381, duration: 0.828s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.262 [15.000, 223.000], mean observation: 0.292 [0.000, 84.000], loss: 0.488996, mean_absolute_error: 0.491069, mean_q: 7.744233, mean_eps: 0.100000\n",
      "  84861/175000: episode: 2382, duration: 0.724s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.189 [15.000, 223.000], mean observation: 0.163 [0.000, 74.000], loss: 0.309252, mean_absolute_error: 0.486607, mean_q: 7.577953, mean_eps: 0.100000\n",
      "  84914/175000: episode: 2383, duration: 1.014s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 160.830 [15.000, 223.000], mean observation: 0.387 [0.000, 106.000], loss: 71.322881, mean_absolute_error: 0.919406, mean_q: 8.721355, mean_eps: 0.100000\n",
      "  84959/175000: episode: 2384, duration: 0.828s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.889 [12.000, 223.000], mean observation: 0.374 [0.000, 90.000], loss: 0.257376, mean_absolute_error: 0.460419, mean_q: 7.574424, mean_eps: 0.100000\n",
      "  84999/175000: episode: 2385, duration: 0.737s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 122.475 [3.000, 223.000], mean observation: 0.441 [0.000, 80.000], loss: 0.503868, mean_absolute_error: 0.615087, mean_q: 8.818742, mean_eps: 0.100000\n",
      "  85047/175000: episode: 2386, duration: 0.882s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.979 [15.000, 223.000], mean observation: 0.406 [0.000, 96.000], loss: 0.398621, mean_absolute_error: 0.488316, mean_q: 7.689701, mean_eps: 0.100000\n",
      "  85087/175000: episode: 2387, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 118.225 [15.000, 207.000], mean observation: 0.268 [0.000, 80.000], loss: 0.266324, mean_absolute_error: 0.483692, mean_q: 7.608816, mean_eps: 0.100000\n",
      "  85130/175000: episode: 2388, duration: 0.860s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 121.581 [3.000, 207.000], mean observation: 0.301 [0.000, 86.000], loss: 148.351097, mean_absolute_error: 1.385244, mean_q: 9.896146, mean_eps: 0.100000\n",
      "  85175/175000: episode: 2389, duration: 0.881s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.111 [15.000, 207.000], mean observation: 0.247 [0.000, 90.000], loss: 0.300842, mean_absolute_error: 0.601552, mean_q: 8.775654, mean_eps: 0.100000\n",
      "  85203/175000: episode: 2390, duration: 0.598s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 134.071 [103.000, 207.000], mean observation: 0.144 [0.000, 56.000], loss: 221.067263, mean_absolute_error: 1.643610, mean_q: 9.339982, mean_eps: 0.100000\n",
      "  85227/175000: episode: 2391, duration: 0.431s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 155.625 [103.000, 207.000], mean observation: 0.101 [0.000, 48.000], loss: 0.311048, mean_absolute_error: 0.474615, mean_q: 7.604729, mean_eps: 0.100000\n",
      "  85265/175000: episode: 2392, duration: 0.734s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.579 [22.000, 207.000], mean observation: 0.225 [0.000, 76.000], loss: 0.422915, mean_absolute_error: 0.469359, mean_q: 7.741198, mean_eps: 0.100000\n",
      "  85314/175000: episode: 2393, duration: 0.847s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 85.020 [17.000, 207.000], mean observation: 0.564 [0.000, 98.000], loss: 0.778063, mean_absolute_error: 0.473179, mean_q: 7.765555, mean_eps: 0.100000\n",
      "  85334/175000: episode: 2394, duration: 0.357s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 90.700 [40.000, 145.000], mean observation: 0.150 [0.000, 40.000], loss: 293.412511, mean_absolute_error: 2.052716, mean_q: 9.860659, mean_eps: 0.100000\n",
      "  85385/175000: episode: 2395, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.392 [40.000, 192.000], mean observation: 0.176 [0.000, 102.000], loss: 0.363388, mean_absolute_error: 0.468193, mean_q: 7.637370, mean_eps: 0.100000\n",
      "  85423/175000: episode: 2396, duration: 0.700s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.263 [40.000, 202.000], mean observation: 0.108 [0.000, 76.000], loss: 0.692890, mean_absolute_error: 0.449498, mean_q: 7.349309, mean_eps: 0.100000\n",
      "  85460/175000: episode: 2397, duration: 0.686s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 121.973 [22.000, 156.000], mean observation: 0.419 [0.000, 74.000], loss: 0.277199, mean_absolute_error: 0.489896, mean_q: 7.823641, mean_eps: 0.100000\n",
      "  85484/175000: episode: 2398, duration: 0.517s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 121.833 [40.000, 177.000], mean observation: 0.221 [0.000, 48.000], loss: 0.394105, mean_absolute_error: 0.473568, mean_q: 7.372667, mean_eps: 0.100000\n",
      "  85505/175000: episode: 2399, duration: 0.469s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.333 [68.000, 177.000], mean observation: 0.121 [0.000, 42.000], loss: 1.007458, mean_absolute_error: 0.486273, mean_q: 7.193049, mean_eps: 0.100000\n",
      "  85556/175000: episode: 2400, duration: 1.000s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.000 [20.000, 223.000], mean observation: 0.370 [0.000, 102.000], loss: 2.183699, mean_absolute_error: 0.479711, mean_q: 7.350345, mean_eps: 0.100000\n",
      "  85588/175000: episode: 2401, duration: 0.695s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 48.969 [9.000, 224.000], mean observation: 0.147 [0.000, 64.000], loss: 2.469721, mean_absolute_error: 0.501028, mean_q: 7.393282, mean_eps: 0.100000\n",
      "  85603/175000: episode: 2402, duration: 0.291s, episode steps: 15, steps per second: 52, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 49.267 [43.000, 137.000], mean observation: 0.037 [0.000, 30.000], loss: 1.703716, mean_absolute_error: 0.470723, mean_q: 7.366169, mean_eps: 0.100000\n",
      "  85637/175000: episode: 2403, duration: 0.615s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.000 [36.000, 223.000], mean observation: 0.274 [0.000, 68.000], loss: 0.844587, mean_absolute_error: 0.469012, mean_q: 7.326246, mean_eps: 0.100000\n",
      "  85689/175000: episode: 2404, duration: 0.942s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 139.635 [2.000, 223.000], mean observation: 0.597 [0.000, 104.000], loss: 56.499636, mean_absolute_error: 0.864046, mean_q: 8.502220, mean_eps: 0.100000\n",
      "  85718/175000: episode: 2405, duration: 0.561s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.345 [5.000, 223.000], mean observation: 0.197 [0.000, 58.000], loss: 1.071922, mean_absolute_error: 0.477396, mean_q: 7.278421, mean_eps: 0.100000\n",
      "  85735/175000: episode: 2406, duration: 0.318s, episode steps: 17, steps per second: 53, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 88.000 [88.000, 88.000], mean observation: 0.041 [0.000, 34.000], loss: 191.099921, mean_absolute_error: 1.682241, mean_q: 10.099900, mean_eps: 0.100000\n",
      "  85751/175000: episode: 2407, duration: 0.310s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 110.875 [88.000, 149.000], mean observation: 0.044 [0.000, 32.000], loss: 0.510797, mean_absolute_error: 0.471428, mean_q: 7.223167, mean_eps: 0.100000\n",
      "  85788/175000: episode: 2408, duration: 0.784s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 158.784 [17.000, 223.000], mean observation: 0.164 [0.000, 74.000], loss: 0.261373, mean_absolute_error: 0.483488, mean_q: 7.343756, mean_eps: 0.100000\n",
      "  85823/175000: episode: 2409, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.000 [5.000, 223.000], mean observation: 0.313 [0.000, 70.000], loss: 0.287084, mean_absolute_error: 0.492282, mean_q: 7.538275, mean_eps: 0.100000\n",
      "  85843/175000: episode: 2410, duration: 0.428s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 108.000 [5.000, 192.000], mean observation: 0.105 [0.000, 40.000], loss: 0.170405, mean_absolute_error: 0.464787, mean_q: 7.195507, mean_eps: 0.100000\n",
      "  85874/175000: episode: 2411, duration: 0.662s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 133.871 [5.000, 191.000], mean observation: 0.133 [0.000, 62.000], loss: 1.638072, mean_absolute_error: 0.472543, mean_q: 7.155114, mean_eps: 0.100000\n",
      "  85918/175000: episode: 2412, duration: 0.874s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.636 [5.000, 217.000], mean observation: 0.232 [0.000, 88.000], loss: 1.608796, mean_absolute_error: 0.468678, mean_q: 7.078382, mean_eps: 0.100000\n",
      "  85960/175000: episode: 2413, duration: 0.924s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 70.690 [18.000, 195.000], mean observation: 0.540 [0.000, 84.000], loss: 90.704927, mean_absolute_error: 1.143265, mean_q: 9.640649, mean_eps: 0.100000\n",
      "  85984/175000: episode: 2414, duration: 0.511s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 116.333 [15.000, 191.000], mean observation: 0.090 [0.000, 48.000], loss: 1.338267, mean_absolute_error: 0.458372, mean_q: 7.240319, mean_eps: 0.100000\n",
      "  86021/175000: episode: 2415, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.649 [4.000, 223.000], mean observation: 0.280 [0.000, 74.000], loss: 3.160050, mean_absolute_error: 0.477376, mean_q: 7.721332, mean_eps: 0.100000\n",
      "  86061/175000: episode: 2416, duration: 0.748s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 203.375 [68.000, 223.000], mean observation: 0.101 [0.000, 80.000], loss: 106.286729, mean_absolute_error: 1.072012, mean_q: 8.818841, mean_eps: 0.100000\n",
      "  86091/175000: episode: 2417, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 59.733 [15.000, 184.000], mean observation: 0.141 [0.000, 60.000], loss: 0.401683, mean_absolute_error: 0.594890, mean_q: 8.564712, mean_eps: 0.100000\n",
      "  86142/175000: episode: 2418, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 51.686 [5.000, 223.000], mean observation: 0.258 [0.000, 102.000], loss: 0.174512, mean_absolute_error: 0.455412, mean_q: 7.574765, mean_eps: 0.100000\n",
      "  86151/175000: episode: 2419, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 37.444 [15.000, 217.000], mean observation: 0.037 [0.000, 18.000], loss: 0.924348, mean_absolute_error: 0.434718, mean_q: 7.300991, mean_eps: 0.100000\n",
      "  86182/175000: episode: 2420, duration: 0.632s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 46.742 [15.000, 202.000], mean observation: 0.129 [0.000, 62.000], loss: 0.362198, mean_absolute_error: 0.526134, mean_q: 8.181639, mean_eps: 0.100000\n",
      "  86220/175000: episode: 2421, duration: 0.737s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.342 [5.000, 222.000], mean observation: 0.311 [0.000, 76.000], loss: 71.028234, mean_absolute_error: 0.919723, mean_q: 8.836143, mean_eps: 0.100000\n",
      "  86256/175000: episode: 2422, duration: 0.711s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 79.444 [15.000, 202.000], mean observation: 0.241 [0.000, 72.000], loss: 0.959331, mean_absolute_error: 0.442230, mean_q: 7.485656, mean_eps: 0.100000\n",
      "  86296/175000: episode: 2423, duration: 0.767s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 120.000 [15.000, 218.000], mean observation: 0.332 [0.000, 80.000], loss: 1.143529, mean_absolute_error: 0.452455, mean_q: 7.473840, mean_eps: 0.100000\n",
      "  86331/175000: episode: 2424, duration: 0.646s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 127.143 [28.000, 218.000], mean observation: 0.179 [0.000, 70.000], loss: 1.521738, mean_absolute_error: 0.448189, mean_q: 7.345949, mean_eps: 0.100000\n",
      "  86370/175000: episode: 2425, duration: 0.773s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.026 [15.000, 222.000], mean observation: 0.395 [0.000, 78.000], loss: 3.430302, mean_absolute_error: 0.487808, mean_q: 7.700622, mean_eps: 0.100000\n",
      "  86406/175000: episode: 2426, duration: 0.656s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.611 [15.000, 222.000], mean observation: 0.245 [0.000, 72.000], loss: 0.934616, mean_absolute_error: 0.476229, mean_q: 7.402344, mean_eps: 0.100000\n",
      "  86432/175000: episode: 2427, duration: 0.525s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 56.500 [15.000, 223.000], mean observation: 0.109 [0.000, 52.000], loss: 58.430319, mean_absolute_error: 0.976192, mean_q: 9.419541, mean_eps: 0.100000\n",
      "  86454/175000: episode: 2428, duration: 0.425s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 183.455 [15.000, 223.000], mean observation: 0.091 [0.000, 44.000], loss: 107.316300, mean_absolute_error: 1.203134, mean_q: 9.451817, mean_eps: 0.100000\n",
      "  86482/175000: episode: 2429, duration: 0.525s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.250 [15.000, 223.000], mean observation: 0.130 [0.000, 56.000], loss: 0.711521, mean_absolute_error: 0.509542, mean_q: 7.731705, mean_eps: 0.100000\n",
      "  86518/175000: episode: 2430, duration: 0.642s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.028 [71.000, 223.000], mean observation: 0.407 [0.000, 72.000], loss: 7.684815, mean_absolute_error: 0.665193, mean_q: 8.812607, mean_eps: 0.100000\n",
      "  86561/175000: episode: 2431, duration: 0.816s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.279 [15.000, 202.000], mean observation: 0.400 [0.000, 86.000], loss: 0.879149, mean_absolute_error: 0.735741, mean_q: 9.817051, mean_eps: 0.100000\n",
      "  86600/175000: episode: 2432, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 77.462 [15.000, 191.000], mean observation: 0.263 [0.000, 78.000], loss: 47.118861, mean_absolute_error: 0.819167, mean_q: 8.746654, mean_eps: 0.100000\n",
      "  86627/175000: episode: 2433, duration: 0.521s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.667 [15.000, 194.000], mean observation: 0.250 [0.000, 54.000], loss: 0.567962, mean_absolute_error: 0.456736, mean_q: 7.571638, mean_eps: 0.100000\n",
      "  86655/175000: episode: 2434, duration: 0.511s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.000 [15.000, 191.000], mean observation: 0.266 [0.000, 56.000], loss: 191.755778, mean_absolute_error: 1.712254, mean_q: 10.845588, mean_eps: 0.100000\n",
      "  86681/175000: episode: 2435, duration: 0.499s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 59.462 [28.000, 213.000], mean observation: 0.112 [0.000, 52.000], loss: 0.676653, mean_absolute_error: 0.436485, mean_q: 7.459019, mean_eps: 0.100000\n",
      "  86732/175000: episode: 2436, duration: 1.003s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 36.118 [15.000, 218.000], mean observation: 0.136 [0.000, 102.000], loss: 30.358702, mean_absolute_error: 0.698266, mean_q: 8.696458, mean_eps: 0.100000\n",
      "  86785/175000: episode: 2437, duration: 1.025s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 80.792 [7.000, 165.000], mean observation: 0.309 [0.000, 106.000], loss: 1.412508, mean_absolute_error: 0.466263, mean_q: 8.083090, mean_eps: 0.100000\n",
      "  86833/175000: episode: 2438, duration: 0.877s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 163.146 [28.000, 223.000], mean observation: 0.360 [0.000, 96.000], loss: 0.256018, mean_absolute_error: 0.461373, mean_q: 8.144729, mean_eps: 0.100000\n",
      "  86874/175000: episode: 2439, duration: 0.726s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.780 [28.000, 223.000], mean observation: 0.335 [0.000, 82.000], loss: 91.730851, mean_absolute_error: 1.014890, mean_q: 9.348973, mean_eps: 0.100000\n",
      "  86917/175000: episode: 2440, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 159.302 [28.000, 223.000], mean observation: 0.145 [0.000, 86.000], loss: 31.875166, mean_absolute_error: 0.743064, mean_q: 9.200145, mean_eps: 0.100000\n",
      "  86954/175000: episode: 2441, duration: 0.751s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 123.135 [57.000, 223.000], mean observation: 0.234 [0.000, 74.000], loss: 0.243292, mean_absolute_error: 0.467146, mean_q: 8.226702, mean_eps: 0.100000\n",
      "  87008/175000: episode: 2442, duration: 1.034s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 121.333 [7.000, 223.000], mean observation: 0.332 [0.000, 108.000], loss: 107.705480, mean_absolute_error: 1.061421, mean_q: 9.384270, mean_eps: 0.100000\n",
      "  87042/175000: episode: 2443, duration: 0.653s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 51.471 [11.000, 195.000], mean observation: 0.240 [0.000, 68.000], loss: 63.963748, mean_absolute_error: 0.734944, mean_q: 7.991534, mean_eps: 0.100000\n",
      "  87076/175000: episode: 2444, duration: 0.671s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 46.912 [28.000, 201.000], mean observation: 0.138 [0.000, 68.000], loss: 55.659641, mean_absolute_error: 0.887467, mean_q: 9.356426, mean_eps: 0.100000\n",
      "  87112/175000: episode: 2445, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 45.250 [6.000, 217.000], mean observation: 0.149 [0.000, 72.000], loss: 61.248158, mean_absolute_error: 1.075916, mean_q: 10.711115, mean_eps: 0.100000\n",
      "  87145/175000: episode: 2446, duration: 0.721s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 55.970 [10.000, 217.000], mean observation: 0.275 [0.000, 66.000], loss: 1.392606, mean_absolute_error: 0.722529, mean_q: 10.214268, mean_eps: 0.100000\n",
      "  87180/175000: episode: 2447, duration: 0.717s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 34.000 [28.000, 126.000], mean observation: 0.121 [0.000, 70.000], loss: 0.337358, mean_absolute_error: 0.485109, mean_q: 8.418480, mean_eps: 0.100000\n",
      "  87204/175000: episode: 2448, duration: 0.517s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.057 [0.000, 48.000], loss: 0.301298, mean_absolute_error: 0.498044, mean_q: 8.665406, mean_eps: 0.100000\n",
      "  87227/175000: episode: 2449, duration: 0.488s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.217 [6.000, 214.000], mean observation: 0.096 [0.000, 46.000], loss: 210.466108, mean_absolute_error: 1.413286, mean_q: 8.416291, mean_eps: 0.100000\n",
      "  87273/175000: episode: 2450, duration: 0.913s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.130 [27.000, 214.000], mean observation: 0.489 [0.000, 92.000], loss: 0.352379, mean_absolute_error: 0.596461, mean_q: 9.209276, mean_eps: 0.100000\n",
      "  87306/175000: episode: 2451, duration: 0.663s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.515 [28.000, 191.000], mean observation: 0.229 [0.000, 66.000], loss: 0.290795, mean_absolute_error: 0.470726, mean_q: 7.801361, mean_eps: 0.100000\n",
      "  87349/175000: episode: 2452, duration: 0.834s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 93.465 [5.000, 191.000], mean observation: 0.274 [0.000, 86.000], loss: 6.681667, mean_absolute_error: 0.674203, mean_q: 9.189968, mean_eps: 0.100000\n",
      "  87370/175000: episode: 2453, duration: 0.402s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 36.143 [28.000, 199.000], mean observation: 0.060 [0.000, 42.000], loss: 0.181966, mean_absolute_error: 0.526135, mean_q: 8.305267, mean_eps: 0.100000\n",
      "  87388/175000: episode: 2454, duration: 0.389s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 39.944 [28.000, 221.000], mean observation: 0.066 [0.000, 36.000], loss: 0.438113, mean_absolute_error: 0.519587, mean_q: 8.156442, mean_eps: 0.100000\n",
      "  87448/175000: episode: 2455, duration: 1.267s, episode steps: 60, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 134.517 [5.000, 215.000], mean observation: 0.502 [0.000, 120.000], loss: 52.255486, mean_absolute_error: 0.910677, mean_q: 9.529406, mean_eps: 0.100000\n",
      "  87473/175000: episode: 2456, duration: 0.564s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 29.800 [19.000, 82.000], mean observation: 0.074 [0.000, 50.000], loss: 182.327471, mean_absolute_error: 1.487294, mean_q: 9.450946, mean_eps: 0.100000\n",
      "  87485/175000: episode: 2457, duration: 0.246s, episode steps: 12, steps per second: 49, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 26.000 [4.000, 28.000], mean observation: 0.035 [0.000, 24.000], loss: 0.107792, mean_absolute_error: 0.542578, mean_q: 8.769643, mean_eps: 0.100000\n",
      "  87519/175000: episode: 2458, duration: 0.689s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 29.824 [21.000, 97.000], mean observation: 0.083 [0.000, 68.000], loss: 0.374673, mean_absolute_error: 0.656632, mean_q: 9.407515, mean_eps: 0.100000\n",
      "  87553/175000: episode: 2459, duration: 0.691s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.382 [7.000, 204.000], mean observation: 0.166 [0.000, 68.000], loss: 335.300790, mean_absolute_error: 2.295189, mean_q: 10.650076, mean_eps: 0.100000\n",
      "  87587/175000: episode: 2460, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.265 [28.000, 218.000], mean observation: 0.121 [0.000, 68.000], loss: 0.230373, mean_absolute_error: 0.463567, mean_q: 7.856064, mean_eps: 0.100000\n",
      "  87622/175000: episode: 2461, duration: 0.728s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.629 [28.000, 218.000], mean observation: 0.093 [0.000, 70.000], loss: 140.716263, mean_absolute_error: 1.430814, mean_q: 10.709095, mean_eps: 0.100000\n",
      "  87653/175000: episode: 2462, duration: 0.591s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 105.710 [7.000, 218.000], mean observation: 0.209 [0.000, 62.000], loss: 0.546300, mean_absolute_error: 0.474800, mean_q: 8.069906, mean_eps: 0.100000\n",
      "  87685/175000: episode: 2463, duration: 0.668s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 28.844 [28.000, 55.000], mean observation: 0.122 [0.000, 64.000], loss: 0.187287, mean_absolute_error: 0.485710, mean_q: 8.057287, mean_eps: 0.100000\n",
      "  87716/175000: episode: 2464, duration: 0.656s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 115.290 [14.000, 218.000], mean observation: 0.171 [0.000, 62.000], loss: 170.020862, mean_absolute_error: 1.856954, mean_q: 13.173429, mean_eps: 0.100000\n",
      "  87731/175000: episode: 2465, duration: 0.302s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 93.067 [28.000, 191.000], mean observation: 0.072 [0.000, 30.000], loss: 0.181074, mean_absolute_error: 0.783600, mean_q: 10.495355, mean_eps: 0.100000\n",
      "  87752/175000: episode: 2466, duration: 0.457s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 93.524 [5.000, 215.000], mean observation: 0.149 [0.000, 42.000], loss: 0.114107, mean_absolute_error: 0.454149, mean_q: 7.650917, mean_eps: 0.100000\n",
      "  87790/175000: episode: 2467, duration: 0.805s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 70.842 [26.000, 218.000], mean observation: 0.284 [0.000, 76.000], loss: 5.101196, mean_absolute_error: 0.660815, mean_q: 9.254169, mean_eps: 0.100000\n",
      "  87826/175000: episode: 2468, duration: 0.729s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 51.333 [28.000, 218.000], mean observation: 0.102 [0.000, 72.000], loss: 0.202190, mean_absolute_error: 0.517713, mean_q: 8.059381, mean_eps: 0.100000\n",
      "  87856/175000: episode: 2469, duration: 0.615s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 91.967 [5.000, 218.000], mean observation: 0.202 [0.000, 60.000], loss: 0.372613, mean_absolute_error: 0.487457, mean_q: 7.810173, mean_eps: 0.100000\n",
      "  87906/175000: episode: 2470, duration: 0.990s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.840 [27.000, 217.000], mean observation: 0.425 [0.000, 100.000], loss: 170.757646, mean_absolute_error: 1.703331, mean_q: 11.705861, mean_eps: 0.100000\n",
      "  87946/175000: episode: 2471, duration: 0.882s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 87.300 [0.000, 221.000], mean observation: 0.295 [0.000, 80.000], loss: 0.127544, mean_absolute_error: 0.493402, mean_q: 7.715311, mean_eps: 0.100000\n",
      "  87985/175000: episode: 2472, duration: 0.753s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.667 [28.000, 218.000], mean observation: 0.245 [0.000, 78.000], loss: 1.314322, mean_absolute_error: 0.634030, mean_q: 8.953956, mean_eps: 0.100000\n",
      "  88032/175000: episode: 2473, duration: 0.955s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.489 [28.000, 184.000], mean observation: 0.245 [0.000, 94.000], loss: 134.685053, mean_absolute_error: 1.597686, mean_q: 12.104947, mean_eps: 0.100000\n",
      "  88066/175000: episode: 2474, duration: 0.705s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 33.088 [28.000, 201.000], mean observation: 0.111 [0.000, 68.000], loss: 1.377372, mean_absolute_error: 0.685115, mean_q: 9.565354, mean_eps: 0.100000\n",
      "  88110/175000: episode: 2475, duration: 0.886s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 97.091 [28.000, 191.000], mean observation: 0.183 [0.000, 88.000], loss: 81.414192, mean_absolute_error: 0.962248, mean_q: 8.830287, mean_eps: 0.100000\n",
      "  88150/175000: episode: 2476, duration: 0.850s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 121.750 [39.000, 192.000], mean observation: 0.305 [0.000, 80.000], loss: 0.383325, mean_absolute_error: 0.461920, mean_q: 7.833321, mean_eps: 0.100000\n",
      "  88199/175000: episode: 2477, duration: 0.935s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 89.571 [3.000, 204.000], mean observation: 0.478 [0.000, 98.000], loss: 0.475822, mean_absolute_error: 0.510838, mean_q: 8.559826, mean_eps: 0.100000\n",
      "  88238/175000: episode: 2478, duration: 0.838s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.231 [27.000, 211.000], mean observation: 0.427 [0.000, 78.000], loss: 123.190334, mean_absolute_error: 1.219605, mean_q: 10.077643, mean_eps: 0.100000\n",
      "  88276/175000: episode: 2479, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.105 [9.000, 224.000], mean observation: 0.397 [0.000, 76.000], loss: 0.207319, mean_absolute_error: 0.489523, mean_q: 8.220252, mean_eps: 0.100000\n",
      "  88330/175000: episode: 2480, duration: 1.098s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 88.222 [6.000, 177.000], mean observation: 0.603 [0.000, 108.000], loss: 2.950122, mean_absolute_error: 0.486906, mean_q: 8.105371, mean_eps: 0.100000\n",
      "  88369/175000: episode: 2481, duration: 0.822s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.154 [2.000, 191.000], mean observation: 0.255 [0.000, 78.000], loss: 0.348317, mean_absolute_error: 0.517231, mean_q: 8.544375, mean_eps: 0.100000\n",
      "  88423/175000: episode: 2482, duration: 1.061s, episode steps: 54, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.370 [7.000, 221.000], mean observation: 0.684 [0.000, 108.000], loss: 100.681623, mean_absolute_error: 1.118051, mean_q: 9.762084, mean_eps: 0.100000\n",
      "  88461/175000: episode: 2483, duration: 0.785s, episode steps: 38, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 126.132 [9.000, 184.000], mean observation: 0.105 [0.000, 76.000], loss: 0.356365, mean_absolute_error: 0.466876, mean_q: 7.817116, mean_eps: 0.100000\n",
      "  88488/175000: episode: 2484, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 163.222 [15.000, 184.000], mean observation: 0.079 [0.000, 54.000], loss: 105.352841, mean_absolute_error: 1.378496, mean_q: 11.709401, mean_eps: 0.100000\n",
      "  88527/175000: episode: 2485, duration: 0.757s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.385 [5.000, 191.000], mean observation: 0.333 [0.000, 78.000], loss: 0.285723, mean_absolute_error: 0.441191, mean_q: 7.730347, mean_eps: 0.100000\n",
      "  88565/175000: episode: 2486, duration: 0.815s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.658 [5.000, 202.000], mean observation: 0.275 [0.000, 76.000], loss: 98.533644, mean_absolute_error: 1.143638, mean_q: 10.171649, mean_eps: 0.100000\n",
      "  88618/175000: episode: 2487, duration: 1.135s, episode steps: 53, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 80.547 [5.000, 191.000], mean observation: 0.669 [0.000, 106.000], loss: 3.764429, mean_absolute_error: 0.480014, mean_q: 7.978821, mean_eps: 0.100000\n",
      "  88649/175000: episode: 2488, duration: 0.684s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 25.065 [15.000, 221.000], mean observation: 0.114 [0.000, 62.000], loss: 118.993635, mean_absolute_error: 1.360903, mean_q: 11.021153, mean_eps: 0.100000\n",
      "  88676/175000: episode: 2489, duration: 0.618s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 25.074 [15.000, 139.000], mean observation: 0.065 [0.000, 54.000], loss: 204.974618, mean_absolute_error: 1.862041, mean_q: 12.028647, mean_eps: 0.100000\n",
      "  88719/175000: episode: 2490, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 23.512 [15.000, 136.000], mean observation: 0.182 [0.000, 86.000], loss: 149.481693, mean_absolute_error: 1.247917, mean_q: 8.860095, mean_eps: 0.100000\n",
      "  88763/175000: episode: 2491, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.045 [15.000, 220.000], mean observation: 0.604 [0.000, 88.000], loss: 0.321108, mean_absolute_error: 0.437211, mean_q: 7.494477, mean_eps: 0.100000\n",
      "  88799/175000: episode: 2492, duration: 0.665s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 42.278 [27.000, 218.000], mean observation: 0.128 [0.000, 72.000], loss: 61.720729, mean_absolute_error: 0.901873, mean_q: 9.182836, mean_eps: 0.100000\n",
      "  88821/175000: episode: 2493, duration: 0.510s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 27.409 [15.000, 28.000], mean observation: 0.082 [0.000, 44.000], loss: 30.219776, mean_absolute_error: 0.833341, mean_q: 9.687392, mean_eps: 0.100000\n",
      "  88864/175000: episode: 2494, duration: 0.855s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 54.093 [27.000, 144.000], mean observation: 0.172 [0.000, 86.000], loss: 19.271317, mean_absolute_error: 0.705968, mean_q: 9.028720, mean_eps: 0.100000\n",
      "  88907/175000: episode: 2495, duration: 0.907s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 52.163 [28.000, 217.000], mean observation: 0.251 [0.000, 86.000], loss: 73.408814, mean_absolute_error: 0.908958, mean_q: 8.832241, mean_eps: 0.100000\n",
      "  88944/175000: episode: 2496, duration: 0.880s, episode steps: 37, steps per second: 42, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 41.054 [12.000, 187.000], mean observation: 0.139 [0.000, 74.000], loss: 173.764122, mean_absolute_error: 1.571386, mean_q: 10.732727, mean_eps: 0.100000\n",
      "  88986/175000: episode: 2497, duration: 0.906s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 49.333 [28.000, 191.000], mean observation: 0.309 [0.000, 84.000], loss: 106.776105, mean_absolute_error: 1.096987, mean_q: 9.178472, mean_eps: 0.100000\n",
      "  89036/175000: episode: 2498, duration: 1.046s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 39.680 [28.000, 199.000], mean observation: 0.187 [0.000, 100.000], loss: 141.631939, mean_absolute_error: 1.345455, mean_q: 9.998538, mean_eps: 0.100000\n",
      "  89082/175000: episode: 2499, duration: 0.977s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.196 [28.000, 223.000], mean observation: 0.394 [0.000, 92.000], loss: 22.289813, mean_absolute_error: 0.689195, mean_q: 8.875698, mean_eps: 0.100000\n",
      "  89108/175000: episode: 2500, duration: 0.559s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 29.692 [10.000, 90.000], mean observation: 0.079 [0.000, 52.000], loss: 91.476906, mean_absolute_error: 1.107698, mean_q: 10.012434, mean_eps: 0.100000\n",
      "  89132/175000: episode: 2501, duration: 0.522s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 27.042 [5.000, 28.000], mean observation: 0.079 [0.000, 48.000], loss: 0.123159, mean_absolute_error: 0.479782, mean_q: 7.979429, mean_eps: 0.100000\n",
      "  89173/175000: episode: 2502, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 38.171 [1.000, 205.000], mean observation: 0.151 [0.000, 82.000], loss: 130.261043, mean_absolute_error: 1.196901, mean_q: 9.099287, mean_eps: 0.100000\n",
      "  89200/175000: episode: 2503, duration: 0.595s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 27.444 [4.000, 37.000], mean observation: 0.088 [0.000, 54.000], loss: 0.153827, mean_absolute_error: 0.481446, mean_q: 8.143456, mean_eps: 0.100000\n",
      "  89243/175000: episode: 2504, duration: 0.870s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 56.744 [20.000, 207.000], mean observation: 0.197 [0.000, 86.000], loss: 0.150690, mean_absolute_error: 0.477538, mean_q: 7.979189, mean_eps: 0.100000\n",
      "  89278/175000: episode: 2505, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 66.714 [6.000, 211.000], mean observation: 0.236 [0.000, 70.000], loss: 0.178568, mean_absolute_error: 0.489373, mean_q: 8.080861, mean_eps: 0.100000\n",
      "  89314/175000: episode: 2506, duration: 0.771s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.056 [27.000, 157.000], mean observation: 0.132 [0.000, 72.000], loss: 1.289481, mean_absolute_error: 0.652418, mean_q: 9.409492, mean_eps: 0.100000\n",
      "  89348/175000: episode: 2507, duration: 0.704s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 32.294 [28.000, 105.000], mean observation: 0.082 [0.000, 68.000], loss: 164.497487, mean_absolute_error: 1.385516, mean_q: 9.378280, mean_eps: 0.100000\n",
      "  89392/175000: episode: 2508, duration: 0.936s, episode steps: 44, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 40.364 [10.000, 224.000], mean observation: 0.178 [0.000, 88.000], loss: 118.697136, mean_absolute_error: 1.103524, mean_q: 8.825823, mean_eps: 0.100000\n",
      "  89415/175000: episode: 2509, duration: 0.516s, episode steps: 23, steps per second: 45, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.055 [0.000, 46.000], loss: 165.677947, mean_absolute_error: 1.467498, mean_q: 10.162690, mean_eps: 0.100000\n",
      "  89432/175000: episode: 2510, duration: 0.394s, episode steps: 17, steps per second: 43, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 26.647 [5.000, 28.000], mean observation: 0.044 [0.000, 34.000], loss: 318.478566, mean_absolute_error: 2.275914, mean_q: 11.188061, mean_eps: 0.100000\n",
      "  89472/175000: episode: 2511, duration: 0.848s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 55.125 [3.000, 214.000], mean observation: 0.245 [0.000, 80.000], loss: 0.202605, mean_absolute_error: 0.477499, mean_q: 7.984460, mean_eps: 0.100000\n",
      "  89502/175000: episode: 2512, duration: 0.590s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 33.600 [28.000, 196.000], mean observation: 0.109 [0.000, 60.000], loss: 68.262215, mean_absolute_error: 0.958536, mean_q: 9.462478, mean_eps: 0.100000\n",
      "  89525/175000: episode: 2513, duration: 0.501s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 48.652 [28.000, 173.000], mean observation: 0.101 [0.000, 46.000], loss: 86.194959, mean_absolute_error: 1.092569, mean_q: 10.185052, mean_eps: 0.100000\n",
      "  89567/175000: episode: 2514, duration: 0.861s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 61.071 [28.000, 201.000], mean observation: 0.206 [0.000, 84.000], loss: 0.293213, mean_absolute_error: 0.491627, mean_q: 8.292763, mean_eps: 0.100000\n",
      "  89602/175000: episode: 2515, duration: 0.763s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 50.714 [16.000, 159.000], mean observation: 0.197 [0.000, 70.000], loss: 1.323099, mean_absolute_error: 0.628705, mean_q: 9.252047, mean_eps: 0.100000\n",
      "  89646/175000: episode: 2516, duration: 0.972s, episode steps: 44, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.750 [1.000, 191.000], mean observation: 0.344 [0.000, 88.000], loss: 35.975392, mean_absolute_error: 0.740514, mean_q: 8.906117, mean_eps: 0.100000\n",
      "  89688/175000: episode: 2517, duration: 0.913s, episode steps: 42, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 32.167 [11.000, 190.000], mean observation: 0.191 [0.000, 84.000], loss: 154.681045, mean_absolute_error: 1.270988, mean_q: 8.981814, mean_eps: 0.100000\n",
      "  89717/175000: episode: 2518, duration: 0.761s, episode steps: 29, steps per second: 38, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 32.655 [28.000, 163.000], mean observation: 0.071 [0.000, 58.000], loss: 0.089293, mean_absolute_error: 0.424132, mean_q: 7.649418, mean_eps: 0.100000\n",
      "  89759/175000: episode: 2519, duration: 0.897s, episode steps: 42, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 58.619 [28.000, 224.000], mean observation: 0.249 [0.000, 84.000], loss: 109.748577, mean_absolute_error: 1.219790, mean_q: 10.222801, mean_eps: 0.100000\n",
      "  89778/175000: episode: 2520, duration: 0.380s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 30.053 [2.000, 90.000], mean observation: 0.066 [0.000, 38.000], loss: 0.215381, mean_absolute_error: 0.461781, mean_q: 8.077061, mean_eps: 0.100000\n",
      "  89813/175000: episode: 2521, duration: 0.722s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 58.686 [2.000, 162.000], mean observation: 0.148 [0.000, 70.000], loss: 43.858338, mean_absolute_error: 0.842482, mean_q: 9.818489, mean_eps: 0.100000\n",
      "  89849/175000: episode: 2522, duration: 0.730s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 46.167 [20.000, 188.000], mean observation: 0.147 [0.000, 72.000], loss: 167.295672, mean_absolute_error: 1.382909, mean_q: 9.675838, mean_eps: 0.100000\n",
      "  89886/175000: episode: 2523, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 48.000 [0.000, 169.000], mean observation: 0.318 [0.000, 74.000], loss: 43.555726, mean_absolute_error: 0.795541, mean_q: 9.439155, mean_eps: 0.100000\n",
      "  89904/175000: episode: 2524, duration: 0.344s, episode steps: 18, steps per second: 52, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 49.278 [28.000, 216.000], mean observation: 0.093 [0.000, 36.000], loss: 0.156962, mean_absolute_error: 0.452929, mean_q: 8.237008, mean_eps: 0.100000\n",
      "  89943/175000: episode: 2525, duration: 0.824s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.949 [28.000, 206.000], mean observation: 0.320 [0.000, 78.000], loss: 0.187642, mean_absolute_error: 0.444638, mean_q: 8.127959, mean_eps: 0.100000\n",
      "  89958/175000: episode: 2526, duration: 0.316s, episode steps: 15, steps per second: 47, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 37.133 [28.000, 121.000], mean observation: 0.075 [0.000, 30.000], loss: 0.123442, mean_absolute_error: 0.459617, mean_q: 8.260682, mean_eps: 0.100000\n",
      "  89994/175000: episode: 2527, duration: 0.744s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.417 [28.000, 213.000], mean observation: 0.251 [0.000, 72.000], loss: 11.235247, mean_absolute_error: 0.506012, mean_q: 8.325526, mean_eps: 0.100000\n",
      "  90039/175000: episode: 2528, duration: 1.004s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 74.222 [14.000, 218.000], mean observation: 0.266 [0.000, 90.000], loss: 0.328830, mean_absolute_error: 0.447939, mean_q: 8.245423, mean_eps: 0.100000\n",
      "  90091/175000: episode: 2529, duration: 1.125s, episode steps: 52, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 31.885 [5.000, 135.000], mean observation: 0.254 [0.000, 104.000], loss: 0.749147, mean_absolute_error: 0.410726, mean_q: 7.765131, mean_eps: 0.100000\n",
      "  90125/175000: episode: 2530, duration: 0.738s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.618 [0.000, 194.000], mean observation: 0.130 [0.000, 68.000], loss: 0.305219, mean_absolute_error: 0.426771, mean_q: 7.892429, mean_eps: 0.100000\n",
      "  90171/175000: episode: 2531, duration: 0.986s, episode steps: 46, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 44.413 [28.000, 187.000], mean observation: 0.335 [0.000, 92.000], loss: 0.489302, mean_absolute_error: 0.429559, mean_q: 7.870919, mean_eps: 0.100000\n",
      "  90199/175000: episode: 2532, duration: 0.579s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 90.107 [1.000, 202.000], mean observation: 0.226 [0.000, 56.000], loss: 0.349463, mean_absolute_error: 0.442523, mean_q: 8.014250, mean_eps: 0.100000\n",
      "  90231/175000: episode: 2533, duration: 0.709s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 27.812 [22.000, 28.000], mean observation: 0.083 [0.000, 64.000], loss: 0.181964, mean_absolute_error: 0.475160, mean_q: 8.503743, mean_eps: 0.100000\n",
      "  90254/175000: episode: 2534, duration: 0.520s, episode steps: 23, steps per second: 44, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 33.913 [28.000, 164.000], mean observation: 0.061 [0.000, 46.000], loss: 30.760811, mean_absolute_error: 0.835777, mean_q: 10.196575, mean_eps: 0.100000\n",
      "  90280/175000: episode: 2535, duration: 0.624s, episode steps: 26, steps per second: 42, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 33.231 [28.000, 93.000], mean observation: 0.077 [0.000, 52.000], loss: 0.967019, mean_absolute_error: 0.462669, mean_q: 8.280913, mean_eps: 0.100000\n",
      "  90327/175000: episode: 2536, duration: 0.945s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 30.872 [28.000, 136.000], mean observation: 0.137 [0.000, 94.000], loss: 11.313858, mean_absolute_error: 0.630717, mean_q: 9.192530, mean_eps: 0.100000\n",
      "  90343/175000: episode: 2537, duration: 0.361s, episode steps: 16, steps per second: 44, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 80.250 [28.000, 208.000], mean observation: 0.058 [0.000, 32.000], loss: 0.588186, mean_absolute_error: 0.450371, mean_q: 7.943981, mean_eps: 0.100000\n",
      "  90363/175000: episode: 2538, duration: 0.429s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 60.000 [9.000, 170.000], mean observation: 0.096 [0.000, 40.000], loss: 6.115019, mean_absolute_error: 0.778873, mean_q: 10.603823, mean_eps: 0.100000\n",
      "  90392/175000: episode: 2539, duration: 0.633s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 63.448 [5.000, 200.000], mean observation: 0.149 [0.000, 58.000], loss: 0.207457, mean_absolute_error: 0.485488, mean_q: 8.138580, mean_eps: 0.100000\n",
      "  90431/175000: episode: 2540, duration: 0.760s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.590 [5.000, 223.000], mean observation: 0.258 [0.000, 78.000], loss: 0.853054, mean_absolute_error: 0.491272, mean_q: 8.044104, mean_eps: 0.100000\n",
      "  90480/175000: episode: 2541, duration: 1.087s, episode steps: 49, steps per second: 45, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.113 [0.000, 98.000], loss: 37.113200, mean_absolute_error: 0.765344, mean_q: 9.185124, mean_eps: 0.100000\n",
      "  90515/175000: episode: 2542, duration: 0.699s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 49.771 [5.000, 184.000], mean observation: 0.156 [0.000, 70.000], loss: 0.297489, mean_absolute_error: 0.479290, mean_q: 8.327115, mean_eps: 0.100000\n",
      "  90539/175000: episode: 2543, duration: 0.446s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 39.125 [28.000, 203.000], mean observation: 0.064 [0.000, 48.000], loss: 0.666204, mean_absolute_error: 0.479747, mean_q: 8.271269, mean_eps: 0.100000\n",
      "  90572/175000: episode: 2544, duration: 0.823s, episode steps: 33, steps per second: 40, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 49.667 [0.000, 191.000], mean observation: 0.177 [0.000, 66.000], loss: 104.131850, mean_absolute_error: 1.269978, mean_q: 10.951070, mean_eps: 0.100000\n",
      "  90612/175000: episode: 2545, duration: 1.078s, episode steps: 40, steps per second: 37, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 45.575 [4.000, 223.000], mean observation: 0.149 [0.000, 80.000], loss: 116.780242, mean_absolute_error: 1.096693, mean_q: 9.215930, mean_eps: 0.100000\n",
      "  90650/175000: episode: 2546, duration: 0.737s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 42.763 [17.000, 221.000], mean observation: 0.170 [0.000, 76.000], loss: 80.366693, mean_absolute_error: 0.976119, mean_q: 9.712427, mean_eps: 0.100000\n",
      "  90677/175000: episode: 2547, duration: 0.493s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 64.815 [28.000, 213.000], mean observation: 0.117 [0.000, 54.000], loss: 0.721420, mean_absolute_error: 0.493532, mean_q: 8.798186, mean_eps: 0.100000\n",
      "  90699/175000: episode: 2548, duration: 0.507s, episode steps: 22, steps per second: 43, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 67.318 [28.000, 182.000], mean observation: 0.128 [0.000, 44.000], loss: 0.198848, mean_absolute_error: 0.549228, mean_q: 9.155834, mean_eps: 0.100000\n",
      "  90740/175000: episode: 2549, duration: 0.914s, episode steps: 41, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 67.195 [20.000, 211.000], mean observation: 0.283 [0.000, 82.000], loss: 0.993531, mean_absolute_error: 0.495960, mean_q: 9.020991, mean_eps: 0.100000\n",
      "  90781/175000: episode: 2550, duration: 0.836s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 47.634 [28.000, 200.000], mean observation: 0.194 [0.000, 82.000], loss: 0.292733, mean_absolute_error: 0.483521, mean_q: 8.378194, mean_eps: 0.100000\n",
      "  90808/175000: episode: 2551, duration: 0.491s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 35.000 [28.000, 217.000], mean observation: 0.068 [0.000, 54.000], loss: 187.388275, mean_absolute_error: 1.586623, mean_q: 10.801030, mean_eps: 0.100000\n",
      "  90831/175000: episode: 2552, duration: 0.498s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 28.000 [28.000, 28.000], mean observation: 0.055 [0.000, 46.000], loss: 0.232829, mean_absolute_error: 0.531504, mean_q: 8.635347, mean_eps: 0.100000\n",
      "  90860/175000: episode: 2553, duration: 0.588s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 33.966 [28.000, 152.000], mean observation: 0.092 [0.000, 58.000], loss: 0.389186, mean_absolute_error: 0.521662, mean_q: 8.638499, mean_eps: 0.100000\n",
      "  90906/175000: episode: 2554, duration: 0.856s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 56.261 [1.000, 202.000], mean observation: 0.378 [0.000, 92.000], loss: 51.902451, mean_absolute_error: 0.853236, mean_q: 9.608098, mean_eps: 0.100000\n",
      "  90948/175000: episode: 2555, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 51.000 [15.000, 202.000], mean observation: 0.164 [0.000, 84.000], loss: 0.167460, mean_absolute_error: 0.515722, mean_q: 8.687827, mean_eps: 0.100000\n",
      "  90995/175000: episode: 2556, duration: 0.932s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.894 [28.000, 218.000], mean observation: 0.384 [0.000, 94.000], loss: 0.363861, mean_absolute_error: 0.554025, mean_q: 8.988152, mean_eps: 0.100000\n",
      "  91034/175000: episode: 2557, duration: 0.956s, episode steps: 39, steps per second: 41, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 33.333 [20.000, 216.000], mean observation: 0.150 [0.000, 78.000], loss: 0.338341, mean_absolute_error: 0.548090, mean_q: 8.952398, mean_eps: 0.100000\n",
      "  91084/175000: episode: 2558, duration: 1.308s, episode steps: 50, steps per second: 38, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 61.380 [7.000, 214.000], mean observation: 0.383 [0.000, 100.000], loss: 0.197598, mean_absolute_error: 0.524419, mean_q: 8.692111, mean_eps: 0.100000\n",
      "  91116/175000: episode: 2559, duration: 0.694s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.875 [28.000, 191.000], mean observation: 0.122 [0.000, 64.000], loss: 0.475331, mean_absolute_error: 0.510297, mean_q: 8.655839, mean_eps: 0.100000\n",
      "  91139/175000: episode: 2560, duration: 0.494s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 71.478 [18.000, 215.000], mean observation: 0.112 [0.000, 46.000], loss: 109.885882, mean_absolute_error: 1.491342, mean_q: 12.995377, mean_eps: 0.100000\n",
      "  91162/175000: episode: 2561, duration: 0.425s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 33.913 [28.000, 164.000], mean observation: 0.078 [0.000, 46.000], loss: 181.044353, mean_absolute_error: 1.636409, mean_q: 11.059922, mean_eps: 0.100000\n",
      "  91213/175000: episode: 2562, duration: 1.068s, episode steps: 51, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.196 [24.000, 210.000], mean observation: 0.482 [0.000, 102.000], loss: 51.958943, mean_absolute_error: 0.878424, mean_q: 9.706241, mean_eps: 0.100000\n",
      "  91236/175000: episode: 2563, duration: 0.513s, episode steps: 23, steps per second: 45, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.522 [28.000, 180.000], mean observation: 0.090 [0.000, 46.000], loss: 0.103833, mean_absolute_error: 0.509826, mean_q: 8.135394, mean_eps: 0.100000\n",
      "  91264/175000: episode: 2564, duration: 0.654s, episode steps: 28, steps per second: 43, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 96.250 [0.000, 138.000], mean observation: 0.108 [0.000, 56.000], loss: 50.566508, mean_absolute_error: 0.934885, mean_q: 9.972462, mean_eps: 0.100000\n",
      "  91281/175000: episode: 2565, duration: 0.438s, episode steps: 17, steps per second: 39, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 131.941 [3.000, 215.000], mean observation: 0.047 [0.000, 34.000], loss: 0.690664, mean_absolute_error: 0.568325, mean_q: 8.644353, mean_eps: 0.100000\n",
      "  91318/175000: episode: 2566, duration: 0.837s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 80.351 [25.000, 191.000], mean observation: 0.252 [0.000, 74.000], loss: 0.132755, mean_absolute_error: 0.498475, mean_q: 8.482059, mean_eps: 0.100000\n",
      "  91348/175000: episode: 2567, duration: 0.707s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 125.867 [28.000, 202.000], mean observation: 0.199 [0.000, 60.000], loss: 0.358406, mean_absolute_error: 0.486137, mean_q: 8.442306, mean_eps: 0.100000\n",
      "  91371/175000: episode: 2568, duration: 0.460s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 32.783 [25.000, 141.000], mean observation: 0.059 [0.000, 46.000], loss: 24.964533, mean_absolute_error: 0.872554, mean_q: 10.861216, mean_eps: 0.100000\n",
      "  91424/175000: episode: 2569, duration: 1.182s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 117.038 [14.000, 191.000], mean observation: 0.598 [0.000, 106.000], loss: 28.119395, mean_absolute_error: 0.758762, mean_q: 9.969551, mean_eps: 0.100000\n",
      "  91466/175000: episode: 2570, duration: 1.084s, episode steps: 42, steps per second: 39, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.167 [28.000, 214.000], mean observation: 0.413 [0.000, 84.000], loss: 0.213772, mean_absolute_error: 0.471276, mean_q: 8.387483, mean_eps: 0.100000\n",
      "  91504/175000: episode: 2571, duration: 0.816s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 90.789 [27.000, 216.000], mean observation: 0.227 [0.000, 76.000], loss: 0.373571, mean_absolute_error: 0.506600, mean_q: 8.803195, mean_eps: 0.100000\n",
      "  91526/175000: episode: 2572, duration: 0.502s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 92.864 [28.000, 222.000], mean observation: 0.105 [0.000, 44.000], loss: 0.203286, mean_absolute_error: 0.511491, mean_q: 8.666027, mean_eps: 0.100000\n",
      "  91581/175000: episode: 2573, duration: 1.073s, episode steps: 55, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 124.927 [1.000, 210.000], mean observation: 0.438 [0.000, 110.000], loss: 0.185893, mean_absolute_error: 0.500128, mean_q: 8.328458, mean_eps: 0.100000\n",
      "  91606/175000: episode: 2574, duration: 0.500s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 176.000 [41.000, 206.000], mean observation: 0.104 [0.000, 50.000], loss: 1.545211, mean_absolute_error: 0.460536, mean_q: 8.141892, mean_eps: 0.100000\n",
      "  91648/175000: episode: 2575, duration: 0.850s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.119 [15.000, 213.000], mean observation: 0.198 [0.000, 84.000], loss: 33.108100, mean_absolute_error: 0.764044, mean_q: 9.412255, mean_eps: 0.100000\n",
      "  91683/175000: episode: 2576, duration: 0.752s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.971 [28.000, 191.000], mean observation: 0.110 [0.000, 70.000], loss: 0.317897, mean_absolute_error: 0.484414, mean_q: 7.935574, mean_eps: 0.100000\n",
      "  91718/175000: episode: 2577, duration: 0.722s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 32.029 [1.000, 196.000], mean observation: 0.086 [0.000, 70.000], loss: 63.853783, mean_absolute_error: 0.941754, mean_q: 9.483491, mean_eps: 0.100000\n",
      "  91761/175000: episode: 2578, duration: 0.874s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.116 [11.000, 191.000], mean observation: 0.262 [0.000, 86.000], loss: 56.377844, mean_absolute_error: 0.849585, mean_q: 9.014160, mean_eps: 0.100000\n",
      "  91792/175000: episode: 2579, duration: 0.602s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 140.161 [28.000, 191.000], mean observation: 0.214 [0.000, 62.000], loss: 148.470901, mean_absolute_error: 1.346236, mean_q: 9.872989, mean_eps: 0.100000\n",
      "  91828/175000: episode: 2580, duration: 0.805s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 37.111 [28.000, 221.000], mean observation: 0.090 [0.000, 72.000], loss: 0.318782, mean_absolute_error: 0.498834, mean_q: 7.993105, mean_eps: 0.100000\n",
      "  91850/175000: episode: 2581, duration: 0.438s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 36.273 [24.000, 214.000], mean observation: 0.074 [0.000, 44.000], loss: 0.162072, mean_absolute_error: 0.523920, mean_q: 8.219584, mean_eps: 0.100000\n",
      "  91869/175000: episode: 2582, duration: 0.379s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 69.053 [28.000, 213.000], mean observation: 0.056 [0.000, 38.000], loss: 96.440649, mean_absolute_error: 1.258200, mean_q: 11.068695, mean_eps: 0.100000\n",
      "  91915/175000: episode: 2583, duration: 0.893s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.543 [28.000, 223.000], mean observation: 0.479 [0.000, 92.000], loss: 0.229147, mean_absolute_error: 0.503619, mean_q: 7.960469, mean_eps: 0.100000\n",
      "  91964/175000: episode: 2584, duration: 1.008s, episode steps: 49, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 73.694 [28.000, 191.000], mean observation: 0.312 [0.000, 98.000], loss: 79.255207, mean_absolute_error: 1.021071, mean_q: 9.399439, mean_eps: 0.100000\n",
      "  92009/175000: episode: 2585, duration: 0.952s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 127.667 [28.000, 218.000], mean observation: 0.335 [0.000, 90.000], loss: 18.571357, mean_absolute_error: 0.732262, mean_q: 9.241104, mean_eps: 0.100000\n",
      "  92050/175000: episode: 2586, duration: 0.770s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.122 [28.000, 179.000], mean observation: 0.246 [0.000, 82.000], loss: 0.237260, mean_absolute_error: 0.557944, mean_q: 8.323836, mean_eps: 0.100000\n",
      "  92097/175000: episode: 2587, duration: 0.997s, episode steps: 47, steps per second: 47, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 43.468 [0.000, 191.000], mean observation: 0.176 [0.000, 94.000], loss: 12.377049, mean_absolute_error: 0.702233, mean_q: 9.285543, mean_eps: 0.100000\n",
      "  92127/175000: episode: 2588, duration: 0.568s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 75.433 [26.000, 213.000], mean observation: 0.272 [0.000, 60.000], loss: 0.484811, mean_absolute_error: 0.475744, mean_q: 7.846240, mean_eps: 0.100000\n",
      "  92174/175000: episode: 2589, duration: 0.958s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.106 [3.000, 197.000], mean observation: 0.293 [0.000, 94.000], loss: 0.309456, mean_absolute_error: 0.463460, mean_q: 7.851255, mean_eps: 0.100000\n",
      "  92197/175000: episode: 2590, duration: 0.478s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 45.783 [15.000, 191.000], mean observation: 0.062 [0.000, 46.000], loss: 0.356239, mean_absolute_error: 0.478493, mean_q: 8.067826, mean_eps: 0.100000\n",
      "  92230/175000: episode: 2591, duration: 0.772s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 29.121 [4.000, 89.000], mean observation: 0.172 [0.000, 66.000], loss: 0.131082, mean_absolute_error: 0.486684, mean_q: 7.960241, mean_eps: 0.100000\n",
      "  92253/175000: episode: 2592, duration: 0.605s, episode steps: 23, steps per second: 38, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.870 [3.000, 215.000], mean observation: 0.089 [0.000, 46.000], loss: 0.220348, mean_absolute_error: 0.469314, mean_q: 7.872440, mean_eps: 0.100000\n",
      "  92302/175000: episode: 2593, duration: 0.944s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.245 [24.000, 222.000], mean observation: 0.508 [0.000, 98.000], loss: 0.247568, mean_absolute_error: 0.474916, mean_q: 7.852905, mean_eps: 0.100000\n",
      "  92339/175000: episode: 2594, duration: 0.690s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 58.541 [2.000, 191.000], mean observation: 0.231 [0.000, 74.000], loss: 0.755310, mean_absolute_error: 0.447067, mean_q: 7.609951, mean_eps: 0.100000\n",
      "  92366/175000: episode: 2595, duration: 0.608s, episode steps: 27, steps per second: 44, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 50.593 [15.000, 191.000], mean observation: 0.203 [0.000, 54.000], loss: 114.739338, mean_absolute_error: 1.223713, mean_q: 10.307020, mean_eps: 0.100000\n",
      "  92411/175000: episode: 2596, duration: 0.971s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 57.200 [8.000, 214.000], mean observation: 0.451 [0.000, 90.000], loss: 0.488740, mean_absolute_error: 0.465606, mean_q: 7.908947, mean_eps: 0.100000\n",
      "  92454/175000: episode: 2597, duration: 1.014s, episode steps: 43, steps per second: 42, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 59.116 [15.000, 146.000], mean observation: 0.210 [0.000, 86.000], loss: 0.841012, mean_absolute_error: 0.476374, mean_q: 7.992984, mean_eps: 0.100000\n",
      "  92469/175000: episode: 2598, duration: 0.304s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 32.000 [27.000, 99.000], mean observation: 0.045 [0.000, 30.000], loss: 0.242352, mean_absolute_error: 0.552831, mean_q: 8.484477, mean_eps: 0.100000\n",
      "  92510/175000: episode: 2599, duration: 0.863s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 28.659 [22.000, 80.000], mean observation: 0.132 [0.000, 82.000], loss: 108.779817, mean_absolute_error: 1.249596, mean_q: 10.634142, mean_eps: 0.100000\n",
      "  92548/175000: episode: 2600, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.658 [0.000, 213.000], mean observation: 0.110 [0.000, 76.000], loss: 49.399566, mean_absolute_error: 0.833397, mean_q: 9.290748, mean_eps: 0.100000\n",
      "  92592/175000: episode: 2601, duration: 0.911s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 42.545 [27.000, 173.000], mean observation: 0.227 [0.000, 88.000], loss: 0.311120, mean_absolute_error: 0.450952, mean_q: 7.928614, mean_eps: 0.100000\n",
      "  92615/175000: episode: 2602, duration: 0.428s, episode steps: 23, steps per second: 54, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 50.000 [27.000, 135.000], mean observation: 0.093 [0.000, 46.000], loss: 0.987925, mean_absolute_error: 0.480254, mean_q: 8.015009, mean_eps: 0.100000\n",
      "  92638/175000: episode: 2603, duration: 0.565s, episode steps: 23, steps per second: 41, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 29.348 [28.000, 59.000], mean observation: 0.083 [0.000, 46.000], loss: 0.513874, mean_absolute_error: 0.542746, mean_q: 8.373893, mean_eps: 0.100000\n",
      "  92660/175000: episode: 2604, duration: 0.586s, episode steps: 22, steps per second: 38, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 99.500 [15.000, 180.000], mean observation: 0.114 [0.000, 44.000], loss: 13.352303, mean_absolute_error: 0.823793, mean_q: 10.334949, mean_eps: 0.100000\n",
      "  92708/175000: episode: 2605, duration: 0.968s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 53.104 [28.000, 158.000], mean observation: 0.247 [0.000, 96.000], loss: 72.903461, mean_absolute_error: 0.931908, mean_q: 8.771240, mean_eps: 0.100000\n",
      "  92722/175000: episode: 2606, duration: 0.398s, episode steps: 14, steps per second: 35, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 94.000 [28.000, 158.000], mean observation: 0.077 [0.000, 28.000], loss: 0.111682, mean_absolute_error: 0.491693, mean_q: 7.509734, mean_eps: 0.100000\n",
      "  92768/175000: episode: 2607, duration: 1.119s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.870 [28.000, 158.000], mean observation: 0.267 [0.000, 92.000], loss: 0.176688, mean_absolute_error: 0.480803, mean_q: 7.519406, mean_eps: 0.100000\n",
      "  92810/175000: episode: 2608, duration: 1.099s, episode steps: 42, steps per second: 38, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 150.143 [63.000, 184.000], mean observation: 0.162 [0.000, 84.000], loss: 7.726036, mean_absolute_error: 0.622621, mean_q: 8.695966, mean_eps: 0.100000\n",
      "  92841/175000: episode: 2609, duration: 0.692s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.710 [24.000, 224.000], mean observation: 0.143 [0.000, 62.000], loss: 0.182998, mean_absolute_error: 0.455031, mean_q: 7.443510, mean_eps: 0.100000\n",
      "  92882/175000: episode: 2610, duration: 0.871s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 154.561 [4.000, 184.000], mean observation: 0.224 [0.000, 82.000], loss: 26.638462, mean_absolute_error: 0.722092, mean_q: 8.725658, mean_eps: 0.100000\n",
      "  92925/175000: episode: 2611, duration: 0.969s, episode steps: 43, steps per second: 44, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 128.674 [7.000, 210.000], mean observation: 0.262 [0.000, 86.000], loss: 61.131991, mean_absolute_error: 0.875181, mean_q: 8.799260, mean_eps: 0.100000\n",
      "  92957/175000: episode: 2612, duration: 0.697s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 153.719 [42.000, 158.000], mean observation: 0.106 [0.000, 64.000], loss: 1.464188, mean_absolute_error: 0.657490, mean_q: 9.399095, mean_eps: 0.100000\n",
      "  93000/175000: episode: 2613, duration: 0.953s, episode steps: 43, steps per second: 45, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.442 [5.000, 222.000], mean observation: 0.367 [0.000, 86.000], loss: 0.181267, mean_absolute_error: 0.452003, mean_q: 7.451328, mean_eps: 0.100000\n",
      "  93043/175000: episode: 2614, duration: 0.898s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 134.791 [7.000, 209.000], mean observation: 0.238 [0.000, 86.000], loss: 0.280961, mean_absolute_error: 0.463824, mean_q: 7.625679, mean_eps: 0.100000\n",
      "  93098/175000: episode: 2615, duration: 1.157s, episode steps: 55, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 103.345 [7.000, 185.000], mean observation: 0.396 [0.000, 110.000], loss: 0.857593, mean_absolute_error: 0.494214, mean_q: 7.955762, mean_eps: 0.100000\n",
      "  93127/175000: episode: 2616, duration: 0.671s, episode steps: 29, steps per second: 43, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 60.069 [24.000, 184.000], mean observation: 0.178 [0.000, 58.000], loss: 0.588092, mean_absolute_error: 0.423253, mean_q: 7.351550, mean_eps: 0.100000\n",
      "  93183/175000: episode: 2617, duration: 1.106s, episode steps: 56, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 87.232 [27.000, 222.000], mean observation: 0.439 [0.000, 112.000], loss: 70.678249, mean_absolute_error: 0.865230, mean_q: 8.572177, mean_eps: 0.100000\n",
      "  93211/175000: episode: 2618, duration: 0.566s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 120.786 [8.000, 184.000], mean observation: 0.145 [0.000, 56.000], loss: 0.178109, mean_absolute_error: 0.453321, mean_q: 7.641269, mean_eps: 0.100000\n",
      "  93263/175000: episode: 2619, duration: 1.184s, episode steps: 52, steps per second: 44, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.654 [29.000, 184.000], mean observation: 0.527 [0.000, 104.000], loss: 13.609320, mean_absolute_error: 0.600344, mean_q: 8.483439, mean_eps: 0.100000\n",
      "  93284/175000: episode: 2620, duration: 0.454s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 115.524 [21.000, 184.000], mean observation: 0.146 [0.000, 42.000], loss: 0.126691, mean_absolute_error: 0.424606, mean_q: 7.458640, mean_eps: 0.100000\n",
      "  93330/175000: episode: 2621, duration: 1.000s, episode steps: 46, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 85.957 [34.000, 221.000], mean observation: 0.376 [0.000, 92.000], loss: 0.160555, mean_absolute_error: 0.438259, mean_q: 7.499789, mean_eps: 0.100000\n",
      "  93361/175000: episode: 2622, duration: 0.669s, episode steps: 31, steps per second: 46, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 102.387 [44.000, 184.000], mean observation: 0.213 [0.000, 62.000], loss: 43.168153, mean_absolute_error: 0.835112, mean_q: 9.552144, mean_eps: 0.100000\n",
      "  93379/175000: episode: 2623, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 85.333 [44.000, 158.000], mean observation: 0.077 [0.000, 36.000], loss: 0.314701, mean_absolute_error: 0.429428, mean_q: 7.410194, mean_eps: 0.100000\n",
      "  93420/175000: episode: 2624, duration: 0.987s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.146 [44.000, 221.000], mean observation: 0.243 [0.000, 82.000], loss: 0.331254, mean_absolute_error: 0.422467, mean_q: 7.299936, mean_eps: 0.100000\n",
      "  93475/175000: episode: 2625, duration: 1.218s, episode steps: 55, steps per second: 45, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 128.236 [49.000, 221.000], mean observation: 0.339 [0.000, 110.000], loss: 24.939055, mean_absolute_error: 0.652533, mean_q: 8.503656, mean_eps: 0.100000\n",
      "  93516/175000: episode: 2626, duration: 0.794s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 124.366 [13.000, 215.000], mean observation: 0.271 [0.000, 82.000], loss: 104.951461, mean_absolute_error: 1.262937, mean_q: 10.879559, mean_eps: 0.100000\n",
      "  93571/175000: episode: 2627, duration: 1.188s, episode steps: 55, steps per second: 46, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 143.109 [14.000, 215.000], mean observation: 0.424 [0.000, 110.000], loss: 0.413063, mean_absolute_error: 0.448230, mean_q: 7.621946, mean_eps: 0.100000\n",
      "  93606/175000: episode: 2628, duration: 0.718s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.086 [48.000, 176.000], mean observation: 0.221 [0.000, 70.000], loss: 0.177896, mean_absolute_error: 0.459565, mean_q: 7.519699, mean_eps: 0.100000\n",
      "  93636/175000: episode: 2629, duration: 0.604s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 111.300 [13.000, 207.000], mean observation: 0.256 [0.000, 60.000], loss: 0.199268, mean_absolute_error: 0.458388, mean_q: 7.409367, mean_eps: 0.100000\n",
      "  93682/175000: episode: 2630, duration: 0.957s, episode steps: 46, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.391 [5.000, 191.000], mean observation: 0.365 [0.000, 92.000], loss: 83.172939, mean_absolute_error: 1.204850, mean_q: 11.157078, mean_eps: 0.100000\n",
      "  93728/175000: episode: 2631, duration: 1.131s, episode steps: 46, steps per second: 41, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.826 [48.000, 213.000], mean observation: 0.431 [0.000, 92.000], loss: 92.947047, mean_absolute_error: 1.004113, mean_q: 8.700242, mean_eps: 0.100000\n",
      "  93761/175000: episode: 2632, duration: 0.749s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 121.030 [8.000, 207.000], mean observation: 0.191 [0.000, 66.000], loss: 0.607103, mean_absolute_error: 0.467487, mean_q: 7.730361, mean_eps: 0.100000\n",
      "  93802/175000: episode: 2633, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.293 [42.000, 202.000], mean observation: 0.266 [0.000, 82.000], loss: 4.840403, mean_absolute_error: 0.607730, mean_q: 8.959512, mean_eps: 0.100000\n",
      "  93826/175000: episode: 2634, duration: 0.557s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 147.167 [101.000, 217.000], mean observation: 0.090 [0.000, 48.000], loss: 0.175338, mean_absolute_error: 0.430320, mean_q: 7.485181, mean_eps: 0.100000\n",
      "  93856/175000: episode: 2635, duration: 0.644s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 134.033 [101.000, 179.000], mean observation: 0.140 [0.000, 60.000], loss: 70.257506, mean_absolute_error: 0.964964, mean_q: 9.543589, mean_eps: 0.100000\n",
      "  93909/175000: episode: 2636, duration: 1.105s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 125.547 [30.000, 207.000], mean observation: 0.309 [0.000, 106.000], loss: 96.289977, mean_absolute_error: 1.082722, mean_q: 9.780517, mean_eps: 0.100000\n",
      "  93944/175000: episode: 2637, duration: 0.793s, episode steps: 35, steps per second: 44, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 147.971 [101.000, 176.000], mean observation: 0.141 [0.000, 70.000], loss: 55.368640, mean_absolute_error: 0.861213, mean_q: 9.433362, mean_eps: 0.100000\n",
      "  93971/175000: episode: 2638, duration: 0.514s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 140.333 [24.000, 178.000], mean observation: 0.210 [0.000, 54.000], loss: 0.219565, mean_absolute_error: 0.457563, mean_q: 8.280657, mean_eps: 0.100000\n",
      "  93984/175000: episode: 2639, duration: 0.287s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 148.308 [15.000, 220.000], mean observation: 0.082 [0.000, 26.000], loss: 205.989818, mean_absolute_error: 1.894198, mean_q: 12.745317, mean_eps: 0.100000\n",
      "  94015/175000: episode: 2640, duration: 0.648s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 129.742 [88.000, 221.000], mean observation: 0.166 [0.000, 62.000], loss: 0.219028, mean_absolute_error: 0.423460, mean_q: 7.759926, mean_eps: 0.100000\n",
      "  94040/175000: episode: 2641, duration: 0.590s, episode steps: 25, steps per second: 42, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 153.680 [24.000, 222.000], mean observation: 0.174 [0.000, 50.000], loss: 0.210605, mean_absolute_error: 0.434503, mean_q: 8.050773, mean_eps: 0.100000\n",
      "  94077/175000: episode: 2642, duration: 0.800s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.486 [38.000, 222.000], mean observation: 0.216 [0.000, 74.000], loss: 0.150479, mean_absolute_error: 0.413220, mean_q: 7.649956, mean_eps: 0.100000\n",
      "  94107/175000: episode: 2643, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.500 [41.000, 222.000], mean observation: 0.188 [0.000, 60.000], loss: 0.259949, mean_absolute_error: 0.440954, mean_q: 7.940844, mean_eps: 0.100000\n",
      "  94138/175000: episode: 2644, duration: 0.630s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 105.419 [58.000, 222.000], mean observation: 0.192 [0.000, 62.000], loss: 99.444502, mean_absolute_error: 1.071318, mean_q: 9.635205, mean_eps: 0.100000\n",
      "  94186/175000: episode: 2645, duration: 0.982s, episode steps: 48, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 94.854 [41.000, 222.000], mean observation: 0.367 [0.000, 96.000], loss: 0.177829, mean_absolute_error: 0.410181, mean_q: 7.397466, mean_eps: 0.100000\n",
      "  94232/175000: episode: 2646, duration: 0.928s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.935 [13.000, 158.000], mean observation: 0.363 [0.000, 92.000], loss: 0.173680, mean_absolute_error: 0.435568, mean_q: 7.705984, mean_eps: 0.100000\n",
      "  94256/175000: episode: 2647, duration: 0.588s, episode steps: 24, steps per second: 41, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 143.792 [21.000, 202.000], mean observation: 0.127 [0.000, 48.000], loss: 0.121214, mean_absolute_error: 0.418449, mean_q: 7.490832, mean_eps: 0.100000\n",
      "  94285/175000: episode: 2648, duration: 0.649s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.517 [16.000, 169.000], mean observation: 0.259 [0.000, 58.000], loss: 0.133251, mean_absolute_error: 0.439996, mean_q: 7.871873, mean_eps: 0.100000\n",
      "  94329/175000: episode: 2649, duration: 0.908s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.023 [35.000, 189.000], mean observation: 0.379 [0.000, 88.000], loss: 0.385164, mean_absolute_error: 0.484467, mean_q: 8.204824, mean_eps: 0.100000\n",
      "  94369/175000: episode: 2650, duration: 0.793s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 175.375 [68.000, 223.000], mean observation: 0.268 [0.000, 80.000], loss: 0.301629, mean_absolute_error: 0.416696, mean_q: 7.539675, mean_eps: 0.100000\n",
      "  94403/175000: episode: 2651, duration: 0.651s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 183.000 [33.000, 223.000], mean observation: 0.212 [0.000, 68.000], loss: 112.148092, mean_absolute_error: 1.132997, mean_q: 9.509701, mean_eps: 0.100000\n",
      "  94437/175000: episode: 2652, duration: 0.862s, episode steps: 34, steps per second: 39, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.676 [64.000, 222.000], mean observation: 0.213 [0.000, 68.000], loss: 0.165410, mean_absolute_error: 0.451743, mean_q: 7.816226, mean_eps: 0.100000\n",
      "  94495/175000: episode: 2653, duration: 1.180s, episode steps: 58, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 155.931 [20.000, 222.000], mean observation: 0.394 [0.000, 116.000], loss: 92.161341, mean_absolute_error: 1.086635, mean_q: 9.949508, mean_eps: 0.100000\n",
      "  94518/175000: episode: 2654, duration: 0.491s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 138.652 [1.000, 222.000], mean observation: 0.123 [0.000, 46.000], loss: 0.116155, mean_absolute_error: 0.444264, mean_q: 7.901022, mean_eps: 0.100000\n",
      "  94555/175000: episode: 2655, duration: 0.822s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 178.216 [1.000, 222.000], mean observation: 0.366 [0.000, 74.000], loss: 0.102941, mean_absolute_error: 0.431569, mean_q: 7.711719, mean_eps: 0.100000\n",
      "  94598/175000: episode: 2656, duration: 0.892s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 182.953 [1.000, 222.000], mean observation: 0.472 [0.000, 86.000], loss: 0.147134, mean_absolute_error: 0.417954, mean_q: 7.500859, mean_eps: 0.100000\n",
      "  94645/175000: episode: 2657, duration: 1.019s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 136.936 [0.000, 215.000], mean observation: 0.563 [0.000, 94.000], loss: 0.148671, mean_absolute_error: 0.414794, mean_q: 7.451059, mean_eps: 0.100000\n",
      "  94663/175000: episode: 2658, duration: 0.332s, episode steps: 18, steps per second: 54, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 85.389 [68.000, 197.000], mean observation: 0.101 [0.000, 36.000], loss: 0.149447, mean_absolute_error: 0.429837, mean_q: 7.619736, mean_eps: 0.100000\n",
      "  94703/175000: episode: 2659, duration: 0.772s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 100.725 [51.000, 215.000], mean observation: 0.153 [0.000, 80.000], loss: 89.658526, mean_absolute_error: 0.968591, mean_q: 8.912697, mean_eps: 0.100000\n",
      "  94734/175000: episode: 2660, duration: 0.653s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.419 [64.000, 216.000], mean observation: 0.273 [0.000, 62.000], loss: 0.109458, mean_absolute_error: 0.407849, mean_q: 7.372326, mean_eps: 0.100000\n",
      "  94775/175000: episode: 2661, duration: 0.883s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.756 [4.000, 213.000], mean observation: 0.494 [0.000, 82.000], loss: 0.204195, mean_absolute_error: 0.440588, mean_q: 7.725591, mean_eps: 0.100000\n",
      "  94795/175000: episode: 2662, duration: 0.413s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 114.200 [32.000, 215.000], mean observation: 0.141 [0.000, 40.000], loss: 0.219120, mean_absolute_error: 0.417066, mean_q: 7.288403, mean_eps: 0.100000\n",
      "  94838/175000: episode: 2663, duration: 0.904s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 70.907 [12.000, 215.000], mean observation: 0.532 [0.000, 86.000], loss: 1.177757, mean_absolute_error: 0.602987, mean_q: 9.093394, mean_eps: 0.100000\n",
      "  94875/175000: episode: 2664, duration: 0.773s, episode steps: 37, steps per second: 48, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 168.811 [32.000, 223.000], mean observation: 0.238 [0.000, 74.000], loss: 43.606074, mean_absolute_error: 0.794446, mean_q: 9.425553, mean_eps: 0.100000\n",
      "  94895/175000: episode: 2665, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 168.850 [126.000, 215.000], mean observation: 0.094 [0.000, 40.000], loss: 198.018938, mean_absolute_error: 1.637680, mean_q: 10.655843, mean_eps: 0.100000\n",
      "  94941/175000: episode: 2666, duration: 0.875s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 160.783 [80.000, 215.000], mean observation: 0.448 [0.000, 92.000], loss: 0.219420, mean_absolute_error: 0.437414, mean_q: 7.853035, mean_eps: 0.100000\n",
      "  94978/175000: episode: 2667, duration: 0.638s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 163.784 [69.000, 221.000], mean observation: 0.270 [0.000, 74.000], loss: 0.609073, mean_absolute_error: 0.437203, mean_q: 7.745652, mean_eps: 0.100000\n",
      "  95009/175000: episode: 2668, duration: 0.595s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 178.774 [70.000, 221.000], mean observation: 0.216 [0.000, 62.000], loss: 83.737986, mean_absolute_error: 1.022289, mean_q: 10.015918, mean_eps: 0.100000\n",
      "  95046/175000: episode: 2669, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.243 [81.000, 221.000], mean observation: 0.318 [0.000, 74.000], loss: 0.130739, mean_absolute_error: 0.434723, mean_q: 7.935575, mean_eps: 0.100000\n",
      "  95094/175000: episode: 2670, duration: 0.950s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.396 [36.000, 223.000], mean observation: 0.577 [0.000, 96.000], loss: 12.538541, mean_absolute_error: 0.628339, mean_q: 9.173884, mean_eps: 0.100000\n",
      "  95150/175000: episode: 2671, duration: 1.029s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 146.679 [46.000, 222.000], mean observation: 0.604 [0.000, 112.000], loss: 0.671952, mean_absolute_error: 0.420170, mean_q: 7.765429, mean_eps: 0.100000\n",
      "  95179/175000: episode: 2672, duration: 0.558s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 122.172 [64.000, 208.000], mean observation: 0.137 [0.000, 58.000], loss: 63.234288, mean_absolute_error: 0.982224, mean_q: 10.692667, mean_eps: 0.100000\n",
      "  95215/175000: episode: 2673, duration: 0.791s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 130.861 [58.000, 208.000], mean observation: 0.295 [0.000, 72.000], loss: 0.221102, mean_absolute_error: 0.431742, mean_q: 8.149587, mean_eps: 0.100000\n",
      "  95254/175000: episode: 2674, duration: 0.822s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.641 [16.000, 208.000], mean observation: 0.243 [0.000, 78.000], loss: 0.421392, mean_absolute_error: 0.421757, mean_q: 8.128333, mean_eps: 0.100000\n",
      "  95304/175000: episode: 2675, duration: 1.013s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.360 [34.000, 208.000], mean observation: 0.390 [0.000, 100.000], loss: 0.246644, mean_absolute_error: 0.550317, mean_q: 9.213711, mean_eps: 0.100000\n",
      "  95344/175000: episode: 2676, duration: 0.882s, episode steps: 40, steps per second: 45, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 83.250 [44.000, 208.000], mean observation: 0.286 [0.000, 80.000], loss: 48.442046, mean_absolute_error: 0.853038, mean_q: 9.714022, mean_eps: 0.100000\n",
      "  95389/175000: episode: 2677, duration: 0.912s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 91.644 [38.000, 213.000], mean observation: 0.541 [0.000, 90.000], loss: 87.043234, mean_absolute_error: 1.133862, mean_q: 10.419504, mean_eps: 0.100000\n",
      "  95441/175000: episode: 2678, duration: 1.114s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.404 [12.000, 177.000], mean observation: 0.417 [0.000, 104.000], loss: 26.418772, mean_absolute_error: 0.703021, mean_q: 8.742313, mean_eps: 0.100000\n",
      "  95487/175000: episode: 2679, duration: 0.919s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 120.783 [13.000, 208.000], mean observation: 0.393 [0.000, 92.000], loss: 0.440909, mean_absolute_error: 0.464040, mean_q: 7.394078, mean_eps: 0.100000\n",
      "  95535/175000: episode: 2680, duration: 0.919s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.042 [4.000, 177.000], mean observation: 0.468 [0.000, 96.000], loss: 85.844832, mean_absolute_error: 0.954213, mean_q: 8.563770, mean_eps: 0.100000\n",
      "  95579/175000: episode: 2681, duration: 0.855s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.864 [12.000, 177.000], mean observation: 0.264 [0.000, 88.000], loss: 79.559955, mean_absolute_error: 1.093020, mean_q: 10.268416, mean_eps: 0.100000\n",
      "  95616/175000: episode: 2682, duration: 0.786s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 124.973 [12.000, 158.000], mean observation: 0.121 [0.000, 74.000], loss: 0.138657, mean_absolute_error: 0.431922, mean_q: 7.297833, mean_eps: 0.100000\n",
      "  95636/175000: episode: 2683, duration: 0.440s, episode steps: 20, steps per second: 45, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 86.550 [46.000, 170.000], mean observation: 0.099 [0.000, 40.000], loss: 0.175921, mean_absolute_error: 0.441593, mean_q: 7.582382, mean_eps: 0.100000\n",
      "  95679/175000: episode: 2684, duration: 0.818s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.093 [17.000, 199.000], mean observation: 0.309 [0.000, 86.000], loss: 0.202087, mean_absolute_error: 0.473118, mean_q: 7.765982, mean_eps: 0.100000\n",
      "  95700/175000: episode: 2685, duration: 0.424s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 105.714 [46.000, 190.000], mean observation: 0.117 [0.000, 42.000], loss: 0.219643, mean_absolute_error: 0.456892, mean_q: 7.444852, mean_eps: 0.100000\n",
      "  95732/175000: episode: 2686, duration: 0.707s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.500 [46.000, 158.000], mean observation: 0.159 [0.000, 64.000], loss: 175.160716, mean_absolute_error: 1.656050, mean_q: 11.320895, mean_eps: 0.100000\n",
      "  95772/175000: episode: 2687, duration: 0.846s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.775 [1.000, 218.000], mean observation: 0.304 [0.000, 80.000], loss: 0.319634, mean_absolute_error: 0.482123, mean_q: 7.584286, mean_eps: 0.100000\n",
      "  95830/175000: episode: 2688, duration: 1.229s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.000 [0.000, 201.000], mean observation: 0.635 [0.000, 116.000], loss: 89.660380, mean_absolute_error: 1.084128, mean_q: 9.579416, mean_eps: 0.100000\n",
      "  95880/175000: episode: 2689, duration: 1.052s, episode steps: 50, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 84.060 [0.000, 219.000], mean observation: 0.407 [0.000, 100.000], loss: 0.211295, mean_absolute_error: 0.452557, mean_q: 7.416929, mean_eps: 0.100000\n",
      "  95915/175000: episode: 2690, duration: 0.712s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 152.343 [24.000, 191.000], mean observation: 0.323 [0.000, 70.000], loss: 0.385304, mean_absolute_error: 0.443817, mean_q: 7.585098, mean_eps: 0.100000\n",
      "  95948/175000: episode: 2691, duration: 0.717s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 146.364 [0.000, 184.000], mean observation: 0.153 [0.000, 66.000], loss: 0.210580, mean_absolute_error: 0.442918, mean_q: 7.459562, mean_eps: 0.100000\n",
      "  95990/175000: episode: 2692, duration: 0.873s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.857 [15.000, 204.000], mean observation: 0.363 [0.000, 84.000], loss: 0.766490, mean_absolute_error: 0.448790, mean_q: 7.534603, mean_eps: 0.100000\n",
      "  96021/175000: episode: 2693, duration: 0.628s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 68.129 [16.000, 184.000], mean observation: 0.265 [0.000, 62.000], loss: 60.214545, mean_absolute_error: 0.926897, mean_q: 9.206897, mean_eps: 0.100000\n",
      "  96043/175000: episode: 2694, duration: 0.379s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 105.000 [14.000, 184.000], mean observation: 0.097 [0.000, 44.000], loss: 0.118817, mean_absolute_error: 0.415220, mean_q: 7.063527, mean_eps: 0.100000\n",
      "  96057/175000: episode: 2695, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 128.429 [16.000, 184.000], mean observation: 0.062 [0.000, 28.000], loss: 0.182980, mean_absolute_error: 0.461077, mean_q: 7.387852, mean_eps: 0.100000\n",
      "  96080/175000: episode: 2696, duration: 0.482s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 106.609 [1.000, 185.000], mean observation: 0.114 [0.000, 46.000], loss: 0.522889, mean_absolute_error: 0.473056, mean_q: 7.538762, mean_eps: 0.100000\n",
      "  96133/175000: episode: 2697, duration: 1.097s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.358 [0.000, 184.000], mean observation: 0.507 [0.000, 106.000], loss: 0.262604, mean_absolute_error: 0.449434, mean_q: 7.153570, mean_eps: 0.100000\n",
      "  96172/175000: episode: 2698, duration: 0.855s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.590 [16.000, 199.000], mean observation: 0.289 [0.000, 78.000], loss: 0.121039, mean_absolute_error: 0.436453, mean_q: 7.073071, mean_eps: 0.100000\n",
      "  96206/175000: episode: 2699, duration: 0.724s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.176 [8.000, 177.000], mean observation: 0.206 [0.000, 68.000], loss: 90.258796, mean_absolute_error: 1.041096, mean_q: 8.998828, mean_eps: 0.100000\n",
      "  96224/175000: episode: 2700, duration: 0.460s, episode steps: 18, steps per second: 39, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 99.222 [51.000, 177.000], mean observation: 0.109 [0.000, 36.000], loss: 45.906425, mean_absolute_error: 1.092210, mean_q: 11.300214, mean_eps: 0.100000\n",
      "  96258/175000: episode: 2701, duration: 0.745s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.235 [51.000, 221.000], mean observation: 0.209 [0.000, 68.000], loss: 3.566904, mean_absolute_error: 0.618674, mean_q: 8.598240, mean_eps: 0.100000\n",
      "  96307/175000: episode: 2702, duration: 0.954s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 115.510 [33.000, 208.000], mean observation: 0.398 [0.000, 98.000], loss: 31.126335, mean_absolute_error: 0.741951, mean_q: 8.546781, mean_eps: 0.100000\n",
      "  96350/175000: episode: 2703, duration: 0.868s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 106.581 [0.000, 160.000], mean observation: 0.201 [0.000, 86.000], loss: 0.276985, mean_absolute_error: 0.475897, mean_q: 7.391874, mean_eps: 0.100000\n",
      "  96388/175000: episode: 2704, duration: 0.780s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 98.132 [0.000, 209.000], mean observation: 0.304 [0.000, 76.000], loss: 0.168369, mean_absolute_error: 0.472552, mean_q: 7.361056, mean_eps: 0.100000\n",
      "  96443/175000: episode: 2705, duration: 1.044s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 68.855 [0.000, 166.000], mean observation: 0.305 [0.000, 110.000], loss: 102.534063, mean_absolute_error: 1.166002, mean_q: 9.702524, mean_eps: 0.100000\n",
      "  96465/175000: episode: 2706, duration: 0.476s, episode steps: 22, steps per second: 46, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 107.318 [17.000, 189.000], mean observation: 0.124 [0.000, 44.000], loss: 0.106163, mean_absolute_error: 0.469524, mean_q: 7.403372, mean_eps: 0.100000\n",
      "  96496/175000: episode: 2707, duration: 0.575s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 101.097 [4.000, 209.000], mean observation: 0.272 [0.000, 62.000], loss: 0.262463, mean_absolute_error: 0.492295, mean_q: 7.523671, mean_eps: 0.100000\n",
      "  96511/175000: episode: 2708, duration: 0.280s, episode steps: 15, steps per second: 54, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 146.600 [52.000, 193.000], mean observation: 0.050 [0.000, 30.000], loss: 0.185672, mean_absolute_error: 0.441484, mean_q: 7.071141, mean_eps: 0.100000\n",
      "  96532/175000: episode: 2709, duration: 0.420s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.048 [52.000, 200.000], mean observation: 0.118 [0.000, 42.000], loss: 0.197361, mean_absolute_error: 0.452136, mean_q: 7.416527, mean_eps: 0.100000\n",
      "  96561/175000: episode: 2710, duration: 0.697s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 115.621 [5.000, 200.000], mean observation: 0.183 [0.000, 58.000], loss: 0.280811, mean_absolute_error: 0.463752, mean_q: 7.320551, mean_eps: 0.100000\n",
      "  96594/175000: episode: 2711, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.515 [25.000, 200.000], mean observation: 0.308 [0.000, 66.000], loss: 0.160143, mean_absolute_error: 0.459209, mean_q: 7.363111, mean_eps: 0.100000\n",
      "  96629/175000: episode: 2712, duration: 0.760s, episode steps: 35, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 171.343 [7.000, 215.000], mean observation: 0.117 [0.000, 70.000], loss: 0.174361, mean_absolute_error: 0.466759, mean_q: 7.228808, mean_eps: 0.100000\n",
      "  96682/175000: episode: 2713, duration: 0.920s, episode steps: 53, steps per second: 58, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 99.415 [8.000, 218.000], mean observation: 0.339 [0.000, 106.000], loss: 0.187287, mean_absolute_error: 0.451751, mean_q: 7.142952, mean_eps: 0.100000\n",
      "  96700/175000: episode: 2714, duration: 0.376s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 116.278 [0.000, 215.000], mean observation: 0.102 [0.000, 36.000], loss: 0.212100, mean_absolute_error: 0.428837, mean_q: 6.959096, mean_eps: 0.100000\n",
      "  96745/175000: episode: 2715, duration: 0.929s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 151.978 [51.000, 215.000], mean observation: 0.261 [0.000, 90.000], loss: 0.152197, mean_absolute_error: 0.430086, mean_q: 6.795277, mean_eps: 0.100000\n",
      "  96781/175000: episode: 2716, duration: 0.671s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 86.917 [0.000, 215.000], mean observation: 0.332 [0.000, 72.000], loss: 84.120888, mean_absolute_error: 1.002022, mean_q: 8.447370, mean_eps: 0.100000\n",
      "  96816/175000: episode: 2717, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 107.657 [0.000, 207.000], mean observation: 0.299 [0.000, 70.000], loss: 149.702731, mean_absolute_error: 1.312736, mean_q: 8.796304, mean_eps: 0.100000\n",
      "  96869/175000: episode: 2718, duration: 1.032s, episode steps: 53, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 157.000 [16.000, 215.000], mean observation: 0.197 [0.000, 106.000], loss: 0.242280, mean_absolute_error: 0.442301, mean_q: 6.982033, mean_eps: 0.100000\n",
      "  96913/175000: episode: 2719, duration: 0.796s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.500 [0.000, 215.000], mean observation: 0.374 [0.000, 88.000], loss: 0.257361, mean_absolute_error: 0.442260, mean_q: 6.820365, mean_eps: 0.100000\n",
      "  96950/175000: episode: 2720, duration: 0.655s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 128.162 [0.000, 215.000], mean observation: 0.316 [0.000, 74.000], loss: 65.568778, mean_absolute_error: 0.929880, mean_q: 8.542952, mean_eps: 0.100000\n",
      "  96967/175000: episode: 2721, duration: 0.313s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 104.235 [0.000, 216.000], mean observation: 0.089 [0.000, 34.000], loss: 0.264831, mean_absolute_error: 0.428714, mean_q: 6.508184, mean_eps: 0.100000\n",
      "  97006/175000: episode: 2722, duration: 0.723s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.179 [0.000, 209.000], mean observation: 0.503 [0.000, 78.000], loss: 0.212909, mean_absolute_error: 0.431635, mean_q: 6.673202, mean_eps: 0.100000\n",
      "  97046/175000: episode: 2723, duration: 0.762s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.300 [12.000, 218.000], mean observation: 0.587 [0.000, 80.000], loss: 0.154932, mean_absolute_error: 0.428189, mean_q: 6.485694, mean_eps: 0.100000\n",
      "  97095/175000: episode: 2724, duration: 0.923s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 82.796 [1.000, 209.000], mean observation: 0.500 [0.000, 98.000], loss: 15.428116, mean_absolute_error: 0.652740, mean_q: 7.922495, mean_eps: 0.100000\n",
      "  97123/175000: episode: 2725, duration: 0.515s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 136.857 [0.000, 218.000], mean observation: 0.180 [0.000, 56.000], loss: 0.155373, mean_absolute_error: 0.440431, mean_q: 6.593488, mean_eps: 0.100000\n",
      "  97150/175000: episode: 2726, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 37.852 [16.000, 169.000], mean observation: 0.086 [0.000, 54.000], loss: 0.332507, mean_absolute_error: 0.440248, mean_q: 6.602644, mean_eps: 0.100000\n",
      "  97211/175000: episode: 2727, duration: 1.070s, episode steps: 61, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 137.803 [16.000, 221.000], mean observation: 0.853 [0.000, 122.000], loss: 62.623522, mean_absolute_error: 1.092869, mean_q: 10.280415, mean_eps: 0.100000\n",
      "  97259/175000: episode: 2728, duration: 0.864s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.417 [7.000, 209.000], mean observation: 0.670 [0.000, 96.000], loss: 32.786629, mean_absolute_error: 0.709953, mean_q: 7.818552, mean_eps: 0.100000\n",
      "  97314/175000: episode: 2729, duration: 0.995s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 144.236 [6.000, 224.000], mean observation: 0.619 [0.000, 110.000], loss: 61.928668, mean_absolute_error: 0.866070, mean_q: 8.040003, mean_eps: 0.100000\n",
      "  97353/175000: episode: 2730, duration: 0.820s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.205 [68.000, 218.000], mean observation: 0.363 [0.000, 78.000], loss: 0.348806, mean_absolute_error: 0.466960, mean_q: 6.866944, mean_eps: 0.100000\n",
      "  97381/175000: episode: 2731, duration: 0.497s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 135.893 [41.000, 207.000], mean observation: 0.206 [0.000, 56.000], loss: 0.225452, mean_absolute_error: 0.471229, mean_q: 6.840784, mean_eps: 0.100000\n",
      "  97409/175000: episode: 2732, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 132.214 [68.000, 158.000], mean observation: 0.134 [0.000, 56.000], loss: 0.155347, mean_absolute_error: 0.480585, mean_q: 7.132651, mean_eps: 0.100000\n",
      "  97441/175000: episode: 2733, duration: 0.586s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 131.688 [4.000, 213.000], mean observation: 0.218 [0.000, 64.000], loss: 0.577683, mean_absolute_error: 0.464277, mean_q: 6.691432, mean_eps: 0.100000\n",
      "  97470/175000: episode: 2734, duration: 0.502s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 133.069 [5.000, 222.000], mean observation: 0.217 [0.000, 58.000], loss: 1.838380, mean_absolute_error: 0.743286, mean_q: 9.565054, mean_eps: 0.100000\n",
      "  97497/175000: episode: 2735, duration: 0.602s, episode steps: 27, steps per second: 45, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 169.593 [98.000, 218.000], mean observation: 0.160 [0.000, 54.000], loss: 0.542681, mean_absolute_error: 0.495977, mean_q: 7.499184, mean_eps: 0.100000\n",
      "  97524/175000: episode: 2736, duration: 0.504s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 38.111 [27.000, 204.000], mean observation: 0.067 [0.000, 54.000], loss: 54.304406, mean_absolute_error: 1.016885, mean_q: 10.341667, mean_eps: 0.100000\n",
      "  97569/175000: episode: 2737, duration: 0.970s, episode steps: 45, steps per second: 46, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.333 [7.000, 212.000], mean observation: 0.304 [0.000, 90.000], loss: 0.277988, mean_absolute_error: 0.485522, mean_q: 7.378133, mean_eps: 0.100000\n",
      "  97609/175000: episode: 2738, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 29.300 [27.000, 119.000], mean observation: 0.093 [0.000, 80.000], loss: 0.249097, mean_absolute_error: 0.481978, mean_q: 7.180239, mean_eps: 0.100000\n",
      "  97654/175000: episode: 2739, duration: 0.777s, episode steps: 45, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 46.978 [5.000, 208.000], mean observation: 0.295 [0.000, 90.000], loss: 0.545713, mean_absolute_error: 0.459562, mean_q: 6.734743, mean_eps: 0.100000\n",
      "  97696/175000: episode: 2740, duration: 0.768s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.429 [5.000, 200.000], mean observation: 0.453 [0.000, 84.000], loss: 0.589505, mean_absolute_error: 0.479132, mean_q: 6.978572, mean_eps: 0.100000\n",
      "  97743/175000: episode: 2741, duration: 0.881s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 69.021 [5.000, 215.000], mean observation: 0.513 [0.000, 94.000], loss: 7.168214, mean_absolute_error: 0.498188, mean_q: 6.749841, mean_eps: 0.100000\n",
      "  97795/175000: episode: 2742, duration: 0.965s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 63.481 [5.000, 221.000], mean observation: 0.591 [0.000, 104.000], loss: 0.238504, mean_absolute_error: 0.469326, mean_q: 6.805117, mean_eps: 0.100000\n",
      "  97807/175000: episode: 2743, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 140.583 [52.000, 215.000], mean observation: 0.080 [0.000, 24.000], loss: 0.340945, mean_absolute_error: 0.490197, mean_q: 6.985286, mean_eps: 0.100000\n",
      "  97857/175000: episode: 2744, duration: 0.920s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.580 [5.000, 215.000], mean observation: 0.739 [0.000, 100.000], loss: 0.192144, mean_absolute_error: 0.496079, mean_q: 7.277159, mean_eps: 0.100000\n",
      "  97891/175000: episode: 2745, duration: 0.605s, episode steps: 34, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.294 [5.000, 184.000], mean observation: 0.216 [0.000, 68.000], loss: 190.113508, mean_absolute_error: 1.943953, mean_q: 13.175636, mean_eps: 0.100000\n",
      "  97936/175000: episode: 2746, duration: 0.967s, episode steps: 45, steps per second: 47, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.978 [7.000, 222.000], mean observation: 0.297 [0.000, 90.000], loss: 25.714665, mean_absolute_error: 0.751099, mean_q: 8.872542, mean_eps: 0.100000\n",
      "  97991/175000: episode: 2747, duration: 1.018s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 109.400 [27.000, 222.000], mean observation: 0.498 [0.000, 110.000], loss: 0.564085, mean_absolute_error: 0.503028, mean_q: 7.631963, mean_eps: 0.100000\n",
      "  98019/175000: episode: 2748, duration: 0.529s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 33.571 [27.000, 211.000], mean observation: 0.104 [0.000, 56.000], loss: 0.722214, mean_absolute_error: 0.515562, mean_q: 8.078150, mean_eps: 0.100000\n",
      "  98067/175000: episode: 2749, duration: 0.957s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 149.146 [16.000, 222.000], mean observation: 0.484 [0.000, 96.000], loss: 0.325153, mean_absolute_error: 0.493672, mean_q: 7.639015, mean_eps: 0.100000\n",
      "  98108/175000: episode: 2750, duration: 0.791s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 49.902 [27.000, 218.000], mean observation: 0.124 [0.000, 82.000], loss: 92.084763, mean_absolute_error: 1.254175, mean_q: 11.120115, mean_eps: 0.100000\n",
      "  98144/175000: episode: 2751, duration: 0.823s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 55.000 [27.000, 221.000], mean observation: 0.096 [0.000, 72.000], loss: 47.123081, mean_absolute_error: 0.858449, mean_q: 9.300199, mean_eps: 0.100000\n",
      "  98170/175000: episode: 2752, duration: 0.487s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 30.077 [27.000, 107.000], mean observation: 0.063 [0.000, 52.000], loss: 0.243889, mean_absolute_error: 0.491724, mean_q: 7.683935, mean_eps: 0.100000\n",
      "  98194/175000: episode: 2753, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 111.875 [27.000, 191.000], mean observation: 0.122 [0.000, 48.000], loss: 0.134129, mean_absolute_error: 0.480053, mean_q: 7.586378, mean_eps: 0.100000\n",
      "  98230/175000: episode: 2754, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 140.333 [81.000, 191.000], mean observation: 0.268 [0.000, 72.000], loss: 0.332161, mean_absolute_error: 0.486940, mean_q: 8.072876, mean_eps: 0.100000\n",
      "  98252/175000: episode: 2755, duration: 0.430s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 150.136 [81.000, 191.000], mean observation: 0.121 [0.000, 44.000], loss: 0.300163, mean_absolute_error: 0.496346, mean_q: 7.883311, mean_eps: 0.100000\n",
      "  98298/175000: episode: 2756, duration: 0.836s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 130.978 [3.000, 210.000], mean observation: 0.446 [0.000, 92.000], loss: 1.405910, mean_absolute_error: 0.619877, mean_q: 9.149370, mean_eps: 0.100000\n",
      "  98344/175000: episode: 2757, duration: 0.865s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.957 [62.000, 210.000], mean observation: 0.388 [0.000, 92.000], loss: 27.700159, mean_absolute_error: 0.717141, mean_q: 8.782033, mean_eps: 0.100000\n",
      "  98376/175000: episode: 2758, duration: 0.638s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 136.500 [6.000, 210.000], mean observation: 0.245 [0.000, 64.000], loss: 0.303019, mean_absolute_error: 0.447997, mean_q: 7.589820, mean_eps: 0.100000\n",
      "  98406/175000: episode: 2759, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 163.800 [81.000, 212.000], mean observation: 0.303 [0.000, 60.000], loss: 0.574139, mean_absolute_error: 0.438741, mean_q: 7.252894, mean_eps: 0.100000\n",
      "  98434/175000: episode: 2760, duration: 0.559s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 146.643 [76.000, 210.000], mean observation: 0.195 [0.000, 56.000], loss: 0.828101, mean_absolute_error: 0.452937, mean_q: 7.197519, mean_eps: 0.100000\n",
      "  98473/175000: episode: 2761, duration: 0.826s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 113.154 [0.000, 210.000], mean observation: 0.446 [0.000, 78.000], loss: 0.167214, mean_absolute_error: 0.445717, mean_q: 7.061623, mean_eps: 0.100000\n",
      "  98515/175000: episode: 2762, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 127.881 [0.000, 215.000], mean observation: 0.617 [0.000, 84.000], loss: 0.148594, mean_absolute_error: 0.455688, mean_q: 7.432096, mean_eps: 0.100000\n",
      "  98563/175000: episode: 2763, duration: 0.850s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 152.042 [11.000, 215.000], mean observation: 0.640 [0.000, 96.000], loss: 4.143004, mean_absolute_error: 1.038326, mean_q: 13.031509, mean_eps: 0.100000\n",
      "  98582/175000: episode: 2764, duration: 0.375s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 45.368 [27.000, 160.000], mean observation: 0.048 [0.000, 38.000], loss: 0.222588, mean_absolute_error: 0.456793, mean_q: 7.756647, mean_eps: 0.100000\n",
      "  98609/175000: episode: 2765, duration: 0.522s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 180.370 [81.000, 219.000], mean observation: 0.085 [0.000, 54.000], loss: 0.358855, mean_absolute_error: 0.480757, mean_q: 7.872363, mean_eps: 0.100000\n",
      "  98643/175000: episode: 2766, duration: 0.567s, episode steps: 34, steps per second: 60, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.324 [9.000, 221.000], mean observation: 0.167 [0.000, 68.000], loss: 0.375419, mean_absolute_error: 0.459665, mean_q: 7.332923, mean_eps: 0.100000\n",
      "  98685/175000: episode: 2767, duration: 0.769s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 54.595 [27.000, 210.000], mean observation: 0.111 [0.000, 84.000], loss: 2.317168, mean_absolute_error: 0.597983, mean_q: 8.606059, mean_eps: 0.100000\n",
      "  98716/175000: episode: 2768, duration: 0.586s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 53.452 [27.000, 210.000], mean observation: 0.087 [0.000, 62.000], loss: 0.639960, mean_absolute_error: 0.701299, mean_q: 9.718486, mean_eps: 0.100000\n",
      "  98764/175000: episode: 2769, duration: 0.919s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.021 [8.000, 214.000], mean observation: 0.479 [0.000, 96.000], loss: 0.321453, mean_absolute_error: 0.461114, mean_q: 7.271057, mean_eps: 0.100000\n",
      "  98794/175000: episode: 2770, duration: 0.567s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 157.833 [81.000, 210.000], mean observation: 0.307 [0.000, 60.000], loss: 0.551561, mean_absolute_error: 0.469386, mean_q: 6.907839, mean_eps: 0.100000\n",
      "  98843/175000: episode: 2771, duration: 0.842s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 152.531 [12.000, 212.000], mean observation: 0.691 [0.000, 98.000], loss: 0.617272, mean_absolute_error: 0.484824, mean_q: 7.436174, mean_eps: 0.100000\n",
      "  98867/175000: episode: 2772, duration: 0.434s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 119.125 [35.000, 210.000], mean observation: 0.155 [0.000, 48.000], loss: 0.193117, mean_absolute_error: 0.469157, mean_q: 7.350373, mean_eps: 0.100000\n",
      "  98884/175000: episode: 2773, duration: 0.375s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 127.647 [48.000, 210.000], mean observation: 0.141 [0.000, 34.000], loss: 0.198453, mean_absolute_error: 0.452718, mean_q: 7.018484, mean_eps: 0.100000\n",
      "  98934/175000: episode: 2774, duration: 0.899s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 162.660 [48.000, 210.000], mean observation: 0.321 [0.000, 100.000], loss: 9.697454, mean_absolute_error: 0.874041, mean_q: 10.594786, mean_eps: 0.100000\n",
      "  98974/175000: episode: 2775, duration: 0.704s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 146.150 [45.000, 210.000], mean observation: 0.269 [0.000, 80.000], loss: 0.519242, mean_absolute_error: 0.458349, mean_q: 6.935048, mean_eps: 0.100000\n",
      "  99007/175000: episode: 2776, duration: 0.577s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.727 [10.000, 139.000], mean observation: 0.138 [0.000, 66.000], loss: 0.122174, mean_absolute_error: 0.448547, mean_q: 6.802012, mean_eps: 0.100000\n",
      "  99026/175000: episode: 2777, duration: 0.390s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 104.579 [28.000, 196.000], mean observation: 0.070 [0.000, 38.000], loss: 1.657517, mean_absolute_error: 0.771724, mean_q: 10.162837, mean_eps: 0.100000\n",
      "  99074/175000: episode: 2778, duration: 0.924s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 101.292 [1.000, 210.000], mean observation: 0.381 [0.000, 96.000], loss: 0.212211, mean_absolute_error: 0.447137, mean_q: 7.017683, mean_eps: 0.100000\n",
      "  99127/175000: episode: 2779, duration: 0.933s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 158.358 [33.000, 213.000], mean observation: 0.683 [0.000, 106.000], loss: 1.083692, mean_absolute_error: 0.576062, mean_q: 8.256540, mean_eps: 0.100000\n",
      "  99160/175000: episode: 2780, duration: 0.636s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.939 [35.000, 210.000], mean observation: 0.318 [0.000, 66.000], loss: 0.434948, mean_absolute_error: 0.455120, mean_q: 7.093408, mean_eps: 0.100000\n",
      "  99192/175000: episode: 2781, duration: 0.622s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 161.250 [76.000, 210.000], mean observation: 0.223 [0.000, 64.000], loss: 0.352982, mean_absolute_error: 0.474639, mean_q: 7.124758, mean_eps: 0.100000\n",
      "  99220/175000: episode: 2782, duration: 0.619s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.643 [13.000, 210.000], mean observation: 0.230 [0.000, 56.000], loss: 0.324982, mean_absolute_error: 0.714821, mean_q: 9.148006, mean_eps: 0.100000\n",
      "  99275/175000: episode: 2783, duration: 1.033s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 131.927 [10.000, 208.000], mean observation: 0.570 [0.000, 110.000], loss: 0.218475, mean_absolute_error: 0.485654, mean_q: 7.197667, mean_eps: 0.100000\n",
      "  99306/175000: episode: 2784, duration: 0.577s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 86.387 [6.000, 209.000], mean observation: 0.303 [0.000, 62.000], loss: 0.319607, mean_absolute_error: 0.468535, mean_q: 7.011353, mean_eps: 0.100000\n",
      "  99330/175000: episode: 2785, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 54.583 [1.000, 209.000], mean observation: 0.138 [0.000, 48.000], loss: 0.937125, mean_absolute_error: 0.744365, mean_q: 9.637947, mean_eps: 0.100000\n",
      "  99386/175000: episode: 2786, duration: 1.166s, episode steps: 56, steps per second: 48, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 47.714 [3.000, 209.000], mean observation: 0.956 [0.000, 112.000], loss: 0.299903, mean_absolute_error: 0.462228, mean_q: 7.020946, mean_eps: 0.100000\n",
      "  99417/175000: episode: 2787, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 149.548 [3.000, 205.000], mean observation: 0.408 [0.000, 62.000], loss: 0.166891, mean_absolute_error: 0.456172, mean_q: 6.925194, mean_eps: 0.100000\n",
      "  99465/175000: episode: 2788, duration: 0.869s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 100.146 [10.000, 210.000], mean observation: 0.587 [0.000, 96.000], loss: 14.522673, mean_absolute_error: 0.639091, mean_q: 8.027593, mean_eps: 0.100000\n",
      "  99501/175000: episode: 2789, duration: 0.632s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.056 [4.000, 210.000], mean observation: 0.502 [0.000, 72.000], loss: 2.619223, mean_absolute_error: 0.647947, mean_q: 8.792446, mean_eps: 0.100000\n",
      "  99540/175000: episode: 2790, duration: 0.702s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.128 [14.000, 209.000], mean observation: 0.309 [0.000, 78.000], loss: 0.160003, mean_absolute_error: 0.452525, mean_q: 6.959724, mean_eps: 0.100000\n",
      "  99574/175000: episode: 2791, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 145.059 [30.000, 209.000], mean observation: 0.439 [0.000, 68.000], loss: 0.126137, mean_absolute_error: 0.446532, mean_q: 6.971879, mean_eps: 0.100000\n",
      "  99625/175000: episode: 2792, duration: 0.961s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 158.333 [43.000, 209.000], mean observation: 0.561 [0.000, 102.000], loss: 0.377687, mean_absolute_error: 0.593075, mean_q: 8.379195, mean_eps: 0.100000\n",
      "  99668/175000: episode: 2793, duration: 0.797s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.419 [12.000, 209.000], mean observation: 0.500 [0.000, 86.000], loss: 13.461345, mean_absolute_error: 1.020798, mean_q: 11.686267, mean_eps: 0.100000\n",
      "  99695/175000: episode: 2794, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 133.444 [4.000, 222.000], mean observation: 0.253 [0.000, 54.000], loss: 44.250924, mean_absolute_error: 1.133060, mean_q: 11.456789, mean_eps: 0.100000\n",
      "  99743/175000: episode: 2795, duration: 0.849s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.812 [40.000, 222.000], mean observation: 0.565 [0.000, 96.000], loss: 36.225963, mean_absolute_error: 0.898555, mean_q: 9.843169, mean_eps: 0.100000\n",
      "  99778/175000: episode: 2796, duration: 0.661s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.371 [1.000, 176.000], mean observation: 0.291 [0.000, 70.000], loss: 0.141129, mean_absolute_error: 0.462901, mean_q: 7.434818, mean_eps: 0.100000\n",
      "  99809/175000: episode: 2797, duration: 0.598s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 134.161 [34.000, 176.000], mean observation: 0.256 [0.000, 62.000], loss: 1.256940, mean_absolute_error: 0.664388, mean_q: 9.315355, mean_eps: 0.100000\n",
      "  99869/175000: episode: 2798, duration: 1.116s, episode steps: 60, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 106.950 [6.000, 176.000], mean observation: 0.750 [0.000, 120.000], loss: 30.169064, mean_absolute_error: 0.817985, mean_q: 9.542542, mean_eps: 0.100000\n",
      "  99900/175000: episode: 2799, duration: 0.565s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 78.968 [24.000, 203.000], mean observation: 0.282 [0.000, 62.000], loss: 0.109688, mean_absolute_error: 0.434947, mean_q: 7.167051, mean_eps: 0.100000\n",
      "  99938/175000: episode: 2800, duration: 0.766s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 72.526 [37.000, 177.000], mean observation: 0.277 [0.000, 76.000], loss: 4.769007, mean_absolute_error: 0.626603, mean_q: 8.992407, mean_eps: 0.100000\n",
      "  99984/175000: episode: 2801, duration: 0.834s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.022 [9.000, 209.000], mean observation: 0.501 [0.000, 92.000], loss: 2.324854, mean_absolute_error: 0.765520, mean_q: 10.115243, mean_eps: 0.100000\n",
      " 100023/175000: episode: 2802, duration: 0.765s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.000 [55.000, 55.000], mean observation: 0.090 [0.000, 78.000], loss: 0.172368, mean_absolute_error: 0.440038, mean_q: 7.219145, mean_eps: 0.100000\n",
      " 100067/175000: episode: 2803, duration: 0.789s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.318 [43.000, 221.000], mean observation: 0.297 [0.000, 88.000], loss: 0.182791, mean_absolute_error: 0.440052, mean_q: 7.544099, mean_eps: 0.100000\n",
      " 100102/175000: episode: 2804, duration: 0.637s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 112.429 [6.000, 207.000], mean observation: 0.260 [0.000, 70.000], loss: 0.243009, mean_absolute_error: 0.452381, mean_q: 7.522562, mean_eps: 0.100000\n",
      " 100143/175000: episode: 2805, duration: 0.715s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.317 [12.000, 207.000], mean observation: 0.355 [0.000, 82.000], loss: 0.198933, mean_absolute_error: 0.444874, mean_q: 7.480418, mean_eps: 0.100000\n",
      " 100179/175000: episode: 2806, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 148.694 [24.000, 213.000], mean observation: 0.265 [0.000, 72.000], loss: 0.156094, mean_absolute_error: 0.445117, mean_q: 7.484464, mean_eps: 0.100000\n",
      " 100213/175000: episode: 2807, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 153.559 [55.000, 213.000], mean observation: 0.279 [0.000, 68.000], loss: 0.201039, mean_absolute_error: 0.434640, mean_q: 7.348600, mean_eps: 0.100000\n",
      " 100239/175000: episode: 2808, duration: 0.455s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 155.731 [49.000, 213.000], mean observation: 0.239 [0.000, 52.000], loss: 0.665891, mean_absolute_error: 0.438511, mean_q: 7.360682, mean_eps: 0.100000\n",
      " 100284/175000: episode: 2809, duration: 0.884s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 175.956 [3.000, 213.000], mean observation: 0.256 [0.000, 90.000], loss: 9.713488, mean_absolute_error: 0.767258, mean_q: 9.968536, mean_eps: 0.100000\n",
      " 100332/175000: episode: 2810, duration: 0.955s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 113.646 [48.000, 176.000], mean observation: 0.354 [0.000, 96.000], loss: 22.544215, mean_absolute_error: 0.677348, mean_q: 8.647352, mean_eps: 0.100000\n",
      " 100356/175000: episode: 2811, duration: 0.499s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 113.208 [48.000, 208.000], mean observation: 0.107 [0.000, 48.000], loss: 0.856483, mean_absolute_error: 0.489053, mean_q: 7.319405, mean_eps: 0.100000\n",
      " 100399/175000: episode: 2812, duration: 0.814s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.581 [16.000, 212.000], mean observation: 0.526 [0.000, 86.000], loss: 0.462267, mean_absolute_error: 0.457240, mean_q: 6.892063, mean_eps: 0.100000\n",
      " 100413/175000: episode: 2813, duration: 0.288s, episode steps: 14, steps per second: 49, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 125.429 [16.000, 208.000], mean observation: 0.064 [0.000, 28.000], loss: 0.469645, mean_absolute_error: 0.479130, mean_q: 7.156640, mean_eps: 0.100000\n",
      " 100448/175000: episode: 2814, duration: 0.662s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.914 [9.000, 208.000], mean observation: 0.191 [0.000, 70.000], loss: 0.260135, mean_absolute_error: 0.431493, mean_q: 7.025555, mean_eps: 0.100000\n",
      " 100475/175000: episode: 2815, duration: 0.477s, episode steps: 27, steps per second: 57, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 154.815 [16.000, 208.000], mean observation: 0.134 [0.000, 54.000], loss: 0.161510, mean_absolute_error: 0.443092, mean_q: 7.383690, mean_eps: 0.100000\n",
      " 100495/175000: episode: 2816, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 153.300 [55.000, 211.000], mean observation: 0.106 [0.000, 40.000], loss: 0.629245, mean_absolute_error: 0.457143, mean_q: 7.530973, mean_eps: 0.100000\n",
      " 100522/175000: episode: 2817, duration: 0.513s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 148.778 [50.000, 196.000], mean observation: 0.166 [0.000, 54.000], loss: 12.241233, mean_absolute_error: 0.719283, mean_q: 9.466365, mean_eps: 0.100000\n",
      " 100564/175000: episode: 2818, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 97.929 [22.000, 192.000], mean observation: 0.487 [0.000, 84.000], loss: 3.725436, mean_absolute_error: 0.629243, mean_q: 9.033531, mean_eps: 0.100000\n",
      " 100582/175000: episode: 2819, duration: 0.358s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 141.722 [53.000, 192.000], mean observation: 0.078 [0.000, 36.000], loss: 0.296717, mean_absolute_error: 0.437102, mean_q: 7.787465, mean_eps: 0.100000\n",
      " 100621/175000: episode: 2820, duration: 0.747s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 148.513 [7.000, 191.000], mean observation: 0.298 [0.000, 78.000], loss: 9.054726, mean_absolute_error: 0.640270, mean_q: 9.227747, mean_eps: 0.100000\n",
      " 100665/175000: episode: 2821, duration: 0.791s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 131.591 [32.000, 203.000], mean observation: 0.485 [0.000, 88.000], loss: 0.232247, mean_absolute_error: 0.460570, mean_q: 7.565595, mean_eps: 0.100000\n",
      " 100696/175000: episode: 2822, duration: 0.550s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 143.323 [41.000, 207.000], mean observation: 0.213 [0.000, 62.000], loss: 5.841234, mean_absolute_error: 0.457441, mean_q: 7.319233, mean_eps: 0.100000\n",
      " 100707/175000: episode: 2823, duration: 0.229s, episode steps: 11, steps per second: 48, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 96.000 [96.000, 96.000], mean observation: 0.028 [0.000, 22.000], loss: 0.470756, mean_absolute_error: 0.446985, mean_q: 7.455961, mean_eps: 0.100000\n",
      " 100750/175000: episode: 2824, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.326 [67.000, 182.000], mean observation: 0.209 [0.000, 86.000], loss: 0.171224, mean_absolute_error: 0.445279, mean_q: 7.647472, mean_eps: 0.100000\n",
      " 100805/175000: episode: 2825, duration: 0.986s, episode steps: 55, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 142.164 [20.000, 193.000], mean observation: 0.522 [0.000, 110.000], loss: 12.490714, mean_absolute_error: 0.614234, mean_q: 8.751237, mean_eps: 0.100000\n",
      " 100844/175000: episode: 2826, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.564 [13.000, 158.000], mean observation: 0.279 [0.000, 78.000], loss: 54.806545, mean_absolute_error: 1.070880, mean_q: 11.665834, mean_eps: 0.100000\n",
      " 100891/175000: episode: 2827, duration: 0.864s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 90.255 [0.000, 205.000], mean observation: 0.339 [0.000, 94.000], loss: 0.208943, mean_absolute_error: 0.470919, mean_q: 8.312669, mean_eps: 0.100000\n",
      " 100937/175000: episode: 2828, duration: 0.819s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 142.109 [28.000, 215.000], mean observation: 0.312 [0.000, 92.000], loss: 2.214883, mean_absolute_error: 0.597724, mean_q: 9.445506, mean_eps: 0.100000\n",
      " 100977/175000: episode: 2829, duration: 0.716s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 110.675 [23.000, 208.000], mean observation: 0.374 [0.000, 80.000], loss: 1.479273, mean_absolute_error: 0.486127, mean_q: 8.108702, mean_eps: 0.100000\n",
      " 101000/175000: episode: 2830, duration: 0.432s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 115.000 [28.000, 208.000], mean observation: 0.125 [0.000, 46.000], loss: 0.156833, mean_absolute_error: 0.447553, mean_q: 7.612701, mean_eps: 0.100000\n",
      " 101020/175000: episode: 2831, duration: 0.408s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 136.300 [42.000, 208.000], mean observation: 0.111 [0.000, 40.000], loss: 101.658739, mean_absolute_error: 1.305761, mean_q: 11.912534, mean_eps: 0.100000\n",
      " 101051/175000: episode: 2832, duration: 0.594s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 155.774 [48.000, 219.000], mean observation: 0.261 [0.000, 62.000], loss: 0.341641, mean_absolute_error: 0.677404, mean_q: 9.887049, mean_eps: 0.100000\n",
      " 101070/175000: episode: 2833, duration: 0.394s, episode steps: 19, steps per second: 48, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 110.421 [105.000, 208.000], mean observation: 0.074 [0.000, 38.000], loss: 0.191397, mean_absolute_error: 0.472016, mean_q: 8.252823, mean_eps: 0.100000\n",
      " 101098/175000: episode: 2834, duration: 0.497s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 102.286 [39.000, 105.000], mean observation: 0.078 [0.000, 56.000], loss: 0.369775, mean_absolute_error: 0.457381, mean_q: 7.801770, mean_eps: 0.100000\n",
      " 101142/175000: episode: 2835, duration: 0.795s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.341 [12.000, 208.000], mean observation: 0.225 [0.000, 88.000], loss: 0.255793, mean_absolute_error: 0.603488, mean_q: 9.270238, mean_eps: 0.100000\n",
      " 101166/175000: episode: 2836, duration: 0.449s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 122.292 [50.000, 208.000], mean observation: 0.113 [0.000, 48.000], loss: 0.450686, mean_absolute_error: 0.460737, mean_q: 7.841183, mean_eps: 0.100000\n",
      " 101222/175000: episode: 2837, duration: 0.991s, episode steps: 56, steps per second: 57, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 102.304 [1.000, 220.000], mean observation: 0.793 [0.000, 112.000], loss: 0.685578, mean_absolute_error: 0.714734, mean_q: 10.392127, mean_eps: 0.100000\n",
      " 101272/175000: episode: 2838, duration: 0.921s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.200 [28.000, 160.000], mean observation: 0.317 [0.000, 100.000], loss: 2.313345, mean_absolute_error: 0.666171, mean_q: 9.696039, mean_eps: 0.100000\n",
      " 101311/175000: episode: 2839, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 87.846 [0.000, 210.000], mean observation: 0.283 [0.000, 78.000], loss: 2.669873, mean_absolute_error: 0.669820, mean_q: 9.673558, mean_eps: 0.100000\n",
      " 101350/175000: episode: 2840, duration: 0.739s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 121.513 [28.000, 209.000], mean observation: 0.309 [0.000, 78.000], loss: 0.561429, mean_absolute_error: 0.661264, mean_q: 9.681194, mean_eps: 0.100000\n",
      " 101371/175000: episode: 2841, duration: 0.364s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 76.952 [49.000, 153.000], mean observation: 0.088 [0.000, 42.000], loss: 0.127179, mean_absolute_error: 0.517369, mean_q: 8.306534, mean_eps: 0.100000\n",
      " 101411/175000: episode: 2842, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 68.125 [28.000, 161.000], mean observation: 0.293 [0.000, 80.000], loss: 0.184843, mean_absolute_error: 0.620227, mean_q: 9.373603, mean_eps: 0.100000\n",
      " 101444/175000: episode: 2843, duration: 0.626s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 89.788 [41.000, 215.000], mean observation: 0.153 [0.000, 66.000], loss: 2.999820, mean_absolute_error: 0.838526, mean_q: 11.240796, mean_eps: 0.100000\n",
      " 101466/175000: episode: 2844, duration: 0.429s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 165.000 [165.000, 165.000], mean observation: 0.053 [0.000, 44.000], loss: 1.459269, mean_absolute_error: 0.475212, mean_q: 7.573398, mean_eps: 0.100000\n",
      " 101492/175000: episode: 2845, duration: 0.497s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 158.269 [38.000, 165.000], mean observation: 0.112 [0.000, 52.000], loss: 2.513429, mean_absolute_error: 0.460678, mean_q: 7.388777, mean_eps: 0.100000\n",
      " 101528/175000: episode: 2846, duration: 0.709s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 154.194 [21.000, 165.000], mean observation: 0.156 [0.000, 72.000], loss: 2.117489, mean_absolute_error: 0.603674, mean_q: 8.608778, mean_eps: 0.100000\n",
      " 101558/175000: episode: 2847, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.633 [22.000, 213.000], mean observation: 0.185 [0.000, 60.000], loss: 25.950384, mean_absolute_error: 0.843861, mean_q: 9.757482, mean_eps: 0.100000\n",
      " 101583/175000: episode: 2848, duration: 0.449s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.240 [4.000, 208.000], mean observation: 0.175 [0.000, 50.000], loss: 0.285319, mean_absolute_error: 0.782011, mean_q: 10.451342, mean_eps: 0.100000\n",
      " 101612/175000: episode: 2849, duration: 0.556s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.862 [10.000, 191.000], mean observation: 0.221 [0.000, 58.000], loss: 0.251973, mean_absolute_error: 0.530597, mean_q: 8.005509, mean_eps: 0.100000\n",
      " 101669/175000: episode: 2850, duration: 1.054s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.684 [10.000, 222.000], mean observation: 0.685 [0.000, 114.000], loss: 1.336367, mean_absolute_error: 0.564730, mean_q: 8.363436, mean_eps: 0.100000\n",
      " 101714/175000: episode: 2851, duration: 0.789s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.378 [12.000, 214.000], mean observation: 0.518 [0.000, 90.000], loss: 30.752351, mean_absolute_error: 0.772430, mean_q: 8.924259, mean_eps: 0.100000\n",
      " 101757/175000: episode: 2852, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 123.186 [20.000, 214.000], mean observation: 0.454 [0.000, 86.000], loss: 1.346444, mean_absolute_error: 0.541661, mean_q: 7.595024, mean_eps: 0.100000\n",
      " 101780/175000: episode: 2853, duration: 0.433s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 118.957 [10.000, 174.000], mean observation: 0.250 [0.000, 46.000], loss: 2.084932, mean_absolute_error: 0.599528, mean_q: 7.869846, mean_eps: 0.100000\n",
      " 101819/175000: episode: 2854, duration: 0.738s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.077 [10.000, 216.000], mean observation: 0.450 [0.000, 78.000], loss: 0.717845, mean_absolute_error: 0.604312, mean_q: 7.903569, mean_eps: 0.100000\n",
      " 101860/175000: episode: 2855, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.415 [5.000, 186.000], mean observation: 0.471 [0.000, 82.000], loss: 4.175936, mean_absolute_error: 0.762095, mean_q: 9.518015, mean_eps: 0.100000\n",
      " 101922/175000: episode: 2856, duration: 1.094s, episode steps: 62, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 121.565 [11.000, 189.000], mean observation: 0.631 [0.000, 124.000], loss: 3.310966, mean_absolute_error: 0.781470, mean_q: 10.113445, mean_eps: 0.100000\n",
      " 101946/175000: episode: 2857, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 116.417 [41.000, 192.000], mean observation: 0.214 [0.000, 48.000], loss: 0.267065, mean_absolute_error: 0.544034, mean_q: 7.859580, mean_eps: 0.100000\n",
      " 101992/175000: episode: 2858, duration: 0.859s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 111.087 [59.000, 208.000], mean observation: 0.696 [0.000, 92.000], loss: 0.495252, mean_absolute_error: 0.546813, mean_q: 7.945515, mean_eps: 0.100000\n",
      " 102039/175000: episode: 2859, duration: 0.887s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.617 [24.000, 222.000], mean observation: 0.463 [0.000, 94.000], loss: 13.724522, mean_absolute_error: 0.733316, mean_q: 9.160885, mean_eps: 0.100000\n",
      " 102083/175000: episode: 2860, duration: 0.790s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.341 [17.000, 220.000], mean observation: 0.487 [0.000, 88.000], loss: 9.010930, mean_absolute_error: 0.717670, mean_q: 9.244735, mean_eps: 0.100000\n",
      " 102120/175000: episode: 2861, duration: 0.692s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 123.027 [44.000, 195.000], mean observation: 0.478 [0.000, 74.000], loss: 0.410172, mean_absolute_error: 0.509116, mean_q: 7.791613, mean_eps: 0.100000\n",
      " 102146/175000: episode: 2862, duration: 0.503s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.654 [46.000, 217.000], mean observation: 0.148 [0.000, 52.000], loss: 44.906553, mean_absolute_error: 1.141123, mean_q: 11.695591, mean_eps: 0.100000\n",
      " 102171/175000: episode: 2863, duration: 0.438s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 143.760 [89.000, 158.000], mean observation: 0.131 [0.000, 50.000], loss: 38.544283, mean_absolute_error: 0.958606, mean_q: 10.483649, mean_eps: 0.100000\n",
      " 102203/175000: episode: 2864, duration: 0.612s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 123.969 [44.000, 202.000], mean observation: 0.275 [0.000, 64.000], loss: 45.489848, mean_absolute_error: 0.977235, mean_q: 10.195415, mean_eps: 0.100000\n",
      " 102233/175000: episode: 2865, duration: 0.589s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 192.467 [89.000, 213.000], mean observation: 0.088 [0.000, 60.000], loss: 0.311652, mean_absolute_error: 0.701303, mean_q: 9.131301, mean_eps: 0.100000\n",
      " 102272/175000: episode: 2866, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 176.846 [17.000, 221.000], mean observation: 0.286 [0.000, 78.000], loss: 0.180030, mean_absolute_error: 0.471198, mean_q: 6.743860, mean_eps: 0.100000\n",
      " 102307/175000: episode: 2867, duration: 0.641s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 129.657 [64.000, 213.000], mean observation: 0.332 [0.000, 70.000], loss: 0.154427, mean_absolute_error: 0.475276, mean_q: 7.181630, mean_eps: 0.100000\n",
      " 102343/175000: episode: 2868, duration: 0.688s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.694 [37.000, 197.000], mean observation: 0.303 [0.000, 72.000], loss: 0.209078, mean_absolute_error: 0.447148, mean_q: 7.191907, mean_eps: 0.100000\n",
      " 102374/175000: episode: 2869, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 129.065 [37.000, 173.000], mean observation: 0.202 [0.000, 62.000], loss: 5.562380, mean_absolute_error: 0.677054, mean_q: 9.130232, mean_eps: 0.100000\n",
      " 102413/175000: episode: 2870, duration: 0.749s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 95.333 [1.000, 173.000], mean observation: 0.360 [0.000, 78.000], loss: 27.376190, mean_absolute_error: 0.752189, mean_q: 9.148122, mean_eps: 0.100000\n",
      " 102442/175000: episode: 2871, duration: 0.553s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.138 [48.000, 213.000], mean observation: 0.188 [0.000, 58.000], loss: 9.526202, mean_absolute_error: 0.709968, mean_q: 9.413848, mean_eps: 0.100000\n",
      " 102451/175000: episode: 2872, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 152.000 [152.000, 152.000], mean observation: 0.024 [0.000, 18.000], loss: 0.119649, mean_absolute_error: 0.449125, mean_q: 7.437580, mean_eps: 0.100000\n",
      " 102490/175000: episode: 2873, duration: 0.723s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.615 [31.000, 185.000], mean observation: 0.323 [0.000, 78.000], loss: 0.278907, mean_absolute_error: 0.519218, mean_q: 7.984105, mean_eps: 0.100000\n",
      " 102522/175000: episode: 2874, duration: 0.581s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 145.594 [23.000, 222.000], mean observation: 0.216 [0.000, 64.000], loss: 0.187016, mean_absolute_error: 0.418581, mean_q: 7.360218, mean_eps: 0.100000\n",
      " 102558/175000: episode: 2875, duration: 0.646s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.806 [9.000, 178.000], mean observation: 0.206 [0.000, 72.000], loss: 0.383353, mean_absolute_error: 0.426744, mean_q: 7.113334, mean_eps: 0.100000\n",
      " 102588/175000: episode: 2876, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.133 [9.000, 152.000], mean observation: 0.170 [0.000, 60.000], loss: 26.283891, mean_absolute_error: 0.532518, mean_q: 7.123005, mean_eps: 0.100000\n",
      " 102603/175000: episode: 2877, duration: 0.353s, episode steps: 15, steps per second: 42, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 154.800 [66.000, 223.000], mean observation: 0.107 [0.000, 30.000], loss: 0.105562, mean_absolute_error: 0.430928, mean_q: 7.172877, mean_eps: 0.100000\n",
      " 102639/175000: episode: 2878, duration: 0.679s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 134.083 [0.000, 213.000], mean observation: 0.236 [0.000, 72.000], loss: 0.777493, mean_absolute_error: 0.429147, mean_q: 7.524368, mean_eps: 0.100000\n",
      " 102681/175000: episode: 2879, duration: 0.850s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.071 [24.000, 224.000], mean observation: 0.524 [0.000, 84.000], loss: 12.642941, mean_absolute_error: 0.628353, mean_q: 8.535334, mean_eps: 0.100000\n",
      " 102725/175000: episode: 2880, duration: 0.843s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 114.659 [0.000, 208.000], mean observation: 0.528 [0.000, 88.000], loss: 0.170821, mean_absolute_error: 0.429026, mean_q: 7.010420, mean_eps: 0.100000\n",
      " 102744/175000: episode: 2881, duration: 0.394s, episode steps: 19, steps per second: 48, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 126.474 [5.000, 224.000], mean observation: 0.147 [0.000, 38.000], loss: 0.324172, mean_absolute_error: 0.430730, mean_q: 7.172357, mean_eps: 0.100000\n",
      " 102772/175000: episode: 2882, duration: 0.556s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 118.250 [7.000, 178.000], mean observation: 0.207 [0.000, 56.000], loss: 0.283510, mean_absolute_error: 0.431077, mean_q: 6.896158, mean_eps: 0.100000\n",
      " 102825/175000: episode: 2883, duration: 1.074s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 110.208 [2.000, 223.000], mean observation: 0.950 [0.000, 106.000], loss: 0.437086, mean_absolute_error: 0.453263, mean_q: 7.032318, mean_eps: 0.100000\n",
      " 102860/175000: episode: 2884, duration: 0.743s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 92.257 [7.000, 171.000], mean observation: 0.222 [0.000, 70.000], loss: 0.332612, mean_absolute_error: 0.474551, mean_q: 7.117369, mean_eps: 0.100000\n",
      " 102905/175000: episode: 2885, duration: 0.890s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.911 [39.000, 181.000], mean observation: 0.449 [0.000, 90.000], loss: 0.568498, mean_absolute_error: 0.657695, mean_q: 8.723546, mean_eps: 0.100000\n",
      " 102953/175000: episode: 2886, duration: 0.863s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.500 [2.000, 222.000], mean observation: 0.598 [0.000, 96.000], loss: 0.355129, mean_absolute_error: 0.511182, mean_q: 7.208681, mean_eps: 0.100000\n",
      " 102986/175000: episode: 2887, duration: 0.567s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.212 [15.000, 222.000], mean observation: 0.413 [0.000, 66.000], loss: 0.443917, mean_absolute_error: 0.517948, mean_q: 7.179989, mean_eps: 0.100000\n",
      " 103025/175000: episode: 2888, duration: 0.745s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.385 [9.000, 222.000], mean observation: 0.447 [0.000, 78.000], loss: 0.342047, mean_absolute_error: 0.510824, mean_q: 6.928340, mean_eps: 0.100000\n",
      " 103064/175000: episode: 2889, duration: 0.801s, episode steps: 39, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 210.923 [158.000, 215.000], mean observation: 0.120 [0.000, 78.000], loss: 0.226518, mean_absolute_error: 0.545136, mean_q: 7.753164, mean_eps: 0.100000\n",
      " 103117/175000: episode: 2890, duration: 1.006s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 172.113 [58.000, 209.000], mean observation: 0.247 [0.000, 106.000], loss: 0.298221, mean_absolute_error: 0.525512, mean_q: 7.112500, mean_eps: 0.100000\n",
      " 103165/175000: episode: 2891, duration: 0.858s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 154.458 [2.000, 209.000], mean observation: 0.265 [0.000, 96.000], loss: 0.517411, mean_absolute_error: 0.496213, mean_q: 6.565011, mean_eps: 0.100000\n",
      " 103194/175000: episode: 2892, duration: 0.498s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 184.276 [2.000, 209.000], mean observation: 0.164 [0.000, 58.000], loss: 17.395701, mean_absolute_error: 0.798731, mean_q: 8.852291, mean_eps: 0.100000\n",
      " 103227/175000: episode: 2893, duration: 0.600s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 139.394 [39.000, 209.000], mean observation: 0.225 [0.000, 66.000], loss: 0.321522, mean_absolute_error: 0.502584, mean_q: 6.496521, mean_eps: 0.100000\n",
      " 103275/175000: episode: 2894, duration: 0.879s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.396 [2.000, 209.000], mean observation: 0.219 [0.000, 96.000], loss: 11.876591, mean_absolute_error: 0.686290, mean_q: 7.671491, mean_eps: 0.100000\n",
      " 103316/175000: episode: 2895, duration: 0.806s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 129.659 [16.000, 209.000], mean observation: 0.224 [0.000, 82.000], loss: 0.613878, mean_absolute_error: 0.492813, mean_q: 6.273957, mean_eps: 0.100000\n",
      " 103350/175000: episode: 2896, duration: 0.653s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 87.882 [15.000, 212.000], mean observation: 0.239 [0.000, 68.000], loss: 0.433879, mean_absolute_error: 0.487861, mean_q: 6.143910, mean_eps: 0.100000\n",
      " 103400/175000: episode: 2897, duration: 0.920s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.080 [15.000, 218.000], mean observation: 0.592 [0.000, 100.000], loss: 0.380017, mean_absolute_error: 0.526418, mean_q: 6.837446, mean_eps: 0.100000\n",
      " 103448/175000: episode: 2898, duration: 0.920s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 92.188 [4.000, 215.000], mean observation: 0.598 [0.000, 96.000], loss: 11.753638, mean_absolute_error: 0.703224, mean_q: 8.116050, mean_eps: 0.100000\n",
      " 103489/175000: episode: 2899, duration: 0.792s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 86.683 [15.000, 212.000], mean observation: 0.557 [0.000, 82.000], loss: 4.007226, mean_absolute_error: 0.529736, mean_q: 6.842635, mean_eps: 0.100000\n",
      " 103530/175000: episode: 2900, duration: 0.737s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.902 [5.000, 222.000], mean observation: 0.441 [0.000, 82.000], loss: 21.983079, mean_absolute_error: 0.753207, mean_q: 8.443795, mean_eps: 0.100000\n",
      " 103572/175000: episode: 2901, duration: 0.777s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 55.167 [5.000, 221.000], mean observation: 0.416 [0.000, 84.000], loss: 0.218809, mean_absolute_error: 0.486990, mean_q: 6.878811, mean_eps: 0.100000\n",
      " 103609/175000: episode: 2902, duration: 0.723s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 71.324 [11.000, 221.000], mean observation: 0.498 [0.000, 74.000], loss: 24.405241, mean_absolute_error: 0.755801, mean_q: 8.438253, mean_eps: 0.100000\n",
      " 103642/175000: episode: 2903, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 55.152 [11.000, 209.000], mean observation: 0.381 [0.000, 66.000], loss: 0.193206, mean_absolute_error: 0.475253, mean_q: 7.015525, mean_eps: 0.100000\n",
      " 103684/175000: episode: 2904, duration: 0.766s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 71.071 [32.000, 209.000], mean observation: 0.540 [0.000, 84.000], loss: 0.129233, mean_absolute_error: 0.457482, mean_q: 6.481644, mean_eps: 0.100000\n",
      " 103740/175000: episode: 2905, duration: 1.108s, episode steps: 56, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 77.714 [5.000, 209.000], mean observation: 0.511 [0.000, 112.000], loss: 0.194415, mean_absolute_error: 0.458909, mean_q: 6.632134, mean_eps: 0.100000\n",
      " 103785/175000: episode: 2906, duration: 0.877s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.067 [37.000, 211.000], mean observation: 0.575 [0.000, 90.000], loss: 0.249538, mean_absolute_error: 0.459255, mean_q: 6.673685, mean_eps: 0.100000\n",
      " 103803/175000: episode: 2907, duration: 0.383s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 130.500 [12.000, 209.000], mean observation: 0.102 [0.000, 36.000], loss: 0.567820, mean_absolute_error: 0.457910, mean_q: 6.470788, mean_eps: 0.100000\n",
      " 103819/175000: episode: 2908, duration: 0.290s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 124.375 [12.000, 209.000], mean observation: 0.134 [0.000, 32.000], loss: 0.157164, mean_absolute_error: 0.470558, mean_q: 6.777156, mean_eps: 0.100000\n",
      " 103872/175000: episode: 2909, duration: 1.061s, episode steps: 53, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.547 [12.000, 211.000], mean observation: 0.641 [0.000, 106.000], loss: 0.382692, mean_absolute_error: 0.461287, mean_q: 6.746661, mean_eps: 0.100000\n",
      " 103924/175000: episode: 2910, duration: 1.067s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 112.038 [12.000, 209.000], mean observation: 0.721 [0.000, 104.000], loss: 86.922699, mean_absolute_error: 1.090086, mean_q: 9.032417, mean_eps: 0.100000\n",
      " 103956/175000: episode: 2911, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 157.625 [19.000, 209.000], mean observation: 0.102 [0.000, 64.000], loss: 22.484183, mean_absolute_error: 0.739391, mean_q: 8.386106, mean_eps: 0.100000\n",
      " 103971/175000: episode: 2912, duration: 0.319s, episode steps: 15, steps per second: 47, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 133.867 [38.000, 184.000], mean observation: 0.129 [0.000, 30.000], loss: 49.518375, mean_absolute_error: 1.085731, mean_q: 10.585015, mean_eps: 0.100000\n",
      " 104034/175000: episode: 2913, duration: 1.428s, episode steps: 63, steps per second: 44, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 116.492 [0.000, 191.000], mean observation: 0.665 [0.000, 126.000], loss: 0.209764, mean_absolute_error: 0.473648, mean_q: 6.816828, mean_eps: 0.100000\n",
      " 104066/175000: episode: 2914, duration: 0.620s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.719 [0.000, 172.000], mean observation: 0.288 [0.000, 64.000], loss: 34.504862, mean_absolute_error: 1.026692, mean_q: 10.474197, mean_eps: 0.100000\n",
      " 104088/175000: episode: 2915, duration: 0.455s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 124.682 [8.000, 172.000], mean observation: 0.188 [0.000, 44.000], loss: 0.322209, mean_absolute_error: 0.472966, mean_q: 6.529872, mean_eps: 0.100000\n",
      " 104124/175000: episode: 2916, duration: 0.662s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 135.667 [12.000, 191.000], mean observation: 0.375 [0.000, 72.000], loss: 0.665499, mean_absolute_error: 0.486049, mean_q: 6.882285, mean_eps: 0.100000\n",
      " 104153/175000: episode: 2917, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.310 [10.000, 211.000], mean observation: 0.217 [0.000, 58.000], loss: 6.220413, mean_absolute_error: 0.692413, mean_q: 8.534625, mean_eps: 0.100000\n",
      " 104197/175000: episode: 2918, duration: 0.793s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.409 [12.000, 211.000], mean observation: 0.563 [0.000, 88.000], loss: 0.138598, mean_absolute_error: 0.459767, mean_q: 6.311599, mean_eps: 0.100000\n",
      " 104225/175000: episode: 2919, duration: 0.507s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 139.500 [16.000, 211.000], mean observation: 0.316 [0.000, 56.000], loss: 0.134713, mean_absolute_error: 0.462328, mean_q: 6.514800, mean_eps: 0.100000\n",
      " 104273/175000: episode: 2920, duration: 0.864s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.000 [59.000, 211.000], mean observation: 0.579 [0.000, 96.000], loss: 22.746791, mean_absolute_error: 0.817736, mean_q: 8.996379, mean_eps: 0.100000\n",
      " 104306/175000: episode: 2921, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 92.909 [16.000, 217.000], mean observation: 0.214 [0.000, 66.000], loss: 0.652250, mean_absolute_error: 0.446266, mean_q: 6.482268, mean_eps: 0.100000\n",
      " 104351/175000: episode: 2922, duration: 0.853s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 89.333 [16.000, 202.000], mean observation: 0.385 [0.000, 90.000], loss: 0.168390, mean_absolute_error: 0.437772, mean_q: 6.444962, mean_eps: 0.100000\n",
      " 104392/175000: episode: 2923, duration: 0.843s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.122 [32.000, 211.000], mean observation: 0.387 [0.000, 82.000], loss: 0.160588, mean_absolute_error: 0.423469, mean_q: 6.694170, mean_eps: 0.100000\n",
      " 104437/175000: episode: 2924, duration: 0.929s, episode steps: 45, steps per second: 48, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 99.600 [14.000, 191.000], mean observation: 0.586 [0.000, 90.000], loss: 15.026196, mean_absolute_error: 0.619967, mean_q: 7.860030, mean_eps: 0.100000\n",
      " 104495/175000: episode: 2925, duration: 1.049s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 141.897 [48.000, 201.000], mean observation: 0.565 [0.000, 116.000], loss: 2.517892, mean_absolute_error: 0.421403, mean_q: 6.452517, mean_eps: 0.100000\n",
      " 104516/175000: episode: 2926, duration: 0.509s, episode steps: 21, steps per second: 41, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.381 [1.000, 182.000], mean observation: 0.106 [0.000, 42.000], loss: 0.148706, mean_absolute_error: 0.433364, mean_q: 6.652026, mean_eps: 0.100000\n",
      " 104569/175000: episode: 2927, duration: 1.141s, episode steps: 53, steps per second: 46, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 116.660 [22.000, 190.000], mean observation: 0.459 [0.000, 106.000], loss: 0.124979, mean_absolute_error: 0.435074, mean_q: 6.727763, mean_eps: 0.100000\n",
      " 104596/175000: episode: 2928, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 100.926 [4.000, 158.000], mean observation: 0.316 [0.000, 54.000], loss: 0.148232, mean_absolute_error: 0.423721, mean_q: 6.321519, mean_eps: 0.100000\n",
      " 104626/175000: episode: 2929, duration: 0.608s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.733 [27.000, 220.000], mean observation: 0.247 [0.000, 60.000], loss: 25.866568, mean_absolute_error: 0.758013, mean_q: 8.664650, mean_eps: 0.100000\n",
      " 104664/175000: episode: 2930, duration: 0.774s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 153.500 [3.000, 222.000], mean observation: 0.368 [0.000, 76.000], loss: 13.204240, mean_absolute_error: 0.675840, mean_q: 8.369831, mean_eps: 0.100000\n",
      " 104705/175000: episode: 2931, duration: 0.783s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.927 [48.000, 202.000], mean observation: 0.455 [0.000, 82.000], loss: 2.014952, mean_absolute_error: 0.570373, mean_q: 7.628856, mean_eps: 0.100000\n",
      " 104746/175000: episode: 2932, duration: 0.713s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.293 [59.000, 220.000], mean observation: 0.384 [0.000, 82.000], loss: 0.475417, mean_absolute_error: 0.437564, mean_q: 6.570168, mean_eps: 0.100000\n",
      " 104780/175000: episode: 2933, duration: 0.668s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.353 [58.000, 216.000], mean observation: 0.162 [0.000, 68.000], loss: 0.205053, mean_absolute_error: 0.474166, mean_q: 6.701890, mean_eps: 0.100000\n",
      " 104798/175000: episode: 2934, duration: 0.367s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 91.611 [0.000, 169.000], mean observation: 0.080 [0.000, 36.000], loss: 0.224077, mean_absolute_error: 0.459133, mean_q: 6.325145, mean_eps: 0.100000\n",
      " 104836/175000: episode: 2935, duration: 0.711s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 106.711 [50.000, 220.000], mean observation: 0.159 [0.000, 76.000], loss: 0.206219, mean_absolute_error: 0.481002, mean_q: 6.806272, mean_eps: 0.100000\n",
      " 104860/175000: episode: 2936, duration: 0.479s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.833 [44.000, 201.000], mean observation: 0.197 [0.000, 48.000], loss: 0.290246, mean_absolute_error: 0.490363, mean_q: 6.771654, mean_eps: 0.100000\n",
      " 104895/175000: episode: 2937, duration: 0.653s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 88.371 [1.000, 158.000], mean observation: 0.260 [0.000, 70.000], loss: 0.221520, mean_absolute_error: 0.506274, mean_q: 6.592922, mean_eps: 0.100000\n",
      " 104912/175000: episode: 2938, duration: 0.373s, episode steps: 17, steps per second: 46, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 99.765 [10.000, 158.000], mean observation: 0.081 [0.000, 34.000], loss: 1.095235, mean_absolute_error: 0.502118, mean_q: 6.454820, mean_eps: 0.100000\n",
      " 104945/175000: episode: 2939, duration: 0.620s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 78.636 [0.000, 181.000], mean observation: 0.288 [0.000, 66.000], loss: 0.679284, mean_absolute_error: 0.539573, mean_q: 6.779442, mean_eps: 0.100000\n",
      " 104982/175000: episode: 2940, duration: 0.655s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.054 [32.000, 158.000], mean observation: 0.336 [0.000, 74.000], loss: 24.343652, mean_absolute_error: 0.996951, mean_q: 10.061372, mean_eps: 0.100000\n",
      " 105017/175000: episode: 2941, duration: 0.636s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.571 [30.000, 216.000], mean observation: 0.273 [0.000, 70.000], loss: 0.279075, mean_absolute_error: 0.540796, mean_q: 6.599501, mean_eps: 0.100000\n",
      " 105049/175000: episode: 2942, duration: 0.588s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 64.594 [8.000, 213.000], mean observation: 0.272 [0.000, 64.000], loss: 1.933668, mean_absolute_error: 0.708297, mean_q: 8.264946, mean_eps: 0.100000\n",
      " 105097/175000: episode: 2943, duration: 0.925s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.917 [10.000, 216.000], mean observation: 0.552 [0.000, 96.000], loss: 14.678691, mean_absolute_error: 0.700070, mean_q: 7.676481, mean_eps: 0.100000\n",
      " 105134/175000: episode: 2944, duration: 0.644s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 122.730 [3.000, 176.000], mean observation: 0.397 [0.000, 74.000], loss: 0.112931, mean_absolute_error: 0.517904, mean_q: 6.574821, mean_eps: 0.100000\n",
      " 105179/175000: episode: 2945, duration: 0.798s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.000 [32.000, 176.000], mean observation: 0.429 [0.000, 90.000], loss: 16.608874, mean_absolute_error: 0.711637, mean_q: 7.921662, mean_eps: 0.100000\n",
      " 105229/175000: episode: 2946, duration: 0.957s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.420 [10.000, 170.000], mean observation: 0.676 [0.000, 100.000], loss: 0.948922, mean_absolute_error: 0.465988, mean_q: 6.387570, mean_eps: 0.100000\n",
      " 105278/175000: episode: 2947, duration: 0.906s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.592 [10.000, 220.000], mean observation: 0.567 [0.000, 98.000], loss: 2.180895, mean_absolute_error: 0.475273, mean_q: 6.160555, mean_eps: 0.100000\n",
      " 105311/175000: episode: 2948, duration: 0.579s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.030 [9.000, 213.000], mean observation: 0.314 [0.000, 66.000], loss: 4.023802, mean_absolute_error: 0.538946, mean_q: 6.264539, mean_eps: 0.100000\n",
      " 105351/175000: episode: 2949, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 87.125 [9.000, 219.000], mean observation: 0.481 [0.000, 80.000], loss: 5.761849, mean_absolute_error: 0.723438, mean_q: 7.784512, mean_eps: 0.100000\n",
      " 105384/175000: episode: 2950, duration: 0.692s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.242 [9.000, 186.000], mean observation: 0.391 [0.000, 66.000], loss: 3.521423, mean_absolute_error: 0.568218, mean_q: 6.305157, mean_eps: 0.100000\n",
      " 105413/175000: episode: 2951, duration: 0.689s, episode steps: 29, steps per second: 42, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 104.000 [7.000, 160.000], mean observation: 0.344 [0.000, 58.000], loss: 38.126684, mean_absolute_error: 0.745118, mean_q: 6.557919, mean_eps: 0.100000\n",
      " 105444/175000: episode: 2952, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 85.710 [7.000, 135.000], mean observation: 0.254 [0.000, 62.000], loss: 0.247037, mean_absolute_error: 0.505826, mean_q: 6.075346, mean_eps: 0.100000\n",
      " 105491/175000: episode: 2953, duration: 0.854s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 96.043 [7.000, 220.000], mean observation: 0.606 [0.000, 94.000], loss: 0.183476, mean_absolute_error: 0.487536, mean_q: 6.172086, mean_eps: 0.100000\n",
      " 105522/175000: episode: 2954, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 62.419 [7.000, 188.000], mean observation: 0.226 [0.000, 62.000], loss: 0.361204, mean_absolute_error: 0.470152, mean_q: 5.954836, mean_eps: 0.100000\n",
      " 105567/175000: episode: 2955, duration: 0.789s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.111 [7.000, 213.000], mean observation: 0.555 [0.000, 90.000], loss: 0.891010, mean_absolute_error: 0.469359, mean_q: 5.955306, mean_eps: 0.100000\n",
      " 105610/175000: episode: 2956, duration: 0.802s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.349 [7.000, 197.000], mean observation: 0.424 [0.000, 86.000], loss: 2.585620, mean_absolute_error: 0.477999, mean_q: 5.992763, mean_eps: 0.100000\n",
      " 105644/175000: episode: 2957, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 85.206 [7.000, 202.000], mean observation: 0.270 [0.000, 68.000], loss: 0.124014, mean_absolute_error: 0.467532, mean_q: 6.243754, mean_eps: 0.100000\n",
      " 105668/175000: episode: 2958, duration: 0.500s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 90.167 [7.000, 162.000], mean observation: 0.156 [0.000, 48.000], loss: 0.592887, mean_absolute_error: 0.709290, mean_q: 8.587264, mean_eps: 0.100000\n",
      " 105688/175000: episode: 2959, duration: 0.421s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 111.250 [37.000, 152.000], mean observation: 0.107 [0.000, 40.000], loss: 4.298558, mean_absolute_error: 0.780340, mean_q: 8.967929, mean_eps: 0.100000\n",
      " 105725/175000: episode: 2960, duration: 0.726s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 112.865 [5.000, 152.000], mean observation: 0.206 [0.000, 74.000], loss: 0.165288, mean_absolute_error: 0.463686, mean_q: 6.447522, mean_eps: 0.100000\n",
      " 105755/175000: episode: 2961, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 123.900 [37.000, 196.000], mean observation: 0.229 [0.000, 60.000], loss: 0.108935, mean_absolute_error: 0.448960, mean_q: 6.366829, mean_eps: 0.100000\n",
      " 105795/175000: episode: 2962, duration: 0.805s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 137.500 [37.000, 222.000], mean observation: 0.447 [0.000, 80.000], loss: 0.352262, mean_absolute_error: 0.609626, mean_q: 7.851747, mean_eps: 0.100000\n",
      " 105809/175000: episode: 2963, duration: 0.289s, episode steps: 14, steps per second: 49, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 66.429 [59.000, 111.000], mean observation: 0.053 [0.000, 28.000], loss: 0.076211, mean_absolute_error: 0.458471, mean_q: 6.241454, mean_eps: 0.100000\n",
      " 105845/175000: episode: 2964, duration: 0.678s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 71.667 [19.000, 183.000], mean observation: 0.316 [0.000, 72.000], loss: 0.182588, mean_absolute_error: 0.433720, mean_q: 5.764679, mean_eps: 0.100000\n",
      " 105882/175000: episode: 2965, duration: 0.641s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 73.135 [0.000, 224.000], mean observation: 0.418 [0.000, 74.000], loss: 8.981838, mean_absolute_error: 0.473176, mean_q: 5.914842, mean_eps: 0.100000\n",
      " 105911/175000: episode: 2966, duration: 0.593s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.586 [43.000, 181.000], mean observation: 0.235 [0.000, 58.000], loss: 0.149450, mean_absolute_error: 0.416657, mean_q: 6.009015, mean_eps: 0.100000\n",
      " 105947/175000: episode: 2967, duration: 0.667s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.861 [59.000, 216.000], mean observation: 0.341 [0.000, 72.000], loss: 0.140816, mean_absolute_error: 0.411027, mean_q: 5.844673, mean_eps: 0.100000\n",
      " 105982/175000: episode: 2968, duration: 0.635s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 126.886 [59.000, 216.000], mean observation: 0.329 [0.000, 70.000], loss: 6.571156, mean_absolute_error: 0.620314, mean_q: 7.584690, mean_eps: 0.100000\n",
      " 106030/175000: episode: 2969, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.083 [41.000, 194.000], mean observation: 0.416 [0.000, 96.000], loss: 0.124049, mean_absolute_error: 0.400757, mean_q: 5.932628, mean_eps: 0.100000\n",
      " 106085/175000: episode: 2970, duration: 1.038s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 76.055 [0.000, 185.000], mean observation: 0.736 [0.000, 110.000], loss: 22.333752, mean_absolute_error: 0.613352, mean_q: 7.046766, mean_eps: 0.100000\n",
      " 106120/175000: episode: 2971, duration: 0.624s, episode steps: 35, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.914 [0.000, 208.000], mean observation: 0.333 [0.000, 70.000], loss: 0.195947, mean_absolute_error: 0.406474, mean_q: 6.057287, mean_eps: 0.100000\n",
      " 106157/175000: episode: 2972, duration: 0.691s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.324 [0.000, 208.000], mean observation: 0.303 [0.000, 74.000], loss: 15.609679, mean_absolute_error: 0.814580, mean_q: 9.070808, mean_eps: 0.100000\n",
      " 106187/175000: episode: 2973, duration: 0.551s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 126.667 [28.000, 208.000], mean observation: 0.204 [0.000, 60.000], loss: 0.122127, mean_absolute_error: 0.418563, mean_q: 6.161015, mean_eps: 0.100000\n",
      " 106215/175000: episode: 2974, duration: 0.503s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 153.786 [28.000, 208.000], mean observation: 0.193 [0.000, 56.000], loss: 0.167932, mean_absolute_error: 0.406373, mean_q: 5.929690, mean_eps: 0.100000\n",
      " 106236/175000: episode: 2975, duration: 0.456s, episode steps: 21, steps per second: 46, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 106.619 [62.000, 158.000], mean observation: 0.142 [0.000, 42.000], loss: 0.347987, mean_absolute_error: 0.406790, mean_q: 6.169731, mean_eps: 0.100000\n",
      " 106279/175000: episode: 2976, duration: 0.811s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 133.535 [48.000, 207.000], mean observation: 0.584 [0.000, 86.000], loss: 0.280071, mean_absolute_error: 0.550881, mean_q: 7.269007, mean_eps: 0.100000\n",
      " 106309/175000: episode: 2977, duration: 0.578s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.533 [43.000, 148.000], mean observation: 0.194 [0.000, 60.000], loss: 32.850687, mean_absolute_error: 1.093619, mean_q: 10.742387, mean_eps: 0.100000\n",
      " 106357/175000: episode: 2978, duration: 0.852s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 126.208 [32.000, 204.000], mean observation: 0.423 [0.000, 96.000], loss: 6.304744, mean_absolute_error: 0.572409, mean_q: 7.249691, mean_eps: 0.100000\n",
      " 106415/175000: episode: 2979, duration: 1.011s, episode steps: 58, steps per second: 57, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 111.207 [14.000, 158.000], mean observation: 0.417 [0.000, 116.000], loss: 0.116277, mean_absolute_error: 0.407226, mean_q: 5.933018, mean_eps: 0.100000\n",
      " 106441/175000: episode: 2980, duration: 0.512s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.077 [82.000, 158.000], mean observation: 0.131 [0.000, 52.000], loss: 0.665143, mean_absolute_error: 0.640205, mean_q: 8.087831, mean_eps: 0.100000\n",
      " 106488/175000: episode: 2981, duration: 0.859s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.191 [32.000, 184.000], mean observation: 0.389 [0.000, 94.000], loss: 1.688653, mean_absolute_error: 0.518818, mean_q: 7.134580, mean_eps: 0.100000\n",
      " 106528/175000: episode: 2982, duration: 0.763s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.250 [53.000, 195.000], mean observation: 0.424 [0.000, 80.000], loss: 2.892140, mean_absolute_error: 0.395053, mean_q: 5.618135, mean_eps: 0.100000\n",
      " 106558/175000: episode: 2983, duration: 0.571s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.433 [49.000, 218.000], mean observation: 0.268 [0.000, 60.000], loss: 0.106391, mean_absolute_error: 0.394782, mean_q: 5.897903, mean_eps: 0.100000\n",
      " 106602/175000: episode: 2984, duration: 0.833s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.659 [46.000, 208.000], mean observation: 0.330 [0.000, 88.000], loss: 22.972019, mean_absolute_error: 0.797225, mean_q: 8.677292, mean_eps: 0.100000\n",
      " 106652/175000: episode: 2985, duration: 0.929s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.440 [33.000, 222.000], mean observation: 0.401 [0.000, 100.000], loss: 2.201744, mean_absolute_error: 0.410210, mean_q: 5.819708, mean_eps: 0.100000\n",
      " 106682/175000: episode: 2986, duration: 0.579s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 128.867 [29.000, 208.000], mean observation: 0.202 [0.000, 60.000], loss: 0.267574, mean_absolute_error: 0.407795, mean_q: 5.948827, mean_eps: 0.100000\n",
      " 106719/175000: episode: 2987, duration: 0.686s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 188.919 [9.000, 208.000], mean observation: 0.112 [0.000, 74.000], loss: 38.421964, mean_absolute_error: 0.745982, mean_q: 7.525387, mean_eps: 0.100000\n",
      " 106759/175000: episode: 2988, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 125.375 [33.000, 208.000], mean observation: 0.313 [0.000, 80.000], loss: 0.160507, mean_absolute_error: 0.394615, mean_q: 5.822921, mean_eps: 0.100000\n",
      " 106808/175000: episode: 2989, duration: 1.034s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.490 [33.000, 224.000], mean observation: 0.570 [0.000, 98.000], loss: 2.088851, mean_absolute_error: 0.550265, mean_q: 7.244041, mean_eps: 0.100000\n",
      " 106854/175000: episode: 2990, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 123.413 [27.000, 223.000], mean observation: 0.506 [0.000, 92.000], loss: 0.190738, mean_absolute_error: 0.387891, mean_q: 5.848193, mean_eps: 0.100000\n",
      " 106902/175000: episode: 2991, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.833 [43.000, 196.000], mean observation: 0.336 [0.000, 96.000], loss: 0.111724, mean_absolute_error: 0.391840, mean_q: 6.016524, mean_eps: 0.100000\n",
      " 106933/175000: episode: 2992, duration: 0.570s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 141.226 [43.000, 183.000], mean observation: 0.150 [0.000, 62.000], loss: 0.167017, mean_absolute_error: 0.368067, mean_q: 5.720748, mean_eps: 0.100000\n",
      " 106959/175000: episode: 2993, duration: 0.452s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 123.500 [5.000, 183.000], mean observation: 0.161 [0.000, 52.000], loss: 0.316488, mean_absolute_error: 0.372623, mean_q: 5.990868, mean_eps: 0.100000\n",
      " 106995/175000: episode: 2994, duration: 0.701s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 149.194 [10.000, 208.000], mean observation: 0.216 [0.000, 72.000], loss: 0.131806, mean_absolute_error: 0.378837, mean_q: 5.758494, mean_eps: 0.100000\n",
      " 107048/175000: episode: 2995, duration: 0.998s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 153.038 [34.000, 208.000], mean observation: 0.468 [0.000, 106.000], loss: 0.819842, mean_absolute_error: 0.384681, mean_q: 5.760172, mean_eps: 0.100000\n",
      " 107094/175000: episode: 2996, duration: 0.828s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 151.283 [0.000, 212.000], mean observation: 0.400 [0.000, 92.000], loss: 0.220232, mean_absolute_error: 0.538472, mean_q: 6.862434, mean_eps: 0.100000\n",
      " 107149/175000: episode: 2997, duration: 1.038s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 151.309 [18.000, 219.000], mean observation: 0.265 [0.000, 110.000], loss: 10.642714, mean_absolute_error: 0.581063, mean_q: 7.076061, mean_eps: 0.100000\n",
      " 107193/175000: episode: 2998, duration: 0.788s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 127.568 [7.000, 187.000], mean observation: 0.329 [0.000, 88.000], loss: 0.219706, mean_absolute_error: 0.415936, mean_q: 5.985616, mean_eps: 0.100000\n",
      " 107231/175000: episode: 2999, duration: 0.681s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 119.026 [8.000, 224.000], mean observation: 0.305 [0.000, 76.000], loss: 0.181544, mean_absolute_error: 0.414686, mean_q: 5.925844, mean_eps: 0.100000\n",
      " 107262/175000: episode: 3000, duration: 0.559s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 148.581 [79.000, 215.000], mean observation: 0.204 [0.000, 62.000], loss: 0.233432, mean_absolute_error: 0.418757, mean_q: 5.925479, mean_eps: 0.100000\n",
      " 107279/175000: episode: 3001, duration: 0.305s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 131.765 [40.000, 169.000], mean observation: 0.077 [0.000, 34.000], loss: 15.463165, mean_absolute_error: 0.912639, mean_q: 9.651766, mean_eps: 0.100000\n",
      " 107312/175000: episode: 3002, duration: 0.659s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 109.879 [0.000, 208.000], mean observation: 0.293 [0.000, 66.000], loss: 0.101852, mean_absolute_error: 0.420473, mean_q: 5.638636, mean_eps: 0.100000\n",
      " 107346/175000: episode: 3003, duration: 0.651s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.941 [18.000, 171.000], mean observation: 0.258 [0.000, 68.000], loss: 0.263007, mean_absolute_error: 0.422283, mean_q: 5.671825, mean_eps: 0.100000\n",
      " 107391/175000: episode: 3004, duration: 0.793s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.156 [0.000, 171.000], mean observation: 0.329 [0.000, 90.000], loss: 0.435030, mean_absolute_error: 0.578476, mean_q: 7.096866, mean_eps: 0.100000\n",
      " 107438/175000: episode: 3005, duration: 0.848s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 139.532 [78.000, 171.000], mean observation: 0.311 [0.000, 94.000], loss: 21.129962, mean_absolute_error: 0.810639, mean_q: 8.286182, mean_eps: 0.100000\n",
      " 107469/175000: episode: 3006, duration: 0.570s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 155.323 [79.000, 180.000], mean observation: 0.197 [0.000, 62.000], loss: 0.128931, mean_absolute_error: 0.667660, mean_q: 7.765375, mean_eps: 0.100000\n",
      " 107485/175000: episode: 3007, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 133.438 [57.000, 188.000], mean observation: 0.132 [0.000, 32.000], loss: 0.119271, mean_absolute_error: 0.456044, mean_q: 5.817832, mean_eps: 0.100000\n",
      " 107505/175000: episode: 3008, duration: 0.367s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 121.950 [20.000, 195.000], mean observation: 0.151 [0.000, 40.000], loss: 37.614394, mean_absolute_error: 1.263503, mean_q: 11.907045, mean_eps: 0.100000\n",
      " 107553/175000: episode: 3009, duration: 0.854s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.667 [37.000, 188.000], mean observation: 0.477 [0.000, 96.000], loss: 0.115912, mean_absolute_error: 0.443357, mean_q: 5.700808, mean_eps: 0.100000\n",
      " 107583/175000: episode: 3010, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 122.167 [47.000, 212.000], mean observation: 0.238 [0.000, 60.000], loss: 0.098803, mean_absolute_error: 0.437913, mean_q: 5.656171, mean_eps: 0.100000\n",
      " 107638/175000: episode: 3011, duration: 1.018s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 155.764 [22.000, 218.000], mean observation: 0.805 [0.000, 110.000], loss: 8.900123, mean_absolute_error: 0.484649, mean_q: 5.779044, mean_eps: 0.100000\n",
      " 107677/175000: episode: 3012, duration: 0.691s, episode steps: 39, steps per second: 56, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 146.410 [91.000, 208.000], mean observation: 0.420 [0.000, 77.000], loss: 21.003176, mean_absolute_error: 0.681517, mean_q: 7.152808, mean_eps: 0.100000\n",
      " 107709/175000: episode: 3013, duration: 0.573s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 138.000 [18.000, 208.000], mean observation: 0.277 [0.000, 64.000], loss: 0.492364, mean_absolute_error: 0.434946, mean_q: 5.392498, mean_eps: 0.100000\n",
      " 107765/175000: episode: 3014, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 118.607 [33.000, 221.000], mean observation: 0.639 [0.000, 112.000], loss: 23.454002, mean_absolute_error: 0.661757, mean_q: 6.613264, mean_eps: 0.100000\n",
      " 107816/175000: episode: 3015, duration: 0.922s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 102.804 [21.000, 208.000], mean observation: 0.533 [0.000, 102.000], loss: 1.069664, mean_absolute_error: 0.604734, mean_q: 6.739196, mean_eps: 0.100000\n",
      " 107860/175000: episode: 3016, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.705 [5.000, 195.000], mean observation: 0.250 [0.000, 88.000], loss: 2.171535, mean_absolute_error: 0.657423, mean_q: 6.985899, mean_eps: 0.100000\n",
      " 107891/175000: episode: 3017, duration: 0.613s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.677 [7.000, 195.000], mean observation: 0.305 [0.000, 62.000], loss: 2.671145, mean_absolute_error: 0.874303, mean_q: 8.856764, mean_eps: 0.100000\n",
      " 107936/175000: episode: 3018, duration: 0.869s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 75.156 [11.000, 201.000], mean observation: 0.667 [0.000, 90.000], loss: 0.603497, mean_absolute_error: 0.512970, mean_q: 5.487448, mean_eps: 0.100000\n",
      " 107977/175000: episode: 3019, duration: 0.804s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 133.683 [27.000, 166.000], mean observation: 0.434 [0.000, 82.000], loss: 0.392295, mean_absolute_error: 0.514524, mean_q: 5.592135, mean_eps: 0.100000\n",
      " 108013/175000: episode: 3020, duration: 0.684s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 87.667 [11.000, 201.000], mean observation: 0.507 [0.000, 72.000], loss: 0.374273, mean_absolute_error: 0.542237, mean_q: 5.762522, mean_eps: 0.100000\n",
      " 108040/175000: episode: 3021, duration: 0.505s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 121.741 [11.000, 166.000], mean observation: 0.178 [0.000, 54.000], loss: 0.178562, mean_absolute_error: 0.552894, mean_q: 5.806014, mean_eps: 0.100000\n",
      " 108086/175000: episode: 3022, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.630 [1.000, 211.000], mean observation: 0.435 [0.000, 92.000], loss: 3.080057, mean_absolute_error: 0.649227, mean_q: 6.219114, mean_eps: 0.100000\n",
      " 108117/175000: episode: 3023, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 122.806 [33.000, 214.000], mean observation: 0.255 [0.000, 62.000], loss: 1.192570, mean_absolute_error: 0.632836, mean_q: 5.958154, mean_eps: 0.100000\n",
      " 108159/175000: episode: 3024, duration: 0.698s, episode steps: 42, steps per second: 60, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 120.429 [32.000, 203.000], mean observation: 0.479 [0.000, 84.000], loss: 0.343933, mean_absolute_error: 0.542489, mean_q: 5.339167, mean_eps: 0.100000\n",
      " 108206/175000: episode: 3025, duration: 0.881s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.426 [18.000, 219.000], mean observation: 0.510 [0.000, 94.000], loss: 0.221738, mean_absolute_error: 0.565954, mean_q: 5.601270, mean_eps: 0.100000\n",
      " 108235/175000: episode: 3026, duration: 0.536s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 118.000 [43.000, 202.000], mean observation: 0.127 [0.000, 58.000], loss: 0.317679, mean_absolute_error: 0.588395, mean_q: 5.975394, mean_eps: 0.100000\n",
      " 108284/175000: episode: 3027, duration: 0.937s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 141.633 [24.000, 203.000], mean observation: 0.602 [0.000, 98.000], loss: 5.681371, mean_absolute_error: 0.712372, mean_q: 7.124727, mean_eps: 0.100000\n",
      " 108313/175000: episode: 3028, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 112.034 [60.000, 224.000], mean observation: 0.241 [0.000, 58.000], loss: 0.604925, mean_absolute_error: 0.537184, mean_q: 5.431834, mean_eps: 0.100000\n",
      " 108346/175000: episode: 3029, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.030 [1.000, 221.000], mean observation: 0.184 [0.000, 66.000], loss: 0.396668, mean_absolute_error: 0.531478, mean_q: 4.984396, mean_eps: 0.100000\n",
      " 108365/175000: episode: 3030, duration: 0.358s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 165.789 [60.000, 222.000], mean observation: 0.126 [0.000, 38.000], loss: 0.263813, mean_absolute_error: 0.556448, mean_q: 5.346349, mean_eps: 0.100000\n",
      " 108399/175000: episode: 3031, duration: 0.571s, episode steps: 34, steps per second: 60, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 163.529 [48.000, 224.000], mean observation: 0.262 [0.000, 68.000], loss: 0.261824, mean_absolute_error: 0.538437, mean_q: 5.050739, mean_eps: 0.100000\n",
      " 108429/175000: episode: 3032, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 152.067 [2.000, 224.000], mean observation: 0.114 [0.000, 60.000], loss: 1.366489, mean_absolute_error: 0.555645, mean_q: 5.167786, mean_eps: 0.100000\n",
      " 108442/175000: episode: 3033, duration: 0.226s, episode steps: 13, steps per second: 58, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 186.385 [81.000, 202.000], mean observation: 0.047 [0.000, 26.000], loss: 1.160328, mean_absolute_error: 0.549814, mean_q: 4.837460, mean_eps: 0.100000\n",
      " 108481/175000: episode: 3034, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 182.821 [32.000, 224.000], mean observation: 0.273 [0.000, 78.000], loss: 0.840531, mean_absolute_error: 0.583795, mean_q: 4.854683, mean_eps: 0.100000\n",
      " 108505/175000: episode: 3035, duration: 0.419s, episode steps: 24, steps per second: 57, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 169.125 [29.000, 202.000], mean observation: 0.106 [0.000, 48.000], loss: 0.733510, mean_absolute_error: 0.748397, mean_q: 6.185853, mean_eps: 0.100000\n",
      " 108529/175000: episode: 3036, duration: 0.442s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 174.083 [60.000, 224.000], mean observation: 0.218 [0.000, 48.000], loss: 5.711268, mean_absolute_error: 0.636161, mean_q: 4.686682, mean_eps: 0.100000\n",
      " 108550/175000: episode: 3037, duration: 0.360s, episode steps: 21, steps per second: 58, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 122.000 [59.000, 224.000], mean observation: 0.163 [0.000, 42.000], loss: 0.875847, mean_absolute_error: 0.644678, mean_q: 4.841585, mean_eps: 0.100000\n",
      " 108585/175000: episode: 3038, duration: 0.772s, episode steps: 35, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 142.971 [15.000, 224.000], mean observation: 0.348 [0.000, 70.000], loss: 0.767883, mean_absolute_error: 0.663096, mean_q: 4.707691, mean_eps: 0.100000\n",
      " 108623/175000: episode: 3039, duration: 0.676s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.553 [11.000, 206.000], mean observation: 0.434 [0.000, 76.000], loss: 18.495750, mean_absolute_error: 0.880564, mean_q: 6.144940, mean_eps: 0.100000\n",
      " 108676/175000: episode: 3040, duration: 0.990s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 148.245 [38.000, 216.000], mean observation: 0.749 [0.000, 106.000], loss: 0.169319, mean_absolute_error: 0.659951, mean_q: 4.996997, mean_eps: 0.100000\n",
      " 108709/175000: episode: 3041, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 103.212 [23.000, 218.000], mean observation: 0.313 [0.000, 66.000], loss: 0.158406, mean_absolute_error: 0.619982, mean_q: 5.047389, mean_eps: 0.100000\n",
      " 108746/175000: episode: 3042, duration: 0.657s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 152.270 [32.000, 216.000], mean observation: 0.268 [0.000, 74.000], loss: 0.166098, mean_absolute_error: 0.602645, mean_q: 4.946201, mean_eps: 0.100000\n",
      " 108767/175000: episode: 3043, duration: 0.374s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 103.381 [10.000, 212.000], mean observation: 0.121 [0.000, 42.000], loss: 0.141257, mean_absolute_error: 0.594626, mean_q: 5.051074, mean_eps: 0.100000\n",
      " 108810/175000: episode: 3044, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 139.860 [1.000, 206.000], mean observation: 0.478 [0.000, 86.000], loss: 0.328338, mean_absolute_error: 0.582121, mean_q: 4.926721, mean_eps: 0.100000\n",
      " 108862/175000: episode: 3045, duration: 0.986s, episode steps: 52, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.519 [0.000, 174.000], mean observation: 0.722 [0.000, 104.000], loss: 15.601877, mean_absolute_error: 0.826947, mean_q: 6.750968, mean_eps: 0.100000\n",
      " 108904/175000: episode: 3046, duration: 0.747s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.500 [7.000, 188.000], mean observation: 0.653 [0.000, 84.000], loss: 2.441762, mean_absolute_error: 0.684467, mean_q: 6.317075, mean_eps: 0.100000\n",
      " 108947/175000: episode: 3047, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 122.744 [0.000, 188.000], mean observation: 0.567 [0.000, 86.000], loss: 26.411034, mean_absolute_error: 0.953883, mean_q: 8.189942, mean_eps: 0.100000\n",
      " 108978/175000: episode: 3048, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 109.516 [1.000, 222.000], mean observation: 0.316 [0.000, 62.000], loss: 0.101698, mean_absolute_error: 0.504052, mean_q: 5.085995, mean_eps: 0.100000\n",
      " 108999/175000: episode: 3049, duration: 0.375s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 76.905 [44.000, 168.000], mean observation: 0.064 [0.000, 42.000], loss: 0.118189, mean_absolute_error: 0.490579, mean_q: 5.060850, mean_eps: 0.100000\n",
      " 109029/175000: episode: 3050, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 129.533 [1.000, 202.000], mean observation: 0.186 [0.000, 60.000], loss: 0.230841, mean_absolute_error: 0.505562, mean_q: 5.137655, mean_eps: 0.100000\n",
      " 109059/175000: episode: 3051, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 174.533 [59.000, 202.000], mean observation: 0.173 [0.000, 60.000], loss: 0.117466, mean_absolute_error: 0.483378, mean_q: 5.193119, mean_eps: 0.100000\n",
      " 109099/175000: episode: 3052, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 157.925 [28.000, 207.000], mean observation: 0.269 [0.000, 80.000], loss: 0.193399, mean_absolute_error: 0.470083, mean_q: 5.297915, mean_eps: 0.100000\n",
      " 109123/175000: episode: 3053, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.042 [1.000, 222.000], mean observation: 0.218 [0.000, 48.000], loss: 30.222127, mean_absolute_error: 0.823888, mean_q: 7.202619, mean_eps: 0.100000\n",
      " 109169/175000: episode: 3054, duration: 0.861s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.022 [6.000, 222.000], mean observation: 0.562 [0.000, 92.000], loss: 0.137150, mean_absolute_error: 0.463640, mean_q: 5.356966, mean_eps: 0.100000\n",
      " 109202/175000: episode: 3055, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 105.606 [39.000, 222.000], mean observation: 0.280 [0.000, 66.000], loss: 15.440144, mean_absolute_error: 0.690569, mean_q: 6.991062, mean_eps: 0.100000\n",
      " 109215/175000: episode: 3056, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 137.692 [2.000, 220.000], mean observation: 0.063 [0.000, 26.000], loss: 1.297575, mean_absolute_error: 0.476287, mean_q: 5.442477, mean_eps: 0.100000\n",
      " 109240/175000: episode: 3057, duration: 0.511s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 127.080 [59.000, 202.000], mean observation: 0.141 [0.000, 50.000], loss: 0.097686, mean_absolute_error: 0.450992, mean_q: 5.652814, mean_eps: 0.100000\n",
      " 109275/175000: episode: 3058, duration: 0.663s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.286 [59.000, 191.000], mean observation: 0.242 [0.000, 70.000], loss: 0.160651, mean_absolute_error: 0.471911, mean_q: 5.904065, mean_eps: 0.100000\n",
      " 109317/175000: episode: 3059, duration: 0.805s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.738 [59.000, 223.000], mean observation: 0.390 [0.000, 84.000], loss: 0.076765, mean_absolute_error: 0.437040, mean_q: 5.800677, mean_eps: 0.100000\n",
      " 109334/175000: episode: 3060, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 185.059 [59.000, 202.000], mean observation: 0.064 [0.000, 34.000], loss: 57.876043, mean_absolute_error: 1.413782, mean_q: 12.273187, mean_eps: 0.100000\n",
      " 109374/175000: episode: 3061, duration: 0.721s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 71.025 [0.000, 202.000], mean observation: 0.350 [0.000, 80.000], loss: 0.077382, mean_absolute_error: 0.446550, mean_q: 6.113369, mean_eps: 0.100000\n",
      " 109409/175000: episode: 3062, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.229 [3.000, 202.000], mean observation: 0.354 [0.000, 70.000], loss: 0.381258, mean_absolute_error: 0.436831, mean_q: 6.088694, mean_eps: 0.100000\n",
      " 109436/175000: episode: 3063, duration: 0.499s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 132.889 [0.000, 207.000], mean observation: 0.172 [0.000, 54.000], loss: 0.077799, mean_absolute_error: 0.418224, mean_q: 5.764527, mean_eps: 0.100000\n",
      " 109477/175000: episode: 3064, duration: 0.778s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.634 [39.000, 206.000], mean observation: 0.399 [0.000, 82.000], loss: 8.492031, mean_absolute_error: 0.592947, mean_q: 6.970044, mean_eps: 0.100000\n",
      " 109511/175000: episode: 3065, duration: 0.615s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 101.088 [43.000, 215.000], mean observation: 0.380 [0.000, 68.000], loss: 0.080554, mean_absolute_error: 0.418406, mean_q: 5.869442, mean_eps: 0.100000\n",
      " 109552/175000: episode: 3066, duration: 0.862s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.341 [21.000, 191.000], mean observation: 0.424 [0.000, 82.000], loss: 14.789354, mean_absolute_error: 0.754149, mean_q: 8.086734, mean_eps: 0.100000\n",
      " 109589/175000: episode: 3067, duration: 0.701s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.757 [22.000, 177.000], mean observation: 0.200 [0.000, 74.000], loss: 0.222254, mean_absolute_error: 0.438375, mean_q: 5.941677, mean_eps: 0.100000\n",
      " 109622/175000: episode: 3068, duration: 0.656s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 138.485 [1.000, 175.000], mean observation: 0.182 [0.000, 66.000], loss: 0.431495, mean_absolute_error: 0.432657, mean_q: 5.964406, mean_eps: 0.100000\n",
      " 109653/175000: episode: 3069, duration: 0.603s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.258 [1.000, 168.000], mean observation: 0.229 [0.000, 62.000], loss: 0.116667, mean_absolute_error: 0.423770, mean_q: 5.935530, mean_eps: 0.100000\n",
      " 109685/175000: episode: 3070, duration: 0.572s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 131.188 [10.000, 199.000], mean observation: 0.233 [0.000, 64.000], loss: 0.151036, mean_absolute_error: 0.433513, mean_q: 5.882959, mean_eps: 0.100000\n",
      " 109734/175000: episode: 3071, duration: 0.895s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 136.041 [93.000, 219.000], mean observation: 0.534 [0.000, 98.000], loss: 0.178085, mean_absolute_error: 0.416287, mean_q: 5.913128, mean_eps: 0.100000\n",
      " 109797/175000: episode: 3072, duration: 1.171s, episode steps: 63, steps per second: 54, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 129.524 [22.000, 193.000], mean observation: 0.642 [0.000, 126.000], loss: 1.859175, mean_absolute_error: 0.619034, mean_q: 7.670086, mean_eps: 0.100000\n",
      " 109832/175000: episode: 3073, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 135.143 [22.000, 206.000], mean observation: 0.306 [0.000, 70.000], loss: 0.274630, mean_absolute_error: 0.431150, mean_q: 6.066507, mean_eps: 0.100000\n",
      " 109849/175000: episode: 3074, duration: 0.378s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 129.412 [43.000, 206.000], mean observation: 0.108 [0.000, 34.000], loss: 0.282027, mean_absolute_error: 0.420860, mean_q: 5.917642, mean_eps: 0.100000\n",
      " 109875/175000: episode: 3075, duration: 0.450s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.654 [22.000, 206.000], mean observation: 0.240 [0.000, 52.000], loss: 0.288061, mean_absolute_error: 0.427065, mean_q: 6.066193, mean_eps: 0.100000\n",
      " 109908/175000: episode: 3076, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 122.606 [8.000, 175.000], mean observation: 0.378 [0.000, 66.000], loss: 0.274726, mean_absolute_error: 0.435313, mean_q: 6.073161, mean_eps: 0.100000\n",
      " 109938/175000: episode: 3077, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 120.567 [44.000, 220.000], mean observation: 0.131 [0.000, 60.000], loss: 0.179615, mean_absolute_error: 0.487273, mean_q: 6.275029, mean_eps: 0.100000\n",
      " 109967/175000: episode: 3078, duration: 0.515s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.862 [1.000, 203.000], mean observation: 0.161 [0.000, 58.000], loss: 0.225604, mean_absolute_error: 0.472074, mean_q: 6.183371, mean_eps: 0.100000\n",
      " 110030/175000: episode: 3079, duration: 1.223s, episode steps: 63, steps per second: 51, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 119.810 [1.000, 206.000], mean observation: 0.690 [0.000, 126.000], loss: 0.255372, mean_absolute_error: 0.476036, mean_q: 6.074331, mean_eps: 0.100000\n",
      " 110082/175000: episode: 3080, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 146.962 [1.000, 212.000], mean observation: 0.570 [0.000, 104.000], loss: 10.400927, mean_absolute_error: 0.731299, mean_q: 8.171459, mean_eps: 0.100000\n",
      " 110122/175000: episode: 3081, duration: 0.716s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 101.150 [30.000, 208.000], mean observation: 0.549 [0.000, 80.000], loss: 0.225848, mean_absolute_error: 0.487069, mean_q: 6.277799, mean_eps: 0.100000\n",
      " 110156/175000: episode: 3082, duration: 0.643s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.265 [1.000, 190.000], mean observation: 0.431 [0.000, 68.000], loss: 0.145204, mean_absolute_error: 0.486370, mean_q: 6.255746, mean_eps: 0.100000\n",
      " 110192/175000: episode: 3083, duration: 0.722s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 103.389 [1.000, 220.000], mean observation: 0.383 [0.000, 72.000], loss: 0.235224, mean_absolute_error: 0.502493, mean_q: 6.371379, mean_eps: 0.100000\n",
      " 110227/175000: episode: 3084, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.657 [1.000, 205.000], mean observation: 0.523 [0.000, 70.000], loss: 0.239526, mean_absolute_error: 0.488768, mean_q: 6.355213, mean_eps: 0.100000\n",
      " 110268/175000: episode: 3085, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.537 [1.000, 219.000], mean observation: 0.486 [0.000, 82.000], loss: 0.312868, mean_absolute_error: 0.495612, mean_q: 6.337172, mean_eps: 0.100000\n",
      " 110318/175000: episode: 3086, duration: 0.935s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 89.020 [22.000, 203.000], mean observation: 0.531 [0.000, 100.000], loss: 31.929172, mean_absolute_error: 0.767420, mean_q: 7.294113, mean_eps: 0.100000\n",
      " 110340/175000: episode: 3087, duration: 0.438s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 127.727 [83.000, 188.000], mean observation: 0.154 [0.000, 44.000], loss: 0.184372, mean_absolute_error: 0.507654, mean_q: 6.145593, mean_eps: 0.100000\n",
      " 110360/175000: episode: 3088, duration: 0.423s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 109.150 [65.000, 204.000], mean observation: 0.195 [0.000, 40.000], loss: 0.244945, mean_absolute_error: 0.523757, mean_q: 6.221249, mean_eps: 0.100000\n",
      " 110412/175000: episode: 3089, duration: 0.998s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.538 [8.000, 207.000], mean observation: 0.493 [0.000, 104.000], loss: 16.868416, mean_absolute_error: 0.578077, mean_q: 6.085587, mean_eps: 0.100000\n",
      " 110450/175000: episode: 3090, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 100.105 [25.000, 160.000], mean observation: 0.144 [0.000, 76.000], loss: 201.931228, mean_absolute_error: 1.502781, mean_q: 7.299170, mean_eps: 0.100000\n",
      " 110466/175000: episode: 3091, duration: 0.299s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 128.875 [103.000, 220.000], mean observation: 0.065 [0.000, 32.000], loss: 0.156734, mean_absolute_error: 0.481835, mean_q: 6.031261, mean_eps: 0.100000\n",
      " 110523/175000: episode: 3092, duration: 1.026s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.649 [15.000, 216.000], mean observation: 0.832 [0.000, 114.000], loss: 97.106483, mean_absolute_error: 1.208099, mean_q: 8.808702, mean_eps: 0.100000\n",
      " 110569/175000: episode: 3093, duration: 0.924s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.913 [3.000, 218.000], mean observation: 0.469 [0.000, 92.000], loss: 220.757660, mean_absolute_error: 1.569871, mean_q: 7.155326, mean_eps: 0.100000\n",
      " 110607/175000: episode: 3094, duration: 0.826s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 101.184 [46.000, 220.000], mean observation: 0.338 [0.000, 76.000], loss: 0.107767, mean_absolute_error: 0.458962, mean_q: 6.133392, mean_eps: 0.100000\n",
      " 110624/175000: episode: 3095, duration: 0.445s, episode steps: 17, steps per second: 38, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.529 [45.000, 202.000], mean observation: 0.074 [0.000, 34.000], loss: 0.147380, mean_absolute_error: 0.501486, mean_q: 6.224082, mean_eps: 0.100000\n",
      " 110643/175000: episode: 3096, duration: 0.476s, episode steps: 19, steps per second: 40, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 116.053 [45.000, 202.000], mean observation: 0.086 [0.000, 38.000], loss: 0.116430, mean_absolute_error: 0.496680, mean_q: 6.138400, mean_eps: 0.100000\n",
      " 110665/175000: episode: 3097, duration: 0.496s, episode steps: 22, steps per second: 44, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 94.455 [48.000, 209.000], mean observation: 0.133 [0.000, 44.000], loss: 0.443196, mean_absolute_error: 0.519780, mean_q: 6.099052, mean_eps: 0.100000\n",
      " 110697/175000: episode: 3098, duration: 0.686s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 61.844 [45.000, 202.000], mean observation: 0.221 [0.000, 64.000], loss: 27.371769, mean_absolute_error: 0.621684, mean_q: 5.814986, mean_eps: 0.100000\n",
      " 110727/175000: episode: 3099, duration: 0.551s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 68.067 [45.000, 222.000], mean observation: 0.277 [0.000, 60.000], loss: 4.106171, mean_absolute_error: 0.723459, mean_q: 8.083295, mean_eps: 0.100000\n",
      " 110740/175000: episode: 3100, duration: 0.307s, episode steps: 13, steps per second: 42, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 91.000 [48.000, 202.000], mean observation: 0.083 [0.000, 26.000], loss: 82.332599, mean_absolute_error: 1.364053, mean_q: 10.495377, mean_eps: 0.100000\n",
      " 110770/175000: episode: 3101, duration: 0.645s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 79.533 [48.000, 202.000], mean observation: 0.189 [0.000, 60.000], loss: 0.206661, mean_absolute_error: 0.574399, mean_q: 6.321118, mean_eps: 0.100000\n",
      " 110808/175000: episode: 3102, duration: 0.763s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.921 [15.000, 168.000], mean observation: 0.339 [0.000, 76.000], loss: 4.605512, mean_absolute_error: 0.728935, mean_q: 7.649837, mean_eps: 0.100000\n",
      " 110833/175000: episode: 3103, duration: 0.504s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 79.760 [25.000, 217.000], mean observation: 0.145 [0.000, 50.000], loss: 0.109412, mean_absolute_error: 0.508061, mean_q: 6.007749, mean_eps: 0.100000\n",
      " 110883/175000: episode: 3104, duration: 0.901s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.860 [27.000, 208.000], mean observation: 0.353 [0.000, 100.000], loss: 0.168599, mean_absolute_error: 0.484347, mean_q: 6.069924, mean_eps: 0.100000\n",
      " 110919/175000: episode: 3105, duration: 0.688s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.861 [4.000, 168.000], mean observation: 0.170 [0.000, 72.000], loss: 0.355312, mean_absolute_error: 0.511728, mean_q: 6.029443, mean_eps: 0.100000\n",
      " 110961/175000: episode: 3106, duration: 0.801s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.833 [0.000, 195.000], mean observation: 0.330 [0.000, 84.000], loss: 0.125248, mean_absolute_error: 0.514821, mean_q: 5.896397, mean_eps: 0.100000\n",
      " 110977/175000: episode: 3107, duration: 0.305s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 109.688 [15.000, 168.000], mean observation: 0.053 [0.000, 32.000], loss: 0.123972, mean_absolute_error: 0.520860, mean_q: 5.892403, mean_eps: 0.100000\n",
      " 111005/175000: episode: 3108, duration: 0.539s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 84.429 [15.000, 192.000], mean observation: 0.191 [0.000, 56.000], loss: 0.150567, mean_absolute_error: 0.527112, mean_q: 6.072569, mean_eps: 0.100000\n",
      " 111035/175000: episode: 3109, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 29.467 [15.000, 155.000], mean observation: 0.081 [0.000, 60.000], loss: 0.383095, mean_absolute_error: 0.516290, mean_q: 5.973230, mean_eps: 0.100000\n",
      " 111065/175000: episode: 3110, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 89.067 [15.000, 217.000], mean observation: 0.262 [0.000, 60.000], loss: 32.446711, mean_absolute_error: 0.848730, mean_q: 7.880097, mean_eps: 0.100000\n",
      " 111090/175000: episode: 3111, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 87.200 [42.000, 181.000], mean observation: 0.208 [0.000, 50.000], loss: 0.571570, mean_absolute_error: 0.506644, mean_q: 5.943733, mean_eps: 0.100000\n",
      " 111131/175000: episode: 3112, duration: 0.716s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 158.024 [15.000, 222.000], mean observation: 0.480 [0.000, 82.000], loss: 0.159788, mean_absolute_error: 0.506187, mean_q: 5.990041, mean_eps: 0.100000\n",
      " 111177/175000: episode: 3113, duration: 1.042s, episode steps: 46, steps per second: 44, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 130.891 [5.000, 222.000], mean observation: 0.550 [0.000, 92.000], loss: 67.921060, mean_absolute_error: 0.914044, mean_q: 6.897966, mean_eps: 0.100000\n",
      " 111198/175000: episode: 3114, duration: 0.423s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 90.381 [5.000, 180.000], mean observation: 0.099 [0.000, 42.000], loss: 39.972871, mean_absolute_error: 0.963934, mean_q: 8.547364, mean_eps: 0.100000\n",
      " 111235/175000: episode: 3115, duration: 0.862s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.838 [5.000, 180.000], mean observation: 0.373 [0.000, 74.000], loss: 0.189391, mean_absolute_error: 0.479788, mean_q: 5.629890, mean_eps: 0.100000\n",
      " 111277/175000: episode: 3116, duration: 0.930s, episode steps: 42, steps per second: 45, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 135.000 [18.000, 222.000], mean observation: 0.286 [0.000, 84.000], loss: 0.130658, mean_absolute_error: 0.481313, mean_q: 5.903411, mean_eps: 0.100000\n",
      " 111312/175000: episode: 3117, duration: 0.675s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.343 [18.000, 218.000], mean observation: 0.442 [0.000, 70.000], loss: 0.128734, mean_absolute_error: 0.489626, mean_q: 6.176365, mean_eps: 0.100000\n",
      " 111356/175000: episode: 3118, duration: 1.032s, episode steps: 44, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.864 [18.000, 208.000], mean observation: 0.466 [0.000, 88.000], loss: 0.599487, mean_absolute_error: 0.571905, mean_q: 6.869402, mean_eps: 0.100000\n",
      " 111391/175000: episode: 3119, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 64.029 [1.000, 209.000], mean observation: 0.287 [0.000, 70.000], loss: 0.437350, mean_absolute_error: 0.478930, mean_q: 5.809193, mean_eps: 0.100000\n",
      " 111431/175000: episode: 3120, duration: 0.720s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 146.150 [1.000, 219.000], mean observation: 0.307 [0.000, 80.000], loss: 0.162104, mean_absolute_error: 0.471006, mean_q: 5.605887, mean_eps: 0.100000\n",
      " 111472/175000: episode: 3121, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 189.195 [24.000, 209.000], mean observation: 0.149 [0.000, 82.000], loss: 0.854433, mean_absolute_error: 0.560492, mean_q: 6.650680, mean_eps: 0.100000\n",
      " 111534/175000: episode: 3122, duration: 1.269s, episode steps: 62, steps per second: 49, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 120.355 [1.000, 209.000], mean observation: 0.742 [0.000, 124.000], loss: 0.275079, mean_absolute_error: 0.419964, mean_q: 5.499731, mean_eps: 0.100000\n",
      " 111576/175000: episode: 3123, duration: 0.857s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.381 [40.000, 210.000], mean observation: 0.262 [0.000, 84.000], loss: 0.200545, mean_absolute_error: 0.438304, mean_q: 5.719891, mean_eps: 0.100000\n",
      " 111629/175000: episode: 3124, duration: 1.007s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 119.717 [40.000, 183.000], mean observation: 0.191 [0.000, 106.000], loss: 1.213848, mean_absolute_error: 0.549909, mean_q: 6.527667, mean_eps: 0.100000\n",
      " 111652/175000: episode: 3125, duration: 0.449s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 100.739 [40.000, 202.000], mean observation: 0.124 [0.000, 46.000], loss: 0.291632, mean_absolute_error: 0.432602, mean_q: 5.525083, mean_eps: 0.100000\n",
      " 111673/175000: episode: 3126, duration: 0.451s, episode steps: 21, steps per second: 47, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 118.571 [40.000, 183.000], mean observation: 0.075 [0.000, 42.000], loss: 0.371510, mean_absolute_error: 0.432240, mean_q: 5.608370, mean_eps: 0.100000\n",
      " 111729/175000: episode: 3127, duration: 1.028s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 89.964 [3.000, 191.000], mean observation: 0.640 [0.000, 112.000], loss: 0.347691, mean_absolute_error: 0.460465, mean_q: 5.589517, mean_eps: 0.100000\n",
      " 111756/175000: episode: 3128, duration: 0.487s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 137.333 [132.000, 196.000], mean observation: 0.069 [0.000, 54.000], loss: 0.254134, mean_absolute_error: 0.496025, mean_q: 5.835351, mean_eps: 0.100000\n",
      " 111789/175000: episode: 3129, duration: 0.618s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 97.364 [23.000, 206.000], mean observation: 0.281 [0.000, 66.000], loss: 0.233307, mean_absolute_error: 0.498840, mean_q: 6.060391, mean_eps: 0.100000\n",
      " 111819/175000: episode: 3130, duration: 0.569s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 135.633 [52.000, 208.000], mean observation: 0.209 [0.000, 60.000], loss: 0.349202, mean_absolute_error: 0.464541, mean_q: 5.777955, mean_eps: 0.100000\n",
      " 111861/175000: episode: 3131, duration: 0.797s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 108.881 [15.000, 220.000], mean observation: 0.235 [0.000, 84.000], loss: 1.436692, mean_absolute_error: 0.443701, mean_q: 5.545090, mean_eps: 0.100000\n",
      " 111883/175000: episode: 3132, duration: 0.374s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 106.455 [28.000, 208.000], mean observation: 0.201 [0.000, 44.000], loss: 5.049010, mean_absolute_error: 0.474179, mean_q: 5.729887, mean_eps: 0.100000\n",
      " 111911/175000: episode: 3133, duration: 0.528s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 65.607 [28.000, 166.000], mean observation: 0.186 [0.000, 56.000], loss: 3.800164, mean_absolute_error: 0.468536, mean_q: 5.374234, mean_eps: 0.100000\n",
      " 111947/175000: episode: 3134, duration: 0.765s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 50.944 [17.000, 158.000], mean observation: 0.362 [0.000, 72.000], loss: 7.665932, mean_absolute_error: 0.513308, mean_q: 5.586228, mean_eps: 0.100000\n",
      " 111983/175000: episode: 3135, duration: 0.803s, episode steps: 36, steps per second: 45, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 52.611 [28.000, 206.000], mean observation: 0.304 [0.000, 72.000], loss: 6.973734, mean_absolute_error: 0.578127, mean_q: 5.841571, mean_eps: 0.100000\n",
      " 112018/175000: episode: 3136, duration: 0.716s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 56.914 [12.000, 174.000], mean observation: 0.379 [0.000, 70.000], loss: 7.012295, mean_absolute_error: 0.585158, mean_q: 5.670373, mean_eps: 0.100000\n",
      " 112078/175000: episode: 3137, duration: 1.089s, episode steps: 60, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 48.617 [28.000, 220.000], mean observation: 0.714 [0.000, 120.000], loss: 160.107564, mean_absolute_error: 1.494879, mean_q: 7.740641, mean_eps: 0.100000\n",
      " 112113/175000: episode: 3138, duration: 0.671s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.086 [1.000, 148.000], mean observation: 0.124 [0.000, 70.000], loss: 2.382023, mean_absolute_error: 0.794826, mean_q: 7.649232, mean_eps: 0.100000\n",
      " 112148/175000: episode: 3139, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 79.514 [1.000, 209.000], mean observation: 0.137 [0.000, 70.000], loss: 50.184493, mean_absolute_error: 1.002621, mean_q: 7.474495, mean_eps: 0.100000\n",
      " 112195/175000: episode: 3140, duration: 0.857s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.915 [1.000, 210.000], mean observation: 0.407 [0.000, 94.000], loss: 0.603047, mean_absolute_error: 0.567377, mean_q: 5.351023, mean_eps: 0.100000\n",
      " 112239/175000: episode: 3141, duration: 0.827s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 74.659 [1.000, 198.000], mean observation: 0.250 [0.000, 88.000], loss: 0.440775, mean_absolute_error: 0.547752, mean_q: 5.463658, mean_eps: 0.100000\n",
      " 112273/175000: episode: 3142, duration: 0.647s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.265 [1.000, 173.000], mean observation: 0.147 [0.000, 68.000], loss: 0.236540, mean_absolute_error: 0.525971, mean_q: 5.392970, mean_eps: 0.100000\n",
      " 112306/175000: episode: 3143, duration: 0.576s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 81.818 [28.000, 215.000], mean observation: 0.260 [0.000, 66.000], loss: 0.567507, mean_absolute_error: 0.487363, mean_q: 5.327946, mean_eps: 0.100000\n",
      " 112335/175000: episode: 3144, duration: 0.527s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 104.172 [1.000, 223.000], mean observation: 0.146 [0.000, 58.000], loss: 26.640369, mean_absolute_error: 0.804842, mean_q: 7.240169, mean_eps: 0.100000\n",
      " 112373/175000: episode: 3145, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 144.526 [3.000, 223.000], mean observation: 0.522 [0.000, 76.000], loss: 1.909914, mean_absolute_error: 0.634875, mean_q: 6.484745, mean_eps: 0.100000\n",
      " 112417/175000: episode: 3146, duration: 0.790s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 141.227 [13.000, 220.000], mean observation: 0.616 [0.000, 88.000], loss: 6.273082, mean_absolute_error: 0.673682, mean_q: 6.764297, mean_eps: 0.100000\n",
      " 112437/175000: episode: 3147, duration: 0.385s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 20.050 [1.000, 215.000], mean observation: 0.053 [0.000, 40.000], loss: 0.162268, mean_absolute_error: 0.507376, mean_q: 5.605628, mean_eps: 0.100000\n",
      " 112462/175000: episode: 3148, duration: 0.434s, episode steps: 25, steps per second: 58, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 74.760 [1.000, 214.000], mean observation: 0.205 [0.000, 50.000], loss: 0.457218, mean_absolute_error: 0.532900, mean_q: 5.747233, mean_eps: 0.100000\n",
      " 112504/175000: episode: 3149, duration: 0.830s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 93.190 [1.000, 218.000], mean observation: 0.536 [0.000, 84.000], loss: 33.220241, mean_absolute_error: 0.772074, mean_q: 6.861128, mean_eps: 0.100000\n",
      " 112559/175000: episode: 3150, duration: 1.029s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 56.709 [1.000, 179.000], mean observation: 0.397 [0.000, 110.000], loss: 45.772481, mean_absolute_error: 0.771568, mean_q: 6.560808, mean_eps: 0.100000\n",
      " 112617/175000: episode: 3151, duration: 1.064s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 56.448 [1.000, 169.000], mean observation: 0.402 [0.000, 116.000], loss: 144.695942, mean_absolute_error: 1.192419, mean_q: 6.531966, mean_eps: 0.100000\n",
      " 112656/175000: episode: 3152, duration: 0.707s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.000 [1.000, 206.000], mean observation: 0.443 [0.000, 78.000], loss: 72.302124, mean_absolute_error: 0.928836, mean_q: 7.090562, mean_eps: 0.100000\n",
      " 112703/175000: episode: 3153, duration: 0.838s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 57.638 [1.000, 206.000], mean observation: 0.555 [0.000, 94.000], loss: 136.203299, mean_absolute_error: 1.171508, mean_q: 6.587148, mean_eps: 0.100000\n",
      " 112731/175000: episode: 3154, duration: 0.520s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 107.393 [41.000, 199.000], mean observation: 0.154 [0.000, 56.000], loss: 0.302849, mean_absolute_error: 0.444053, mean_q: 5.659665, mean_eps: 0.100000\n",
      " 112776/175000: episode: 3155, duration: 0.862s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 63.578 [1.000, 205.000], mean observation: 0.400 [0.000, 90.000], loss: 0.268262, mean_absolute_error: 0.450125, mean_q: 5.668703, mean_eps: 0.100000\n",
      " 112815/175000: episode: 3156, duration: 0.747s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 31.256 [1.000, 176.000], mean observation: 0.345 [0.000, 78.000], loss: 0.559891, mean_absolute_error: 0.424395, mean_q: 5.333301, mean_eps: 0.100000\n",
      " 112857/175000: episode: 3157, duration: 0.783s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.405 [1.000, 211.000], mean observation: 0.273 [0.000, 84.000], loss: 0.374543, mean_absolute_error: 0.412971, mean_q: 5.293510, mean_eps: 0.100000\n",
      " 112895/175000: episode: 3158, duration: 0.666s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.132 [10.000, 209.000], mean observation: 0.221 [0.000, 76.000], loss: 25.249362, mean_absolute_error: 0.703715, mean_q: 6.956276, mean_eps: 0.100000\n",
      " 112927/175000: episode: 3159, duration: 0.601s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 61.875 [42.000, 184.000], mean observation: 0.138 [0.000, 64.000], loss: 0.156893, mean_absolute_error: 0.530782, mean_q: 6.206510, mean_eps: 0.100000\n",
      " 112972/175000: episode: 3160, duration: 0.876s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.711 [15.000, 209.000], mean observation: 0.371 [0.000, 90.000], loss: 11.227861, mean_absolute_error: 0.720939, mean_q: 7.562842, mean_eps: 0.100000\n",
      " 113020/175000: episode: 3161, duration: 0.916s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 72.000 [0.000, 208.000], mean observation: 0.218 [0.000, 96.000], loss: 121.309660, mean_absolute_error: 1.232235, mean_q: 7.704222, mean_eps: 0.100000\n",
      " 113047/175000: episode: 3162, duration: 0.545s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 43.556 [11.000, 215.000], mean observation: 0.070 [0.000, 54.000], loss: 0.305784, mean_absolute_error: 0.463024, mean_q: 5.193744, mean_eps: 0.100000\n",
      " 113087/175000: episode: 3163, duration: 0.748s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 43.175 [38.000, 181.000], mean observation: 0.099 [0.000, 80.000], loss: 0.146696, mean_absolute_error: 0.476226, mean_q: 5.593756, mean_eps: 0.100000\n",
      " 113122/175000: episode: 3164, duration: 0.671s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.457 [33.000, 177.000], mean observation: 0.144 [0.000, 70.000], loss: 335.215041, mean_absolute_error: 2.137258, mean_q: 7.062722, mean_eps: 0.100000\n",
      " 113162/175000: episode: 3165, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 38.075 [20.000, 59.000], mean observation: 0.144 [0.000, 80.000], loss: 0.205128, mean_absolute_error: 0.487849, mean_q: 5.637808, mean_eps: 0.100000\n",
      " 113205/175000: episode: 3166, duration: 0.792s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 38.000 [38.000, 38.000], mean observation: 0.099 [0.000, 86.000], loss: 223.074321, mean_absolute_error: 1.744447, mean_q: 8.272706, mean_eps: 0.100000\n",
      " 113268/175000: episode: 3167, duration: 1.108s, episode steps: 63, steps per second: 57, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 69.714 [1.000, 177.000], mean observation: 0.436 [0.000, 126.000], loss: 58.751739, mean_absolute_error: 0.851348, mean_q: 6.719086, mean_eps: 0.100000\n",
      " 113310/175000: episode: 3168, duration: 0.794s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 30.500 [1.000, 188.000], mean observation: 0.170 [0.000, 84.000], loss: 286.939103, mean_absolute_error: 1.976940, mean_q: 7.609518, mean_eps: 0.100000\n",
      " 113353/175000: episode: 3169, duration: 0.891s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.442 [1.000, 188.000], mean observation: 0.415 [0.000, 86.000], loss: 0.176314, mean_absolute_error: 0.534849, mean_q: 5.832171, mean_eps: 0.100000\n",
      " 113400/175000: episode: 3170, duration: 0.950s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 102.553 [21.000, 206.000], mean observation: 0.589 [0.000, 94.000], loss: 0.561268, mean_absolute_error: 0.569443, mean_q: 5.936355, mean_eps: 0.100000\n",
      " 113438/175000: episode: 3171, duration: 0.771s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.579 [1.000, 206.000], mean observation: 0.272 [0.000, 76.000], loss: 0.265465, mean_absolute_error: 0.525828, mean_q: 5.694280, mean_eps: 0.100000\n",
      " 113474/175000: episode: 3172, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.333 [1.000, 181.000], mean observation: 0.196 [0.000, 72.000], loss: 0.147585, mean_absolute_error: 0.560927, mean_q: 5.721726, mean_eps: 0.100000\n",
      " 113517/175000: episode: 3173, duration: 0.772s, episode steps: 43, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.442 [1.000, 188.000], mean observation: 0.406 [0.000, 86.000], loss: 22.650054, mean_absolute_error: 0.919223, mean_q: 7.748175, mean_eps: 0.100000\n",
      " 113555/175000: episode: 3174, duration: 0.685s, episode steps: 38, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 42.474 [1.000, 212.000], mean observation: 0.151 [0.000, 76.000], loss: 24.527216, mean_absolute_error: 0.733358, mean_q: 5.613969, mean_eps: 0.100000\n",
      " 113585/175000: episode: 3175, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 53.500 [38.000, 148.000], mean observation: 0.079 [0.000, 60.000], loss: 2.538476, mean_absolute_error: 0.641410, mean_q: 5.411280, mean_eps: 0.100000\n",
      " 113616/175000: episode: 3176, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 44.452 [1.000, 72.000], mean observation: 0.096 [0.000, 62.000], loss: 0.201517, mean_absolute_error: 0.610147, mean_q: 5.470203, mean_eps: 0.100000\n",
      " 113643/175000: episode: 3177, duration: 0.541s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 45.481 [30.000, 68.000], mean observation: 0.085 [0.000, 54.000], loss: 0.750377, mean_absolute_error: 0.775559, mean_q: 7.148769, mean_eps: 0.100000\n",
      " 113699/175000: episode: 3178, duration: 1.008s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 107.339 [1.000, 188.000], mean observation: 0.605 [0.000, 112.000], loss: 371.869903, mean_absolute_error: 2.328253, mean_q: 6.304360, mean_eps: 0.100000\n",
      " 113726/175000: episode: 3179, duration: 0.493s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.741 [26.000, 177.000], mean observation: 0.197 [0.000, 54.000], loss: 0.208494, mean_absolute_error: 0.630295, mean_q: 5.979032, mean_eps: 0.100000\n",
      " 113756/175000: episode: 3180, duration: 0.564s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 61.800 [38.000, 194.000], mean observation: 0.122 [0.000, 60.000], loss: 0.094467, mean_absolute_error: 0.554767, mean_q: 5.526941, mean_eps: 0.100000\n",
      " 113779/175000: episode: 3181, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 81.130 [38.000, 205.000], mean observation: 0.129 [0.000, 46.000], loss: 665.606831, mean_absolute_error: 3.740754, mean_q: 7.951030, mean_eps: 0.100000\n",
      " 113831/175000: episode: 3182, duration: 0.930s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.500 [27.000, 223.000], mean observation: 0.820 [0.000, 104.000], loss: 0.217062, mean_absolute_error: 0.546334, mean_q: 5.465712, mean_eps: 0.100000\n",
      " 113860/175000: episode: 3183, duration: 0.589s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 57.000 [18.000, 221.000], mean observation: 0.141 [0.000, 58.000], loss: 805.660636, mean_absolute_error: 4.306668, mean_q: 7.308063, mean_eps: 0.100000\n",
      " 113907/175000: episode: 3184, duration: 0.978s, episode steps: 47, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 107.298 [18.000, 221.000], mean observation: 0.472 [0.000, 94.000], loss: 0.530235, mean_absolute_error: 0.548095, mean_q: 5.396431, mean_eps: 0.100000\n",
      " 113948/175000: episode: 3185, duration: 0.777s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.707 [5.000, 176.000], mean observation: 0.278 [0.000, 82.000], loss: 11.329474, mean_absolute_error: 0.592874, mean_q: 5.193910, mean_eps: 0.100000\n",
      " 113986/175000: episode: 3186, duration: 0.735s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.816 [5.000, 221.000], mean observation: 0.302 [0.000, 76.000], loss: 200.197448, mean_absolute_error: 1.533890, mean_q: 6.281808, mean_eps: 0.100000\n",
      " 114044/175000: episode: 3187, duration: 1.083s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 115.776 [3.000, 221.000], mean observation: 0.528 [0.000, 116.000], loss: 1.809197, mean_absolute_error: 0.594267, mean_q: 5.850010, mean_eps: 0.100000\n",
      " 114099/175000: episode: 3188, duration: 1.035s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 108.636 [0.000, 223.000], mean observation: 0.676 [0.000, 110.000], loss: 0.782905, mean_absolute_error: 0.581335, mean_q: 5.745673, mean_eps: 0.100000\n",
      " 114139/175000: episode: 3189, duration: 0.769s, episode steps: 40, steps per second: 52, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.075 [49.000, 178.000], mean observation: 0.468 [0.000, 80.000], loss: 1.847092, mean_absolute_error: 0.502579, mean_q: 4.972106, mean_eps: 0.100000\n",
      " 114180/175000: episode: 3190, duration: 0.782s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.049 [9.000, 212.000], mean observation: 0.421 [0.000, 82.000], loss: 84.181674, mean_absolute_error: 0.999743, mean_q: 6.143812, mean_eps: 0.100000\n",
      " 114209/175000: episode: 3191, duration: 0.569s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 138.103 [55.000, 215.000], mean observation: 0.270 [0.000, 58.000], loss: 0.152097, mean_absolute_error: 0.523498, mean_q: 5.092046, mean_eps: 0.100000\n",
      " 114243/175000: episode: 3192, duration: 0.645s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.471 [52.000, 221.000], mean observation: 0.399 [0.000, 68.000], loss: 99.153780, mean_absolute_error: 1.273274, mean_q: 8.071585, mean_eps: 0.100000\n",
      " 114285/175000: episode: 3193, duration: 0.822s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 140.548 [3.000, 217.000], mean observation: 0.359 [0.000, 84.000], loss: 0.309231, mean_absolute_error: 0.518911, mean_q: 4.974310, mean_eps: 0.100000\n",
      " 114309/175000: episode: 3194, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 146.250 [24.000, 222.000], mean observation: 0.228 [0.000, 48.000], loss: 0.096720, mean_absolute_error: 0.529159, mean_q: 5.298711, mean_eps: 0.100000\n",
      " 114348/175000: episode: 3195, duration: 0.750s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 141.949 [1.000, 222.000], mean observation: 0.279 [0.000, 78.000], loss: 19.279829, mean_absolute_error: 0.713260, mean_q: 6.232649, mean_eps: 0.100000\n",
      " 114377/175000: episode: 3196, duration: 0.616s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 117.690 [16.000, 222.000], mean observation: 0.307 [0.000, 58.000], loss: 0.755605, mean_absolute_error: 0.531463, mean_q: 5.614920, mean_eps: 0.100000\n",
      " 114408/175000: episode: 3197, duration: 0.560s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.774 [44.000, 215.000], mean observation: 0.170 [0.000, 62.000], loss: 131.062915, mean_absolute_error: 1.274157, mean_q: 7.250130, mean_eps: 0.100000\n",
      " 114445/175000: episode: 3198, duration: 0.865s, episode steps: 37, steps per second: 43, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 135.054 [1.000, 222.000], mean observation: 0.524 [0.000, 74.000], loss: 3.206900, mean_absolute_error: 0.639533, mean_q: 6.759229, mean_eps: 0.100000\n",
      " 114478/175000: episode: 3199, duration: 0.581s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 118.242 [44.000, 178.000], mean observation: 0.260 [0.000, 66.000], loss: 50.425157, mean_absolute_error: 0.922945, mean_q: 7.247900, mean_eps: 0.100000\n",
      " 114526/175000: episode: 3200, duration: 0.886s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.021 [27.000, 169.000], mean observation: 0.416 [0.000, 96.000], loss: 122.338362, mean_absolute_error: 1.292740, mean_q: 7.953076, mean_eps: 0.100000\n",
      " 114558/175000: episode: 3201, duration: 0.593s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.688 [6.000, 169.000], mean observation: 0.179 [0.000, 64.000], loss: 121.340917, mean_absolute_error: 1.221889, mean_q: 7.392531, mean_eps: 0.100000\n",
      " 114570/175000: episode: 3202, duration: 0.227s, episode steps: 12, steps per second: 53, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 107.000 [55.000, 224.000], mean observation: 0.097 [0.000, 24.000], loss: 0.125959, mean_absolute_error: 0.526508, mean_q: 5.759363, mean_eps: 0.100000\n",
      " 114605/175000: episode: 3203, duration: 0.653s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.543 [28.000, 169.000], mean observation: 0.230 [0.000, 70.000], loss: 53.081876, mean_absolute_error: 0.902941, mean_q: 7.351235, mean_eps: 0.100000\n",
      " 114644/175000: episode: 3204, duration: 0.804s, episode steps: 39, steps per second: 48, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.538 [29.000, 185.000], mean observation: 0.388 [0.000, 78.000], loss: 16.184331, mean_absolute_error: 0.737104, mean_q: 7.311669, mean_eps: 0.100000\n",
      " 114680/175000: episode: 3205, duration: 0.723s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.472 [22.000, 213.000], mean observation: 0.301 [0.000, 72.000], loss: 0.247713, mean_absolute_error: 0.507381, mean_q: 5.760230, mean_eps: 0.100000\n",
      " 114723/175000: episode: 3206, duration: 0.894s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.721 [38.000, 210.000], mean observation: 0.358 [0.000, 86.000], loss: 82.270686, mean_absolute_error: 1.127481, mean_q: 8.267279, mean_eps: 0.100000\n",
      " 114757/175000: episode: 3207, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 89.176 [18.000, 169.000], mean observation: 0.219 [0.000, 68.000], loss: 0.136463, mean_absolute_error: 0.528520, mean_q: 5.685848, mean_eps: 0.100000\n",
      " 114785/175000: episode: 3208, duration: 0.531s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 37.857 [1.000, 215.000], mean observation: 0.125 [0.000, 56.000], loss: 0.115184, mean_absolute_error: 0.558333, mean_q: 5.809732, mean_eps: 0.100000\n",
      " 114820/175000: episode: 3209, duration: 0.631s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 71.857 [1.000, 208.000], mean observation: 0.350 [0.000, 70.000], loss: 7.388201, mean_absolute_error: 0.747445, mean_q: 7.256573, mean_eps: 0.100000\n",
      " 114873/175000: episode: 3210, duration: 1.037s, episode steps: 53, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 93.604 [1.000, 222.000], mean observation: 0.514 [0.000, 106.000], loss: 319.012830, mean_absolute_error: 2.251450, mean_q: 8.537870, mean_eps: 0.100000\n",
      " 114914/175000: episode: 3211, duration: 0.707s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 76.146 [7.000, 208.000], mean observation: 0.387 [0.000, 82.000], loss: 0.106049, mean_absolute_error: 0.548817, mean_q: 5.699363, mean_eps: 0.100000\n",
      " 114956/175000: episode: 3212, duration: 0.804s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 101.405 [28.000, 211.000], mean observation: 0.250 [0.000, 84.000], loss: 0.090225, mean_absolute_error: 0.530581, mean_q: 5.693082, mean_eps: 0.100000\n",
      " 114998/175000: episode: 3213, duration: 0.795s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.048 [0.000, 211.000], mean observation: 0.395 [0.000, 84.000], loss: 0.217681, mean_absolute_error: 0.490940, mean_q: 5.463237, mean_eps: 0.100000\n",
      " 115041/175000: episode: 3214, duration: 0.832s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 100.977 [33.000, 218.000], mean observation: 0.570 [0.000, 86.000], loss: 0.367244, mean_absolute_error: 0.454369, mean_q: 5.517276, mean_eps: 0.100000\n",
      " 115090/175000: episode: 3215, duration: 0.873s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.612 [17.000, 207.000], mean observation: 0.407 [0.000, 98.000], loss: 26.503226, mean_absolute_error: 0.709191, mean_q: 6.603236, mean_eps: 0.100000\n",
      " 115115/175000: episode: 3216, duration: 0.464s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 158.040 [108.000, 211.000], mean observation: 0.184 [0.000, 50.000], loss: 31.419718, mean_absolute_error: 0.863561, mean_q: 7.686232, mean_eps: 0.100000\n",
      " 115141/175000: episode: 3217, duration: 0.495s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 163.692 [38.000, 211.000], mean observation: 0.221 [0.000, 52.000], loss: 0.303667, mean_absolute_error: 0.503391, mean_q: 5.710592, mean_eps: 0.100000\n",
      " 115177/175000: episode: 3218, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.472 [37.000, 211.000], mean observation: 0.288 [0.000, 72.000], loss: 137.703395, mean_absolute_error: 1.262475, mean_q: 7.269855, mean_eps: 0.100000\n",
      " 115215/175000: episode: 3219, duration: 0.706s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.579 [38.000, 222.000], mean observation: 0.592 [0.000, 76.000], loss: 22.830857, mean_absolute_error: 0.753241, mean_q: 7.009398, mean_eps: 0.100000\n",
      " 115261/175000: episode: 3220, duration: 0.846s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 83.804 [1.000, 211.000], mean observation: 0.233 [0.000, 92.000], loss: 0.517819, mean_absolute_error: 0.493042, mean_q: 5.275254, mean_eps: 0.100000\n",
      " 115317/175000: episode: 3221, duration: 1.010s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 113.482 [28.000, 207.000], mean observation: 0.504 [0.000, 112.000], loss: 98.210164, mean_absolute_error: 1.117040, mean_q: 7.434852, mean_eps: 0.100000\n",
      " 115359/175000: episode: 3222, duration: 0.750s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 136.262 [48.000, 207.000], mean observation: 0.527 [0.000, 84.000], loss: 36.315257, mean_absolute_error: 0.766165, mean_q: 6.759755, mean_eps: 0.100000\n",
      " 115391/175000: episode: 3223, duration: 0.643s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 164.656 [66.000, 215.000], mean observation: 0.304 [0.000, 64.000], loss: 0.214774, mean_absolute_error: 0.452843, mean_q: 5.510098, mean_eps: 0.100000\n",
      " 115429/175000: episode: 3224, duration: 0.723s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.842 [68.000, 215.000], mean observation: 0.241 [0.000, 76.000], loss: 172.440147, mean_absolute_error: 1.499797, mean_q: 8.218418, mean_eps: 0.100000\n",
      " 115462/175000: episode: 3225, duration: 0.604s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 95.485 [17.000, 221.000], mean observation: 0.325 [0.000, 66.000], loss: 0.095884, mean_absolute_error: 0.512726, mean_q: 5.873794, mean_eps: 0.100000\n",
      " 115492/175000: episode: 3226, duration: 0.563s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 142.867 [17.000, 203.000], mean observation: 0.243 [0.000, 60.000], loss: 0.123092, mean_absolute_error: 0.507869, mean_q: 6.011984, mean_eps: 0.100000\n",
      " 115535/175000: episode: 3227, duration: 0.809s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.791 [6.000, 202.000], mean observation: 0.623 [0.000, 86.000], loss: 0.138960, mean_absolute_error: 0.478928, mean_q: 5.690228, mean_eps: 0.100000\n",
      " 115593/175000: episode: 3228, duration: 1.083s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 93.776 [9.000, 216.000], mean observation: 0.773 [0.000, 116.000], loss: 0.098372, mean_absolute_error: 0.475647, mean_q: 5.733857, mean_eps: 0.100000\n",
      " 115648/175000: episode: 3229, duration: 0.992s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 71.127 [1.000, 209.000], mean observation: 0.754 [0.000, 110.000], loss: 0.096235, mean_absolute_error: 0.478875, mean_q: 5.828708, mean_eps: 0.100000\n",
      " 115686/175000: episode: 3230, duration: 0.718s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 149.105 [6.000, 207.000], mean observation: 0.498 [0.000, 76.000], loss: 0.121052, mean_absolute_error: 0.499402, mean_q: 5.923804, mean_eps: 0.100000\n",
      " 115723/175000: episode: 3231, duration: 0.671s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 86.649 [3.000, 193.000], mean observation: 0.324 [0.000, 74.000], loss: 182.332557, mean_absolute_error: 1.610165, mean_q: 8.758766, mean_eps: 0.100000\n",
      " 115767/175000: episode: 3232, duration: 0.786s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.136 [1.000, 215.000], mean observation: 0.690 [0.000, 88.000], loss: 0.149614, mean_absolute_error: 0.458760, mean_q: 5.948889, mean_eps: 0.100000\n",
      " 115813/175000: episode: 3233, duration: 0.847s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 63.413 [8.000, 208.000], mean observation: 0.459 [0.000, 92.000], loss: 0.155289, mean_absolute_error: 0.478155, mean_q: 6.083828, mean_eps: 0.100000\n",
      " 115848/175000: episode: 3234, duration: 0.690s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 103.057 [6.000, 222.000], mean observation: 0.253 [0.000, 70.000], loss: 0.230681, mean_absolute_error: 0.479589, mean_q: 5.994357, mean_eps: 0.100000\n",
      " 115906/175000: episode: 3235, duration: 1.054s, episode steps: 58, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 89.138 [12.000, 201.000], mean observation: 0.903 [0.000, 116.000], loss: 36.009012, mean_absolute_error: 0.816735, mean_q: 7.442138, mean_eps: 0.100000\n",
      " 115939/175000: episode: 3236, duration: 0.598s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.000 [38.000, 224.000], mean observation: 0.169 [0.000, 66.000], loss: 0.105700, mean_absolute_error: 0.472318, mean_q: 5.772073, mean_eps: 0.100000\n",
      " 115963/175000: episode: 3237, duration: 0.440s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 36.375 [28.000, 179.000], mean observation: 0.115 [0.000, 48.000], loss: 0.106153, mean_absolute_error: 0.509659, mean_q: 5.978981, mean_eps: 0.100000\n",
      " 116003/175000: episode: 3238, duration: 0.759s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 92.025 [28.000, 171.000], mean observation: 0.428 [0.000, 80.000], loss: 58.942339, mean_absolute_error: 0.958593, mean_q: 7.572701, mean_eps: 0.100000\n",
      " 116043/175000: episode: 3239, duration: 0.720s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 90.075 [0.000, 178.000], mean observation: 0.326 [0.000, 80.000], loss: 229.046858, mean_absolute_error: 1.962939, mean_q: 9.743101, mean_eps: 0.100000\n",
      " 116079/175000: episode: 3240, duration: 0.682s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [18.000, 188.000], mean observation: 0.347 [0.000, 72.000], loss: 0.457816, mean_absolute_error: 0.566886, mean_q: 6.148844, mean_eps: 0.100000\n",
      " 116114/175000: episode: 3241, duration: 0.663s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.600 [23.000, 186.000], mean observation: 0.301 [0.000, 70.000], loss: 0.167167, mean_absolute_error: 0.540497, mean_q: 5.909683, mean_eps: 0.100000\n",
      " 116170/175000: episode: 3242, duration: 1.005s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 128.286 [14.000, 214.000], mean observation: 0.530 [0.000, 112.000], loss: 129.011281, mean_absolute_error: 1.418308, mean_q: 9.055720, mean_eps: 0.100000\n",
      " 116197/175000: episode: 3243, duration: 0.519s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 66.519 [0.000, 169.000], mean observation: 0.171 [0.000, 54.000], loss: 23.118690, mean_absolute_error: 0.596476, mean_q: 5.906392, mean_eps: 0.100000\n",
      " 116226/175000: episode: 3244, duration: 0.513s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 47.310 [18.000, 137.000], mean observation: 0.169 [0.000, 58.000], loss: 0.143400, mean_absolute_error: 0.493248, mean_q: 6.153224, mean_eps: 0.100000\n",
      " 116247/175000: episode: 3245, duration: 0.368s, episode steps: 21, steps per second: 57, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 58.714 [38.000, 139.000], mean observation: 0.077 [0.000, 42.000], loss: 0.161739, mean_absolute_error: 0.470309, mean_q: 5.953909, mean_eps: 0.100000\n",
      " 116271/175000: episode: 3246, duration: 0.452s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 67.417 [0.000, 217.000], mean observation: 0.179 [0.000, 48.000], loss: 0.167603, mean_absolute_error: 0.482884, mean_q: 5.963179, mean_eps: 0.100000\n",
      " 116305/175000: episode: 3247, duration: 0.670s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 63.618 [6.000, 176.000], mean observation: 0.332 [0.000, 68.000], loss: 65.135093, mean_absolute_error: 0.914321, mean_q: 7.467612, mean_eps: 0.100000\n",
      " 116344/175000: episode: 3248, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 53.077 [6.000, 147.000], mean observation: 0.308 [0.000, 78.000], loss: 33.253471, mean_absolute_error: 0.763863, mean_q: 7.337153, mean_eps: 0.100000\n",
      " 116387/175000: episode: 3249, duration: 0.806s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.163 [4.000, 221.000], mean observation: 0.436 [0.000, 86.000], loss: 19.836638, mean_absolute_error: 0.712827, mean_q: 7.562065, mean_eps: 0.100000\n",
      " 116416/175000: episode: 3250, duration: 0.578s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 97.931 [6.000, 158.000], mean observation: 0.156 [0.000, 58.000], loss: 435.316556, mean_absolute_error: 3.073104, mean_q: 12.361218, mean_eps: 0.100000\n",
      " 116436/175000: episode: 3251, duration: 0.462s, episode steps: 20, steps per second: 43, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 89.300 [38.000, 169.000], mean observation: 0.086 [0.000, 40.000], loss: 0.163590, mean_absolute_error: 0.494123, mean_q: 6.246423, mean_eps: 0.100000\n",
      " 116474/175000: episode: 3252, duration: 0.748s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.579 [2.000, 191.000], mean observation: 0.244 [0.000, 76.000], loss: 0.181625, mean_absolute_error: 0.512114, mean_q: 6.509327, mean_eps: 0.100000\n",
      " 116503/175000: episode: 3253, duration: 0.507s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.552 [38.000, 139.000], mean observation: 0.166 [0.000, 58.000], loss: 10.318951, mean_absolute_error: 0.985687, mean_q: 10.618787, mean_eps: 0.100000\n",
      " 116541/175000: episode: 3254, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.763 [12.000, 215.000], mean observation: 0.360 [0.000, 76.000], loss: 9.721661, mean_absolute_error: 0.739624, mean_q: 8.386594, mean_eps: 0.100000\n",
      " 116596/175000: episode: 3255, duration: 1.033s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.127 [1.000, 217.000], mean observation: 0.494 [0.000, 110.000], loss: 149.768785, mean_absolute_error: 1.291587, mean_q: 7.866825, mean_eps: 0.100000\n",
      " 116625/175000: episode: 3256, duration: 0.587s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 93.138 [21.000, 208.000], mean observation: 0.176 [0.000, 58.000], loss: 6.624415, mean_absolute_error: 0.970560, mean_q: 11.242095, mean_eps: 0.100000\n",
      " 116652/175000: episode: 3257, duration: 0.485s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 119.963 [38.000, 176.000], mean observation: 0.157 [0.000, 54.000], loss: 0.152503, mean_absolute_error: 0.453549, mean_q: 6.505231, mean_eps: 0.100000\n",
      " 116692/175000: episode: 3258, duration: 0.842s, episode steps: 40, steps per second: 48, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 130.325 [38.000, 221.000], mean observation: 0.307 [0.000, 80.000], loss: 153.868190, mean_absolute_error: 1.276397, mean_q: 7.587237, mean_eps: 0.100000\n",
      " 116719/175000: episode: 3259, duration: 0.557s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 120.148 [44.000, 176.000], mean observation: 0.172 [0.000, 54.000], loss: 3.713821, mean_absolute_error: 0.683645, mean_q: 8.190705, mean_eps: 0.100000\n",
      " 116769/175000: episode: 3260, duration: 0.933s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 68.040 [28.000, 176.000], mean observation: 0.376 [0.000, 100.000], loss: 0.206350, mean_absolute_error: 0.481137, mean_q: 6.480811, mean_eps: 0.100000\n",
      " 116813/175000: episode: 3261, duration: 0.820s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.227 [28.000, 158.000], mean observation: 0.246 [0.000, 88.000], loss: 0.184860, mean_absolute_error: 0.457523, mean_q: 6.172488, mean_eps: 0.100000\n",
      " 116859/175000: episode: 3262, duration: 0.818s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.348 [38.000, 208.000], mean observation: 0.224 [0.000, 92.000], loss: 0.160373, mean_absolute_error: 0.488552, mean_q: 6.266604, mean_eps: 0.100000\n",
      " 116889/175000: episode: 3263, duration: 0.604s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 76.567 [28.000, 192.000], mean observation: 0.093 [0.000, 60.000], loss: 0.152067, mean_absolute_error: 0.488818, mean_q: 5.894333, mean_eps: 0.100000\n",
      " 116932/175000: episode: 3264, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 70.651 [25.000, 169.000], mean observation: 0.268 [0.000, 86.000], loss: 3.959421, mean_absolute_error: 0.685276, mean_q: 7.430901, mean_eps: 0.100000\n",
      " 116974/175000: episode: 3265, duration: 0.800s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.905 [1.000, 222.000], mean observation: 0.414 [0.000, 84.000], loss: 18.980164, mean_absolute_error: 0.786660, mean_q: 7.389423, mean_eps: 0.100000\n",
      " 117007/175000: episode: 3266, duration: 0.571s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 73.152 [37.000, 156.000], mean observation: 0.130 [0.000, 66.000], loss: 0.124547, mean_absolute_error: 0.613770, mean_q: 6.512831, mean_eps: 0.100000\n",
      " 117064/175000: episode: 3267, duration: 1.051s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 49.579 [1.000, 200.000], mean observation: 0.265 [0.000, 114.000], loss: 25.097615, mean_absolute_error: 0.794038, mean_q: 7.256647, mean_eps: 0.100000\n",
      " 117113/175000: episode: 3268, duration: 0.932s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.082 [1.000, 218.000], mean observation: 0.356 [0.000, 98.000], loss: 59.394876, mean_absolute_error: 0.948604, mean_q: 7.076615, mean_eps: 0.100000\n",
      " 117121/175000: episode: 3269, duration: 0.147s, episode steps: 8, steps per second: 54, episode reward: -1.000, mean reward: -0.125 [-1.000, 0.000], mean action: 38.000 [38.000, 38.000], mean observation: 0.021 [0.000, 16.000], loss: 0.116048, mean_absolute_error: 0.608407, mean_q: 6.209617, mean_eps: 0.100000\n",
      " 117161/175000: episode: 3270, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 91.150 [1.000, 221.000], mean observation: 0.247 [0.000, 80.000], loss: 47.480322, mean_absolute_error: 1.114661, mean_q: 9.090202, mean_eps: 0.100000\n",
      " 117198/175000: episode: 3271, duration: 0.673s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 100.514 [1.000, 215.000], mean observation: 0.368 [0.000, 74.000], loss: 0.124110, mean_absolute_error: 0.593990, mean_q: 5.933127, mean_eps: 0.100000\n",
      " 117234/175000: episode: 3272, duration: 0.681s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.000 [1.000, 218.000], mean observation: 0.352 [0.000, 72.000], loss: 0.158817, mean_absolute_error: 0.564964, mean_q: 5.746640, mean_eps: 0.100000\n",
      " 117253/175000: episode: 3273, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 67.158 [1.000, 221.000], mean observation: 0.146 [0.000, 38.000], loss: 6.247247, mean_absolute_error: 0.860880, mean_q: 8.614882, mean_eps: 0.100000\n",
      " 117289/175000: episode: 3274, duration: 0.674s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 62.667 [1.000, 203.000], mean observation: 0.385 [0.000, 72.000], loss: 0.183592, mean_absolute_error: 0.548573, mean_q: 5.956403, mean_eps: 0.100000\n",
      " 117336/175000: episode: 3275, duration: 0.840s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 70.447 [1.000, 210.000], mean observation: 0.455 [0.000, 94.000], loss: 0.148509, mean_absolute_error: 0.556621, mean_q: 5.960176, mean_eps: 0.100000\n",
      " 117376/175000: episode: 3276, duration: 0.806s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 85.925 [0.000, 203.000], mean observation: 0.326 [0.000, 80.000], loss: 0.185240, mean_absolute_error: 0.532915, mean_q: 6.034584, mean_eps: 0.100000\n",
      " 117406/175000: episode: 3277, duration: 0.588s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 87.833 [38.000, 203.000], mean observation: 0.313 [0.000, 60.000], loss: 84.941000, mean_absolute_error: 1.047536, mean_q: 7.525250, mean_eps: 0.100000\n",
      " 117447/175000: episode: 3278, duration: 0.765s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 145.024 [41.000, 216.000], mean observation: 0.328 [0.000, 82.000], loss: 0.137525, mean_absolute_error: 0.483292, mean_q: 5.837098, mean_eps: 0.100000\n",
      " 117480/175000: episode: 3279, duration: 0.643s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 141.788 [31.000, 203.000], mean observation: 0.305 [0.000, 66.000], loss: 0.438656, mean_absolute_error: 0.672185, mean_q: 7.745419, mean_eps: 0.100000\n",
      " 117513/175000: episode: 3280, duration: 0.624s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 147.970 [55.000, 203.000], mean observation: 0.219 [0.000, 66.000], loss: 11.650019, mean_absolute_error: 0.556774, mean_q: 5.949332, mean_eps: 0.100000\n",
      " 117558/175000: episode: 3281, duration: 0.825s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.356 [1.000, 211.000], mean observation: 0.580 [0.000, 90.000], loss: 1.800161, mean_absolute_error: 0.642291, mean_q: 7.390762, mean_eps: 0.100000\n",
      " 117599/175000: episode: 3282, duration: 0.722s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.244 [1.000, 203.000], mean observation: 0.445 [0.000, 82.000], loss: 0.478640, mean_absolute_error: 0.666420, mean_q: 7.432093, mean_eps: 0.100000\n",
      " 117657/175000: episode: 3283, duration: 1.067s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 102.224 [28.000, 203.000], mean observation: 0.805 [0.000, 116.000], loss: 11.946080, mean_absolute_error: 0.618706, mean_q: 6.031122, mean_eps: 0.100000\n",
      " 117688/175000: episode: 3284, duration: 0.592s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 66.839 [9.000, 170.000], mean observation: 0.325 [0.000, 62.000], loss: 172.425897, mean_absolute_error: 1.539059, mean_q: 8.259335, mean_eps: 0.100000\n",
      " 117721/175000: episode: 3285, duration: 0.653s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 103.455 [7.000, 222.000], mean observation: 0.213 [0.000, 66.000], loss: 0.154990, mean_absolute_error: 0.591803, mean_q: 6.428162, mean_eps: 0.100000\n",
      " 117763/175000: episode: 3286, duration: 0.785s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 62.857 [28.000, 222.000], mean observation: 0.203 [0.000, 84.000], loss: 3.002248, mean_absolute_error: 0.584146, mean_q: 6.268770, mean_eps: 0.100000\n",
      " 117800/175000: episode: 3287, duration: 0.735s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 58.946 [9.000, 158.000], mean observation: 0.207 [0.000, 74.000], loss: 0.180342, mean_absolute_error: 0.547890, mean_q: 5.948199, mean_eps: 0.100000\n",
      " 117833/175000: episode: 3288, duration: 0.621s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.273 [1.000, 158.000], mean observation: 0.283 [0.000, 66.000], loss: 0.180754, mean_absolute_error: 0.557039, mean_q: 5.945328, mean_eps: 0.100000\n",
      " 117880/175000: episode: 3289, duration: 0.906s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 81.681 [1.000, 221.000], mean observation: 0.461 [0.000, 94.000], loss: 0.154909, mean_absolute_error: 0.552238, mean_q: 6.033552, mean_eps: 0.100000\n",
      " 117907/175000: episode: 3290, duration: 0.525s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 7.852 [1.000, 149.000], mean observation: 0.104 [0.000, 54.000], loss: 52.932893, mean_absolute_error: 0.989749, mean_q: 8.444560, mean_eps: 0.100000\n",
      " 117940/175000: episode: 3291, duration: 0.637s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 66.879 [1.000, 212.000], mean observation: 0.154 [0.000, 66.000], loss: 29.468166, mean_absolute_error: 0.880923, mean_q: 8.246283, mean_eps: 0.100000\n",
      " 117967/175000: episode: 3292, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 115.778 [27.000, 208.000], mean observation: 0.230 [0.000, 54.000], loss: 3.485041, mean_absolute_error: 0.767560, mean_q: 8.285068, mean_eps: 0.100000\n",
      " 117991/175000: episode: 3293, duration: 0.433s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 100.625 [11.000, 208.000], mean observation: 0.178 [0.000, 48.000], loss: 0.219541, mean_absolute_error: 0.532346, mean_q: 6.321199, mean_eps: 0.100000\n",
      " 118030/175000: episode: 3294, duration: 0.758s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.897 [38.000, 208.000], mean observation: 0.118 [0.000, 78.000], loss: 2.129223, mean_absolute_error: 0.822221, mean_q: 9.197016, mean_eps: 0.100000\n",
      " 118065/175000: episode: 3295, duration: 0.642s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.743 [15.000, 210.000], mean observation: 0.124 [0.000, 70.000], loss: 1.987783, mean_absolute_error: 0.631946, mean_q: 7.174042, mean_eps: 0.100000\n",
      " 118112/175000: episode: 3296, duration: 0.946s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.957 [1.000, 216.000], mean observation: 0.304 [0.000, 94.000], loss: 0.150119, mean_absolute_error: 0.513202, mean_q: 6.444194, mean_eps: 0.100000\n",
      " 118153/175000: episode: 3297, duration: 0.752s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 27.439 [1.000, 207.000], mean observation: 0.118 [0.000, 82.000], loss: 0.115965, mean_absolute_error: 0.489628, mean_q: 6.408526, mean_eps: 0.100000\n",
      " 118190/175000: episode: 3298, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 62.919 [1.000, 210.000], mean observation: 0.275 [0.000, 74.000], loss: 0.124674, mean_absolute_error: 0.447480, mean_q: 6.164605, mean_eps: 0.100000\n",
      " 118240/175000: episode: 3299, duration: 0.924s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 56.420 [1.000, 224.000], mean observation: 0.456 [0.000, 100.000], loss: 258.970974, mean_absolute_error: 1.681504, mean_q: 7.053914, mean_eps: 0.100000\n",
      " 118278/175000: episode: 3300, duration: 0.718s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 130.921 [24.000, 214.000], mean observation: 0.536 [0.000, 76.000], loss: 1.082704, mean_absolute_error: 0.448212, mean_q: 6.349963, mean_eps: 0.100000\n",
      " 118305/175000: episode: 3301, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.064 [0.000, 54.000], loss: 1.131424, mean_absolute_error: 0.413571, mean_q: 5.890719, mean_eps: 0.100000\n",
      " 118343/175000: episode: 3302, duration: 0.667s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 28.211 [1.000, 201.000], mean observation: 0.144 [0.000, 76.000], loss: 318.810911, mean_absolute_error: 1.879234, mean_q: 6.384884, mean_eps: 0.100000\n",
      " 118381/175000: episode: 3303, duration: 0.743s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.763 [1.000, 195.000], mean observation: 0.287 [0.000, 76.000], loss: 6.619569, mean_absolute_error: 0.477303, mean_q: 6.397955, mean_eps: 0.100000\n",
      " 118405/175000: episode: 3304, duration: 0.452s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 75.375 [30.000, 113.000], mean observation: 0.083 [0.000, 48.000], loss: 21.179245, mean_absolute_error: 0.615877, mean_q: 6.707220, mean_eps: 0.100000\n",
      " 118439/175000: episode: 3305, duration: 0.617s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.000 [63.000, 224.000], mean observation: 0.148 [0.000, 68.000], loss: 8.495500, mean_absolute_error: 0.607012, mean_q: 6.442679, mean_eps: 0.100000\n",
      " 118468/175000: episode: 3306, duration: 0.591s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 106.552 [39.000, 191.000], mean observation: 0.196 [0.000, 58.000], loss: 2.204389, mean_absolute_error: 0.634708, mean_q: 6.306974, mean_eps: 0.100000\n",
      " 118506/175000: episode: 3307, duration: 0.731s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 83.474 [27.000, 191.000], mean observation: 0.348 [0.000, 76.000], loss: 668.859998, mean_absolute_error: 3.752916, mean_q: 7.154589, mean_eps: 0.100000\n",
      " 118538/175000: episode: 3308, duration: 0.574s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 90.281 [27.000, 211.000], mean observation: 0.350 [0.000, 64.000], loss: 5.985994, mean_absolute_error: 0.763126, mean_q: 6.224617, mean_eps: 0.100000\n",
      " 118572/175000: episode: 3309, duration: 0.674s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 81.588 [8.000, 188.000], mean observation: 0.246 [0.000, 68.000], loss: 10.627077, mean_absolute_error: 0.797640, mean_q: 6.153581, mean_eps: 0.100000\n",
      " 118605/175000: episode: 3310, duration: 0.668s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 86.030 [27.000, 188.000], mean observation: 0.249 [0.000, 66.000], loss: 3.570109, mean_absolute_error: 0.775424, mean_q: 5.816463, mean_eps: 0.100000\n",
      " 118638/175000: episode: 3311, duration: 0.567s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.576 [29.000, 216.000], mean observation: 0.185 [0.000, 66.000], loss: 1.265554, mean_absolute_error: 0.725357, mean_q: 5.394679, mean_eps: 0.100000\n",
      " 118663/175000: episode: 3312, duration: 0.460s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 109.000 [34.000, 215.000], mean observation: 0.200 [0.000, 50.000], loss: 1.221806, mean_absolute_error: 0.779014, mean_q: 5.691370, mean_eps: 0.100000\n",
      " 118683/175000: episode: 3313, duration: 0.369s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 71.850 [1.000, 166.000], mean observation: 0.147 [0.000, 40.000], loss: 3.006384, mean_absolute_error: 0.776822, mean_q: 5.549116, mean_eps: 0.100000\n",
      " 118726/175000: episode: 3314, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 50.023 [1.000, 166.000], mean observation: 0.370 [0.000, 86.000], loss: 2.006672, mean_absolute_error: 0.805528, mean_q: 5.482223, mean_eps: 0.100000\n",
      " 118755/175000: episode: 3315, duration: 0.564s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 63.138 [1.000, 222.000], mean observation: 0.188 [0.000, 58.000], loss: 1.428780, mean_absolute_error: 0.808000, mean_q: 5.420827, mean_eps: 0.100000\n",
      " 118777/175000: episode: 3316, duration: 0.458s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 75.773 [1.000, 181.000], mean observation: 0.142 [0.000, 44.000], loss: 1.057858, mean_absolute_error: 0.745064, mean_q: 5.125698, mean_eps: 0.100000\n",
      " 118817/175000: episode: 3317, duration: 0.792s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 92.200 [3.000, 218.000], mean observation: 0.469 [0.000, 80.000], loss: 0.940554, mean_absolute_error: 0.755544, mean_q: 5.187999, mean_eps: 0.100000\n",
      " 118857/175000: episode: 3318, duration: 0.748s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 72.400 [7.000, 200.000], mean observation: 0.297 [0.000, 80.000], loss: 67.840529, mean_absolute_error: 1.176318, mean_q: 6.414129, mean_eps: 0.100000\n",
      " 118895/175000: episode: 3319, duration: 0.650s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.395 [7.000, 170.000], mean observation: 0.255 [0.000, 76.000], loss: 0.644831, mean_absolute_error: 0.745032, mean_q: 4.940937, mean_eps: 0.100000\n",
      " 118932/175000: episode: 3320, duration: 0.713s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 73.541 [3.000, 170.000], mean observation: 0.254 [0.000, 74.000], loss: 0.417646, mean_absolute_error: 0.734489, mean_q: 4.696008, mean_eps: 0.100000\n",
      " 118977/175000: episode: 3321, duration: 0.867s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 79.200 [7.000, 183.000], mean observation: 0.324 [0.000, 90.000], loss: 0.665307, mean_absolute_error: 0.710229, mean_q: 4.646764, mean_eps: 0.100000\n",
      " 119011/175000: episode: 3322, duration: 0.620s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.676 [32.000, 220.000], mean observation: 0.202 [0.000, 68.000], loss: 22.134166, mean_absolute_error: 0.778510, mean_q: 4.375026, mean_eps: 0.100000\n",
      " 119050/175000: episode: 3323, duration: 0.738s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.385 [1.000, 191.000], mean observation: 0.487 [0.000, 78.000], loss: 68.140804, mean_absolute_error: 0.971000, mean_q: 4.428814, mean_eps: 0.100000\n",
      " 119094/175000: episode: 3324, duration: 0.821s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.545 [6.000, 189.000], mean observation: 0.226 [0.000, 88.000], loss: 0.554244, mean_absolute_error: 0.659920, mean_q: 4.386712, mean_eps: 0.100000\n",
      " 119121/175000: episode: 3325, duration: 0.526s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 100.741 [7.000, 209.000], mean observation: 0.173 [0.000, 54.000], loss: 0.564671, mean_absolute_error: 0.654452, mean_q: 4.312640, mean_eps: 0.100000\n",
      " 119148/175000: episode: 3326, duration: 0.591s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 104.481 [7.000, 209.000], mean observation: 0.248 [0.000, 54.000], loss: 26.509436, mean_absolute_error: 0.766703, mean_q: 4.657492, mean_eps: 0.100000\n",
      " 119173/175000: episode: 3327, duration: 0.519s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.080 [17.000, 223.000], mean observation: 0.145 [0.000, 50.000], loss: 0.921607, mean_absolute_error: 0.653938, mean_q: 4.565216, mean_eps: 0.100000\n",
      " 119219/175000: episode: 3328, duration: 0.773s, episode steps: 46, steps per second: 60, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 108.957 [0.000, 223.000], mean observation: 0.523 [0.000, 92.000], loss: 0.883459, mean_absolute_error: 0.649495, mean_q: 4.538515, mean_eps: 0.100000\n",
      " 119238/175000: episode: 3329, duration: 0.431s, episode steps: 19, steps per second: 44, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 111.053 [46.000, 170.000], mean observation: 0.074 [0.000, 38.000], loss: 1365.418111, mean_absolute_error: 7.013194, mean_q: 7.352099, mean_eps: 0.100000\n",
      " 119264/175000: episode: 3330, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 103.615 [7.000, 170.000], mean observation: 0.155 [0.000, 52.000], loss: 0.707438, mean_absolute_error: 0.657192, mean_q: 4.343320, mean_eps: 0.100000\n",
      " 119280/175000: episode: 3331, duration: 0.393s, episode steps: 16, steps per second: 41, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 112.438 [46.000, 208.000], mean observation: 0.090 [0.000, 32.000], loss: 0.496455, mean_absolute_error: 0.655836, mean_q: 4.390616, mean_eps: 0.100000\n",
      " 119313/175000: episode: 3332, duration: 0.657s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 108.242 [9.000, 191.000], mean observation: 0.183 [0.000, 66.000], loss: 248.924780, mean_absolute_error: 1.897555, mean_q: 5.788568, mean_eps: 0.100000\n",
      " 119344/175000: episode: 3333, duration: 0.574s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 107.581 [8.000, 218.000], mean observation: 0.219 [0.000, 62.000], loss: 0.524322, mean_absolute_error: 0.654648, mean_q: 4.282407, mean_eps: 0.100000\n",
      " 119403/175000: episode: 3334, duration: 1.148s, episode steps: 59, steps per second: 51, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 84.542 [8.000, 223.000], mean observation: 0.738 [0.000, 118.000], loss: 14.646413, mean_absolute_error: 0.695605, mean_q: 4.229749, mean_eps: 0.100000\n",
      " 119438/175000: episode: 3335, duration: 0.662s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 61.829 [8.000, 213.000], mean observation: 0.271 [0.000, 70.000], loss: 0.187084, mean_absolute_error: 0.621680, mean_q: 3.929694, mean_eps: 0.100000\n",
      " 119477/175000: episode: 3336, duration: 0.718s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.846 [8.000, 191.000], mean observation: 0.312 [0.000, 78.000], loss: 3.691571, mean_absolute_error: 0.646002, mean_q: 3.842233, mean_eps: 0.100000\n",
      " 119513/175000: episode: 3337, duration: 0.625s, episode steps: 36, steps per second: 58, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 102.583 [9.000, 222.000], mean observation: 0.215 [0.000, 72.000], loss: 1.289769, mean_absolute_error: 0.761848, mean_q: 5.513056, mean_eps: 0.100000\n",
      " 119520/175000: episode: 3338, duration: 0.137s, episode steps: 7, steps per second: 51, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 132.143 [49.000, 146.000], mean observation: 0.027 [0.000, 14.000], loss: 0.081239, mean_absolute_error: 0.587342, mean_q: 3.828480, mean_eps: 0.100000\n",
      " 119538/175000: episode: 3339, duration: 0.389s, episode steps: 18, steps per second: 46, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 129.611 [38.000, 222.000], mean observation: 0.067 [0.000, 36.000], loss: 7.572870, mean_absolute_error: 0.624519, mean_q: 4.046635, mean_eps: 0.100000\n",
      " 119567/175000: episode: 3340, duration: 0.528s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 125.207 [38.000, 222.000], mean observation: 0.166 [0.000, 58.000], loss: 1.288277, mean_absolute_error: 0.592216, mean_q: 4.092980, mean_eps: 0.100000\n",
      " 119622/175000: episode: 3341, duration: 1.057s, episode steps: 55, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 101.836 [6.000, 222.000], mean observation: 0.546 [0.000, 110.000], loss: 0.416693, mean_absolute_error: 0.594023, mean_q: 4.051154, mean_eps: 0.100000\n",
      " 119652/175000: episode: 3342, duration: 0.569s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 119.000 [1.000, 222.000], mean observation: 0.326 [0.000, 60.000], loss: 0.172841, mean_absolute_error: 0.571312, mean_q: 3.953138, mean_eps: 0.100000\n",
      " 119686/175000: episode: 3343, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.471 [9.000, 184.000], mean observation: 0.214 [0.000, 68.000], loss: 0.297501, mean_absolute_error: 0.573151, mean_q: 4.085296, mean_eps: 0.100000\n",
      " 119723/175000: episode: 3344, duration: 0.669s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 101.892 [23.000, 220.000], mean observation: 0.256 [0.000, 74.000], loss: 1.045157, mean_absolute_error: 0.558823, mean_q: 4.186719, mean_eps: 0.100000\n",
      " 119764/175000: episode: 3345, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.122 [38.000, 183.000], mean observation: 0.224 [0.000, 82.000], loss: 0.182096, mean_absolute_error: 0.548789, mean_q: 4.189117, mean_eps: 0.100000\n",
      " 119816/175000: episode: 3346, duration: 1.024s, episode steps: 52, steps per second: 51, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.000 [38.000, 183.000], mean observation: 0.492 [0.000, 104.000], loss: 0.792038, mean_absolute_error: 0.547980, mean_q: 4.212227, mean_eps: 0.100000\n",
      " 119832/175000: episode: 3347, duration: 0.377s, episode steps: 16, steps per second: 42, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 132.875 [36.000, 186.000], mean observation: 0.108 [0.000, 32.000], loss: 1467.675163, mean_absolute_error: 7.302837, mean_q: 6.746137, mean_eps: 0.100000\n",
      " 119857/175000: episode: 3348, duration: 0.501s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 146.520 [67.000, 208.000], mean observation: 0.079 [0.000, 50.000], loss: 18.850626, mean_absolute_error: 0.618406, mean_q: 4.428603, mean_eps: 0.100000\n",
      " 119877/175000: episode: 3349, duration: 0.360s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 113.550 [17.000, 221.000], mean observation: 0.159 [0.000, 40.000], loss: 3.789884, mean_absolute_error: 0.563490, mean_q: 4.559057, mean_eps: 0.100000\n",
      " 119912/175000: episode: 3350, duration: 0.634s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 133.114 [59.000, 178.000], mean observation: 0.241 [0.000, 70.000], loss: 171.177145, mean_absolute_error: 1.325556, mean_q: 4.719632, mean_eps: 0.100000\n",
      " 119939/175000: episode: 3351, duration: 0.514s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 93.185 [59.000, 178.000], mean observation: 0.262 [0.000, 54.000], loss: 6.273500, mean_absolute_error: 0.606752, mean_q: 4.685559, mean_eps: 0.100000\n",
      " 119963/175000: episode: 3352, duration: 0.456s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 155.083 [43.000, 178.000], mean observation: 0.119 [0.000, 48.000], loss: 9.281392, mean_absolute_error: 0.623442, mean_q: 4.647483, mean_eps: 0.100000\n",
      " 120002/175000: episode: 3353, duration: 0.764s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 67.103 [28.000, 195.000], mean observation: 0.405 [0.000, 78.000], loss: 11.120242, mean_absolute_error: 0.652312, mean_q: 4.683188, mean_eps: 0.100000\n",
      " 120040/175000: episode: 3354, duration: 0.743s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 66.158 [28.000, 216.000], mean observation: 0.314 [0.000, 76.000], loss: 7.058369, mean_absolute_error: 0.660669, mean_q: 4.957942, mean_eps: 0.100000\n",
      " 120066/175000: episode: 3355, duration: 0.545s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 69.538 [28.000, 216.000], mean observation: 0.251 [0.000, 52.000], loss: 1217.298580, mean_absolute_error: 6.253443, mean_q: 7.046480, mean_eps: 0.100000\n",
      " 120077/175000: episode: 3356, duration: 0.216s, episode steps: 11, steps per second: 51, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 112.818 [10.000, 216.000], mean observation: 0.074 [0.000, 22.000], loss: 5.262609, mean_absolute_error: 0.669109, mean_q: 5.307583, mean_eps: 0.100000\n",
      " 120141/175000: episode: 3357, duration: 1.158s, episode steps: 64, steps per second: 55, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 112.562 [7.000, 216.000], mean observation: 0.856 [0.000, 128.000], loss: 1.666475, mean_absolute_error: 0.619762, mean_q: 4.869968, mean_eps: 0.100000\n",
      " 120193/175000: episode: 3358, duration: 0.994s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.038 [16.000, 216.000], mean observation: 0.289 [0.000, 104.000], loss: 1.223894, mean_absolute_error: 0.605994, mean_q: 4.716243, mean_eps: 0.100000\n",
      " 120230/175000: episode: 3359, duration: 0.665s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 119.757 [28.000, 216.000], mean observation: 0.253 [0.000, 74.000], loss: 1.239017, mean_absolute_error: 0.605775, mean_q: 4.641448, mean_eps: 0.100000\n",
      " 120275/175000: episode: 3360, duration: 0.804s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 127.933 [9.000, 216.000], mean observation: 0.329 [0.000, 90.000], loss: 0.872403, mean_absolute_error: 0.651369, mean_q: 4.925944, mean_eps: 0.100000\n",
      " 120302/175000: episode: 3361, duration: 0.524s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 199.407 [150.000, 223.000], mean observation: 0.145 [0.000, 54.000], loss: 0.814723, mean_absolute_error: 0.618589, mean_q: 4.878822, mean_eps: 0.100000\n",
      " 120331/175000: episode: 3362, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 191.448 [91.000, 223.000], mean observation: 0.165 [0.000, 58.000], loss: 0.538121, mean_absolute_error: 0.581327, mean_q: 4.915948, mean_eps: 0.100000\n",
      " 120366/175000: episode: 3363, duration: 0.673s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 162.371 [35.000, 221.000], mean observation: 0.258 [0.000, 70.000], loss: 1.579336, mean_absolute_error: 0.572063, mean_q: 4.892103, mean_eps: 0.100000\n",
      " 120419/175000: episode: 3364, duration: 1.002s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 92.283 [5.000, 204.000], mean observation: 0.572 [0.000, 106.000], loss: 1.326077, mean_absolute_error: 0.561832, mean_q: 4.876759, mean_eps: 0.100000\n",
      " 120469/175000: episode: 3365, duration: 0.925s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.960 [5.000, 207.000], mean observation: 0.550 [0.000, 100.000], loss: 281.539609, mean_absolute_error: 1.909384, mean_q: 5.838466, mean_eps: 0.100000\n",
      " 120511/175000: episode: 3366, duration: 0.724s, episode steps: 42, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 98.190 [7.000, 207.000], mean observation: 0.296 [0.000, 84.000], loss: 0.783449, mean_absolute_error: 0.545644, mean_q: 4.368340, mean_eps: 0.100000\n",
      " 120552/175000: episode: 3367, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 103.756 [9.000, 216.000], mean observation: 0.435 [0.000, 82.000], loss: 0.333644, mean_absolute_error: 0.531870, mean_q: 4.432915, mean_eps: 0.100000\n",
      " 120581/175000: episode: 3368, duration: 0.731s, episode steps: 29, steps per second: 40, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 159.621 [25.000, 216.000], mean observation: 0.159 [0.000, 58.000], loss: 0.255675, mean_absolute_error: 0.515088, mean_q: 4.337353, mean_eps: 0.100000\n",
      " 120608/175000: episode: 3369, duration: 0.675s, episode steps: 27, steps per second: 40, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 63.519 [9.000, 216.000], mean observation: 0.169 [0.000, 54.000], loss: 0.196620, mean_absolute_error: 0.514767, mean_q: 4.424800, mean_eps: 0.100000\n",
      " 120657/175000: episode: 3370, duration: 1.045s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.714 [9.000, 207.000], mean observation: 0.481 [0.000, 98.000], loss: 25.584452, mean_absolute_error: 0.630358, mean_q: 4.605425, mean_eps: 0.100000\n",
      " 120703/175000: episode: 3371, duration: 0.846s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 53.804 [16.000, 209.000], mean observation: 0.263 [0.000, 92.000], loss: 37.429541, mean_absolute_error: 0.815720, mean_q: 6.023541, mean_eps: 0.100000\n",
      " 120747/175000: episode: 3372, duration: 0.811s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 149.432 [16.000, 208.000], mean observation: 0.277 [0.000, 88.000], loss: 22.053519, mean_absolute_error: 0.758543, mean_q: 5.786524, mean_eps: 0.100000\n",
      " 120785/175000: episode: 3373, duration: 0.742s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 146.237 [43.000, 221.000], mean observation: 0.275 [0.000, 76.000], loss: 0.690902, mean_absolute_error: 0.555358, mean_q: 4.618719, mean_eps: 0.100000\n",
      " 120810/175000: episode: 3374, duration: 0.442s, episode steps: 25, steps per second: 57, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 127.680 [1.000, 219.000], mean observation: 0.139 [0.000, 50.000], loss: 307.988682, mean_absolute_error: 2.164950, mean_q: 7.425714, mean_eps: 0.100000\n",
      " 120833/175000: episode: 3375, duration: 0.446s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 99.826 [1.000, 181.000], mean observation: 0.110 [0.000, 46.000], loss: 0.387820, mean_absolute_error: 0.552445, mean_q: 4.866091, mean_eps: 0.100000\n",
      " 120884/175000: episode: 3376, duration: 1.042s, episode steps: 51, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 94.490 [1.000, 196.000], mean observation: 0.430 [0.000, 102.000], loss: 0.663830, mean_absolute_error: 0.554393, mean_q: 4.500808, mean_eps: 0.100000\n",
      " 120910/175000: episode: 3377, duration: 0.519s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 82.000 [57.000, 214.000], mean observation: 0.114 [0.000, 52.000], loss: 0.509841, mean_absolute_error: 0.552476, mean_q: 4.495105, mean_eps: 0.100000\n",
      " 120935/175000: episode: 3378, duration: 0.443s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 96.240 [72.000, 194.000], mean observation: 0.103 [0.000, 50.000], loss: 0.478061, mean_absolute_error: 0.534092, mean_q: 4.114309, mean_eps: 0.100000\n",
      " 120979/175000: episode: 3379, duration: 0.844s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.955 [10.000, 218.000], mean observation: 0.301 [0.000, 88.000], loss: 0.367633, mean_absolute_error: 0.533604, mean_q: 3.947500, mean_eps: 0.100000\n",
      " 121007/175000: episode: 3380, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 102.321 [20.000, 215.000], mean observation: 0.152 [0.000, 56.000], loss: 183.073738, mean_absolute_error: 1.586060, mean_q: 6.325318, mean_eps: 0.100000\n",
      " 121047/175000: episode: 3381, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 77.475 [15.000, 224.000], mean observation: 0.430 [0.000, 80.000], loss: 0.335016, mean_absolute_error: 0.557428, mean_q: 3.983498, mean_eps: 0.100000\n",
      " 121079/175000: episode: 3382, duration: 0.598s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 134.531 [26.000, 215.000], mean observation: 0.298 [0.000, 64.000], loss: 0.379691, mean_absolute_error: 0.550011, mean_q: 3.957925, mean_eps: 0.100000\n",
      " 121091/175000: episode: 3383, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 92.083 [1.000, 194.000], mean observation: 0.053 [0.000, 24.000], loss: 31.255966, mean_absolute_error: 1.184523, mean_q: 9.260473, mean_eps: 0.100000\n",
      " 121141/175000: episode: 3384, duration: 1.026s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.420 [1.000, 215.000], mean observation: 0.259 [0.000, 100.000], loss: 0.349151, mean_absolute_error: 0.547503, mean_q: 4.326438, mean_eps: 0.100000\n",
      " 121173/175000: episode: 3385, duration: 0.654s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 130.938 [14.000, 215.000], mean observation: 0.195 [0.000, 64.000], loss: 0.277659, mean_absolute_error: 0.522265, mean_q: 4.365517, mean_eps: 0.100000\n",
      " 121223/175000: episode: 3386, duration: 0.974s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 112.060 [7.000, 216.000], mean observation: 0.315 [0.000, 100.000], loss: 102.905209, mean_absolute_error: 1.088085, mean_q: 5.641270, mean_eps: 0.100000\n",
      " 121268/175000: episode: 3387, duration: 0.883s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.178 [6.000, 195.000], mean observation: 0.363 [0.000, 90.000], loss: 0.261367, mean_absolute_error: 0.498249, mean_q: 4.162871, mean_eps: 0.100000\n",
      " 121304/175000: episode: 3388, duration: 0.842s, episode steps: 36, steps per second: 43, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 129.833 [7.000, 216.000], mean observation: 0.219 [0.000, 72.000], loss: 0.281131, mean_absolute_error: 0.498024, mean_q: 4.031725, mean_eps: 0.100000\n",
      " 121342/175000: episode: 3389, duration: 0.816s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.211 [18.000, 191.000], mean observation: 0.318 [0.000, 76.000], loss: 26.063917, mean_absolute_error: 0.780854, mean_q: 6.041895, mean_eps: 0.100000\n",
      " 121366/175000: episode: 3390, duration: 0.438s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.625 [27.000, 218.000], mean observation: 0.239 [0.000, 48.000], loss: 0.309419, mean_absolute_error: 0.504633, mean_q: 4.396437, mean_eps: 0.100000\n",
      " 121393/175000: episode: 3391, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 98.185 [15.000, 211.000], mean observation: 0.274 [0.000, 54.000], loss: 0.554063, mean_absolute_error: 0.499499, mean_q: 4.136162, mean_eps: 0.100000\n",
      " 121437/175000: episode: 3392, duration: 0.807s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 104.932 [38.000, 218.000], mean observation: 0.396 [0.000, 88.000], loss: 0.769732, mean_absolute_error: 0.504076, mean_q: 4.064246, mean_eps: 0.100000\n",
      " 121458/175000: episode: 3393, duration: 0.380s, episode steps: 21, steps per second: 55, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.381 [38.000, 218.000], mean observation: 0.112 [0.000, 42.000], loss: 0.496736, mean_absolute_error: 0.502375, mean_q: 3.796411, mean_eps: 0.100000\n",
      " 121503/175000: episode: 3394, duration: 0.848s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.356 [1.000, 218.000], mean observation: 0.573 [0.000, 90.000], loss: 0.382724, mean_absolute_error: 0.516674, mean_q: 3.748082, mean_eps: 0.100000\n",
      " 121553/175000: episode: 3395, duration: 0.976s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.240 [1.000, 218.000], mean observation: 0.643 [0.000, 100.000], loss: 1.230132, mean_absolute_error: 0.532050, mean_q: 3.880037, mean_eps: 0.100000\n",
      " 121599/175000: episode: 3396, duration: 0.841s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 177.783 [40.000, 219.000], mean observation: 0.472 [0.000, 92.000], loss: 35.329889, mean_absolute_error: 0.834538, mean_q: 5.401915, mean_eps: 0.100000\n",
      " 121663/175000: episode: 3397, duration: 1.206s, episode steps: 64, steps per second: 53, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 158.328 [40.000, 218.000], mean observation: 0.505 [0.000, 128.000], loss: 0.586394, mean_absolute_error: 0.665508, mean_q: 5.159316, mean_eps: 0.100000\n",
      " 121689/175000: episode: 3398, duration: 0.498s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 162.769 [67.000, 215.000], mean observation: 0.102 [0.000, 52.000], loss: 0.703652, mean_absolute_error: 0.531384, mean_q: 3.598827, mean_eps: 0.100000\n",
      " 121730/175000: episode: 3399, duration: 0.772s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.146 [38.000, 220.000], mean observation: 0.323 [0.000, 82.000], loss: 1.743641, mean_absolute_error: 0.667134, mean_q: 4.982510, mean_eps: 0.100000\n",
      " 121776/175000: episode: 3400, duration: 0.907s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.000 [7.000, 176.000], mean observation: 0.666 [0.000, 92.000], loss: 351.889083, mean_absolute_error: 2.240697, mean_q: 5.010626, mean_eps: 0.100000\n",
      " 121816/175000: episode: 3401, duration: 0.804s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 81.325 [20.000, 202.000], mean observation: 0.351 [0.000, 80.000], loss: 0.501927, mean_absolute_error: 0.575844, mean_q: 3.627690, mean_eps: 0.100000\n",
      " 121858/175000: episode: 3402, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 86.452 [28.000, 191.000], mean observation: 0.428 [0.000, 84.000], loss: 0.355625, mean_absolute_error: 0.605393, mean_q: 3.689577, mean_eps: 0.100000\n",
      " 121887/175000: episode: 3403, duration: 0.524s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 100.931 [14.000, 209.000], mean observation: 0.122 [0.000, 58.000], loss: 0.276028, mean_absolute_error: 0.616214, mean_q: 3.643122, mean_eps: 0.100000\n",
      " 121897/175000: episode: 3404, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 121.500 [38.000, 158.000], mean observation: 0.052 [0.000, 20.000], loss: 0.297324, mean_absolute_error: 0.599257, mean_q: 3.438491, mean_eps: 0.100000\n",
      " 121939/175000: episode: 3405, duration: 0.814s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 102.571 [18.000, 202.000], mean observation: 0.306 [0.000, 84.000], loss: 302.863042, mean_absolute_error: 2.085366, mean_q: 5.076415, mean_eps: 0.100000\n",
      " 121969/175000: episode: 3406, duration: 0.633s, episode steps: 30, steps per second: 47, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 113.533 [37.000, 202.000], mean observation: 0.210 [0.000, 60.000], loss: 23.853842, mean_absolute_error: 0.879965, mean_q: 5.533307, mean_eps: 0.100000\n",
      " 121994/175000: episode: 3407, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 99.760 [2.000, 148.000], mean observation: 0.172 [0.000, 50.000], loss: 0.349941, mean_absolute_error: 0.544550, mean_q: 3.475622, mean_eps: 0.100000\n",
      " 122015/175000: episode: 3408, duration: 0.384s, episode steps: 21, steps per second: 55, episode reward: 1.000, mean reward: 0.048 [0.000, 1.000], mean action: 90.238 [37.000, 148.000], mean observation: 0.168 [0.000, 41.000], loss: 0.287608, mean_absolute_error: 0.545773, mean_q: 3.631539, mean_eps: 0.100000\n",
      " 122069/175000: episode: 3409, duration: 1.089s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 111.037 [5.000, 222.000], mean observation: 0.552 [0.000, 108.000], loss: 473.639879, mean_absolute_error: 2.872057, mean_q: 6.167657, mean_eps: 0.100000\n",
      " 122082/175000: episode: 3410, duration: 0.246s, episode steps: 13, steps per second: 53, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 44.000 [44.000, 44.000], mean observation: 0.033 [0.000, 26.000], loss: 0.272011, mean_absolute_error: 0.541581, mean_q: 3.753347, mean_eps: 0.100000\n",
      " 122123/175000: episode: 3411, duration: 0.753s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.927 [4.000, 212.000], mean observation: 0.285 [0.000, 82.000], loss: 5.255069, mean_absolute_error: 0.553737, mean_q: 3.772392, mean_eps: 0.100000\n",
      " 122139/175000: episode: 3412, duration: 0.317s, episode steps: 16, steps per second: 50, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 75.500 [44.000, 202.000], mean observation: 0.070 [0.000, 32.000], loss: 0.244805, mean_absolute_error: 0.511313, mean_q: 3.440807, mean_eps: 0.100000\n",
      " 122191/175000: episode: 3413, duration: 0.946s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 46.192 [10.000, 102.000], mean observation: 0.210 [0.000, 104.000], loss: 0.310437, mean_absolute_error: 0.551229, mean_q: 4.021922, mean_eps: 0.100000\n",
      " 122231/175000: episode: 3414, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 51.475 [1.000, 183.000], mean observation: 0.231 [0.000, 80.000], loss: 178.552826, mean_absolute_error: 1.490854, mean_q: 5.616265, mean_eps: 0.100000\n",
      " 122268/175000: episode: 3415, duration: 0.848s, episode steps: 37, steps per second: 44, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 65.405 [44.000, 209.000], mean observation: 0.242 [0.000, 74.000], loss: 0.451344, mean_absolute_error: 0.538416, mean_q: 3.863690, mean_eps: 0.100000\n",
      " 122314/175000: episode: 3416, duration: 0.862s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.152 [10.000, 207.000], mean observation: 0.567 [0.000, 92.000], loss: 0.464866, mean_absolute_error: 0.675502, mean_q: 5.246316, mean_eps: 0.100000\n",
      " 122354/175000: episode: 3417, duration: 0.731s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 68.950 [12.000, 199.000], mean observation: 0.435 [0.000, 80.000], loss: 0.254431, mean_absolute_error: 0.521964, mean_q: 3.381446, mean_eps: 0.100000\n",
      " 122381/175000: episode: 3418, duration: 0.545s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.815 [49.000, 167.000], mean observation: 0.164 [0.000, 54.000], loss: 0.226494, mean_absolute_error: 0.510536, mean_q: 3.351086, mean_eps: 0.100000\n",
      " 122408/175000: episode: 3419, duration: 0.500s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.556 [49.000, 219.000], mean observation: 0.191 [0.000, 54.000], loss: 4.542024, mean_absolute_error: 0.525942, mean_q: 3.372497, mean_eps: 0.100000\n",
      " 122426/175000: episode: 3420, duration: 0.359s, episode steps: 18, steps per second: 50, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 140.778 [118.000, 201.000], mean observation: 0.089 [0.000, 36.000], loss: 1956.936299, mean_absolute_error: 9.531384, mean_q: 6.751471, mean_eps: 0.100000\n",
      " 122456/175000: episode: 3421, duration: 0.607s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 107.167 [1.000, 196.000], mean observation: 0.182 [0.000, 60.000], loss: 0.543831, mean_absolute_error: 0.501744, mean_q: 3.283683, mean_eps: 0.100000\n",
      " 122481/175000: episode: 3422, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 130.320 [16.000, 182.000], mean observation: 0.111 [0.000, 50.000], loss: 0.260932, mean_absolute_error: 0.506750, mean_q: 3.418292, mean_eps: 0.100000\n",
      " 122503/175000: episode: 3423, duration: 0.373s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 131.500 [1.000, 139.000], mean observation: 0.068 [0.000, 44.000], loss: 0.174882, mean_absolute_error: 0.519615, mean_q: 3.537548, mean_eps: 0.100000\n",
      " 122530/175000: episode: 3424, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 104.370 [71.000, 218.000], mean observation: 0.169 [0.000, 54.000], loss: 0.271575, mean_absolute_error: 0.515654, mean_q: 3.461151, mean_eps: 0.100000\n",
      " 122573/175000: episode: 3425, duration: 0.786s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 80.674 [14.000, 139.000], mean observation: 0.242 [0.000, 86.000], loss: 4.234349, mean_absolute_error: 0.543167, mean_q: 3.785309, mean_eps: 0.100000\n",
      " 122619/175000: episode: 3426, duration: 0.800s, episode steps: 46, steps per second: 58, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 72.500 [9.000, 214.000], mean observation: 0.362 [0.000, 92.000], loss: 0.614166, mean_absolute_error: 0.507745, mean_q: 3.556896, mean_eps: 0.100000\n",
      " 122671/175000: episode: 3427, duration: 0.942s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 81.442 [9.000, 223.000], mean observation: 0.454 [0.000, 104.000], loss: 0.607204, mean_absolute_error: 0.522005, mean_q: 3.668446, mean_eps: 0.100000\n",
      " 122684/175000: episode: 3428, duration: 0.287s, episode steps: 13, steps per second: 45, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 109.385 [102.000, 150.000], mean observation: 0.034 [0.000, 26.000], loss: 0.656128, mean_absolute_error: 0.528587, mean_q: 3.666033, mean_eps: 0.100000\n",
      " 122702/175000: episode: 3429, duration: 0.353s, episode steps: 18, steps per second: 51, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 94.833 [15.000, 198.000], mean observation: 0.097 [0.000, 36.000], loss: 0.773628, mean_absolute_error: 0.535521, mean_q: 3.667515, mean_eps: 0.100000\n",
      " 122745/175000: episode: 3430, duration: 0.810s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 113.674 [15.000, 203.000], mean observation: 0.474 [0.000, 86.000], loss: 0.802035, mean_absolute_error: 0.516615, mean_q: 3.433314, mean_eps: 0.100000\n",
      " 122792/175000: episode: 3431, duration: 0.842s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 115.915 [15.000, 152.000], mean observation: 0.238 [0.000, 94.000], loss: 0.882471, mean_absolute_error: 0.544313, mean_q: 3.654680, mean_eps: 0.100000\n",
      " 122817/175000: episode: 3432, duration: 0.658s, episode steps: 25, steps per second: 38, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.480 [39.000, 132.000], mean observation: 0.102 [0.000, 50.000], loss: 0.806138, mean_absolute_error: 0.567162, mean_q: 3.815827, mean_eps: 0.100000\n",
      " 122848/175000: episode: 3433, duration: 0.584s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 90.710 [9.000, 193.000], mean observation: 0.215 [0.000, 62.000], loss: 0.874689, mean_absolute_error: 0.578293, mean_q: 3.836319, mean_eps: 0.100000\n",
      " 122880/175000: episode: 3434, duration: 0.630s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 88.062 [9.000, 186.000], mean observation: 0.243 [0.000, 64.000], loss: 0.567837, mean_absolute_error: 0.598772, mean_q: 3.725401, mean_eps: 0.100000\n",
      " 122917/175000: episode: 3435, duration: 0.718s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 143.757 [11.000, 224.000], mean observation: 0.405 [0.000, 74.000], loss: 1.000147, mean_absolute_error: 0.588948, mean_q: 4.096115, mean_eps: 0.100000\n",
      " 122960/175000: episode: 3436, duration: 0.791s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 137.977 [1.000, 224.000], mean observation: 0.529 [0.000, 86.000], loss: 2.566919, mean_absolute_error: 0.610327, mean_q: 4.814884, mean_eps: 0.100000\n",
      " 123015/175000: episode: 3437, duration: 1.037s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 130.909 [1.000, 221.000], mean observation: 0.458 [0.000, 110.000], loss: 0.344275, mean_absolute_error: 0.548613, mean_q: 4.286374, mean_eps: 0.100000\n",
      " 123048/175000: episode: 3438, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 138.000 [102.000, 150.000], mean observation: 0.114 [0.000, 66.000], loss: 0.808793, mean_absolute_error: 0.533508, mean_q: 4.239369, mean_eps: 0.100000\n",
      " 123063/175000: episode: 3439, duration: 0.307s, episode steps: 15, steps per second: 49, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 124.467 [12.000, 163.000], mean observation: 0.057 [0.000, 30.000], loss: 512.786180, mean_absolute_error: 3.193041, mean_q: 8.230790, mean_eps: 0.100000\n",
      " 123084/175000: episode: 3440, duration: 0.418s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.524 [102.000, 193.000], mean observation: 0.073 [0.000, 42.000], loss: 0.764273, mean_absolute_error: 0.549772, mean_q: 4.358855, mean_eps: 0.100000\n",
      " 123112/175000: episode: 3441, duration: 0.590s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 100.679 [11.000, 220.000], mean observation: 0.238 [0.000, 56.000], loss: 0.959359, mean_absolute_error: 0.582995, mean_q: 4.561975, mean_eps: 0.100000\n",
      " 123141/175000: episode: 3442, duration: 0.585s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 137.069 [102.000, 163.000], mean observation: 0.100 [0.000, 58.000], loss: 0.781108, mean_absolute_error: 0.578412, mean_q: 4.589819, mean_eps: 0.100000\n",
      " 123159/175000: episode: 3443, duration: 0.313s, episode steps: 18, steps per second: 57, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 130.111 [71.000, 176.000], mean observation: 0.092 [0.000, 36.000], loss: 0.562436, mean_absolute_error: 0.570883, mean_q: 4.094010, mean_eps: 0.100000\n",
      " 123195/175000: episode: 3444, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.083 [55.000, 176.000], mean observation: 0.172 [0.000, 72.000], loss: 0.508073, mean_absolute_error: 0.518154, mean_q: 3.630157, mean_eps: 0.100000\n",
      " 123233/175000: episode: 3445, duration: 0.740s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 109.868 [35.000, 176.000], mean observation: 0.238 [0.000, 76.000], loss: 259.565860, mean_absolute_error: 1.844827, mean_q: 5.694495, mean_eps: 0.100000\n",
      " 123267/175000: episode: 3446, duration: 0.595s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.500 [6.000, 219.000], mean observation: 0.198 [0.000, 68.000], loss: 340.503549, mean_absolute_error: 2.249907, mean_q: 6.088808, mean_eps: 0.100000\n",
      " 123300/175000: episode: 3447, duration: 0.663s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 108.636 [4.000, 193.000], mean observation: 0.258 [0.000, 66.000], loss: 0.942573, mean_absolute_error: 0.535203, mean_q: 3.965233, mean_eps: 0.100000\n",
      " 123324/175000: episode: 3448, duration: 0.557s, episode steps: 24, steps per second: 43, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 97.875 [28.000, 176.000], mean observation: 0.122 [0.000, 48.000], loss: 0.842007, mean_absolute_error: 0.522353, mean_q: 3.929054, mean_eps: 0.100000\n",
      " 123356/175000: episode: 3449, duration: 0.626s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 97.344 [55.000, 176.000], mean observation: 0.272 [0.000, 64.000], loss: 0.156532, mean_absolute_error: 0.503876, mean_q: 3.841315, mean_eps: 0.100000\n",
      " 123365/175000: episode: 3450, duration: 0.201s, episode steps: 9, steps per second: 45, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 176.222 [125.000, 221.000], mean observation: 0.051 [0.000, 18.000], loss: 0.189286, mean_absolute_error: 0.496031, mean_q: 3.691969, mean_eps: 0.100000\n",
      " 123399/175000: episode: 3451, duration: 0.627s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 97.676 [11.000, 221.000], mean observation: 0.316 [0.000, 68.000], loss: 0.295536, mean_absolute_error: 0.515670, mean_q: 3.971659, mean_eps: 0.100000\n",
      " 123447/175000: episode: 3452, duration: 0.876s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 108.417 [55.000, 221.000], mean observation: 0.483 [0.000, 96.000], loss: 0.337824, mean_absolute_error: 0.512328, mean_q: 3.908031, mean_eps: 0.100000\n",
      " 123489/175000: episode: 3453, duration: 0.805s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 75.881 [38.000, 221.000], mean observation: 0.237 [0.000, 84.000], loss: 1.304110, mean_absolute_error: 0.520318, mean_q: 3.843514, mean_eps: 0.100000\n",
      " 123523/175000: episode: 3454, duration: 0.614s, episode steps: 34, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 95.324 [44.000, 221.000], mean observation: 0.225 [0.000, 68.000], loss: 3.016282, mean_absolute_error: 0.521192, mean_q: 3.688642, mean_eps: 0.100000\n",
      " 123554/175000: episode: 3455, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 110.323 [41.000, 176.000], mean observation: 0.256 [0.000, 62.000], loss: 0.294524, mean_absolute_error: 0.522625, mean_q: 3.729949, mean_eps: 0.100000\n",
      " 123584/175000: episode: 3456, duration: 0.592s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 111.533 [55.000, 213.000], mean observation: 0.218 [0.000, 60.000], loss: 0.203975, mean_absolute_error: 0.522170, mean_q: 3.518092, mean_eps: 0.100000\n",
      " 123632/175000: episode: 3457, duration: 0.922s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.542 [33.000, 212.000], mean observation: 0.445 [0.000, 96.000], loss: 0.290365, mean_absolute_error: 0.527005, mean_q: 3.464804, mean_eps: 0.100000\n",
      " 123660/175000: episode: 3458, duration: 0.574s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.036 [55.000, 207.000], mean observation: 0.223 [0.000, 56.000], loss: 0.581194, mean_absolute_error: 0.520865, mean_q: 3.232617, mean_eps: 0.100000\n",
      " 123712/175000: episode: 3459, duration: 0.971s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.173 [43.000, 218.000], mean observation: 0.431 [0.000, 104.000], loss: 43.871644, mean_absolute_error: 0.839853, mean_q: 4.553685, mean_eps: 0.100000\n",
      " 123754/175000: episode: 3460, duration: 0.791s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.095 [44.000, 218.000], mean observation: 0.596 [0.000, 84.000], loss: 0.401482, mean_absolute_error: 0.667036, mean_q: 4.910468, mean_eps: 0.100000\n",
      " 123784/175000: episode: 3461, duration: 0.575s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 109.967 [60.000, 210.000], mean observation: 0.187 [0.000, 60.000], loss: 0.259096, mean_absolute_error: 0.513684, mean_q: 3.402478, mean_eps: 0.100000\n",
      " 123823/175000: episode: 3462, duration: 0.706s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.615 [19.000, 194.000], mean observation: 0.297 [0.000, 78.000], loss: 0.298919, mean_absolute_error: 0.520182, mean_q: 3.414245, mean_eps: 0.100000\n",
      " 123849/175000: episode: 3463, duration: 0.527s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 135.231 [43.000, 150.000], mean observation: 0.191 [0.000, 52.000], loss: 0.383243, mean_absolute_error: 0.513074, mean_q: 3.394184, mean_eps: 0.100000\n",
      " 123878/175000: episode: 3464, duration: 0.533s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 127.483 [15.000, 150.000], mean observation: 0.200 [0.000, 58.000], loss: 0.403783, mean_absolute_error: 0.509972, mean_q: 3.357273, mean_eps: 0.100000\n",
      " 123924/175000: episode: 3465, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 135.043 [4.000, 221.000], mean observation: 0.569 [0.000, 92.000], loss: 0.467334, mean_absolute_error: 0.510895, mean_q: 3.240386, mean_eps: 0.100000\n",
      " 123974/175000: episode: 3466, duration: 0.913s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.060 [17.000, 209.000], mean observation: 0.657 [0.000, 100.000], loss: 1.581067, mean_absolute_error: 0.510248, mean_q: 3.180776, mean_eps: 0.100000\n",
      " 124007/175000: episode: 3467, duration: 0.576s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 64.636 [28.000, 117.000], mean observation: 0.217 [0.000, 66.000], loss: 0.345675, mean_absolute_error: 0.517100, mean_q: 3.303645, mean_eps: 0.100000\n",
      " 124026/175000: episode: 3468, duration: 0.348s, episode steps: 19, steps per second: 55, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.263 [52.000, 192.000], mean observation: 0.160 [0.000, 38.000], loss: 0.241854, mean_absolute_error: 0.504516, mean_q: 3.355510, mean_eps: 0.100000\n",
      " 124073/175000: episode: 3469, duration: 0.879s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 98.979 [28.000, 191.000], mean observation: 0.221 [0.000, 94.000], loss: 5.018178, mean_absolute_error: 0.531086, mean_q: 3.423724, mean_eps: 0.100000\n",
      " 124121/175000: episode: 3470, duration: 0.849s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.208 [20.000, 222.000], mean observation: 0.339 [0.000, 96.000], loss: 0.793529, mean_absolute_error: 0.509378, mean_q: 3.404156, mean_eps: 0.100000\n",
      " 124153/175000: episode: 3471, duration: 0.583s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 85.000 [5.000, 102.000], mean observation: 0.192 [0.000, 64.000], loss: 340.890852, mean_absolute_error: 2.231280, mean_q: 5.511586, mean_eps: 0.100000\n",
      " 124189/175000: episode: 3472, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.861 [9.000, 209.000], mean observation: 0.280 [0.000, 72.000], loss: 0.476183, mean_absolute_error: 0.520922, mean_q: 3.448145, mean_eps: 0.100000\n",
      " 124212/175000: episode: 3473, duration: 0.432s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 145.957 [72.000, 203.000], mean observation: 0.137 [0.000, 46.000], loss: 0.634003, mean_absolute_error: 0.525503, mean_q: 3.379005, mean_eps: 0.100000\n",
      " 124249/175000: episode: 3474, duration: 0.746s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.243 [56.000, 223.000], mean observation: 0.344 [0.000, 74.000], loss: 108.287747, mean_absolute_error: 1.162360, mean_q: 5.027713, mean_eps: 0.100000\n",
      " 124284/175000: episode: 3475, duration: 0.719s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.714 [56.000, 203.000], mean observation: 0.250 [0.000, 70.000], loss: 0.336496, mean_absolute_error: 0.511529, mean_q: 3.328397, mean_eps: 0.100000\n",
      " 124333/175000: episode: 3476, duration: 1.053s, episode steps: 49, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 120.000 [5.000, 209.000], mean observation: 0.329 [0.000, 98.000], loss: 103.569509, mean_absolute_error: 1.095771, mean_q: 4.674199, mean_eps: 0.100000\n",
      " 124386/175000: episode: 3477, duration: 0.916s, episode steps: 53, steps per second: 58, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.717 [7.000, 209.000], mean observation: 0.556 [0.000, 106.000], loss: 0.201893, mean_absolute_error: 0.514569, mean_q: 3.419150, mean_eps: 0.100000\n",
      " 124425/175000: episode: 3478, duration: 0.697s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 54.256 [3.000, 206.000], mean observation: 0.185 [0.000, 78.000], loss: 0.272628, mean_absolute_error: 0.515232, mean_q: 3.308407, mean_eps: 0.100000\n",
      " 124441/175000: episode: 3479, duration: 0.291s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 59.938 [5.000, 201.000], mean observation: 0.076 [0.000, 32.000], loss: 0.558480, mean_absolute_error: 0.897387, mean_q: 7.201980, mean_eps: 0.100000\n",
      " 124491/175000: episode: 3480, duration: 0.988s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.680 [44.000, 208.000], mean observation: 0.246 [0.000, 100.000], loss: 0.201773, mean_absolute_error: 0.516726, mean_q: 3.275982, mean_eps: 0.100000\n",
      " 124516/175000: episode: 3481, duration: 0.498s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 49.000 [44.000, 134.000], mean observation: 0.083 [0.000, 50.000], loss: 0.263112, mean_absolute_error: 0.525839, mean_q: 3.295943, mean_eps: 0.100000\n",
      " 124547/175000: episode: 3482, duration: 0.632s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.645 [35.000, 215.000], mean observation: 0.285 [0.000, 62.000], loss: 0.480741, mean_absolute_error: 0.530807, mean_q: 3.417156, mean_eps: 0.100000\n",
      " 124579/175000: episode: 3483, duration: 0.570s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 96.375 [29.000, 215.000], mean observation: 0.280 [0.000, 64.000], loss: 9.432618, mean_absolute_error: 0.767337, mean_q: 5.577066, mean_eps: 0.100000\n",
      " 124598/175000: episode: 3484, duration: 0.354s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.316 [38.000, 185.000], mean observation: 0.167 [0.000, 38.000], loss: 0.232838, mean_absolute_error: 0.501874, mean_q: 3.230895, mean_eps: 0.100000\n",
      " 124639/175000: episode: 3485, duration: 0.785s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 75.805 [16.000, 176.000], mean observation: 0.497 [0.000, 82.000], loss: 0.485107, mean_absolute_error: 0.506698, mean_q: 3.251881, mean_eps: 0.100000\n",
      " 124688/175000: episode: 3486, duration: 0.919s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.184 [2.000, 221.000], mean observation: 0.419 [0.000, 98.000], loss: 0.322705, mean_absolute_error: 0.513788, mean_q: 3.359440, mean_eps: 0.100000\n",
      " 124726/175000: episode: 3487, duration: 0.808s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 88.921 [38.000, 222.000], mean observation: 0.197 [0.000, 76.000], loss: 0.537327, mean_absolute_error: 0.507491, mean_q: 3.324096, mean_eps: 0.100000\n",
      " 124766/175000: episode: 3488, duration: 0.745s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 74.875 [33.000, 172.000], mean observation: 0.240 [0.000, 80.000], loss: 0.481970, mean_absolute_error: 0.504173, mean_q: 3.234693, mean_eps: 0.100000\n",
      " 124802/175000: episode: 3489, duration: 0.643s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 122.694 [15.000, 172.000], mean observation: 0.307 [0.000, 72.000], loss: 1.533693, mean_absolute_error: 0.690328, mean_q: 5.032971, mean_eps: 0.100000\n",
      " 124832/175000: episode: 3490, duration: 0.577s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 125.267 [11.000, 191.000], mean observation: 0.233 [0.000, 60.000], loss: 0.841512, mean_absolute_error: 0.510221, mean_q: 3.272190, mean_eps: 0.100000\n",
      " 124862/175000: episode: 3491, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 144.467 [139.000, 213.000], mean observation: 0.128 [0.000, 60.000], loss: 0.271802, mean_absolute_error: 0.512407, mean_q: 3.508172, mean_eps: 0.100000\n",
      " 124899/175000: episode: 3492, duration: 0.645s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.081 [6.000, 199.000], mean observation: 0.300 [0.000, 74.000], loss: 0.457762, mean_absolute_error: 0.519833, mean_q: 3.532192, mean_eps: 0.100000\n",
      " 124930/175000: episode: 3493, duration: 0.655s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 90.935 [42.000, 178.000], mean observation: 0.234 [0.000, 62.000], loss: 0.533638, mean_absolute_error: 0.505356, mean_q: 3.359013, mean_eps: 0.100000\n",
      " 124969/175000: episode: 3494, duration: 0.742s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.949 [18.000, 167.000], mean observation: 0.358 [0.000, 78.000], loss: 0.479689, mean_absolute_error: 0.510055, mean_q: 3.455650, mean_eps: 0.100000\n",
      " 124995/175000: episode: 3495, duration: 0.433s, episode steps: 26, steps per second: 60, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 81.077 [16.000, 146.000], mean observation: 0.189 [0.000, 52.000], loss: 3.946534, mean_absolute_error: 0.823338, mean_q: 6.549776, mean_eps: 0.100000\n",
      " 125022/175000: episode: 3496, duration: 0.508s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.963 [24.000, 217.000], mean observation: 0.282 [0.000, 54.000], loss: 4.460188, mean_absolute_error: 0.792988, mean_q: 6.243047, mean_eps: 0.100000\n",
      " 125071/175000: episode: 3497, duration: 0.881s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.490 [0.000, 182.000], mean observation: 0.585 [0.000, 98.000], loss: 0.383385, mean_absolute_error: 0.547255, mean_q: 3.770320, mean_eps: 0.100000\n",
      " 125091/175000: episode: 3498, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 95.800 [18.000, 203.000], mean observation: 0.053 [0.000, 40.000], loss: 0.346982, mean_absolute_error: 0.524052, mean_q: 3.439687, mean_eps: 0.100000\n",
      " 125143/175000: episode: 3499, duration: 0.924s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 132.788 [14.000, 203.000], mean observation: 0.368 [0.000, 104.000], loss: 14.320304, mean_absolute_error: 0.706926, mean_q: 5.063376, mean_eps: 0.100000\n",
      " 125187/175000: episode: 3500, duration: 0.801s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 142.364 [3.000, 204.000], mean observation: 0.474 [0.000, 88.000], loss: 0.326332, mean_absolute_error: 0.548012, mean_q: 4.158083, mean_eps: 0.100000\n",
      " 125221/175000: episode: 3501, duration: 0.629s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 109.794 [14.000, 221.000], mean observation: 0.337 [0.000, 68.000], loss: 0.557241, mean_absolute_error: 0.516010, mean_q: 3.759441, mean_eps: 0.100000\n",
      " 125249/175000: episode: 3502, duration: 0.509s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 75.000 [4.000, 203.000], mean observation: 0.123 [0.000, 56.000], loss: 1.101046, mean_absolute_error: 0.527289, mean_q: 3.821419, mean_eps: 0.100000\n",
      " 125301/175000: episode: 3503, duration: 0.972s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 103.808 [8.000, 223.000], mean observation: 0.500 [0.000, 104.000], loss: 0.495249, mean_absolute_error: 0.509230, mean_q: 3.535183, mean_eps: 0.100000\n",
      " 125325/175000: episode: 3504, duration: 0.449s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 102.750 [14.000, 185.000], mean observation: 0.213 [0.000, 48.000], loss: 0.302249, mean_absolute_error: 0.507029, mean_q: 3.391144, mean_eps: 0.100000\n",
      " 125348/175000: episode: 3505, duration: 0.435s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 90.261 [12.000, 203.000], mean observation: 0.196 [0.000, 46.000], loss: 0.207702, mean_absolute_error: 0.518220, mean_q: 3.595043, mean_eps: 0.100000\n",
      " 125394/175000: episode: 3506, duration: 0.888s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.174 [18.000, 203.000], mean observation: 0.560 [0.000, 92.000], loss: 15.284140, mean_absolute_error: 0.851728, mean_q: 6.407502, mean_eps: 0.100000\n",
      " 125426/175000: episode: 3507, duration: 0.610s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.781 [13.000, 184.000], mean observation: 0.170 [0.000, 64.000], loss: 0.462806, mean_absolute_error: 0.526030, mean_q: 3.878094, mean_eps: 0.100000\n",
      " 125458/175000: episode: 3508, duration: 0.716s, episode steps: 32, steps per second: 45, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 77.438 [31.000, 218.000], mean observation: 0.252 [0.000, 64.000], loss: 0.418136, mean_absolute_error: 0.515096, mean_q: 4.262031, mean_eps: 0.100000\n",
      " 125502/175000: episode: 3509, duration: 0.949s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.409 [1.000, 189.000], mean observation: 0.205 [0.000, 88.000], loss: 0.271105, mean_absolute_error: 0.522324, mean_q: 4.334863, mean_eps: 0.100000\n",
      " 125524/175000: episode: 3510, duration: 0.527s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 86.500 [46.000, 116.000], mean observation: 0.101 [0.000, 44.000], loss: 0.188480, mean_absolute_error: 0.494914, mean_q: 3.901399, mean_eps: 0.100000\n",
      " 125572/175000: episode: 3511, duration: 1.053s, episode steps: 48, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 78.208 [60.000, 211.000], mean observation: 0.340 [0.000, 96.000], loss: 32.789698, mean_absolute_error: 0.752097, mean_q: 5.017333, mean_eps: 0.100000\n",
      " 125603/175000: episode: 3512, duration: 0.723s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 88.258 [60.000, 164.000], mean observation: 0.132 [0.000, 62.000], loss: 0.229062, mean_absolute_error: 0.472203, mean_q: 3.442027, mean_eps: 0.100000\n",
      " 125644/175000: episode: 3513, duration: 0.970s, episode steps: 41, steps per second: 42, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.268 [60.000, 204.000], mean observation: 0.261 [0.000, 82.000], loss: 0.732906, mean_absolute_error: 0.490524, mean_q: 3.463175, mean_eps: 0.100000\n",
      " 125680/175000: episode: 3514, duration: 0.852s, episode steps: 36, steps per second: 42, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 62.722 [29.000, 182.000], mean observation: 0.253 [0.000, 72.000], loss: 0.774706, mean_absolute_error: 0.507738, mean_q: 3.475286, mean_eps: 0.100000\n",
      " 125742/175000: episode: 3515, duration: 1.289s, episode steps: 62, steps per second: 48, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 81.645 [29.000, 190.000], mean observation: 0.851 [0.000, 124.000], loss: 0.480429, mean_absolute_error: 0.524336, mean_q: 3.758067, mean_eps: 0.100000\n",
      " 125782/175000: episode: 3516, duration: 0.846s, episode steps: 40, steps per second: 47, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 126.300 [57.000, 213.000], mean observation: 0.271 [0.000, 80.000], loss: 302.960457, mean_absolute_error: 2.030077, mean_q: 5.108784, mean_eps: 0.100000\n",
      " 125821/175000: episode: 3517, duration: 0.750s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 134.282 [20.000, 191.000], mean observation: 0.361 [0.000, 78.000], loss: 0.224183, mean_absolute_error: 0.524629, mean_q: 3.377042, mean_eps: 0.100000\n",
      " 125866/175000: episode: 3518, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 66.600 [16.000, 186.000], mean observation: 0.456 [0.000, 90.000], loss: 0.222044, mean_absolute_error: 0.503716, mean_q: 3.212377, mean_eps: 0.100000\n",
      " 125899/175000: episode: 3519, duration: 0.599s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.424 [17.000, 213.000], mean observation: 0.286 [0.000, 66.000], loss: 126.344328, mean_absolute_error: 1.272914, mean_q: 5.406444, mean_eps: 0.100000\n",
      " 125925/175000: episode: 3520, duration: 0.602s, episode steps: 26, steps per second: 43, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 131.385 [38.000, 197.000], mean observation: 0.155 [0.000, 52.000], loss: 0.242170, mean_absolute_error: 0.500058, mean_q: 3.307289, mean_eps: 0.100000\n",
      " 125962/175000: episode: 3521, duration: 0.654s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 80.541 [15.000, 139.000], mean observation: 0.178 [0.000, 74.000], loss: 0.299351, mean_absolute_error: 0.682829, mean_q: 5.333951, mean_eps: 0.100000\n",
      " 126000/175000: episode: 3522, duration: 0.822s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.632 [14.000, 222.000], mean observation: 0.279 [0.000, 76.000], loss: 40.437343, mean_absolute_error: 0.875876, mean_q: 5.307431, mean_eps: 0.100000\n",
      " 126039/175000: episode: 3523, duration: 0.705s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.615 [28.000, 203.000], mean observation: 0.229 [0.000, 78.000], loss: 365.761792, mean_absolute_error: 2.306234, mean_q: 5.212840, mean_eps: 0.100000\n",
      " 126081/175000: episode: 3524, duration: 0.817s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.357 [28.000, 139.000], mean observation: 0.269 [0.000, 84.000], loss: 0.210754, mean_absolute_error: 0.514121, mean_q: 3.400698, mean_eps: 0.100000\n",
      " 126109/175000: episode: 3525, duration: 0.640s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.107 [6.000, 170.000], mean observation: 0.161 [0.000, 56.000], loss: 0.235327, mean_absolute_error: 0.513990, mean_q: 3.507980, mean_eps: 0.100000\n",
      " 126147/175000: episode: 3526, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 109.105 [9.000, 215.000], mean observation: 0.283 [0.000, 76.000], loss: 0.330985, mean_absolute_error: 0.526564, mean_q: 3.620597, mean_eps: 0.100000\n",
      " 126175/175000: episode: 3527, duration: 0.558s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.214 [11.000, 215.000], mean observation: 0.126 [0.000, 56.000], loss: 0.901089, mean_absolute_error: 0.530663, mean_q: 3.990204, mean_eps: 0.100000\n",
      " 126220/175000: episode: 3528, duration: 1.008s, episode steps: 45, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 164.444 [55.000, 216.000], mean observation: 0.275 [0.000, 90.000], loss: 0.253894, mean_absolute_error: 0.522228, mean_q: 3.823935, mean_eps: 0.100000\n",
      " 126243/175000: episode: 3529, duration: 0.538s, episode steps: 23, steps per second: 43, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 76.870 [12.000, 194.000], mean observation: 0.252 [0.000, 46.000], loss: 0.363432, mean_absolute_error: 0.507923, mean_q: 3.611731, mean_eps: 0.100000\n",
      " 126278/175000: episode: 3530, duration: 0.807s, episode steps: 35, steps per second: 43, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.771 [50.000, 190.000], mean observation: 0.402 [0.000, 70.000], loss: 0.374956, mean_absolute_error: 0.502062, mean_q: 3.574550, mean_eps: 0.100000\n",
      " 126310/175000: episode: 3531, duration: 0.629s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 109.719 [50.000, 199.000], mean observation: 0.328 [0.000, 64.000], loss: 3.363633, mean_absolute_error: 0.730473, mean_q: 5.662558, mean_eps: 0.100000\n",
      " 126351/175000: episode: 3532, duration: 0.795s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.659 [17.000, 190.000], mean observation: 0.378 [0.000, 82.000], loss: 0.316473, mean_absolute_error: 0.527952, mean_q: 3.433277, mean_eps: 0.100000\n",
      " 126385/175000: episode: 3533, duration: 0.626s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.500 [8.000, 208.000], mean observation: 0.478 [0.000, 68.000], loss: 0.270450, mean_absolute_error: 0.517454, mean_q: 3.394039, mean_eps: 0.100000\n",
      " 126440/175000: episode: 3534, duration: 1.127s, episode steps: 55, steps per second: 49, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 91.818 [14.000, 190.000], mean observation: 0.529 [0.000, 110.000], loss: 0.291404, mean_absolute_error: 0.535965, mean_q: 3.430994, mean_eps: 0.100000\n",
      " 126461/175000: episode: 3535, duration: 0.485s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 109.952 [24.000, 215.000], mean observation: 0.138 [0.000, 42.000], loss: 0.430203, mean_absolute_error: 0.526425, mean_q: 3.442352, mean_eps: 0.100000\n",
      " 126504/175000: episode: 3536, duration: 0.808s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 116.070 [6.000, 222.000], mean observation: 0.505 [0.000, 86.000], loss: 0.402989, mean_absolute_error: 0.527584, mean_q: 3.770078, mean_eps: 0.100000\n",
      " 126548/175000: episode: 3537, duration: 0.861s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.682 [12.000, 220.000], mean observation: 0.507 [0.000, 88.000], loss: 0.418257, mean_absolute_error: 0.528890, mean_q: 3.657067, mean_eps: 0.100000\n",
      " 126585/175000: episode: 3538, duration: 0.706s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 93.730 [1.000, 222.000], mean observation: 0.429 [0.000, 74.000], loss: 0.481764, mean_absolute_error: 0.552068, mean_q: 3.857113, mean_eps: 0.100000\n",
      " 126625/175000: episode: 3539, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 132.925 [1.000, 203.000], mean observation: 0.426 [0.000, 80.000], loss: 0.466242, mean_absolute_error: 0.533735, mean_q: 3.796753, mean_eps: 0.100000\n",
      " 126675/175000: episode: 3540, duration: 0.926s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 105.920 [5.000, 222.000], mean observation: 0.647 [0.000, 100.000], loss: 309.487936, mean_absolute_error: 2.021218, mean_q: 5.078961, mean_eps: 0.100000\n",
      " 126705/175000: episode: 3541, duration: 0.685s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 94.333 [26.000, 219.000], mean observation: 0.272 [0.000, 60.000], loss: 2.237784, mean_absolute_error: 0.526752, mean_q: 3.518445, mean_eps: 0.100000\n",
      " 126750/175000: episode: 3542, duration: 0.851s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 91.556 [1.000, 223.000], mean observation: 0.583 [0.000, 90.000], loss: 0.676661, mean_absolute_error: 0.541337, mean_q: 3.570561, mean_eps: 0.100000\n",
      " 126800/175000: episode: 3543, duration: 0.928s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 98.760 [1.000, 223.000], mean observation: 0.566 [0.000, 100.000], loss: 2.595602, mean_absolute_error: 0.545434, mean_q: 3.593604, mean_eps: 0.100000\n",
      " 126825/175000: episode: 3544, duration: 0.508s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 87.360 [1.000, 212.000], mean observation: 0.235 [0.000, 50.000], loss: 1.162008, mean_absolute_error: 0.545634, mean_q: 3.478778, mean_eps: 0.100000\n",
      " 126848/175000: episode: 3545, duration: 0.438s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 131.696 [26.000, 223.000], mean observation: 0.202 [0.000, 46.000], loss: 0.717668, mean_absolute_error: 0.528612, mean_q: 3.400663, mean_eps: 0.100000\n",
      " 126880/175000: episode: 3546, duration: 0.638s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 171.938 [51.000, 223.000], mean observation: 0.085 [0.000, 64.000], loss: 0.572810, mean_absolute_error: 0.535164, mean_q: 3.565619, mean_eps: 0.100000\n",
      " 126927/175000: episode: 3547, duration: 0.868s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.149 [10.000, 212.000], mean observation: 0.582 [0.000, 94.000], loss: 0.411739, mean_absolute_error: 0.519783, mean_q: 3.256411, mean_eps: 0.100000\n",
      " 126974/175000: episode: 3548, duration: 0.843s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.660 [3.000, 214.000], mean observation: 0.589 [0.000, 94.000], loss: 0.422410, mean_absolute_error: 0.558797, mean_q: 3.606100, mean_eps: 0.100000\n",
      " 127005/175000: episode: 3549, duration: 0.580s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 85.677 [15.000, 175.000], mean observation: 0.262 [0.000, 62.000], loss: 0.925059, mean_absolute_error: 0.548233, mean_q: 3.509199, mean_eps: 0.100000\n",
      " 127037/175000: episode: 3550, duration: 0.573s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.500 [8.000, 170.000], mean observation: 0.384 [0.000, 64.000], loss: 1.404719, mean_absolute_error: 0.557435, mean_q: 3.643502, mean_eps: 0.100000\n",
      " 127067/175000: episode: 3551, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 101.500 [26.000, 212.000], mean observation: 0.206 [0.000, 60.000], loss: 1.644070, mean_absolute_error: 0.529714, mean_q: 3.427626, mean_eps: 0.100000\n",
      " 127085/175000: episode: 3552, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 102.611 [36.000, 198.000], mean observation: 0.107 [0.000, 36.000], loss: 0.542854, mean_absolute_error: 0.543130, mean_q: 3.332769, mean_eps: 0.100000\n",
      " 127103/175000: episode: 3553, duration: 0.313s, episode steps: 18, steps per second: 58, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 84.889 [26.000, 212.000], mean observation: 0.090 [0.000, 36.000], loss: 0.527044, mean_absolute_error: 0.531432, mean_q: 3.324576, mean_eps: 0.100000\n",
      " 127125/175000: episode: 3554, duration: 0.427s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 95.909 [14.000, 212.000], mean observation: 0.130 [0.000, 44.000], loss: 0.858811, mean_absolute_error: 0.539284, mean_q: 3.132129, mean_eps: 0.100000\n",
      " 127165/175000: episode: 3555, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 103.225 [14.000, 212.000], mean observation: 0.463 [0.000, 80.000], loss: 0.994897, mean_absolute_error: 0.541697, mean_q: 3.186725, mean_eps: 0.100000\n",
      " 127194/175000: episode: 3556, duration: 0.514s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 114.345 [14.000, 202.000], mean observation: 0.303 [0.000, 58.000], loss: 416.661026, mean_absolute_error: 2.620106, mean_q: 5.844128, mean_eps: 0.100000\n",
      " 127203/175000: episode: 3557, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 96.778 [14.000, 167.000], mean observation: 0.058 [0.000, 18.000], loss: 0.713383, mean_absolute_error: 0.558942, mean_q: 3.151331, mean_eps: 0.100000\n",
      " 127252/175000: episode: 3558, duration: 0.910s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.592 [14.000, 187.000], mean observation: 0.295 [0.000, 98.000], loss: 0.592284, mean_absolute_error: 0.573370, mean_q: 3.368042, mean_eps: 0.100000\n",
      " 127283/175000: episode: 3559, duration: 0.625s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 114.161 [28.000, 224.000], mean observation: 0.146 [0.000, 62.000], loss: 7.524185, mean_absolute_error: 0.602108, mean_q: 3.609215, mean_eps: 0.100000\n",
      " 127335/175000: episode: 3560, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.673 [11.000, 194.000], mean observation: 0.676 [0.000, 104.000], loss: 1.206801, mean_absolute_error: 0.706521, mean_q: 4.994285, mean_eps: 0.100000\n",
      " 127376/175000: episode: 3561, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 100.927 [1.000, 223.000], mean observation: 0.291 [0.000, 82.000], loss: 0.449848, mean_absolute_error: 0.595843, mean_q: 3.703047, mean_eps: 0.100000\n",
      " 127396/175000: episode: 3562, duration: 0.436s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 94.050 [14.000, 215.000], mean observation: 0.173 [0.000, 40.000], loss: 0.695302, mean_absolute_error: 0.571534, mean_q: 3.527244, mean_eps: 0.100000\n",
      " 127412/175000: episode: 3563, duration: 0.362s, episode steps: 16, steps per second: 44, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 87.062 [1.000, 167.000], mean observation: 0.081 [0.000, 32.000], loss: 1.194519, mean_absolute_error: 0.540110, mean_q: 3.600570, mean_eps: 0.100000\n",
      " 127441/175000: episode: 3564, duration: 0.555s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 76.897 [1.000, 185.000], mean observation: 0.186 [0.000, 58.000], loss: 0.597759, mean_absolute_error: 0.580890, mean_q: 3.638386, mean_eps: 0.100000\n",
      " 127462/175000: episode: 3565, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 119.000 [28.000, 202.000], mean observation: 0.064 [0.000, 42.000], loss: 0.719299, mean_absolute_error: 0.589932, mean_q: 3.747940, mean_eps: 0.100000\n",
      " 127479/175000: episode: 3566, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 73.235 [28.000, 167.000], mean observation: 0.073 [0.000, 34.000], loss: 0.990370, mean_absolute_error: 0.607386, mean_q: 3.986584, mean_eps: 0.100000\n",
      " 127526/175000: episode: 3567, duration: 0.866s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 66.979 [14.000, 202.000], mean observation: 0.354 [0.000, 94.000], loss: 1.019363, mean_absolute_error: 0.587404, mean_q: 3.741050, mean_eps: 0.100000\n",
      " 127559/175000: episode: 3568, duration: 0.608s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 112.576 [14.000, 208.000], mean observation: 0.272 [0.000, 66.000], loss: 0.370118, mean_absolute_error: 0.564400, mean_q: 3.577529, mean_eps: 0.100000\n",
      " 127589/175000: episode: 3569, duration: 0.561s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.800 [14.000, 213.000], mean observation: 0.273 [0.000, 60.000], loss: 0.479693, mean_absolute_error: 0.552785, mean_q: 3.598689, mean_eps: 0.100000\n",
      " 127640/175000: episode: 3570, duration: 0.924s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 164.784 [3.000, 206.000], mean observation: 0.523 [0.000, 102.000], loss: 0.445471, mean_absolute_error: 0.546544, mean_q: 3.560387, mean_eps: 0.100000\n",
      " 127659/175000: episode: 3571, duration: 0.373s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 205.684 [146.000, 213.000], mean observation: 0.050 [0.000, 38.000], loss: 0.403448, mean_absolute_error: 0.528481, mean_q: 3.169145, mean_eps: 0.100000\n",
      " 127683/175000: episode: 3572, duration: 0.433s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 192.542 [71.000, 213.000], mean observation: 0.131 [0.000, 48.000], loss: 0.403419, mean_absolute_error: 0.544484, mean_q: 3.323479, mean_eps: 0.100000\n",
      " 127723/175000: episode: 3573, duration: 0.726s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 172.475 [37.000, 213.000], mean observation: 0.190 [0.000, 80.000], loss: 2.036983, mean_absolute_error: 0.561217, mean_q: 3.244885, mean_eps: 0.100000\n",
      " 127763/175000: episode: 3574, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 154.450 [8.000, 219.000], mean observation: 0.251 [0.000, 80.000], loss: 496.738157, mean_absolute_error: 2.931091, mean_q: 5.047822, mean_eps: 0.100000\n",
      " 127798/175000: episode: 3575, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.371 [27.000, 215.000], mean observation: 0.407 [0.000, 70.000], loss: 0.546218, mean_absolute_error: 0.582765, mean_q: 3.214991, mean_eps: 0.100000\n",
      " 127829/175000: episode: 3576, duration: 0.583s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.903 [8.000, 215.000], mean observation: 0.273 [0.000, 62.000], loss: 0.414216, mean_absolute_error: 0.570934, mean_q: 3.156609, mean_eps: 0.100000\n",
      " 127859/175000: episode: 3577, duration: 0.524s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 130.000 [8.000, 187.000], mean observation: 0.322 [0.000, 60.000], loss: 0.394531, mean_absolute_error: 0.582463, mean_q: 3.144711, mean_eps: 0.100000\n",
      " 127877/175000: episode: 3578, duration: 0.375s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.000 [45.000, 209.000], mean observation: 0.097 [0.000, 36.000], loss: 0.364727, mean_absolute_error: 0.585183, mean_q: 3.113227, mean_eps: 0.100000\n",
      " 127915/175000: episode: 3579, duration: 0.663s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.500 [21.000, 209.000], mean observation: 0.353 [0.000, 76.000], loss: 0.687984, mean_absolute_error: 0.577347, mean_q: 3.173232, mean_eps: 0.100000\n",
      " 127951/175000: episode: 3580, duration: 0.645s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 126.056 [8.000, 209.000], mean observation: 0.359 [0.000, 72.000], loss: 0.315776, mean_absolute_error: 0.573340, mean_q: 3.093900, mean_eps: 0.100000\n",
      " 127988/175000: episode: 3581, duration: 0.731s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.784 [8.000, 209.000], mean observation: 0.446 [0.000, 74.000], loss: 1.406724, mean_absolute_error: 0.573826, mean_q: 3.314662, mean_eps: 0.100000\n",
      " 128029/175000: episode: 3582, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.732 [27.000, 210.000], mean observation: 0.518 [0.000, 82.000], loss: 590.714331, mean_absolute_error: 3.406276, mean_q: 5.908381, mean_eps: 0.100000\n",
      " 128065/175000: episode: 3583, duration: 0.670s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.889 [12.000, 210.000], mean observation: 0.437 [0.000, 72.000], loss: 0.482896, mean_absolute_error: 0.555784, mean_q: 3.259116, mean_eps: 0.100000\n",
      " 128078/175000: episode: 3584, duration: 0.234s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 104.615 [33.000, 220.000], mean observation: 0.084 [0.000, 26.000], loss: 0.389400, mean_absolute_error: 0.556254, mean_q: 3.190699, mean_eps: 0.100000\n",
      " 128104/175000: episode: 3585, duration: 0.496s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 126.115 [38.000, 222.000], mean observation: 0.214 [0.000, 52.000], loss: 0.479402, mean_absolute_error: 0.567136, mean_q: 3.425162, mean_eps: 0.100000\n",
      " 128157/175000: episode: 3586, duration: 0.983s, episode steps: 53, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.019 [4.000, 210.000], mean observation: 0.705 [0.000, 106.000], loss: 0.422398, mean_absolute_error: 0.573332, mean_q: 3.335007, mean_eps: 0.100000\n",
      " 128179/175000: episode: 3587, duration: 0.374s, episode steps: 22, steps per second: 59, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 108.182 [38.000, 200.000], mean observation: 0.098 [0.000, 44.000], loss: 0.570116, mean_absolute_error: 0.579771, mean_q: 3.366786, mean_eps: 0.100000\n",
      " 128205/175000: episode: 3588, duration: 0.500s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 94.500 [40.000, 188.000], mean observation: 0.204 [0.000, 52.000], loss: 0.577544, mean_absolute_error: 0.578763, mean_q: 3.454317, mean_eps: 0.100000\n",
      " 128261/175000: episode: 3589, duration: 1.048s, episode steps: 56, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 87.125 [39.000, 194.000], mean observation: 0.666 [0.000, 112.000], loss: 0.441167, mean_absolute_error: 0.596777, mean_q: 3.570285, mean_eps: 0.100000\n",
      " 128314/175000: episode: 3590, duration: 0.926s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.528 [59.000, 217.000], mean observation: 0.356 [0.000, 106.000], loss: 202.277115, mean_absolute_error: 1.637287, mean_q: 5.011849, mean_eps: 0.100000\n",
      " 128345/175000: episode: 3591, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.000 [1.000, 221.000], mean observation: 0.208 [0.000, 62.000], loss: 0.372905, mean_absolute_error: 0.616205, mean_q: 3.590327, mean_eps: 0.100000\n",
      " 128371/175000: episode: 3592, duration: 0.453s, episode steps: 26, steps per second: 57, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 111.731 [58.000, 169.000], mean observation: 0.220 [0.000, 52.000], loss: 0.379711, mean_absolute_error: 0.620730, mean_q: 3.581445, mean_eps: 0.100000\n",
      " 128393/175000: episode: 3593, duration: 0.436s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 135.818 [11.000, 170.000], mean observation: 0.081 [0.000, 44.000], loss: 0.273045, mean_absolute_error: 0.604519, mean_q: 3.513789, mean_eps: 0.100000\n",
      " 128422/175000: episode: 3594, duration: 0.498s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 136.345 [1.000, 177.000], mean observation: 0.135 [0.000, 58.000], loss: 0.197773, mean_absolute_error: 0.606501, mean_q: 3.534604, mean_eps: 0.100000\n",
      " 128437/175000: episode: 3595, duration: 0.295s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 130.067 [39.000, 220.000], mean observation: 0.102 [0.000, 30.000], loss: 0.237699, mean_absolute_error: 0.592864, mean_q: 3.378774, mean_eps: 0.100000\n",
      " 128500/175000: episode: 3596, duration: 1.144s, episode steps: 63, steps per second: 55, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 100.175 [37.000, 190.000], mean observation: 0.785 [0.000, 126.000], loss: 5.494102, mean_absolute_error: 0.647790, mean_q: 4.033875, mean_eps: 0.100000\n",
      " 128541/175000: episode: 3597, duration: 0.776s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 44.415 [8.000, 199.000], mean observation: 0.129 [0.000, 82.000], loss: 1.118752, mean_absolute_error: 0.574831, mean_q: 3.486389, mean_eps: 0.100000\n",
      " 128556/175000: episode: 3598, duration: 0.302s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 87.800 [8.000, 178.000], mean observation: 0.109 [0.000, 30.000], loss: 5.003156, mean_absolute_error: 0.577846, mean_q: 3.384846, mean_eps: 0.100000\n",
      " 128604/175000: episode: 3599, duration: 0.903s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.167 [20.000, 127.000], mean observation: 0.276 [0.000, 96.000], loss: 479.768698, mean_absolute_error: 2.844369, mean_q: 5.153778, mean_eps: 0.100000\n",
      " 128636/175000: episode: 3600, duration: 0.629s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 123.781 [8.000, 216.000], mean observation: 0.348 [0.000, 64.000], loss: 0.974103, mean_absolute_error: 0.619413, mean_q: 3.839998, mean_eps: 0.100000\n",
      " 128674/175000: episode: 3601, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.105 [0.000, 209.000], mean observation: 0.309 [0.000, 76.000], loss: 0.708273, mean_absolute_error: 0.627786, mean_q: 3.704149, mean_eps: 0.100000\n",
      " 128699/175000: episode: 3602, duration: 0.453s, episode steps: 25, steps per second: 55, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 135.440 [84.000, 150.000], mean observation: 0.065 [0.000, 50.000], loss: 0.569192, mean_absolute_error: 0.620319, mean_q: 3.557416, mean_eps: 0.100000\n",
      " 128723/175000: episode: 3603, duration: 0.443s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.917 [37.000, 210.000], mean observation: 0.115 [0.000, 48.000], loss: 0.608405, mean_absolute_error: 0.620195, mean_q: 3.532404, mean_eps: 0.100000\n",
      " 128738/175000: episode: 3604, duration: 0.298s, episode steps: 15, steps per second: 50, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 150.000 [150.000, 150.000], mean observation: 0.037 [0.000, 30.000], loss: 0.544433, mean_absolute_error: 0.625166, mean_q: 3.569087, mean_eps: 0.100000\n",
      " 128778/175000: episode: 3605, duration: 0.708s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.250 [24.000, 201.000], mean observation: 0.493 [0.000, 80.000], loss: 0.660495, mean_absolute_error: 0.635023, mean_q: 3.684779, mean_eps: 0.100000\n",
      " 128817/175000: episode: 3606, duration: 0.704s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.308 [12.000, 220.000], mean observation: 0.288 [0.000, 78.000], loss: 0.442746, mean_absolute_error: 0.629968, mean_q: 3.508737, mean_eps: 0.100000\n",
      " 128859/175000: episode: 3607, duration: 0.739s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.595 [12.000, 197.000], mean observation: 0.411 [0.000, 84.000], loss: 0.255886, mean_absolute_error: 0.611753, mean_q: 3.472210, mean_eps: 0.100000\n",
      " 128892/175000: episode: 3608, duration: 0.630s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 126.212 [12.000, 201.000], mean observation: 0.359 [0.000, 66.000], loss: 0.426614, mean_absolute_error: 0.618672, mean_q: 3.591818, mean_eps: 0.100000\n",
      " 128942/175000: episode: 3609, duration: 0.928s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 91.980 [12.000, 201.000], mean observation: 0.590 [0.000, 100.000], loss: 0.463767, mean_absolute_error: 0.603585, mean_q: 3.514729, mean_eps: 0.100000\n",
      " 128989/175000: episode: 3610, duration: 0.857s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 95.085 [13.000, 201.000], mean observation: 0.357 [0.000, 94.000], loss: 42.459951, mean_absolute_error: 0.752182, mean_q: 3.171797, mean_eps: 0.100000\n",
      " 129018/175000: episode: 3611, duration: 0.582s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 102.931 [38.000, 174.000], mean observation: 0.218 [0.000, 58.000], loss: 0.307896, mean_absolute_error: 0.556989, mean_q: 3.111554, mean_eps: 0.100000\n",
      " 129042/175000: episode: 3612, duration: 0.533s, episode steps: 24, steps per second: 45, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 76.708 [38.000, 163.000], mean observation: 0.070 [0.000, 48.000], loss: 0.417401, mean_absolute_error: 0.548384, mean_q: 3.129805, mean_eps: 0.100000\n",
      " 129084/175000: episode: 3613, duration: 0.799s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.881 [2.000, 187.000], mean observation: 0.348 [0.000, 84.000], loss: 0.331742, mean_absolute_error: 0.549525, mean_q: 3.050434, mean_eps: 0.100000\n",
      " 129103/175000: episode: 3614, duration: 0.356s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 130.263 [27.000, 157.000], mean observation: 0.139 [0.000, 38.000], loss: 0.204994, mean_absolute_error: 0.556903, mean_q: 3.007799, mean_eps: 0.100000\n",
      " 129130/175000: episode: 3615, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 108.222 [13.000, 163.000], mean observation: 0.243 [0.000, 54.000], loss: 1.955586, mean_absolute_error: 0.572038, mean_q: 3.088417, mean_eps: 0.100000\n",
      " 129159/175000: episode: 3616, duration: 0.545s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 146.655 [27.000, 223.000], mean observation: 0.283 [0.000, 58.000], loss: 1.076074, mean_absolute_error: 0.589818, mean_q: 3.213732, mean_eps: 0.100000\n",
      " 129204/175000: episode: 3617, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 128.956 [1.000, 179.000], mean observation: 0.604 [0.000, 90.000], loss: 1.845944, mean_absolute_error: 0.636551, mean_q: 3.305037, mean_eps: 0.100000\n",
      " 129240/175000: episode: 3618, duration: 0.707s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 83.611 [7.000, 218.000], mean observation: 0.397 [0.000, 72.000], loss: 141.854478, mean_absolute_error: 1.287973, mean_q: 3.319499, mean_eps: 0.100000\n",
      " 129289/175000: episode: 3619, duration: 0.887s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 69.837 [27.000, 208.000], mean observation: 0.651 [0.000, 98.000], loss: 1.054753, mean_absolute_error: 0.689588, mean_q: 3.539051, mean_eps: 0.100000\n",
      " 129321/175000: episode: 3620, duration: 0.563s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 120.156 [35.000, 183.000], mean observation: 0.199 [0.000, 64.000], loss: 0.553516, mean_absolute_error: 0.742596, mean_q: 3.514395, mean_eps: 0.100000\n",
      " 129338/175000: episode: 3621, duration: 0.296s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 126.824 [34.000, 204.000], mean observation: 0.080 [0.000, 34.000], loss: 3.237536, mean_absolute_error: 0.757686, mean_q: 3.753367, mean_eps: 0.100000\n",
      " 129356/175000: episode: 3622, duration: 0.384s, episode steps: 18, steps per second: 47, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.611 [63.000, 180.000], mean observation: 0.050 [0.000, 36.000], loss: 1.229889, mean_absolute_error: 0.737996, mean_q: 3.556135, mean_eps: 0.100000\n",
      " 129397/175000: episode: 3623, duration: 0.741s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.244 [14.000, 194.000], mean observation: 0.547 [0.000, 82.000], loss: 1.118538, mean_absolute_error: 0.809428, mean_q: 3.825257, mean_eps: 0.100000\n",
      " 129435/175000: episode: 3624, duration: 0.659s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.526 [23.000, 222.000], mean observation: 0.334 [0.000, 76.000], loss: 4.456684, mean_absolute_error: 0.821689, mean_q: 4.041973, mean_eps: 0.100000\n",
      " 129470/175000: episode: 3625, duration: 0.681s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 103.971 [8.000, 163.000], mean observation: 0.365 [0.000, 70.000], loss: 1401.643558, mean_absolute_error: 7.224309, mean_q: 5.934914, mean_eps: 0.100000\n",
      " 129512/175000: episode: 3626, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 134.548 [10.000, 222.000], mean observation: 0.187 [0.000, 84.000], loss: 0.692809, mean_absolute_error: 0.834530, mean_q: 4.119097, mean_eps: 0.100000\n",
      " 129560/175000: episode: 3627, duration: 0.911s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.979 [24.000, 223.000], mean observation: 0.233 [0.000, 96.000], loss: 3.809686, mean_absolute_error: 0.889313, mean_q: 4.221591, mean_eps: 0.100000\n",
      " 129591/175000: episode: 3628, duration: 0.583s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.355 [27.000, 186.000], mean observation: 0.097 [0.000, 62.000], loss: 0.472132, mean_absolute_error: 0.856962, mean_q: 4.266146, mean_eps: 0.100000\n",
      " 129606/175000: episode: 3629, duration: 0.324s, episode steps: 15, steps per second: 46, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 131.133 [28.000, 150.000], mean observation: 0.044 [0.000, 30.000], loss: 0.698738, mean_absolute_error: 0.839205, mean_q: 4.294201, mean_eps: 0.100000\n",
      " 129652/175000: episode: 3630, duration: 0.848s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 126.087 [5.000, 222.000], mean observation: 0.311 [0.000, 92.000], loss: 0.864957, mean_absolute_error: 0.864146, mean_q: 4.216767, mean_eps: 0.100000\n",
      " 129680/175000: episode: 3631, duration: 0.594s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.357 [27.000, 186.000], mean observation: 0.125 [0.000, 56.000], loss: 0.935377, mean_absolute_error: 0.849338, mean_q: 4.171364, mean_eps: 0.100000\n",
      " 129710/175000: episode: 3632, duration: 0.598s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 116.933 [27.000, 186.000], mean observation: 0.179 [0.000, 60.000], loss: 601.958022, mean_absolute_error: 3.765912, mean_q: 6.365867, mean_eps: 0.100000\n",
      " 129727/175000: episode: 3633, duration: 0.315s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 142.353 [27.000, 199.000], mean observation: 0.105 [0.000, 34.000], loss: 24.592506, mean_absolute_error: 1.004973, mean_q: 4.419399, mean_eps: 0.100000\n",
      " 129762/175000: episode: 3634, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.914 [51.000, 186.000], mean observation: 0.296 [0.000, 70.000], loss: 2.390234, mean_absolute_error: 0.935577, mean_q: 4.190409, mean_eps: 0.100000\n",
      " 129800/175000: episode: 3635, duration: 0.713s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 117.868 [1.000, 223.000], mean observation: 0.354 [0.000, 76.000], loss: 3.625397, mean_absolute_error: 0.971548, mean_q: 4.153429, mean_eps: 0.100000\n",
      " 129832/175000: episode: 3636, duration: 0.610s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 106.094 [27.000, 186.000], mean observation: 0.186 [0.000, 64.000], loss: 2.856937, mean_absolute_error: 0.992120, mean_q: 4.212979, mean_eps: 0.100000\n",
      " 129867/175000: episode: 3637, duration: 0.665s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.686 [25.000, 187.000], mean observation: 0.273 [0.000, 70.000], loss: 2.587954, mean_absolute_error: 1.054453, mean_q: 4.476918, mean_eps: 0.100000\n",
      " 129888/175000: episode: 3638, duration: 0.429s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 93.619 [27.000, 186.000], mean observation: 0.123 [0.000, 42.000], loss: 3.313933, mean_absolute_error: 1.000352, mean_q: 4.014992, mean_eps: 0.100000\n",
      " 129934/175000: episode: 3639, duration: 0.853s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.261 [4.000, 186.000], mean observation: 0.443 [0.000, 92.000], loss: 2.593043, mean_absolute_error: 1.009899, mean_q: 4.228441, mean_eps: 0.100000\n",
      " 129951/175000: episode: 3640, duration: 0.314s, episode steps: 17, steps per second: 54, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 38.294 [27.000, 183.000], mean observation: 0.064 [0.000, 34.000], loss: 1.944304, mean_absolute_error: 0.930238, mean_q: 3.910518, mean_eps: 0.100000\n",
      " 129976/175000: episode: 3641, duration: 0.495s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 27.080 [27.000, 29.000], mean observation: 0.088 [0.000, 50.000], loss: 24.087564, mean_absolute_error: 0.928309, mean_q: 4.145514, mean_eps: 0.100000\n",
      " 129993/175000: episode: 3642, duration: 0.350s, episode steps: 17, steps per second: 49, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 110.765 [27.000, 209.000], mean observation: 0.122 [0.000, 34.000], loss: 6.210322, mean_absolute_error: 0.786657, mean_q: 4.407597, mean_eps: 0.100000\n",
      " 130028/175000: episode: 3643, duration: 0.701s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.514 [17.000, 223.000], mean observation: 0.160 [0.000, 70.000], loss: 0.852398, mean_absolute_error: 0.782716, mean_q: 4.175989, mean_eps: 0.100000\n",
      " 130074/175000: episode: 3644, duration: 0.852s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.652 [16.000, 223.000], mean observation: 0.346 [0.000, 92.000], loss: 0.605259, mean_absolute_error: 0.720491, mean_q: 4.005926, mean_eps: 0.100000\n",
      " 130126/175000: episode: 3645, duration: 0.924s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 52.923 [20.000, 189.000], mean observation: 0.395 [0.000, 104.000], loss: 0.369989, mean_absolute_error: 0.654057, mean_q: 3.960711, mean_eps: 0.100000\n",
      " 130168/175000: episode: 3646, duration: 0.816s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 84.643 [22.000, 200.000], mean observation: 0.362 [0.000, 84.000], loss: 0.914162, mean_absolute_error: 0.599531, mean_q: 3.742400, mean_eps: 0.100000\n",
      " 130185/175000: episode: 3647, duration: 0.361s, episode steps: 17, steps per second: 47, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 102.588 [28.000, 189.000], mean observation: 0.066 [0.000, 34.000], loss: 1.374615, mean_absolute_error: 0.602174, mean_q: 3.805383, mean_eps: 0.100000\n",
      " 130214/175000: episode: 3648, duration: 0.510s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 59.345 [28.000, 173.000], mean observation: 0.219 [0.000, 58.000], loss: 5.613004, mean_absolute_error: 0.630889, mean_q: 3.828072, mean_eps: 0.100000\n",
      " 130240/175000: episode: 3649, duration: 0.532s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 63.808 [6.000, 157.000], mean observation: 0.155 [0.000, 52.000], loss: 1.491495, mean_absolute_error: 0.618890, mean_q: 3.736296, mean_eps: 0.100000\n",
      " 130274/175000: episode: 3650, duration: 0.646s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 99.088 [24.000, 207.000], mean observation: 0.373 [0.000, 68.000], loss: 1.078933, mean_absolute_error: 0.629184, mean_q: 3.816081, mean_eps: 0.100000\n",
      " 130294/175000: episode: 3651, duration: 0.371s, episode steps: 20, steps per second: 54, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 78.550 [1.000, 139.000], mean observation: 0.133 [0.000, 40.000], loss: 0.655840, mean_absolute_error: 0.648246, mean_q: 3.854421, mean_eps: 0.100000\n",
      " 130337/175000: episode: 3652, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 72.651 [32.000, 200.000], mean observation: 0.306 [0.000, 86.000], loss: 0.428331, mean_absolute_error: 0.632226, mean_q: 3.507676, mean_eps: 0.100000\n",
      " 130356/175000: episode: 3653, duration: 0.351s, episode steps: 19, steps per second: 54, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 50.368 [41.000, 219.000], mean observation: 0.050 [0.000, 38.000], loss: 0.440125, mean_absolute_error: 0.635835, mean_q: 3.402803, mean_eps: 0.100000\n",
      " 130373/175000: episode: 3654, duration: 0.356s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 56.118 [23.000, 206.000], mean observation: 0.051 [0.000, 34.000], loss: 0.350216, mean_absolute_error: 0.625151, mean_q: 3.471101, mean_eps: 0.100000\n",
      " 130410/175000: episode: 3655, duration: 0.674s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 115.919 [23.000, 196.000], mean observation: 0.225 [0.000, 74.000], loss: 0.279376, mean_absolute_error: 0.613194, mean_q: 3.360482, mean_eps: 0.100000\n",
      " 130462/175000: episode: 3656, duration: 0.931s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 110.596 [23.000, 210.000], mean observation: 0.498 [0.000, 104.000], loss: 0.283660, mean_absolute_error: 0.599034, mean_q: 3.424862, mean_eps: 0.100000\n",
      " 130499/175000: episode: 3657, duration: 0.642s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 47.486 [23.000, 173.000], mean observation: 0.111 [0.000, 74.000], loss: 0.371749, mean_absolute_error: 0.605651, mean_q: 3.540690, mean_eps: 0.100000\n",
      " 130528/175000: episode: 3658, duration: 0.650s, episode steps: 29, steps per second: 45, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 113.138 [35.000, 223.000], mean observation: 0.195 [0.000, 58.000], loss: 0.301701, mean_absolute_error: 0.605117, mean_q: 3.475440, mean_eps: 0.100000\n",
      " 130557/175000: episode: 3659, duration: 0.614s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 60.793 [35.000, 187.000], mean observation: 0.100 [0.000, 58.000], loss: 0.537731, mean_absolute_error: 0.596647, mean_q: 3.506650, mean_eps: 0.100000\n",
      " 130597/175000: episode: 3660, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 163.350 [17.000, 223.000], mean observation: 0.511 [0.000, 80.000], loss: 1.578488, mean_absolute_error: 0.617673, mean_q: 3.544824, mean_eps: 0.100000\n",
      " 130654/175000: episode: 3661, duration: 1.025s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 71.719 [6.000, 191.000], mean observation: 0.583 [0.000, 114.000], loss: 2.021233, mean_absolute_error: 0.608339, mean_q: 3.395658, mean_eps: 0.100000\n",
      " 130694/175000: episode: 3662, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 73.625 [6.000, 196.000], mean observation: 0.361 [0.000, 80.000], loss: 0.435388, mean_absolute_error: 0.586241, mean_q: 3.334767, mean_eps: 0.100000\n",
      " 130730/175000: episode: 3663, duration: 0.659s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 65.861 [1.000, 178.000], mean observation: 0.361 [0.000, 72.000], loss: 0.279337, mean_absolute_error: 0.581880, mean_q: 3.225936, mean_eps: 0.100000\n",
      " 130769/175000: episode: 3664, duration: 0.724s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 77.026 [29.000, 213.000], mean observation: 0.436 [0.000, 78.000], loss: 0.301146, mean_absolute_error: 0.576343, mean_q: 3.140556, mean_eps: 0.100000\n",
      " 130810/175000: episode: 3665, duration: 0.724s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 90.561 [29.000, 223.000], mean observation: 0.503 [0.000, 82.000], loss: 0.281140, mean_absolute_error: 0.586675, mean_q: 3.054858, mean_eps: 0.100000\n",
      " 130826/175000: episode: 3666, duration: 0.317s, episode steps: 16, steps per second: 50, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 98.312 [29.000, 179.000], mean observation: 0.104 [0.000, 32.000], loss: 5.900777, mean_absolute_error: 0.610094, mean_q: 3.209400, mean_eps: 0.100000\n",
      " 130862/175000: episode: 3667, duration: 0.653s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 98.083 [29.000, 218.000], mean observation: 0.253 [0.000, 72.000], loss: 0.412902, mean_absolute_error: 0.582978, mean_q: 3.164816, mean_eps: 0.100000\n",
      " 130904/175000: episode: 3668, duration: 0.762s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 121.095 [29.000, 197.000], mean observation: 0.478 [0.000, 84.000], loss: 0.419984, mean_absolute_error: 0.580662, mean_q: 3.092430, mean_eps: 0.100000\n",
      " 130941/175000: episode: 3669, duration: 0.730s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 129.514 [15.000, 209.000], mean observation: 0.425 [0.000, 74.000], loss: 0.659588, mean_absolute_error: 0.571697, mean_q: 3.135048, mean_eps: 0.100000\n",
      " 130985/175000: episode: 3670, duration: 0.798s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 126.500 [30.000, 213.000], mean observation: 0.518 [0.000, 88.000], loss: 1.265486, mean_absolute_error: 0.581791, mean_q: 3.066949, mean_eps: 0.100000\n",
      " 131014/175000: episode: 3671, duration: 0.554s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 72.172 [0.000, 217.000], mean observation: 0.188 [0.000, 58.000], loss: 0.686622, mean_absolute_error: 0.601636, mean_q: 3.126481, mean_eps: 0.100000\n",
      " 131057/175000: episode: 3672, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 61.628 [5.000, 198.000], mean observation: 0.282 [0.000, 86.000], loss: 1.163984, mean_absolute_error: 0.642769, mean_q: 3.222549, mean_eps: 0.100000\n",
      " 131093/175000: episode: 3673, duration: 0.641s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.250 [4.000, 219.000], mean observation: 0.258 [0.000, 72.000], loss: 1.486869, mean_absolute_error: 0.628902, mean_q: 3.009526, mean_eps: 0.100000\n",
      " 131123/175000: episode: 3674, duration: 0.528s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 65.300 [5.000, 178.000], mean observation: 0.236 [0.000, 60.000], loss: 1.124218, mean_absolute_error: 0.631772, mean_q: 3.145489, mean_eps: 0.100000\n",
      " 131156/175000: episode: 3675, duration: 0.628s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 61.788 [28.000, 218.000], mean observation: 0.226 [0.000, 66.000], loss: 1.087428, mean_absolute_error: 0.645517, mean_q: 3.223879, mean_eps: 0.100000\n",
      " 131204/175000: episode: 3676, duration: 1.003s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 59.438 [20.000, 212.000], mean observation: 0.648 [0.000, 96.000], loss: 62.311923, mean_absolute_error: 0.996708, mean_q: 4.142325, mean_eps: 0.100000\n",
      " 131242/175000: episode: 3677, duration: 0.735s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.816 [20.000, 208.000], mean observation: 0.366 [0.000, 76.000], loss: 0.875640, mean_absolute_error: 0.671686, mean_q: 3.608519, mean_eps: 0.100000\n",
      " 131274/175000: episode: 3678, duration: 0.599s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 99.750 [20.000, 208.000], mean observation: 0.209 [0.000, 64.000], loss: 0.798355, mean_absolute_error: 0.676104, mean_q: 3.612171, mean_eps: 0.100000\n",
      " 131305/175000: episode: 3679, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 83.839 [20.000, 208.000], mean observation: 0.174 [0.000, 62.000], loss: 0.586707, mean_absolute_error: 0.632964, mean_q: 3.521489, mean_eps: 0.100000\n",
      " 131345/175000: episode: 3680, duration: 0.706s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 105.000 [20.000, 208.000], mean observation: 0.441 [0.000, 80.000], loss: 0.747886, mean_absolute_error: 0.607989, mean_q: 3.384921, mean_eps: 0.100000\n",
      " 131379/175000: episode: 3681, duration: 0.709s, episode steps: 34, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.676 [11.000, 208.000], mean observation: 0.215 [0.000, 68.000], loss: 0.918016, mean_absolute_error: 0.616677, mean_q: 3.641045, mean_eps: 0.100000\n",
      " 131414/175000: episode: 3682, duration: 0.645s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 135.457 [14.000, 212.000], mean observation: 0.442 [0.000, 70.000], loss: 0.737029, mean_absolute_error: 0.584749, mean_q: 3.255798, mean_eps: 0.100000\n",
      " 131454/175000: episode: 3683, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.025 [14.000, 208.000], mean observation: 0.349 [0.000, 80.000], loss: 107.006154, mean_absolute_error: 1.368869, mean_q: 6.709452, mean_eps: 0.100000\n",
      " 131503/175000: episode: 3684, duration: 0.892s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 83.633 [8.000, 218.000], mean observation: 0.338 [0.000, 98.000], loss: 0.935174, mean_absolute_error: 0.697243, mean_q: 4.527341, mean_eps: 0.100000\n",
      " 131538/175000: episode: 3685, duration: 0.687s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 106.114 [14.000, 196.000], mean observation: 0.141 [0.000, 70.000], loss: 0.284735, mean_absolute_error: 0.567580, mean_q: 3.131251, mean_eps: 0.100000\n",
      " 131588/175000: episode: 3686, duration: 0.908s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.240 [14.000, 219.000], mean observation: 0.765 [0.000, 100.000], loss: 0.379515, mean_absolute_error: 0.548053, mean_q: 2.776371, mean_eps: 0.100000\n",
      " 131629/175000: episode: 3687, duration: 0.780s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 167.878 [14.000, 215.000], mean observation: 0.476 [0.000, 82.000], loss: 1.236627, mean_absolute_error: 0.577746, mean_q: 2.936871, mean_eps: 0.100000\n",
      " 131666/175000: episode: 3688, duration: 0.750s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 85.189 [14.000, 215.000], mean observation: 0.573 [0.000, 74.000], loss: 0.451824, mean_absolute_error: 0.566133, mean_q: 2.672189, mean_eps: 0.100000\n",
      " 131687/175000: episode: 3689, duration: 0.375s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 73.714 [27.000, 180.000], mean observation: 0.054 [0.000, 42.000], loss: 0.433836, mean_absolute_error: 0.543810, mean_q: 2.367139, mean_eps: 0.100000\n",
      " 131732/175000: episode: 3690, duration: 0.904s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.000 [27.000, 215.000], mean observation: 0.320 [0.000, 90.000], loss: 0.369604, mean_absolute_error: 0.558267, mean_q: 2.461267, mean_eps: 0.100000\n",
      " 131784/175000: episode: 3691, duration: 0.997s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.442 [44.000, 206.000], mean observation: 0.415 [0.000, 104.000], loss: 0.622983, mean_absolute_error: 0.585803, mean_q: 2.483027, mean_eps: 0.100000\n",
      " 131825/175000: episode: 3692, duration: 0.787s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.902 [13.000, 206.000], mean observation: 0.305 [0.000, 82.000], loss: 1.181101, mean_absolute_error: 0.626391, mean_q: 2.642010, mean_eps: 0.100000\n",
      " 131844/175000: episode: 3693, duration: 0.366s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 104.000 [13.000, 217.000], mean observation: 0.186 [0.000, 38.000], loss: 0.589623, mean_absolute_error: 0.611120, mean_q: 2.368542, mean_eps: 0.100000\n",
      " 131879/175000: episode: 3694, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.229 [13.000, 206.000], mean observation: 0.452 [0.000, 70.000], loss: 1.134844, mean_absolute_error: 0.629402, mean_q: 2.501056, mean_eps: 0.100000\n",
      " 131932/175000: episode: 3695, duration: 1.005s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 72.113 [5.000, 193.000], mean observation: 0.530 [0.000, 106.000], loss: 847.445151, mean_absolute_error: 4.505709, mean_q: 3.689996, mean_eps: 0.100000\n",
      " 131952/175000: episode: 3696, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 92.000 [44.000, 220.000], mean observation: 0.115 [0.000, 40.000], loss: 0.226587, mean_absolute_error: 0.633416, mean_q: 2.595882, mean_eps: 0.100000\n",
      " 131970/175000: episode: 3697, duration: 0.371s, episode steps: 18, steps per second: 49, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 92.778 [5.000, 192.000], mean observation: 0.123 [0.000, 36.000], loss: 0.218522, mean_absolute_error: 0.622028, mean_q: 2.507516, mean_eps: 0.100000\n",
      " 132010/175000: episode: 3698, duration: 0.735s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.375 [0.000, 208.000], mean observation: 0.468 [0.000, 80.000], loss: 0.257637, mean_absolute_error: 0.598014, mean_q: 2.587761, mean_eps: 0.100000\n",
      " 132042/175000: episode: 3699, duration: 0.557s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 97.031 [24.000, 208.000], mean observation: 0.228 [0.000, 64.000], loss: 0.218922, mean_absolute_error: 0.576668, mean_q: 2.427312, mean_eps: 0.100000\n",
      " 132078/175000: episode: 3700, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 81.528 [24.000, 150.000], mean observation: 0.248 [0.000, 72.000], loss: 0.193336, mean_absolute_error: 0.574746, mean_q: 2.355269, mean_eps: 0.100000\n",
      " 132117/175000: episode: 3701, duration: 0.717s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.051 [1.000, 174.000], mean observation: 0.331 [0.000, 78.000], loss: 0.150329, mean_absolute_error: 0.545586, mean_q: 2.127898, mean_eps: 0.100000\n",
      " 132162/175000: episode: 3702, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 100.400 [1.000, 194.000], mean observation: 0.519 [0.000, 90.000], loss: 1.496205, mean_absolute_error: 0.559247, mean_q: 2.484883, mean_eps: 0.100000\n",
      " 132214/175000: episode: 3703, duration: 0.958s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 104.981 [1.000, 197.000], mean observation: 0.525 [0.000, 104.000], loss: 0.261174, mean_absolute_error: 0.549997, mean_q: 2.412943, mean_eps: 0.100000\n",
      " 132252/175000: episode: 3704, duration: 0.702s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 79.789 [1.000, 214.000], mean observation: 0.244 [0.000, 76.000], loss: 0.159099, mean_absolute_error: 0.538367, mean_q: 2.428031, mean_eps: 0.100000\n",
      " 132286/175000: episode: 3705, duration: 0.652s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.029 [1.000, 199.000], mean observation: 0.309 [0.000, 68.000], loss: 0.223668, mean_absolute_error: 0.533532, mean_q: 2.218878, mean_eps: 0.100000\n",
      " 132343/175000: episode: 3706, duration: 1.040s, episode steps: 57, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 83.246 [1.000, 202.000], mean observation: 0.702 [0.000, 114.000], loss: 0.448779, mean_absolute_error: 0.545307, mean_q: 2.159959, mean_eps: 0.100000\n",
      " 132379/175000: episode: 3707, duration: 0.647s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 67.611 [1.000, 219.000], mean observation: 0.352 [0.000, 72.000], loss: 2.832831, mean_absolute_error: 0.557208, mean_q: 2.143560, mean_eps: 0.100000\n",
      " 132407/175000: episode: 3708, duration: 0.500s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 104.786 [1.000, 206.000], mean observation: 0.265 [0.000, 56.000], loss: 0.254257, mean_absolute_error: 0.538661, mean_q: 2.010134, mean_eps: 0.100000\n",
      " 132457/175000: episode: 3709, duration: 0.897s, episode steps: 50, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.180 [1.000, 199.000], mean observation: 0.386 [0.000, 100.000], loss: 3.776992, mean_absolute_error: 0.584512, mean_q: 2.227363, mean_eps: 0.100000\n",
      " 132495/175000: episode: 3710, duration: 0.665s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 92.184 [1.000, 219.000], mean observation: 0.479 [0.000, 76.000], loss: 0.967349, mean_absolute_error: 0.591555, mean_q: 2.285349, mean_eps: 0.100000\n",
      " 132535/175000: episode: 3711, duration: 0.711s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 96.600 [1.000, 219.000], mean observation: 0.436 [0.000, 80.000], loss: 0.164898, mean_absolute_error: 0.580506, mean_q: 2.043131, mean_eps: 0.100000\n",
      " 132547/175000: episode: 3712, duration: 0.218s, episode steps: 12, steps per second: 55, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 99.000 [59.000, 195.000], mean observation: 0.066 [0.000, 24.000], loss: 0.212237, mean_absolute_error: 0.599266, mean_q: 2.117752, mean_eps: 0.100000\n",
      " 132570/175000: episode: 3713, duration: 0.458s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 160.217 [59.000, 195.000], mean observation: 0.179 [0.000, 46.000], loss: 0.355786, mean_absolute_error: 0.574315, mean_q: 1.912826, mean_eps: 0.100000\n",
      " 132609/175000: episode: 3714, duration: 0.719s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 151.231 [37.000, 195.000], mean observation: 0.200 [0.000, 78.000], loss: 0.339586, mean_absolute_error: 0.599739, mean_q: 2.343626, mean_eps: 0.100000\n",
      " 132649/175000: episode: 3715, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 105.450 [20.000, 184.000], mean observation: 0.257 [0.000, 80.000], loss: 0.768724, mean_absolute_error: 0.582560, mean_q: 2.085097, mean_eps: 0.100000\n",
      " 132689/175000: episode: 3716, duration: 0.717s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 127.000 [33.000, 163.000], mean observation: 0.313 [0.000, 80.000], loss: 0.327896, mean_absolute_error: 0.581308, mean_q: 1.894551, mean_eps: 0.100000\n",
      " 132718/175000: episode: 3717, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 148.690 [59.000, 222.000], mean observation: 0.183 [0.000, 58.000], loss: 0.288913, mean_absolute_error: 0.598415, mean_q: 2.113366, mean_eps: 0.100000\n",
      " 132749/175000: episode: 3718, duration: 0.645s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 115.677 [39.000, 194.000], mean observation: 0.287 [0.000, 62.000], loss: 2.406123, mean_absolute_error: 0.615672, mean_q: 2.358591, mean_eps: 0.100000\n",
      " 132786/175000: episode: 3719, duration: 0.662s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 111.054 [58.000, 210.000], mean observation: 0.511 [0.000, 74.000], loss: 0.284343, mean_absolute_error: 0.592474, mean_q: 2.113033, mean_eps: 0.100000\n",
      " 132828/175000: episode: 3720, duration: 0.802s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 115.238 [2.000, 223.000], mean observation: 0.619 [0.000, 84.000], loss: 0.364915, mean_absolute_error: 0.597440, mean_q: 2.100705, mean_eps: 0.100000\n",
      " 132866/175000: episode: 3721, duration: 0.732s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 89.316 [17.000, 224.000], mean observation: 0.319 [0.000, 76.000], loss: 0.277463, mean_absolute_error: 0.604630, mean_q: 2.143502, mean_eps: 0.100000\n",
      " 132893/175000: episode: 3722, duration: 0.501s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 93.667 [0.000, 193.000], mean observation: 0.135 [0.000, 54.000], loss: 0.224007, mean_absolute_error: 0.594047, mean_q: 2.122752, mean_eps: 0.100000\n",
      " 132927/175000: episode: 3723, duration: 0.594s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.647 [6.000, 193.000], mean observation: 0.274 [0.000, 68.000], loss: 0.184677, mean_absolute_error: 0.576000, mean_q: 1.947337, mean_eps: 0.100000\n",
      " 132944/175000: episode: 3724, duration: 0.371s, episode steps: 17, steps per second: 46, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 64.941 [17.000, 173.000], mean observation: 0.115 [0.000, 34.000], loss: 0.522398, mean_absolute_error: 0.582885, mean_q: 2.176656, mean_eps: 0.100000\n",
      " 132970/175000: episode: 3725, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.308 [63.000, 152.000], mean observation: 0.084 [0.000, 52.000], loss: 43.897601, mean_absolute_error: 0.773136, mean_q: 1.922375, mean_eps: 0.100000\n",
      " 132989/175000: episode: 3726, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 95.947 [91.000, 185.000], mean observation: 0.047 [0.000, 38.000], loss: 1.545707, mean_absolute_error: 0.595389, mean_q: 1.940484, mean_eps: 0.100000\n",
      " 133025/175000: episode: 3727, duration: 0.706s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 104.000 [3.000, 221.000], mean observation: 0.116 [0.000, 72.000], loss: 5.834896, mean_absolute_error: 0.636477, mean_q: 2.014998, mean_eps: 0.100000\n",
      " 133055/175000: episode: 3728, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 73.800 [1.000, 176.000], mean observation: 0.336 [0.000, 60.000], loss: 0.742578, mean_absolute_error: 0.606983, mean_q: 1.713984, mean_eps: 0.100000\n",
      " 133095/175000: episode: 3729, duration: 0.729s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 70.200 [0.000, 218.000], mean observation: 0.440 [0.000, 80.000], loss: 0.777250, mean_absolute_error: 0.641929, mean_q: 1.882025, mean_eps: 0.100000\n",
      " 133119/175000: episode: 3730, duration: 0.448s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 80.500 [2.000, 223.000], mean observation: 0.215 [0.000, 48.000], loss: 1.231791, mean_absolute_error: 0.627555, mean_q: 1.540307, mean_eps: 0.100000\n",
      " 133133/175000: episode: 3731, duration: 0.289s, episode steps: 14, steps per second: 48, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 78.929 [50.000, 209.000], mean observation: 0.053 [0.000, 28.000], loss: 1.220127, mean_absolute_error: 0.637713, mean_q: 1.421175, mean_eps: 0.100000\n",
      " 133176/175000: episode: 3732, duration: 0.796s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 94.419 [17.000, 210.000], mean observation: 0.255 [0.000, 86.000], loss: 4.108211, mean_absolute_error: 0.665366, mean_q: 1.740311, mean_eps: 0.100000\n",
      " 133231/175000: episode: 3733, duration: 1.022s, episode steps: 55, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 78.345 [17.000, 211.000], mean observation: 0.826 [0.000, 110.000], loss: 0.548676, mean_absolute_error: 0.639662, mean_q: 1.530007, mean_eps: 0.100000\n",
      " 133260/175000: episode: 3734, duration: 0.599s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 82.724 [50.000, 214.000], mean observation: 0.213 [0.000, 58.000], loss: 0.679558, mean_absolute_error: 0.654628, mean_q: 1.605863, mean_eps: 0.100000\n",
      " 133298/175000: episode: 3735, duration: 0.693s, episode steps: 38, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 94.132 [12.000, 218.000], mean observation: 0.479 [0.000, 76.000], loss: 0.608557, mean_absolute_error: 0.661840, mean_q: 1.676779, mean_eps: 0.100000\n",
      " 133339/175000: episode: 3736, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.829 [32.000, 222.000], mean observation: 0.254 [0.000, 82.000], loss: 1.878552, mean_absolute_error: 0.663429, mean_q: 1.687801, mean_eps: 0.100000\n",
      " 133387/175000: episode: 3737, duration: 0.886s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 83.854 [10.000, 167.000], mean observation: 0.236 [0.000, 96.000], loss: 1.696935, mean_absolute_error: 0.652430, mean_q: 1.424403, mean_eps: 0.100000\n",
      " 133431/175000: episode: 3738, duration: 0.812s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.568 [4.000, 222.000], mean observation: 0.352 [0.000, 88.000], loss: 0.800445, mean_absolute_error: 0.674116, mean_q: 1.482439, mean_eps: 0.100000\n",
      " 133440/175000: episode: 3739, duration: 0.215s, episode steps: 9, steps per second: 42, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 75.111 [33.000, 108.000], mean observation: 0.049 [0.000, 18.000], loss: 0.509157, mean_absolute_error: 0.680839, mean_q: 1.521871, mean_eps: 0.100000\n",
      " 133479/175000: episode: 3740, duration: 0.761s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 103.000 [10.000, 202.000], mean observation: 0.417 [0.000, 78.000], loss: 1.167907, mean_absolute_error: 0.694395, mean_q: 1.617161, mean_eps: 0.100000\n",
      " 133518/175000: episode: 3741, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 73.103 [27.000, 191.000], mean observation: 0.276 [0.000, 78.000], loss: 1.580846, mean_absolute_error: 0.713575, mean_q: 1.948324, mean_eps: 0.100000\n",
      " 133557/175000: episode: 3742, duration: 0.737s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.923 [12.000, 163.000], mean observation: 0.346 [0.000, 78.000], loss: 1.170370, mean_absolute_error: 0.694548, mean_q: 1.544208, mean_eps: 0.100000\n",
      " 133580/175000: episode: 3743, duration: 0.489s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 62.174 [10.000, 189.000], mean observation: 0.217 [0.000, 46.000], loss: 0.938035, mean_absolute_error: 0.720124, mean_q: 2.026390, mean_eps: 0.100000\n",
      " 133624/175000: episode: 3744, duration: 0.863s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 92.705 [0.000, 221.000], mean observation: 0.306 [0.000, 88.000], loss: 0.997739, mean_absolute_error: 0.865018, mean_q: 3.645490, mean_eps: 0.100000\n",
      " 133657/175000: episode: 3745, duration: 0.670s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 116.545 [12.000, 217.000], mean observation: 0.235 [0.000, 66.000], loss: 0.378645, mean_absolute_error: 0.708820, mean_q: 1.821981, mean_eps: 0.100000\n",
      " 133702/175000: episode: 3746, duration: 0.797s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.822 [12.000, 220.000], mean observation: 0.467 [0.000, 90.000], loss: 0.345858, mean_absolute_error: 0.728680, mean_q: 2.259645, mean_eps: 0.100000\n",
      " 133736/175000: episode: 3747, duration: 0.640s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.324 [12.000, 165.000], mean observation: 0.309 [0.000, 68.000], loss: 0.356642, mean_absolute_error: 0.712177, mean_q: 2.050875, mean_eps: 0.100000\n",
      " 133763/175000: episode: 3748, duration: 0.523s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 107.926 [12.000, 190.000], mean observation: 0.254 [0.000, 54.000], loss: 6.555716, mean_absolute_error: 0.743474, mean_q: 2.131310, mean_eps: 0.100000\n",
      " 133779/175000: episode: 3749, duration: 0.305s, episode steps: 16, steps per second: 52, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 116.312 [12.000, 190.000], mean observation: 0.123 [0.000, 32.000], loss: 0.352033, mean_absolute_error: 0.702583, mean_q: 2.030176, mean_eps: 0.100000\n",
      " 133811/175000: episode: 3750, duration: 0.594s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 112.719 [12.000, 190.000], mean observation: 0.332 [0.000, 64.000], loss: 0.426881, mean_absolute_error: 0.710867, mean_q: 2.223257, mean_eps: 0.100000\n",
      " 133837/175000: episode: 3751, duration: 0.540s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 113.231 [12.000, 190.000], mean observation: 0.215 [0.000, 52.000], loss: 0.328654, mean_absolute_error: 0.705885, mean_q: 2.113338, mean_eps: 0.100000\n",
      " 133883/175000: episode: 3752, duration: 0.832s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.000 [31.000, 190.000], mean observation: 0.342 [0.000, 92.000], loss: 0.320065, mean_absolute_error: 0.689165, mean_q: 2.038761, mean_eps: 0.100000\n",
      " 133926/175000: episode: 3753, duration: 0.784s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 89.395 [30.000, 214.000], mean observation: 0.500 [0.000, 86.000], loss: 0.539910, mean_absolute_error: 0.702117, mean_q: 2.261067, mean_eps: 0.100000\n",
      " 133965/175000: episode: 3754, duration: 0.728s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.436 [22.000, 223.000], mean observation: 0.508 [0.000, 78.000], loss: 7.446080, mean_absolute_error: 0.719317, mean_q: 1.946641, mean_eps: 0.100000\n",
      " 134006/175000: episode: 3755, duration: 0.732s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.780 [22.000, 163.000], mean observation: 0.490 [0.000, 82.000], loss: 0.692102, mean_absolute_error: 0.679500, mean_q: 1.920932, mean_eps: 0.100000\n",
      " 134039/175000: episode: 3756, duration: 0.572s, episode steps: 33, steps per second: 58, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 98.000 [0.000, 216.000], mean observation: 0.477 [0.000, 66.000], loss: 0.591741, mean_absolute_error: 0.689390, mean_q: 1.955482, mean_eps: 0.100000\n",
      " 134082/175000: episode: 3757, duration: 0.807s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 124.140 [0.000, 197.000], mean observation: 0.522 [0.000, 86.000], loss: 2.910571, mean_absolute_error: 0.698558, mean_q: 1.802913, mean_eps: 0.100000\n",
      " 134095/175000: episode: 3758, duration: 0.231s, episode steps: 13, steps per second: 56, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 67.692 [43.000, 197.000], mean observation: 0.068 [0.000, 26.000], loss: 0.633282, mean_absolute_error: 0.679299, mean_q: 1.669717, mean_eps: 0.100000\n",
      " 134152/175000: episode: 3759, duration: 1.089s, episode steps: 57, steps per second: 52, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 115.035 [7.000, 204.000], mean observation: 0.700 [0.000, 114.000], loss: 0.738437, mean_absolute_error: 0.688852, mean_q: 1.802013, mean_eps: 0.100000\n",
      " 134199/175000: episode: 3760, duration: 0.852s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 128.234 [0.000, 197.000], mean observation: 0.450 [0.000, 94.000], loss: 0.725171, mean_absolute_error: 0.697227, mean_q: 1.886327, mean_eps: 0.100000\n",
      " 134247/175000: episode: 3761, duration: 0.862s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 104.396 [0.000, 171.000], mean observation: 0.395 [0.000, 96.000], loss: 6.652617, mean_absolute_error: 0.735183, mean_q: 2.020563, mean_eps: 0.100000\n",
      " 134305/175000: episode: 3762, duration: 1.071s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 135.897 [13.000, 217.000], mean observation: 0.805 [0.000, 116.000], loss: 3.437141, mean_absolute_error: 0.711965, mean_q: 1.969393, mean_eps: 0.100000\n",
      " 134334/175000: episode: 3763, duration: 0.519s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 121.379 [0.000, 197.000], mean observation: 0.221 [0.000, 58.000], loss: 0.400251, mean_absolute_error: 0.690029, mean_q: 1.792187, mean_eps: 0.100000\n",
      " 134371/175000: episode: 3764, duration: 0.664s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 106.297 [0.000, 216.000], mean observation: 0.247 [0.000, 74.000], loss: 0.381192, mean_absolute_error: 0.680137, mean_q: 1.949058, mean_eps: 0.100000\n",
      " 134402/175000: episode: 3765, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.581 [0.000, 197.000], mean observation: 0.196 [0.000, 62.000], loss: 0.591890, mean_absolute_error: 0.673786, mean_q: 1.877829, mean_eps: 0.100000\n",
      " 134449/175000: episode: 3766, duration: 0.839s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 91.787 [22.000, 148.000], mean observation: 0.388 [0.000, 94.000], loss: 0.649264, mean_absolute_error: 0.678785, mean_q: 1.769759, mean_eps: 0.100000\n",
      " 134509/175000: episode: 3767, duration: 1.081s, episode steps: 60, steps per second: 56, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 107.283 [24.000, 217.000], mean observation: 0.443 [0.000, 120.000], loss: 365.993470, mean_absolute_error: 2.385799, mean_q: 2.889530, mean_eps: 0.100000\n",
      " 134555/175000: episode: 3768, duration: 0.805s, episode steps: 46, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 51.696 [5.000, 148.000], mean observation: 0.390 [0.000, 92.000], loss: 0.584130, mean_absolute_error: 0.710292, mean_q: 2.045505, mean_eps: 0.100000\n",
      " 134593/175000: episode: 3769, duration: 0.704s, episode steps: 38, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 52.921 [5.000, 223.000], mean observation: 0.369 [0.000, 76.000], loss: 2.402189, mean_absolute_error: 0.723585, mean_q: 1.904268, mean_eps: 0.100000\n",
      " 134631/175000: episode: 3770, duration: 0.649s, episode steps: 38, steps per second: 59, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.316 [0.000, 210.000], mean observation: 0.313 [0.000, 76.000], loss: 0.369667, mean_absolute_error: 0.721340, mean_q: 1.842670, mean_eps: 0.100000\n",
      " 134664/175000: episode: 3771, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 75.455 [3.000, 197.000], mean observation: 0.184 [0.000, 66.000], loss: 12.737385, mean_absolute_error: 0.799874, mean_q: 2.234440, mean_eps: 0.100000\n",
      " 134709/175000: episode: 3772, duration: 0.879s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.622 [13.000, 197.000], mean observation: 0.511 [0.000, 90.000], loss: 1.604388, mean_absolute_error: 0.723261, mean_q: 1.897792, mean_eps: 0.100000\n",
      " 134723/175000: episode: 3773, duration: 0.260s, episode steps: 14, steps per second: 54, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 199.857 [197.000, 207.000], mean observation: 0.048 [0.000, 28.000], loss: 0.228804, mean_absolute_error: 0.706031, mean_q: 1.801645, mean_eps: 0.100000\n",
      " 134768/175000: episode: 3774, duration: 0.858s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.578 [37.000, 223.000], mean observation: 0.430 [0.000, 90.000], loss: 0.340792, mean_absolute_error: 0.695149, mean_q: 1.759529, mean_eps: 0.100000\n",
      " 134801/175000: episode: 3775, duration: 0.658s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 96.455 [2.000, 191.000], mean observation: 0.316 [0.000, 66.000], loss: 0.260298, mean_absolute_error: 0.690834, mean_q: 1.776124, mean_eps: 0.100000\n",
      " 134850/175000: episode: 3776, duration: 0.842s, episode steps: 49, steps per second: 58, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 104.041 [6.000, 191.000], mean observation: 0.346 [0.000, 98.000], loss: 27.197885, mean_absolute_error: 0.931729, mean_q: 3.203474, mean_eps: 0.100000\n",
      " 134863/175000: episode: 3777, duration: 0.203s, episode steps: 13, steps per second: 64, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 88.385 [59.000, 137.000], mean observation: 0.054 [0.000, 26.000], loss: 0.263875, mean_absolute_error: 0.666675, mean_q: 1.539549, mean_eps: 0.100000\n",
      " 134903/175000: episode: 3778, duration: 0.800s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 55.975 [16.000, 137.000], mean observation: 0.250 [0.000, 80.000], loss: 0.149721, mean_absolute_error: 0.681995, mean_q: 1.811052, mean_eps: 0.100000\n",
      " 134942/175000: episode: 3779, duration: 0.787s, episode steps: 39, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.821 [12.000, 222.000], mean observation: 0.305 [0.000, 78.000], loss: 0.589734, mean_absolute_error: 0.686180, mean_q: 1.940940, mean_eps: 0.100000\n",
      " 134995/175000: episode: 3780, duration: 1.009s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 78.698 [5.000, 207.000], mean observation: 0.436 [0.000, 106.000], loss: 24.489792, mean_absolute_error: 0.849712, mean_q: 2.651399, mean_eps: 0.100000\n",
      " 135036/175000: episode: 3781, duration: 0.886s, episode steps: 41, steps per second: 46, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 88.976 [6.000, 207.000], mean observation: 0.368 [0.000, 82.000], loss: 0.238574, mean_absolute_error: 0.664222, mean_q: 1.725931, mean_eps: 0.100000\n",
      " 135068/175000: episode: 3782, duration: 0.691s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 78.250 [22.000, 207.000], mean observation: 0.301 [0.000, 64.000], loss: 0.293621, mean_absolute_error: 0.676472, mean_q: 1.827198, mean_eps: 0.100000\n",
      " 135106/175000: episode: 3783, duration: 0.903s, episode steps: 38, steps per second: 42, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.868 [27.000, 208.000], mean observation: 0.194 [0.000, 76.000], loss: 0.148628, mean_absolute_error: 0.676227, mean_q: 1.877291, mean_eps: 0.100000\n",
      " 135154/175000: episode: 3784, duration: 0.903s, episode steps: 48, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 180.208 [27.000, 222.000], mean observation: 0.193 [0.000, 96.000], loss: 0.243587, mean_absolute_error: 0.656528, mean_q: 1.763883, mean_eps: 0.100000\n",
      " 135180/175000: episode: 3785, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 171.885 [4.000, 222.000], mean observation: 0.083 [0.000, 52.000], loss: 0.182582, mean_absolute_error: 0.656356, mean_q: 1.886933, mean_eps: 0.100000\n",
      " 135217/175000: episode: 3786, duration: 0.715s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 134.432 [6.000, 222.000], mean observation: 0.248 [0.000, 74.000], loss: 0.254817, mean_absolute_error: 0.643602, mean_q: 1.784762, mean_eps: 0.100000\n",
      " 135251/175000: episode: 3787, duration: 0.612s, episode steps: 34, steps per second: 56, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.765 [34.000, 191.000], mean observation: 0.263 [0.000, 68.000], loss: 0.251198, mean_absolute_error: 0.635298, mean_q: 1.642757, mean_eps: 0.100000\n",
      " 135301/175000: episode: 3788, duration: 0.972s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.680 [1.000, 207.000], mean observation: 0.408 [0.000, 100.000], loss: 0.404421, mean_absolute_error: 0.661156, mean_q: 1.930922, mean_eps: 0.100000\n",
      " 135332/175000: episode: 3789, duration: 0.582s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 148.419 [1.000, 222.000], mean observation: 0.114 [0.000, 62.000], loss: 1.220613, mean_absolute_error: 0.666674, mean_q: 1.841701, mean_eps: 0.100000\n",
      " 135364/175000: episode: 3790, duration: 0.664s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 148.469 [16.000, 222.000], mean observation: 0.147 [0.000, 64.000], loss: 0.143095, mean_absolute_error: 0.655356, mean_q: 1.588555, mean_eps: 0.100000\n",
      " 135392/175000: episode: 3791, duration: 0.588s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 79.500 [1.000, 195.000], mean observation: 0.166 [0.000, 56.000], loss: 0.294844, mean_absolute_error: 0.654628, mean_q: 1.591682, mean_eps: 0.100000\n",
      " 135417/175000: episode: 3792, duration: 0.491s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 105.920 [1.000, 153.000], mean observation: 0.207 [0.000, 50.000], loss: 559.417766, mean_absolute_error: 3.373379, mean_q: 4.453767, mean_eps: 0.100000\n",
      " 135473/175000: episode: 3793, duration: 1.008s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 113.339 [1.000, 221.000], mean observation: 0.912 [0.000, 112.000], loss: 0.252655, mean_absolute_error: 0.662918, mean_q: 2.111843, mean_eps: 0.100000\n",
      " 135516/175000: episode: 3794, duration: 0.787s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 96.791 [1.000, 216.000], mean observation: 0.641 [0.000, 86.000], loss: 325.345023, mean_absolute_error: 2.260189, mean_q: 3.625146, mean_eps: 0.100000\n",
      " 135538/175000: episode: 3795, duration: 0.419s, episode steps: 22, steps per second: 52, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 127.227 [39.000, 216.000], mean observation: 0.162 [0.000, 44.000], loss: 1.215053, mean_absolute_error: 0.823837, mean_q: 3.725122, mean_eps: 0.100000\n",
      " 135580/175000: episode: 3796, duration: 0.817s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.310 [38.000, 216.000], mean observation: 0.367 [0.000, 84.000], loss: 0.234470, mean_absolute_error: 0.655891, mean_q: 1.934980, mean_eps: 0.100000\n",
      " 135597/175000: episode: 3797, duration: 0.353s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 138.176 [50.000, 217.000], mean observation: 0.082 [0.000, 34.000], loss: 0.172995, mean_absolute_error: 0.643877, mean_q: 1.915245, mean_eps: 0.100000\n",
      " 135641/175000: episode: 3798, duration: 0.764s, episode steps: 44, steps per second: 58, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 147.455 [42.000, 184.000], mean observation: 0.374 [0.000, 88.000], loss: 0.547582, mean_absolute_error: 0.652330, mean_q: 2.077958, mean_eps: 0.100000\n",
      " 135690/175000: episode: 3799, duration: 0.874s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.796 [7.000, 180.000], mean observation: 0.395 [0.000, 98.000], loss: 0.295886, mean_absolute_error: 0.641326, mean_q: 2.066190, mean_eps: 0.100000\n",
      " 135714/175000: episode: 3800, duration: 0.437s, episode steps: 24, steps per second: 55, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.375 [0.000, 211.000], mean observation: 0.250 [0.000, 48.000], loss: 0.155115, mean_absolute_error: 0.637879, mean_q: 1.961490, mean_eps: 0.100000\n",
      " 135748/175000: episode: 3801, duration: 0.679s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.706 [12.000, 217.000], mean observation: 0.415 [0.000, 68.000], loss: 0.303882, mean_absolute_error: 0.648519, mean_q: 2.030952, mean_eps: 0.100000\n",
      " 135775/175000: episode: 3802, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 106.407 [38.000, 165.000], mean observation: 0.175 [0.000, 54.000], loss: 6.312944, mean_absolute_error: 0.655649, mean_q: 1.682698, mean_eps: 0.100000\n",
      " 135810/175000: episode: 3803, duration: 0.648s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.457 [9.000, 186.000], mean observation: 0.355 [0.000, 70.000], loss: 0.522535, mean_absolute_error: 0.636995, mean_q: 1.829273, mean_eps: 0.100000\n",
      " 135833/175000: episode: 3804, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 91.826 [0.000, 171.000], mean observation: 0.131 [0.000, 46.000], loss: 0.239965, mean_absolute_error: 0.633361, mean_q: 1.834290, mean_eps: 0.100000\n",
      " 135870/175000: episode: 3805, duration: 0.675s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 163.811 [76.000, 215.000], mean observation: 0.216 [0.000, 74.000], loss: 0.255383, mean_absolute_error: 0.631181, mean_q: 1.508950, mean_eps: 0.100000\n",
      " 135898/175000: episode: 3806, duration: 0.511s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 147.750 [9.000, 191.000], mean observation: 0.153 [0.000, 56.000], loss: 0.282706, mean_absolute_error: 0.642851, mean_q: 1.493522, mean_eps: 0.100000\n",
      " 135944/175000: episode: 3807, duration: 0.850s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.717 [1.000, 222.000], mean observation: 0.326 [0.000, 92.000], loss: 0.280272, mean_absolute_error: 0.656851, mean_q: 1.505647, mean_eps: 0.100000\n",
      " 135994/175000: episode: 3808, duration: 0.945s, episode steps: 50, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.100 [1.000, 211.000], mean observation: 0.683 [0.000, 100.000], loss: 0.728508, mean_absolute_error: 0.782709, mean_q: 2.925727, mean_eps: 0.100000\n",
      " 136016/175000: episode: 3809, duration: 0.427s, episode steps: 22, steps per second: 51, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 157.227 [82.000, 214.000], mean observation: 0.130 [0.000, 44.000], loss: 0.245124, mean_absolute_error: 0.666346, mean_q: 1.707981, mean_eps: 0.100000\n",
      " 136057/175000: episode: 3810, duration: 0.879s, episode steps: 41, steps per second: 47, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.512 [29.000, 211.000], mean observation: 0.277 [0.000, 82.000], loss: 0.374696, mean_absolute_error: 0.652875, mean_q: 1.569782, mean_eps: 0.100000\n",
      " 136088/175000: episode: 3811, duration: 0.595s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 150.290 [27.000, 211.000], mean observation: 0.334 [0.000, 62.000], loss: 0.294179, mean_absolute_error: 0.645508, mean_q: 1.553539, mean_eps: 0.100000\n",
      " 136120/175000: episode: 3812, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 105.844 [5.000, 197.000], mean observation: 0.227 [0.000, 64.000], loss: 0.319156, mean_absolute_error: 0.636016, mean_q: 1.504578, mean_eps: 0.100000\n",
      " 136145/175000: episode: 3813, duration: 0.507s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 54.000 [27.000, 150.000], mean observation: 0.113 [0.000, 50.000], loss: 0.244753, mean_absolute_error: 0.644425, mean_q: 1.545103, mean_eps: 0.100000\n",
      " 136185/175000: episode: 3814, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 115.275 [28.000, 211.000], mean observation: 0.367 [0.000, 80.000], loss: 0.190155, mean_absolute_error: 0.638719, mean_q: 1.558710, mean_eps: 0.100000\n",
      " 136208/175000: episode: 3815, duration: 0.501s, episode steps: 23, steps per second: 46, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 104.652 [6.000, 171.000], mean observation: 0.198 [0.000, 46.000], loss: 0.330065, mean_absolute_error: 0.635562, mean_q: 1.664374, mean_eps: 0.100000\n",
      " 136261/175000: episode: 3816, duration: 0.999s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 136.925 [6.000, 212.000], mean observation: 0.552 [0.000, 106.000], loss: 1.427604, mean_absolute_error: 0.636445, mean_q: 1.631233, mean_eps: 0.100000\n",
      " 136292/175000: episode: 3817, duration: 0.605s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.387 [6.000, 216.000], mean observation: 0.258 [0.000, 62.000], loss: 0.201471, mean_absolute_error: 0.622502, mean_q: 1.445160, mean_eps: 0.100000\n",
      " 136341/175000: episode: 3818, duration: 0.969s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 93.408 [39.000, 209.000], mean observation: 0.554 [0.000, 98.000], loss: 0.279197, mean_absolute_error: 0.631595, mean_q: 1.649738, mean_eps: 0.100000\n",
      " 136383/175000: episode: 3819, duration: 0.804s, episode steps: 42, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.619 [0.000, 171.000], mean observation: 0.319 [0.000, 84.000], loss: 0.368433, mean_absolute_error: 0.634486, mean_q: 1.566964, mean_eps: 0.100000\n",
      " 136443/175000: episode: 3820, duration: 1.096s, episode steps: 60, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 107.400 [0.000, 211.000], mean observation: 0.689 [0.000, 120.000], loss: 1.489700, mean_absolute_error: 0.758223, mean_q: 2.980415, mean_eps: 0.100000\n",
      " 136477/175000: episode: 3821, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.206 [0.000, 211.000], mean observation: 0.351 [0.000, 68.000], loss: 0.305507, mean_absolute_error: 0.640423, mean_q: 1.721179, mean_eps: 0.100000\n",
      " 136504/175000: episode: 3822, duration: 0.486s, episode steps: 27, steps per second: 56, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 147.630 [0.000, 193.000], mean observation: 0.195 [0.000, 54.000], loss: 0.241805, mean_absolute_error: 0.643089, mean_q: 1.716161, mean_eps: 0.100000\n",
      " 136541/175000: episode: 3823, duration: 0.712s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 143.946 [0.000, 190.000], mean observation: 0.311 [0.000, 74.000], loss: 79.595697, mean_absolute_error: 1.267256, mean_q: 5.051995, mean_eps: 0.100000\n",
      " 136586/175000: episode: 3824, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 124.444 [0.000, 224.000], mean observation: 0.521 [0.000, 90.000], loss: 0.415629, mean_absolute_error: 0.621149, mean_q: 1.559428, mean_eps: 0.100000\n",
      " 136631/175000: episode: 3825, duration: 0.811s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 103.889 [7.000, 175.000], mean observation: 0.437 [0.000, 90.000], loss: 0.366691, mean_absolute_error: 0.644458, mean_q: 1.722704, mean_eps: 0.100000\n",
      " 136667/175000: episode: 3826, duration: 0.660s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 108.000 [10.000, 177.000], mean observation: 0.308 [0.000, 72.000], loss: 4.930415, mean_absolute_error: 0.655978, mean_q: 1.644081, mean_eps: 0.100000\n",
      " 136693/175000: episode: 3827, duration: 0.497s, episode steps: 26, steps per second: 52, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 107.962 [10.000, 219.000], mean observation: 0.198 [0.000, 52.000], loss: 0.518813, mean_absolute_error: 0.654927, mean_q: 1.834488, mean_eps: 0.100000\n",
      " 136723/175000: episode: 3828, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 102.433 [10.000, 214.000], mean observation: 0.235 [0.000, 60.000], loss: 0.259787, mean_absolute_error: 0.654189, mean_q: 1.753247, mean_eps: 0.100000\n",
      " 136780/175000: episode: 3829, duration: 1.057s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 66.877 [13.000, 219.000], mean observation: 0.719 [0.000, 114.000], loss: 0.204823, mean_absolute_error: 0.648150, mean_q: 1.605890, mean_eps: 0.100000\n",
      " 136821/175000: episode: 3830, duration: 0.807s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.122 [24.000, 219.000], mean observation: 0.409 [0.000, 82.000], loss: 0.143664, mean_absolute_error: 0.652716, mean_q: 1.633553, mean_eps: 0.100000\n",
      " 136843/175000: episode: 3831, duration: 0.381s, episode steps: 22, steps per second: 58, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 135.045 [24.000, 211.000], mean observation: 0.121 [0.000, 44.000], loss: 0.225894, mean_absolute_error: 0.642352, mean_q: 1.487339, mean_eps: 0.100000\n",
      " 136876/175000: episode: 3832, duration: 0.660s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 70.212 [24.000, 179.000], mean observation: 0.313 [0.000, 66.000], loss: 0.179841, mean_absolute_error: 0.646844, mean_q: 1.576528, mean_eps: 0.100000\n",
      " 136924/175000: episode: 3833, duration: 0.965s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 122.396 [43.000, 219.000], mean observation: 0.499 [0.000, 96.000], loss: 18.548258, mean_absolute_error: 0.746036, mean_q: 1.753920, mean_eps: 0.100000\n",
      " 136945/175000: episode: 3834, duration: 0.430s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 150.000 [43.000, 217.000], mean observation: 0.182 [0.000, 42.000], loss: 0.274560, mean_absolute_error: 0.630690, mean_q: 1.361190, mean_eps: 0.100000\n",
      " 136973/175000: episode: 3835, duration: 0.509s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 144.643 [43.000, 222.000], mean observation: 0.158 [0.000, 56.000], loss: 103.605595, mean_absolute_error: 1.292720, mean_q: 3.901068, mean_eps: 0.100000\n",
      " 137029/175000: episode: 3836, duration: 1.022s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 164.625 [43.000, 211.000], mean observation: 0.773 [0.000, 112.000], loss: 40.002589, mean_absolute_error: 0.915258, mean_q: 2.749201, mean_eps: 0.100000\n",
      " 137051/175000: episode: 3837, duration: 0.369s, episode steps: 22, steps per second: 60, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 138.045 [46.000, 211.000], mean observation: 0.200 [0.000, 44.000], loss: 1.054851, mean_absolute_error: 0.662634, mean_q: 1.709627, mean_eps: 0.100000\n",
      " 137092/175000: episode: 3838, duration: 0.777s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 153.829 [96.000, 211.000], mean observation: 0.331 [0.000, 82.000], loss: 0.134523, mean_absolute_error: 0.630532, mean_q: 1.392143, mean_eps: 0.100000\n",
      " 137137/175000: episode: 3839, duration: 0.860s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 109.222 [1.000, 211.000], mean observation: 0.639 [0.000, 90.000], loss: 46.693919, mean_absolute_error: 0.958212, mean_q: 2.882610, mean_eps: 0.100000\n",
      " 137168/175000: episode: 3840, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 97.452 [31.000, 211.000], mean observation: 0.299 [0.000, 62.000], loss: 0.145698, mean_absolute_error: 0.625785, mean_q: 1.457106, mean_eps: 0.100000\n",
      " 137218/175000: episode: 3841, duration: 0.961s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 97.540 [7.000, 210.000], mean observation: 0.615 [0.000, 100.000], loss: 94.342699, mean_absolute_error: 1.148210, mean_q: 2.705589, mean_eps: 0.100000\n",
      " 137256/175000: episode: 3842, duration: 0.759s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 116.158 [28.000, 211.000], mean observation: 0.308 [0.000, 76.000], loss: 0.325130, mean_absolute_error: 0.645976, mean_q: 1.843278, mean_eps: 0.100000\n",
      " 137299/175000: episode: 3843, duration: 0.869s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.279 [1.000, 221.000], mean observation: 0.296 [0.000, 86.000], loss: 0.218952, mean_absolute_error: 0.631998, mean_q: 1.821954, mean_eps: 0.100000\n",
      " 137340/175000: episode: 3844, duration: 0.781s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 107.049 [11.000, 221.000], mean observation: 0.301 [0.000, 82.000], loss: 0.180871, mean_absolute_error: 0.622261, mean_q: 1.913335, mean_eps: 0.100000\n",
      " 137360/175000: episode: 3845, duration: 0.431s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 106.350 [41.000, 195.000], mean observation: 0.142 [0.000, 40.000], loss: 0.955242, mean_absolute_error: 0.618348, mean_q: 1.830582, mean_eps: 0.100000\n",
      " 137390/175000: episode: 3846, duration: 0.590s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 134.967 [43.000, 221.000], mean observation: 0.163 [0.000, 60.000], loss: 0.215224, mean_absolute_error: 0.613971, mean_q: 1.750408, mean_eps: 0.100000\n",
      " 137435/175000: episode: 3847, duration: 0.807s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 172.556 [33.000, 221.000], mean observation: 0.499 [0.000, 90.000], loss: 0.458920, mean_absolute_error: 0.621023, mean_q: 1.630527, mean_eps: 0.100000\n",
      " 137490/175000: episode: 3848, duration: 0.993s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 114.855 [27.000, 222.000], mean observation: 0.362 [0.000, 110.000], loss: 0.209371, mean_absolute_error: 0.606039, mean_q: 1.452222, mean_eps: 0.100000\n",
      " 137539/175000: episode: 3849, duration: 0.879s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 168.347 [42.000, 221.000], mean observation: 0.624 [0.000, 98.000], loss: 0.214124, mean_absolute_error: 0.612455, mean_q: 1.552286, mean_eps: 0.100000\n",
      " 137576/175000: episode: 3850, duration: 0.785s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 97.135 [38.000, 187.000], mean observation: 0.286 [0.000, 74.000], loss: 0.168558, mean_absolute_error: 0.613347, mean_q: 1.613095, mean_eps: 0.100000\n",
      " 137608/175000: episode: 3851, duration: 0.700s, episode steps: 32, steps per second: 46, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 71.125 [38.000, 208.000], mean observation: 0.104 [0.000, 64.000], loss: 0.167610, mean_absolute_error: 0.596860, mean_q: 1.534140, mean_eps: 0.100000\n",
      " 137643/175000: episode: 3852, duration: 0.712s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 104.314 [38.000, 208.000], mean observation: 0.318 [0.000, 70.000], loss: 0.610891, mean_absolute_error: 0.590189, mean_q: 1.387547, mean_eps: 0.100000\n",
      " 137668/175000: episode: 3853, duration: 0.514s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 112.200 [38.000, 208.000], mean observation: 0.154 [0.000, 50.000], loss: 0.155989, mean_absolute_error: 0.573466, mean_q: 1.296103, mean_eps: 0.100000\n",
      " 137699/175000: episode: 3854, duration: 0.651s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 134.871 [38.000, 208.000], mean observation: 0.285 [0.000, 62.000], loss: 0.196934, mean_absolute_error: 0.584987, mean_q: 1.539079, mean_eps: 0.100000\n",
      " 137744/175000: episode: 3855, duration: 0.905s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.289 [11.000, 208.000], mean observation: 0.367 [0.000, 90.000], loss: 0.754382, mean_absolute_error: 0.588599, mean_q: 1.451277, mean_eps: 0.100000\n",
      " 137771/175000: episode: 3856, duration: 0.547s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 178.333 [40.000, 218.000], mean observation: 0.182 [0.000, 54.000], loss: 0.303192, mean_absolute_error: 0.594516, mean_q: 1.424875, mean_eps: 0.100000\n",
      " 137825/175000: episode: 3857, duration: 1.034s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 102.926 [22.000, 208.000], mean observation: 0.440 [0.000, 108.000], loss: 0.201891, mean_absolute_error: 0.584289, mean_q: 1.353402, mean_eps: 0.100000\n",
      " 137859/175000: episode: 3858, duration: 0.572s, episode steps: 34, steps per second: 59, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 115.235 [4.000, 208.000], mean observation: 0.372 [0.000, 68.000], loss: 0.153053, mean_absolute_error: 0.571803, mean_q: 1.256937, mean_eps: 0.100000\n",
      " 137891/175000: episode: 3859, duration: 0.589s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 141.781 [4.000, 224.000], mean observation: 0.346 [0.000, 64.000], loss: 0.144297, mean_absolute_error: 0.583052, mean_q: 1.285403, mean_eps: 0.100000\n",
      " 137927/175000: episode: 3860, duration: 0.658s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.056 [9.000, 208.000], mean observation: 0.353 [0.000, 72.000], loss: 0.256052, mean_absolute_error: 0.583230, mean_q: 1.174982, mean_eps: 0.100000\n",
      " 137960/175000: episode: 3861, duration: 0.645s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 119.758 [7.000, 211.000], mean observation: 0.416 [0.000, 66.000], loss: 0.136400, mean_absolute_error: 0.587905, mean_q: 1.299464, mean_eps: 0.100000\n",
      " 137994/175000: episode: 3862, duration: 0.673s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 84.853 [12.000, 89.000], mean observation: 0.119 [0.000, 68.000], loss: 0.836925, mean_absolute_error: 0.603906, mean_q: 1.350849, mean_eps: 0.100000\n",
      " 138042/175000: episode: 3863, duration: 0.846s, episode steps: 48, steps per second: 57, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 123.208 [35.000, 208.000], mean observation: 0.259 [0.000, 96.000], loss: 1.370634, mean_absolute_error: 0.615075, mean_q: 1.411194, mean_eps: 0.100000\n",
      " 138085/175000: episode: 3864, duration: 0.779s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 73.674 [1.000, 208.000], mean observation: 0.397 [0.000, 86.000], loss: 0.350588, mean_absolute_error: 0.591269, mean_q: 1.301167, mean_eps: 0.100000\n",
      " 138139/175000: episode: 3865, duration: 1.011s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 82.611 [1.000, 223.000], mean observation: 0.388 [0.000, 108.000], loss: 92.462596, mean_absolute_error: 1.108674, mean_q: 2.637831, mean_eps: 0.100000\n",
      " 138198/175000: episode: 3866, duration: 1.074s, episode steps: 59, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 124.881 [23.000, 223.000], mean observation: 0.577 [0.000, 118.000], loss: 0.191025, mean_absolute_error: 0.588304, mean_q: 1.384303, mean_eps: 0.100000\n",
      " 138239/175000: episode: 3867, duration: 0.711s, episode steps: 41, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 169.683 [91.000, 207.000], mean observation: 0.342 [0.000, 82.000], loss: 0.179820, mean_absolute_error: 0.589977, mean_q: 1.307222, mean_eps: 0.100000\n",
      " 138279/175000: episode: 3868, duration: 0.751s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 111.125 [4.000, 207.000], mean observation: 0.269 [0.000, 80.000], loss: 0.176808, mean_absolute_error: 0.595927, mean_q: 1.345108, mean_eps: 0.100000\n",
      " 138312/175000: episode: 3869, duration: 0.644s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 123.273 [4.000, 195.000], mean observation: 0.253 [0.000, 66.000], loss: 0.122663, mean_absolute_error: 0.590739, mean_q: 1.308750, mean_eps: 0.100000\n",
      " 138338/175000: episode: 3870, duration: 0.520s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 156.769 [4.000, 195.000], mean observation: 0.085 [0.000, 52.000], loss: 0.134095, mean_absolute_error: 0.566545, mean_q: 1.174903, mean_eps: 0.100000\n",
      " 138382/175000: episode: 3871, duration: 0.839s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 81.773 [4.000, 217.000], mean observation: 0.552 [0.000, 88.000], loss: 7.422230, mean_absolute_error: 0.609099, mean_q: 1.352642, mean_eps: 0.100000\n",
      " 138411/175000: episode: 3872, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 107.931 [39.000, 195.000], mean observation: 0.292 [0.000, 58.000], loss: 0.146353, mean_absolute_error: 0.580382, mean_q: 1.329701, mean_eps: 0.100000\n",
      " 138444/175000: episode: 3873, duration: 0.631s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 82.424 [4.000, 197.000], mean observation: 0.366 [0.000, 66.000], loss: 0.167219, mean_absolute_error: 0.575278, mean_q: 1.328243, mean_eps: 0.100000\n",
      " 138480/175000: episode: 3874, duration: 0.764s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 96.222 [27.000, 208.000], mean observation: 0.447 [0.000, 72.000], loss: 0.865452, mean_absolute_error: 0.581554, mean_q: 1.415773, mean_eps: 0.100000\n",
      " 138506/175000: episode: 3875, duration: 0.517s, episode steps: 26, steps per second: 50, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 95.077 [27.000, 208.000], mean observation: 0.156 [0.000, 52.000], loss: 1.910477, mean_absolute_error: 0.584872, mean_q: 1.327834, mean_eps: 0.100000\n",
      " 138541/175000: episode: 3876, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.914 [6.000, 217.000], mean observation: 0.401 [0.000, 70.000], loss: 0.255531, mean_absolute_error: 0.582844, mean_q: 1.418498, mean_eps: 0.100000\n",
      " 138575/175000: episode: 3877, duration: 0.599s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 31.824 [27.000, 176.000], mean observation: 0.114 [0.000, 68.000], loss: 0.217612, mean_absolute_error: 0.608062, mean_q: 1.651174, mean_eps: 0.100000\n",
      " 138602/175000: episode: 3878, duration: 0.511s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 59.111 [27.000, 221.000], mean observation: 0.121 [0.000, 54.000], loss: 477.073761, mean_absolute_error: 2.927080, mean_q: 3.930896, mean_eps: 0.100000\n",
      " 138639/175000: episode: 3879, duration: 0.679s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 69.892 [27.000, 224.000], mean observation: 0.189 [0.000, 74.000], loss: 52.245083, mean_absolute_error: 1.013245, mean_q: 3.628356, mean_eps: 0.100000\n",
      " 138675/175000: episode: 3880, duration: 0.680s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 100.139 [27.000, 148.000], mean observation: 0.164 [0.000, 72.000], loss: 11.094770, mean_absolute_error: 0.671236, mean_q: 1.693483, mean_eps: 0.100000\n",
      " 138694/175000: episode: 3881, duration: 0.381s, episode steps: 19, steps per second: 50, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.046 [0.000, 38.000], loss: 0.246204, mean_absolute_error: 0.621442, mean_q: 1.656425, mean_eps: 0.100000\n",
      " 138711/175000: episode: 3882, duration: 0.301s, episode steps: 17, steps per second: 56, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.041 [0.000, 34.000], loss: 0.236945, mean_absolute_error: 0.601698, mean_q: 1.646163, mean_eps: 0.100000\n",
      " 138736/175000: episode: 3883, duration: 0.537s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 27.000 [27.000, 27.000], mean observation: 0.059 [0.000, 50.000], loss: 0.249648, mean_absolute_error: 0.611278, mean_q: 1.648682, mean_eps: 0.100000\n",
      " 138785/175000: episode: 3884, duration: 0.941s, episode steps: 49, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 122.796 [3.000, 213.000], mean observation: 0.543 [0.000, 98.000], loss: 0.212357, mean_absolute_error: 0.611315, mean_q: 1.530847, mean_eps: 0.100000\n",
      " 138823/175000: episode: 3885, duration: 0.665s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 58.500 [19.000, 222.000], mean observation: 0.178 [0.000, 76.000], loss: 0.256816, mean_absolute_error: 0.619317, mean_q: 1.829315, mean_eps: 0.100000\n",
      " 138843/175000: episode: 3886, duration: 0.397s, episode steps: 20, steps per second: 50, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 83.050 [20.000, 190.000], mean observation: 0.093 [0.000, 40.000], loss: 0.162074, mean_absolute_error: 0.605527, mean_q: 1.758949, mean_eps: 0.100000\n",
      " 138894/175000: episode: 3887, duration: 0.924s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 62.471 [27.000, 166.000], mean observation: 0.193 [0.000, 102.000], loss: 0.222180, mean_absolute_error: 0.595612, mean_q: 1.692150, mean_eps: 0.100000\n",
      " 138931/175000: episode: 3888, duration: 0.688s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 49.865 [24.000, 139.000], mean observation: 0.213 [0.000, 74.000], loss: 0.208253, mean_absolute_error: 0.586559, mean_q: 1.751700, mean_eps: 0.100000\n",
      " 138965/175000: episode: 3889, duration: 0.650s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 36.529 [23.000, 207.000], mean observation: 0.111 [0.000, 68.000], loss: 0.453079, mean_absolute_error: 0.602513, mean_q: 1.588042, mean_eps: 0.100000\n",
      " 139008/175000: episode: 3890, duration: 0.837s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 84.767 [24.000, 222.000], mean observation: 0.293 [0.000, 86.000], loss: 0.578423, mean_absolute_error: 0.589962, mean_q: 1.556711, mean_eps: 0.100000\n",
      " 139063/175000: episode: 3891, duration: 1.002s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 140.364 [2.000, 222.000], mean observation: 0.578 [0.000, 110.000], loss: 0.211817, mean_absolute_error: 0.595333, mean_q: 1.815074, mean_eps: 0.100000\n",
      " 139082/175000: episode: 3892, duration: 0.386s, episode steps: 19, steps per second: 49, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.684 [30.000, 222.000], mean observation: 0.083 [0.000, 38.000], loss: 0.172316, mean_absolute_error: 0.578719, mean_q: 1.855969, mean_eps: 0.100000\n",
      " 139113/175000: episode: 3893, duration: 0.572s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 170.710 [165.000, 222.000], mean observation: 0.076 [0.000, 62.000], loss: 0.181042, mean_absolute_error: 0.574969, mean_q: 1.720375, mean_eps: 0.100000\n",
      " 139149/175000: episode: 3894, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 154.278 [29.000, 222.000], mean observation: 0.387 [0.000, 72.000], loss: 0.137960, mean_absolute_error: 0.578446, mean_q: 1.454111, mean_eps: 0.100000\n",
      " 139169/175000: episode: 3895, duration: 0.405s, episode steps: 20, steps per second: 49, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 123.950 [1.000, 222.000], mean observation: 0.131 [0.000, 40.000], loss: 0.197428, mean_absolute_error: 0.588530, mean_q: 1.535812, mean_eps: 0.100000\n",
      " 139191/175000: episode: 3896, duration: 0.394s, episode steps: 22, steps per second: 56, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 165.000 [165.000, 165.000], mean observation: 0.053 [0.000, 44.000], loss: 0.204434, mean_absolute_error: 0.582040, mean_q: 1.536315, mean_eps: 0.100000\n",
      " 139222/175000: episode: 3897, duration: 0.579s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 156.000 [85.000, 195.000], mean observation: 0.091 [0.000, 62.000], loss: 90.578115, mean_absolute_error: 1.198567, mean_q: 3.958969, mean_eps: 0.100000\n",
      " 139259/175000: episode: 3898, duration: 0.674s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 154.784 [61.000, 220.000], mean observation: 0.224 [0.000, 74.000], loss: 0.652197, mean_absolute_error: 0.759449, mean_q: 3.536944, mean_eps: 0.100000\n",
      " 139299/175000: episode: 3899, duration: 0.734s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 156.550 [17.000, 214.000], mean observation: 0.391 [0.000, 80.000], loss: 1.614958, mean_absolute_error: 0.603513, mean_q: 1.710616, mean_eps: 0.100000\n",
      " 139324/175000: episode: 3900, duration: 0.487s, episode steps: 25, steps per second: 51, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 118.560 [43.000, 195.000], mean observation: 0.160 [0.000, 50.000], loss: 0.158423, mean_absolute_error: 0.589956, mean_q: 1.605429, mean_eps: 0.100000\n",
      " 139347/175000: episode: 3901, duration: 0.450s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 84.000 [43.000, 189.000], mean observation: 0.163 [0.000, 46.000], loss: 0.114460, mean_absolute_error: 0.580181, mean_q: 1.697928, mean_eps: 0.100000\n",
      " 139384/175000: episode: 3902, duration: 0.718s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 75.784 [7.000, 217.000], mean observation: 0.448 [0.000, 74.000], loss: 0.198584, mean_absolute_error: 0.562528, mean_q: 1.605618, mean_eps: 0.100000\n",
      " 139417/175000: episode: 3903, duration: 0.679s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.212 [13.000, 189.000], mean observation: 0.372 [0.000, 66.000], loss: 0.213776, mean_absolute_error: 0.545003, mean_q: 1.603011, mean_eps: 0.100000\n",
      " 139450/175000: episode: 3904, duration: 0.600s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.030 [4.000, 223.000], mean observation: 0.374 [0.000, 66.000], loss: 0.160768, mean_absolute_error: 0.573578, mean_q: 1.935186, mean_eps: 0.100000\n",
      " 139489/175000: episode: 3905, duration: 0.720s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.205 [7.000, 202.000], mean observation: 0.440 [0.000, 78.000], loss: 0.160649, mean_absolute_error: 0.575479, mean_q: 1.866565, mean_eps: 0.100000\n",
      " 139525/175000: episode: 3906, duration: 0.647s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 76.278 [7.000, 207.000], mean observation: 0.405 [0.000, 72.000], loss: 0.160776, mean_absolute_error: 0.578238, mean_q: 1.768710, mean_eps: 0.100000\n",
      " 139563/175000: episode: 3907, duration: 0.712s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 63.316 [5.000, 185.000], mean observation: 0.411 [0.000, 76.000], loss: 0.240369, mean_absolute_error: 0.575807, mean_q: 1.701322, mean_eps: 0.100000\n",
      " 139596/175000: episode: 3908, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 101.485 [2.000, 185.000], mean observation: 0.440 [0.000, 66.000], loss: 0.242548, mean_absolute_error: 0.572841, mean_q: 1.640331, mean_eps: 0.100000\n",
      " 139630/175000: episode: 3909, duration: 0.722s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 102.294 [4.000, 187.000], mean observation: 0.348 [0.000, 68.000], loss: 0.218324, mean_absolute_error: 0.559006, mean_q: 1.520971, mean_eps: 0.100000\n",
      " 139677/175000: episode: 3910, duration: 0.883s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 90.702 [4.000, 204.000], mean observation: 0.518 [0.000, 94.000], loss: 0.193812, mean_absolute_error: 0.539134, mean_q: 1.269006, mean_eps: 0.100000\n",
      " 139719/175000: episode: 3911, duration: 0.742s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 91.000 [14.000, 221.000], mean observation: 0.358 [0.000, 84.000], loss: 0.499223, mean_absolute_error: 0.558684, mean_q: 1.334623, mean_eps: 0.100000\n",
      " 139741/175000: episode: 3912, duration: 0.441s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 88.136 [14.000, 167.000], mean observation: 0.128 [0.000, 44.000], loss: 0.223200, mean_absolute_error: 0.574227, mean_q: 1.329504, mean_eps: 0.100000\n",
      " 139762/175000: episode: 3913, duration: 0.429s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 127.571 [49.000, 220.000], mean observation: 0.098 [0.000, 42.000], loss: 0.372637, mean_absolute_error: 0.550882, mean_q: 1.124279, mean_eps: 0.100000\n",
      " 139808/175000: episode: 3914, duration: 0.867s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 162.043 [14.000, 220.000], mean observation: 0.477 [0.000, 92.000], loss: 0.236748, mean_absolute_error: 0.564575, mean_q: 1.186024, mean_eps: 0.100000\n",
      " 139853/175000: episode: 3915, duration: 0.897s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 101.400 [13.000, 220.000], mean observation: 0.453 [0.000, 90.000], loss: 0.272766, mean_absolute_error: 0.568684, mean_q: 1.172704, mean_eps: 0.100000\n",
      " 139897/175000: episode: 3916, duration: 0.834s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 85.455 [28.000, 223.000], mean observation: 0.446 [0.000, 88.000], loss: 0.203230, mean_absolute_error: 0.572482, mean_q: 1.325011, mean_eps: 0.100000\n",
      " 139937/175000: episode: 3917, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 112.900 [7.000, 220.000], mean observation: 0.460 [0.000, 80.000], loss: 0.243718, mean_absolute_error: 0.562725, mean_q: 1.259263, mean_eps: 0.100000\n",
      " 139976/175000: episode: 3918, duration: 0.709s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 134.872 [33.000, 220.000], mean observation: 0.418 [0.000, 78.000], loss: 0.208449, mean_absolute_error: 0.564567, mean_q: 1.276894, mean_eps: 0.100000\n",
      " 140005/175000: episode: 3919, duration: 0.624s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 160.759 [3.000, 220.000], mean observation: 0.299 [0.000, 58.000], loss: 0.428741, mean_absolute_error: 0.564009, mean_q: 1.259741, mean_eps: 0.100000\n",
      " 140035/175000: episode: 3920, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 104.100 [1.000, 220.000], mean observation: 0.227 [0.000, 60.000], loss: 0.392730, mean_absolute_error: 0.559652, mean_q: 1.198535, mean_eps: 0.100000\n",
      " 140092/175000: episode: 3921, duration: 1.070s, episode steps: 57, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 139.842 [1.000, 223.000], mean observation: 0.459 [0.000, 114.000], loss: 0.264959, mean_absolute_error: 0.562694, mean_q: 1.187116, mean_eps: 0.100000\n",
      " 140132/175000: episode: 3922, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 147.225 [1.000, 220.000], mean observation: 0.489 [0.000, 80.000], loss: 0.520871, mean_absolute_error: 0.545939, mean_q: 1.231174, mean_eps: 0.100000\n",
      " 140170/175000: episode: 3923, duration: 0.736s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.974 [61.000, 221.000], mean observation: 0.449 [0.000, 76.000], loss: 0.371969, mean_absolute_error: 0.551984, mean_q: 1.477900, mean_eps: 0.100000\n",
      " 140199/175000: episode: 3924, duration: 0.508s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 107.103 [10.000, 200.000], mean observation: 0.209 [0.000, 58.000], loss: 14.576858, mean_absolute_error: 0.622996, mean_q: 1.515582, mean_eps: 0.100000\n",
      " 140228/175000: episode: 3925, duration: 0.542s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 114.655 [24.000, 200.000], mean observation: 0.121 [0.000, 58.000], loss: 0.269289, mean_absolute_error: 0.550089, mean_q: 1.362919, mean_eps: 0.100000\n",
      " 140269/175000: episode: 3926, duration: 0.772s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.707 [4.000, 222.000], mean observation: 0.332 [0.000, 82.000], loss: 1.186801, mean_absolute_error: 0.564499, mean_q: 1.462777, mean_eps: 0.100000\n",
      " 140281/175000: episode: 3927, duration: 0.214s, episode steps: 12, steps per second: 56, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 119.500 [59.000, 205.000], mean observation: 0.073 [0.000, 24.000], loss: 1.647897, mean_absolute_error: 0.555590, mean_q: 1.398005, mean_eps: 0.100000\n",
      " 140337/175000: episode: 3928, duration: 1.020s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 99.661 [4.000, 205.000], mean observation: 0.547 [0.000, 112.000], loss: 0.656371, mean_absolute_error: 0.655473, mean_q: 2.675812, mean_eps: 0.100000\n",
      " 140370/175000: episode: 3929, duration: 0.588s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.485 [35.000, 220.000], mean observation: 0.193 [0.000, 66.000], loss: 0.260780, mean_absolute_error: 0.545053, mean_q: 1.498005, mean_eps: 0.100000\n",
      " 140421/175000: episode: 3930, duration: 0.954s, episode steps: 51, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 140.235 [4.000, 220.000], mean observation: 0.242 [0.000, 102.000], loss: 0.453544, mean_absolute_error: 0.545008, mean_q: 1.492179, mean_eps: 0.100000\n",
      " 140451/175000: episode: 3931, duration: 0.573s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.933 [89.000, 215.000], mean observation: 0.073 [0.000, 60.000], loss: 2289.558239, mean_absolute_error: 10.948433, mean_q: 4.032811, mean_eps: 0.100000\n",
      " 140485/175000: episode: 3932, duration: 0.655s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.824 [4.000, 222.000], mean observation: 0.403 [0.000, 68.000], loss: 0.221309, mean_absolute_error: 0.558319, mean_q: 1.654886, mean_eps: 0.100000\n",
      " 140527/175000: episode: 3933, duration: 0.736s, episode steps: 42, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 144.738 [4.000, 220.000], mean observation: 0.478 [0.000, 84.000], loss: 0.306535, mean_absolute_error: 0.546331, mean_q: 1.582573, mean_eps: 0.100000\n",
      " 140564/175000: episode: 3934, duration: 0.714s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 76.054 [9.000, 195.000], mean observation: 0.427 [0.000, 74.000], loss: 0.145805, mean_absolute_error: 0.533869, mean_q: 1.428890, mean_eps: 0.100000\n",
      " 140611/175000: episode: 3935, duration: 0.849s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 111.787 [3.000, 222.000], mean observation: 0.534 [0.000, 94.000], loss: 2879.278646, mean_absolute_error: 13.565975, mean_q: 4.193565, mean_eps: 0.100000\n",
      " 140639/175000: episode: 3936, duration: 0.523s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 128.143 [4.000, 222.000], mean observation: 0.241 [0.000, 56.000], loss: 0.181688, mean_absolute_error: 0.768186, mean_q: 4.038550, mean_eps: 0.100000\n",
      " 140653/175000: episode: 3937, duration: 0.312s, episode steps: 14, steps per second: 45, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 107.357 [67.000, 148.000], mean observation: 0.051 [0.000, 28.000], loss: 0.148960, mean_absolute_error: 0.546775, mean_q: 1.534121, mean_eps: 0.100000\n",
      " 140699/175000: episode: 3938, duration: 0.826s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 112.783 [9.000, 220.000], mean observation: 0.457 [0.000, 92.000], loss: 0.420402, mean_absolute_error: 0.534931, mean_q: 1.590352, mean_eps: 0.100000\n",
      " 140739/175000: episode: 3939, duration: 0.733s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 131.100 [1.000, 220.000], mean observation: 0.418 [0.000, 80.000], loss: 0.179667, mean_absolute_error: 0.538302, mean_q: 1.565987, mean_eps: 0.100000\n",
      " 140763/175000: episode: 3940, duration: 0.450s, episode steps: 24, steps per second: 53, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.792 [20.000, 220.000], mean observation: 0.141 [0.000, 48.000], loss: 0.141220, mean_absolute_error: 0.637286, mean_q: 2.782218, mean_eps: 0.100000\n",
      " 140793/175000: episode: 3941, duration: 0.596s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 122.800 [1.000, 212.000], mean observation: 0.170 [0.000, 60.000], loss: 0.146506, mean_absolute_error: 0.530309, mean_q: 1.585197, mean_eps: 0.100000\n",
      " 140825/175000: episode: 3942, duration: 0.572s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.219 [31.000, 222.000], mean observation: 0.223 [0.000, 64.000], loss: 0.198619, mean_absolute_error: 0.516977, mean_q: 1.561338, mean_eps: 0.100000\n",
      " 140859/175000: episode: 3943, duration: 0.589s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 153.118 [24.000, 220.000], mean observation: 0.325 [0.000, 68.000], loss: 0.321377, mean_absolute_error: 0.528177, mean_q: 1.629776, mean_eps: 0.100000\n",
      " 140901/175000: episode: 3944, duration: 0.799s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 157.738 [6.000, 220.000], mean observation: 0.384 [0.000, 84.000], loss: 0.168035, mean_absolute_error: 0.514412, mean_q: 1.483347, mean_eps: 0.100000\n",
      " 140932/175000: episode: 3945, duration: 0.578s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 156.516 [37.000, 220.000], mean observation: 0.222 [0.000, 62.000], loss: 2116.801785, mean_absolute_error: 10.129425, mean_q: 3.888890, mean_eps: 0.100000\n",
      " 140954/175000: episode: 3946, duration: 0.472s, episode steps: 22, steps per second: 47, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 119.091 [24.000, 220.000], mean observation: 0.091 [0.000, 44.000], loss: 0.088201, mean_absolute_error: 0.508587, mean_q: 1.508790, mean_eps: 0.100000\n",
      " 140971/175000: episode: 3947, duration: 0.301s, episode steps: 17, steps per second: 57, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 80.000 [24.000, 220.000], mean observation: 0.049 [0.000, 34.000], loss: 0.127334, mean_absolute_error: 0.502414, mean_q: 1.368551, mean_eps: 0.100000\n",
      " 141014/175000: episode: 3948, duration: 0.760s, episode steps: 43, steps per second: 57, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 108.744 [24.000, 220.000], mean observation: 0.381 [0.000, 86.000], loss: 0.362869, mean_absolute_error: 0.502281, mean_q: 1.260458, mean_eps: 0.100000\n",
      " 141059/175000: episode: 3949, duration: 0.821s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.778 [24.000, 207.000], mean observation: 0.460 [0.000, 90.000], loss: 1.143965, mean_absolute_error: 0.523888, mean_q: 1.460932, mean_eps: 0.100000\n",
      " 141084/175000: episode: 3950, duration: 0.480s, episode steps: 25, steps per second: 52, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 107.080 [37.000, 195.000], mean observation: 0.215 [0.000, 50.000], loss: 0.160626, mean_absolute_error: 0.527009, mean_q: 1.601441, mean_eps: 0.100000\n",
      " 141124/175000: episode: 3951, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 173.225 [37.000, 204.000], mean observation: 0.380 [0.000, 80.000], loss: 0.133616, mean_absolute_error: 0.516041, mean_q: 1.477730, mean_eps: 0.100000\n",
      " 141165/175000: episode: 3952, duration: 0.809s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 141.024 [16.000, 204.000], mean observation: 0.475 [0.000, 82.000], loss: 1.183739, mean_absolute_error: 0.511047, mean_q: 1.338305, mean_eps: 0.100000\n",
      " 141205/175000: episode: 3953, duration: 0.714s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 99.875 [18.000, 222.000], mean observation: 0.282 [0.000, 80.000], loss: 0.815046, mean_absolute_error: 0.512993, mean_q: 1.459620, mean_eps: 0.100000\n",
      " 141247/175000: episode: 3954, duration: 0.727s, episode steps: 42, steps per second: 58, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.071 [4.000, 222.000], mean observation: 0.506 [0.000, 84.000], loss: 0.487210, mean_absolute_error: 0.506434, mean_q: 1.518403, mean_eps: 0.100000\n",
      " 141277/175000: episode: 3955, duration: 0.580s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 130.933 [61.000, 222.000], mean observation: 0.185 [0.000, 60.000], loss: 0.152439, mean_absolute_error: 0.508826, mean_q: 1.417747, mean_eps: 0.100000\n",
      " 141317/175000: episode: 3956, duration: 0.778s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.600 [12.000, 222.000], mean observation: 0.299 [0.000, 80.000], loss: 510.912857, mean_absolute_error: 2.945116, mean_q: 3.240228, mean_eps: 0.100000\n",
      " 141365/175000: episode: 3957, duration: 0.926s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 146.229 [7.000, 215.000], mean observation: 0.680 [0.000, 96.000], loss: 0.150007, mean_absolute_error: 0.536258, mean_q: 1.549514, mean_eps: 0.100000\n",
      " 141419/175000: episode: 3958, duration: 0.953s, episode steps: 54, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 127.630 [19.000, 201.000], mean observation: 0.543 [0.000, 108.000], loss: 6.264453, mean_absolute_error: 0.683977, mean_q: 2.819124, mean_eps: 0.100000\n",
      " 141465/175000: episode: 3959, duration: 0.841s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 124.457 [14.000, 191.000], mean observation: 0.584 [0.000, 92.000], loss: 0.162301, mean_absolute_error: 0.540830, mean_q: 1.549853, mean_eps: 0.100000\n",
      " 141502/175000: episode: 3960, duration: 0.712s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 126.757 [14.000, 222.000], mean observation: 0.377 [0.000, 74.000], loss: 0.193379, mean_absolute_error: 0.531443, mean_q: 1.467613, mean_eps: 0.100000\n",
      " 141528/175000: episode: 3961, duration: 0.580s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 108.077 [8.000, 222.000], mean observation: 0.235 [0.000, 52.000], loss: 0.094763, mean_absolute_error: 0.533081, mean_q: 1.495136, mean_eps: 0.100000\n",
      " 141579/175000: episode: 3962, duration: 0.937s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.510 [8.000, 222.000], mean observation: 0.468 [0.000, 102.000], loss: 0.112999, mean_absolute_error: 0.538015, mean_q: 1.413553, mean_eps: 0.100000\n",
      " 141619/175000: episode: 3963, duration: 0.863s, episode steps: 40, steps per second: 46, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 66.500 [8.000, 222.000], mean observation: 0.302 [0.000, 80.000], loss: 0.105926, mean_absolute_error: 0.535905, mean_q: 1.186683, mean_eps: 0.100000\n",
      " 141636/175000: episode: 3964, duration: 0.384s, episode steps: 17, steps per second: 44, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 149.118 [92.000, 224.000], mean observation: 0.045 [0.000, 34.000], loss: 0.179320, mean_absolute_error: 0.537822, mean_q: 1.097270, mean_eps: 0.100000\n",
      " 141685/175000: episode: 3965, duration: 0.931s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 119.653 [33.000, 215.000], mean observation: 0.458 [0.000, 98.000], loss: 0.467793, mean_absolute_error: 0.542058, mean_q: 1.106273, mean_eps: 0.100000\n",
      " 141714/175000: episode: 3966, duration: 0.504s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 42.655 [14.000, 103.000], mean observation: 0.134 [0.000, 58.000], loss: 0.505576, mean_absolute_error: 0.542400, mean_q: 1.112208, mean_eps: 0.100000\n",
      " 141735/175000: episode: 3967, duration: 0.404s, episode steps: 21, steps per second: 52, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 39.571 [33.000, 56.000], mean observation: 0.054 [0.000, 42.000], loss: 0.166334, mean_absolute_error: 0.538338, mean_q: 1.113261, mean_eps: 0.100000\n",
      " 141768/175000: episode: 3968, duration: 0.646s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 44.848 [33.000, 56.000], mean observation: 0.092 [0.000, 66.000], loss: 0.641732, mean_absolute_error: 0.733125, mean_q: 3.291359, mean_eps: 0.100000\n",
      " 141791/175000: episode: 3969, duration: 0.460s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 103.565 [24.000, 208.000], mean observation: 0.147 [0.000, 46.000], loss: 0.246527, mean_absolute_error: 0.530808, mean_q: 1.095159, mean_eps: 0.100000\n",
      " 141843/175000: episode: 3970, duration: 0.949s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 119.442 [24.000, 208.000], mean observation: 0.841 [0.000, 104.000], loss: 49.109581, mean_absolute_error: 0.988400, mean_q: 3.634993, mean_eps: 0.100000\n",
      " 141874/175000: episode: 3971, duration: 0.571s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 69.581 [8.000, 210.000], mean observation: 0.215 [0.000, 62.000], loss: 83.932510, mean_absolute_error: 0.905736, mean_q: 0.963928, mean_eps: 0.100000\n",
      " 141926/175000: episode: 3972, duration: 0.935s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 91.942 [8.000, 210.000], mean observation: 0.525 [0.000, 104.000], loss: 3754.959798, mean_absolute_error: 17.440036, mean_q: 3.578379, mean_eps: 0.100000\n",
      " 141959/175000: episode: 3973, duration: 0.581s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 46.636 [8.000, 209.000], mean observation: 0.088 [0.000, 66.000], loss: 0.318476, mean_absolute_error: 0.702475, mean_q: 2.979700, mean_eps: 0.100000\n",
      " 142013/175000: episode: 3974, duration: 1.021s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.124 [0.000, 108.000], loss: 0.121185, mean_absolute_error: 0.528584, mean_q: 1.107325, mean_eps: 0.100000\n",
      " 142063/175000: episode: 3975, duration: 0.881s, episode steps: 50, steps per second: 57, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.060 [5.000, 187.000], mean observation: 0.239 [0.000, 100.000], loss: 0.137814, mean_absolute_error: 0.527598, mean_q: 1.168397, mean_eps: 0.100000\n",
      " 142114/175000: episode: 3976, duration: 1.096s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 113.157 [8.000, 208.000], mean observation: 0.522 [0.000, 102.000], loss: 0.324150, mean_absolute_error: 0.524871, mean_q: 1.028265, mean_eps: 0.100000\n",
      " 142130/175000: episode: 3977, duration: 0.296s, episode steps: 16, steps per second: 54, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 183.250 [177.000, 187.000], mean observation: 0.044 [0.000, 32.000], loss: 0.117606, mean_absolute_error: 0.535820, mean_q: 1.053711, mean_eps: 0.100000\n",
      " 142152/175000: episode: 3978, duration: 0.457s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 109.045 [8.000, 187.000], mean observation: 0.095 [0.000, 44.000], loss: 0.089954, mean_absolute_error: 0.527116, mean_q: 0.922870, mean_eps: 0.100000\n",
      " 142185/175000: episode: 3979, duration: 0.687s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.424 [2.000, 221.000], mean observation: 0.292 [0.000, 66.000], loss: 0.228636, mean_absolute_error: 0.545127, mean_q: 1.024906, mean_eps: 0.100000\n",
      " 142228/175000: episode: 3980, duration: 0.798s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 148.884 [14.000, 187.000], mean observation: 0.278 [0.000, 86.000], loss: 0.128655, mean_absolute_error: 0.552134, mean_q: 1.108089, mean_eps: 0.100000\n",
      " 142249/175000: episode: 3981, duration: 0.437s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 131.952 [37.000, 193.000], mean observation: 0.124 [0.000, 42.000], loss: 0.114378, mean_absolute_error: 0.550733, mean_q: 1.041149, mean_eps: 0.100000\n",
      " 142280/175000: episode: 3982, duration: 0.579s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 122.419 [5.000, 197.000], mean observation: 0.132 [0.000, 62.000], loss: 0.185135, mean_absolute_error: 0.547094, mean_q: 1.090072, mean_eps: 0.100000\n",
      " 142324/175000: episode: 3983, duration: 0.848s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.523 [4.000, 200.000], mean observation: 0.434 [0.000, 88.000], loss: 0.138809, mean_absolute_error: 0.551424, mean_q: 1.017455, mean_eps: 0.100000\n",
      " 142363/175000: episode: 3984, duration: 0.716s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 44.179 [5.000, 196.000], mean observation: 0.203 [0.000, 78.000], loss: 0.376027, mean_absolute_error: 0.550268, mean_q: 1.097086, mean_eps: 0.100000\n",
      " 142384/175000: episode: 3985, duration: 0.440s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 113.476 [6.000, 215.000], mean observation: 0.152 [0.000, 42.000], loss: 0.550265, mean_absolute_error: 0.525656, mean_q: 1.071920, mean_eps: 0.100000\n",
      " 142419/175000: episode: 3986, duration: 0.715s, episode steps: 35, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 22.543 [4.000, 175.000], mean observation: 0.203 [0.000, 70.000], loss: 0.475456, mean_absolute_error: 0.530577, mean_q: 1.117152, mean_eps: 0.100000\n",
      " 142455/175000: episode: 3987, duration: 0.673s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 13.750 [7.000, 97.000], mean observation: 0.159 [0.000, 72.000], loss: 0.121765, mean_absolute_error: 0.517891, mean_q: 1.124832, mean_eps: 0.100000\n",
      " 142494/175000: episode: 3988, duration: 0.714s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 46.513 [7.000, 211.000], mean observation: 0.183 [0.000, 78.000], loss: 1.792099, mean_absolute_error: 0.522314, mean_q: 1.055189, mean_eps: 0.100000\n",
      " 142531/175000: episode: 3989, duration: 0.656s, episode steps: 37, steps per second: 56, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 7.865 [7.000, 39.000], mean observation: 0.088 [0.000, 74.000], loss: 0.174854, mean_absolute_error: 0.522617, mean_q: 1.052570, mean_eps: 0.100000\n",
      " 142567/175000: episode: 3990, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 102.694 [7.000, 206.000], mean observation: 0.373 [0.000, 72.000], loss: 0.089188, mean_absolute_error: 0.527518, mean_q: 1.074849, mean_eps: 0.100000\n",
      " 142607/175000: episode: 3991, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 13.625 [7.000, 145.000], mean observation: 0.164 [0.000, 80.000], loss: 0.126614, mean_absolute_error: 0.542462, mean_q: 1.136509, mean_eps: 0.100000\n",
      " 142634/175000: episode: 3992, duration: 0.500s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 31.667 [1.000, 187.000], mean observation: 0.191 [0.000, 54.000], loss: 0.080492, mean_absolute_error: 0.538368, mean_q: 1.156251, mean_eps: 0.100000\n",
      " 142673/175000: episode: 3993, duration: 0.703s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.410 [7.000, 196.000], mean observation: 0.374 [0.000, 78.000], loss: 0.150002, mean_absolute_error: 0.556429, mean_q: 1.429647, mean_eps: 0.100000\n",
      " 142710/175000: episode: 3994, duration: 0.668s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.378 [8.000, 223.000], mean observation: 0.391 [0.000, 74.000], loss: 1.146314, mean_absolute_error: 0.566426, mean_q: 1.319845, mean_eps: 0.100000\n",
      " 142759/175000: episode: 3995, duration: 0.890s, episode steps: 49, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.224 [43.000, 211.000], mean observation: 0.552 [0.000, 98.000], loss: 5185.757220, mean_absolute_error: 23.732254, mean_q: 2.762558, mean_eps: 0.100000\n",
      " 142786/175000: episode: 3996, duration: 0.494s, episode steps: 27, steps per second: 55, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 103.074 [49.000, 196.000], mean observation: 0.120 [0.000, 54.000], loss: 0.117219, mean_absolute_error: 0.563722, mean_q: 1.304754, mean_eps: 0.100000\n",
      " 142828/175000: episode: 3997, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.429 [13.000, 137.000], mean observation: 0.173 [0.000, 84.000], loss: 1103.201534, mean_absolute_error: 5.614434, mean_q: 3.047626, mean_eps: 0.100000\n",
      " 142852/175000: episode: 3998, duration: 0.511s, episode steps: 24, steps per second: 47, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 81.292 [49.000, 137.000], mean observation: 0.084 [0.000, 48.000], loss: 0.192465, mean_absolute_error: 0.555087, mean_q: 1.359892, mean_eps: 0.100000\n",
      " 142878/175000: episode: 3999, duration: 0.492s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.615 [49.000, 187.000], mean observation: 0.177 [0.000, 52.000], loss: 0.204351, mean_absolute_error: 0.543258, mean_q: 1.081954, mean_eps: 0.100000\n",
      " 142905/175000: episode: 4000, duration: 0.533s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 151.000 [49.000, 213.000], mean observation: 0.201 [0.000, 54.000], loss: 0.137936, mean_absolute_error: 0.552723, mean_q: 1.141036, mean_eps: 0.100000\n",
      " 142943/175000: episode: 4001, duration: 0.664s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 156.553 [0.000, 182.000], mean observation: 0.225 [0.000, 76.000], loss: 0.208239, mean_absolute_error: 0.544844, mean_q: 1.018621, mean_eps: 0.100000\n",
      " 142971/175000: episode: 4002, duration: 0.551s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 148.071 [11.000, 182.000], mean observation: 0.110 [0.000, 56.000], loss: 0.087313, mean_absolute_error: 0.557005, mean_q: 1.155514, mean_eps: 0.100000\n",
      " 143015/175000: episode: 4003, duration: 0.781s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 77.545 [6.000, 141.000], mean observation: 0.287 [0.000, 88.000], loss: 0.172219, mean_absolute_error: 0.544982, mean_q: 1.127832, mean_eps: 0.100000\n",
      " 143040/175000: episode: 4004, duration: 0.515s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 94.280 [6.000, 171.000], mean observation: 0.177 [0.000, 50.000], loss: 0.075356, mean_absolute_error: 0.537440, mean_q: 0.991296, mean_eps: 0.100000\n",
      " 143061/175000: episode: 4005, duration: 0.424s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 135.381 [6.000, 200.000], mean observation: 0.142 [0.000, 42.000], loss: 0.102263, mean_absolute_error: 0.529388, mean_q: 1.001341, mean_eps: 0.100000\n",
      " 143089/175000: episode: 4006, duration: 0.530s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 164.429 [86.000, 206.000], mean observation: 0.107 [0.000, 56.000], loss: 0.123307, mean_absolute_error: 0.544151, mean_q: 1.152573, mean_eps: 0.100000\n",
      " 143132/175000: episode: 4007, duration: 0.809s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 129.233 [27.000, 206.000], mean observation: 0.288 [0.000, 86.000], loss: 0.152659, mean_absolute_error: 0.543003, mean_q: 0.993069, mean_eps: 0.100000\n",
      " 143152/175000: episode: 4008, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 161.850 [38.000, 215.000], mean observation: 0.133 [0.000, 40.000], loss: 0.126053, mean_absolute_error: 0.534413, mean_q: 0.647858, mean_eps: 0.100000\n",
      " 143177/175000: episode: 4009, duration: 0.536s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 152.280 [38.000, 215.000], mean observation: 0.182 [0.000, 50.000], loss: 0.130730, mean_absolute_error: 0.557679, mean_q: 1.007892, mean_eps: 0.100000\n",
      " 143209/175000: episode: 4010, duration: 0.577s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 149.844 [73.000, 215.000], mean observation: 0.308 [0.000, 64.000], loss: 0.098536, mean_absolute_error: 0.545776, mean_q: 0.785621, mean_eps: 0.100000\n",
      " 143268/175000: episode: 4011, duration: 1.116s, episode steps: 59, steps per second: 53, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 129.508 [10.000, 224.000], mean observation: 0.763 [0.000, 118.000], loss: 0.275438, mean_absolute_error: 0.663219, mean_q: 2.095268, mean_eps: 0.100000\n",
      " 143327/175000: episode: 4012, duration: 1.080s, episode steps: 59, steps per second: 55, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 141.305 [48.000, 198.000], mean observation: 0.759 [0.000, 118.000], loss: 0.109993, mean_absolute_error: 0.598645, mean_q: 1.320157, mean_eps: 0.100000\n",
      " 143357/175000: episode: 4013, duration: 0.562s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.000 [23.000, 191.000], mean observation: 0.279 [0.000, 60.000], loss: 0.201215, mean_absolute_error: 0.562297, mean_q: 0.850997, mean_eps: 0.100000\n",
      " 143389/175000: episode: 4014, duration: 0.584s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 135.031 [2.000, 222.000], mean observation: 0.249 [0.000, 64.000], loss: 0.142178, mean_absolute_error: 0.566902, mean_q: 0.959000, mean_eps: 0.100000\n",
      " 143441/175000: episode: 4015, duration: 0.969s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 151.173 [48.000, 221.000], mean observation: 0.551 [0.000, 104.000], loss: 1.121604, mean_absolute_error: 0.566682, mean_q: 0.968361, mean_eps: 0.100000\n",
      " 143504/175000: episode: 4016, duration: 1.134s, episode steps: 63, steps per second: 56, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 156.762 [61.000, 199.000], mean observation: 0.943 [0.000, 126.000], loss: 0.212298, mean_absolute_error: 0.554314, mean_q: 0.827938, mean_eps: 0.100000\n",
      " 143547/175000: episode: 4017, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 166.372 [6.000, 213.000], mean observation: 0.530 [0.000, 86.000], loss: 0.998690, mean_absolute_error: 0.562566, mean_q: 0.920988, mean_eps: 0.100000\n",
      " 143587/175000: episode: 4018, duration: 0.737s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 177.225 [32.000, 215.000], mean observation: 0.441 [0.000, 80.000], loss: 0.147163, mean_absolute_error: 0.554813, mean_q: 1.054495, mean_eps: 0.100000\n",
      " 143648/175000: episode: 4019, duration: 1.229s, episode steps: 61, steps per second: 50, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 165.525 [4.000, 215.000], mean observation: 0.737 [0.000, 122.000], loss: 0.263847, mean_absolute_error: 0.546199, mean_q: 0.932493, mean_eps: 0.100000\n",
      " 143692/175000: episode: 4020, duration: 0.841s, episode steps: 44, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 156.159 [0.000, 220.000], mean observation: 0.522 [0.000, 88.000], loss: 558.887601, mean_absolute_error: 3.183392, mean_q: 2.561247, mean_eps: 0.100000\n",
      " 143728/175000: episode: 4021, duration: 0.707s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 131.306 [87.000, 215.000], mean observation: 0.193 [0.000, 72.000], loss: 0.105291, mean_absolute_error: 0.559870, mean_q: 0.916959, mean_eps: 0.100000\n",
      " 143755/175000: episode: 4022, duration: 0.564s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 158.556 [109.000, 215.000], mean observation: 0.189 [0.000, 54.000], loss: 0.209979, mean_absolute_error: 0.552517, mean_q: 1.057799, mean_eps: 0.100000\n",
      " 143778/175000: episode: 4023, duration: 0.431s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 164.609 [52.000, 221.000], mean observation: 0.249 [0.000, 46.000], loss: 0.139719, mean_absolute_error: 0.547516, mean_q: 1.108073, mean_eps: 0.100000\n",
      " 143827/175000: episode: 4024, duration: 0.877s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 125.041 [4.000, 221.000], mean observation: 0.625 [0.000, 98.000], loss: 1.398701, mean_absolute_error: 0.555027, mean_q: 1.069478, mean_eps: 0.100000\n",
      " 143872/175000: episode: 4025, duration: 0.837s, episode steps: 45, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 146.489 [43.000, 222.000], mean observation: 0.180 [0.000, 90.000], loss: 0.200696, mean_absolute_error: 0.561985, mean_q: 1.200983, mean_eps: 0.100000\n",
      " 143898/175000: episode: 4026, duration: 0.477s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 165.808 [44.000, 221.000], mean observation: 0.131 [0.000, 52.000], loss: 0.305423, mean_absolute_error: 0.551686, mean_q: 0.967882, mean_eps: 0.100000\n",
      " 143928/175000: episode: 4027, duration: 0.583s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 112.733 [44.000, 193.000], mean observation: 0.326 [0.000, 60.000], loss: 0.129636, mean_absolute_error: 0.551916, mean_q: 0.968169, mean_eps: 0.100000\n",
      " 143958/175000: episode: 4028, duration: 0.568s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 135.500 [0.000, 193.000], mean observation: 0.319 [0.000, 60.000], loss: 0.132553, mean_absolute_error: 0.549308, mean_q: 0.833951, mean_eps: 0.100000\n",
      " 143987/175000: episode: 4029, duration: 0.509s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 110.310 [9.000, 193.000], mean observation: 0.191 [0.000, 58.000], loss: 0.118813, mean_absolute_error: 0.566822, mean_q: 1.010568, mean_eps: 0.100000\n",
      " 144012/175000: episode: 4030, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 135.520 [33.000, 200.000], mean observation: 0.158 [0.000, 50.000], loss: 2006.812506, mean_absolute_error: 9.747885, mean_q: 4.095227, mean_eps: 0.100000\n",
      " 144038/175000: episode: 4031, duration: 0.506s, episode steps: 26, steps per second: 51, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 133.423 [11.000, 211.000], mean observation: 0.225 [0.000, 52.000], loss: 0.585298, mean_absolute_error: 0.551079, mean_q: 0.854780, mean_eps: 0.100000\n",
      " 144085/175000: episode: 4032, duration: 0.843s, episode steps: 47, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 129.872 [6.000, 211.000], mean observation: 0.523 [0.000, 94.000], loss: 1367.356344, mean_absolute_error: 6.751324, mean_q: 2.438937, mean_eps: 0.100000\n",
      " 144095/175000: episode: 4033, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 70.400 [37.000, 105.000], mean observation: 0.041 [0.000, 20.000], loss: 0.107904, mean_absolute_error: 0.542710, mean_q: 0.790725, mean_eps: 0.100000\n",
      " 144105/175000: episode: 4034, duration: 0.231s, episode steps: 10, steps per second: 43, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 130.100 [37.000, 211.000], mean observation: 0.057 [0.000, 20.000], loss: 193.679864, mean_absolute_error: 1.471331, mean_q: 1.761314, mean_eps: 0.100000\n",
      " 144153/175000: episode: 4035, duration: 0.851s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 133.083 [3.000, 180.000], mean observation: 0.434 [0.000, 96.000], loss: 0.332722, mean_absolute_error: 0.541358, mean_q: 0.988188, mean_eps: 0.100000\n",
      " 144169/175000: episode: 4036, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 149.312 [58.000, 180.000], mean observation: 0.114 [0.000, 32.000], loss: 0.276233, mean_absolute_error: 0.538910, mean_q: 1.081470, mean_eps: 0.100000\n",
      " 144207/175000: episode: 4037, duration: 0.654s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 125.579 [3.000, 220.000], mean observation: 0.394 [0.000, 76.000], loss: 0.107439, mean_absolute_error: 0.541976, mean_q: 1.033462, mean_eps: 0.100000\n",
      " 144246/175000: episode: 4038, duration: 0.722s, episode steps: 39, steps per second: 54, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 138.744 [76.000, 180.000], mean observation: 0.283 [0.000, 78.000], loss: 4.201980, mean_absolute_error: 0.560562, mean_q: 0.945123, mean_eps: 0.100000\n",
      " 144281/175000: episode: 4039, duration: 0.733s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 91.343 [1.000, 222.000], mean observation: 0.262 [0.000, 70.000], loss: 0.318634, mean_absolute_error: 0.555331, mean_q: 1.137123, mean_eps: 0.100000\n",
      " 144320/175000: episode: 4040, duration: 0.753s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 107.641 [23.000, 222.000], mean observation: 0.396 [0.000, 78.000], loss: 0.227332, mean_absolute_error: 0.539149, mean_q: 0.945706, mean_eps: 0.100000\n",
      " 144351/175000: episode: 4041, duration: 0.607s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 127.419 [15.000, 215.000], mean observation: 0.376 [0.000, 62.000], loss: 0.113905, mean_absolute_error: 0.535944, mean_q: 1.024499, mean_eps: 0.100000\n",
      " 144387/175000: episode: 4042, duration: 0.652s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 155.639 [15.000, 219.000], mean observation: 0.466 [0.000, 72.000], loss: 0.420258, mean_absolute_error: 0.536720, mean_q: 0.888937, mean_eps: 0.100000\n",
      " 144440/175000: episode: 4043, duration: 1.008s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 59.849 [0.000, 199.000], mean observation: 0.880 [0.000, 106.000], loss: 0.132657, mean_absolute_error: 0.544673, mean_q: 0.924816, mean_eps: 0.100000\n",
      " 144473/175000: episode: 4044, duration: 0.796s, episode steps: 33, steps per second: 41, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 85.303 [0.000, 168.000], mean observation: 0.204 [0.000, 66.000], loss: 2.163287, mean_absolute_error: 0.559263, mean_q: 0.909877, mean_eps: 0.100000\n",
      " 144521/175000: episode: 4045, duration: 0.997s, episode steps: 48, steps per second: 48, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 65.167 [0.000, 188.000], mean observation: 0.435 [0.000, 96.000], loss: 0.733935, mean_absolute_error: 0.574160, mean_q: 1.009333, mean_eps: 0.100000\n",
      " 144559/175000: episode: 4046, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 111.421 [0.000, 200.000], mean observation: 0.229 [0.000, 76.000], loss: 0.136111, mean_absolute_error: 0.560064, mean_q: 1.099488, mean_eps: 0.100000\n",
      " 144610/175000: episode: 4047, duration: 1.076s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 111.824 [36.000, 203.000], mean observation: 0.548 [0.000, 102.000], loss: 0.851209, mean_absolute_error: 0.567529, mean_q: 1.309761, mean_eps: 0.100000\n",
      " 144646/175000: episode: 4048, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.917 [20.000, 204.000], mean observation: 0.306 [0.000, 72.000], loss: 0.619010, mean_absolute_error: 0.567069, mean_q: 1.167463, mean_eps: 0.100000\n",
      " 144683/175000: episode: 4049, duration: 0.708s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 104.568 [71.000, 207.000], mean observation: 0.255 [0.000, 74.000], loss: 0.346745, mean_absolute_error: 0.558908, mean_q: 1.104161, mean_eps: 0.100000\n",
      " 144729/175000: episode: 4050, duration: 0.894s, episode steps: 46, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 94.304 [25.000, 201.000], mean observation: 0.269 [0.000, 92.000], loss: 0.180927, mean_absolute_error: 0.568302, mean_q: 1.092291, mean_eps: 0.100000\n",
      " 144769/175000: episode: 4051, duration: 0.701s, episode steps: 40, steps per second: 57, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 75.125 [19.000, 173.000], mean observation: 0.273 [0.000, 80.000], loss: 0.405124, mean_absolute_error: 0.563866, mean_q: 0.967999, mean_eps: 0.100000\n",
      " 144807/175000: episode: 4052, duration: 0.682s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 85.184 [25.000, 170.000], mean observation: 0.209 [0.000, 76.000], loss: 0.122961, mean_absolute_error: 0.574986, mean_q: 0.969102, mean_eps: 0.100000\n",
      " 144846/175000: episode: 4053, duration: 0.736s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 76.359 [5.000, 177.000], mean observation: 0.214 [0.000, 78.000], loss: 0.302718, mean_absolute_error: 0.581787, mean_q: 1.084818, mean_eps: 0.100000\n",
      " 144878/175000: episode: 4054, duration: 0.739s, episode steps: 32, steps per second: 43, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 143.906 [55.000, 216.000], mean observation: 0.179 [0.000, 64.000], loss: 0.321158, mean_absolute_error: 0.576404, mean_q: 0.776534, mean_eps: 0.100000\n",
      " 144918/175000: episode: 4055, duration: 0.755s, episode steps: 40, steps per second: 53, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 145.750 [55.000, 217.000], mean observation: 0.338 [0.000, 80.000], loss: 1.105594, mean_absolute_error: 0.571267, mean_q: 0.715099, mean_eps: 0.100000\n",
      " 144953/175000: episode: 4056, duration: 0.682s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.914 [5.000, 216.000], mean observation: 0.310 [0.000, 70.000], loss: 0.113841, mean_absolute_error: 0.561892, mean_q: 0.757164, mean_eps: 0.100000\n",
      " 145003/175000: episode: 4057, duration: 0.972s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 70.420 [28.000, 218.000], mean observation: 0.338 [0.000, 100.000], loss: 0.268939, mean_absolute_error: 0.569436, mean_q: 0.768155, mean_eps: 0.100000\n",
      " 145022/175000: episode: 4058, duration: 0.429s, episode steps: 19, steps per second: 44, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 81.421 [14.000, 192.000], mean observation: 0.131 [0.000, 38.000], loss: 0.129507, mean_absolute_error: 0.575197, mean_q: 0.733341, mean_eps: 0.100000\n",
      " 145046/175000: episode: 4059, duration: 0.469s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 26.000 [26.000, 26.000], mean observation: 0.057 [0.000, 48.000], loss: 0.259555, mean_absolute_error: 0.574804, mean_q: 0.699971, mean_eps: 0.100000\n",
      " 145105/175000: episode: 4060, duration: 1.213s, episode steps: 59, steps per second: 49, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 84.475 [26.000, 214.000], mean observation: 0.336 [0.000, 118.000], loss: 0.237074, mean_absolute_error: 0.579415, mean_q: 0.811929, mean_eps: 0.100000\n",
      " 145135/175000: episode: 4061, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 64.533 [26.000, 192.000], mean observation: 0.149 [0.000, 60.000], loss: 0.620735, mean_absolute_error: 0.587115, mean_q: 0.713546, mean_eps: 0.100000\n",
      " 145163/175000: episode: 4062, duration: 0.530s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.893 [26.000, 206.000], mean observation: 0.176 [0.000, 56.000], loss: 0.155712, mean_absolute_error: 0.593651, mean_q: 0.772826, mean_eps: 0.100000\n",
      " 145192/175000: episode: 4063, duration: 0.595s, episode steps: 29, steps per second: 49, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.138 [9.000, 206.000], mean observation: 0.230 [0.000, 58.000], loss: 0.311243, mean_absolute_error: 0.586570, mean_q: 0.763813, mean_eps: 0.100000\n",
      " 145238/175000: episode: 4064, duration: 0.835s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.522 [26.000, 181.000], mean observation: 0.482 [0.000, 92.000], loss: 0.466572, mean_absolute_error: 0.585138, mean_q: 0.803608, mean_eps: 0.100000\n",
      " 145263/175000: episode: 4065, duration: 0.467s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 149.840 [18.000, 211.000], mean observation: 0.126 [0.000, 50.000], loss: 0.136841, mean_absolute_error: 0.583355, mean_q: 0.838639, mean_eps: 0.100000\n",
      " 145297/175000: episode: 4066, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 41.588 [6.000, 213.000], mean observation: 0.140 [0.000, 68.000], loss: 0.297592, mean_absolute_error: 0.586958, mean_q: 0.905423, mean_eps: 0.100000\n",
      " 145345/175000: episode: 4067, duration: 0.880s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 89.083 [6.000, 201.000], mean observation: 0.237 [0.000, 96.000], loss: 0.616613, mean_absolute_error: 0.593289, mean_q: 0.903110, mean_eps: 0.100000\n",
      " 145384/175000: episode: 4068, duration: 0.733s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 84.051 [1.000, 218.000], mean observation: 0.172 [0.000, 78.000], loss: 0.147787, mean_absolute_error: 0.591382, mean_q: 0.872019, mean_eps: 0.100000\n",
      " 145412/175000: episode: 4069, duration: 0.630s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 95.321 [30.000, 211.000], mean observation: 0.164 [0.000, 56.000], loss: 0.147100, mean_absolute_error: 0.593435, mean_q: 0.753519, mean_eps: 0.100000\n",
      " 145454/175000: episode: 4070, duration: 0.784s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.881 [14.000, 217.000], mean observation: 0.334 [0.000, 84.000], loss: 0.639363, mean_absolute_error: 0.607738, mean_q: 0.798088, mean_eps: 0.100000\n",
      " 145498/175000: episode: 4071, duration: 0.830s, episode steps: 44, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 64.205 [1.000, 129.000], mean observation: 0.340 [0.000, 88.000], loss: 0.497786, mean_absolute_error: 0.613593, mean_q: 0.836408, mean_eps: 0.100000\n",
      " 145542/175000: episode: 4072, duration: 0.792s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.727 [1.000, 215.000], mean observation: 0.574 [0.000, 88.000], loss: 0.165140, mean_absolute_error: 0.625804, mean_q: 0.931249, mean_eps: 0.100000\n",
      " 145587/175000: episode: 4073, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 80.511 [5.000, 201.000], mean observation: 0.430 [0.000, 90.000], loss: 0.196383, mean_absolute_error: 0.636663, mean_q: 0.843938, mean_eps: 0.100000\n",
      " 145619/175000: episode: 4074, duration: 0.597s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 85.406 [6.000, 211.000], mean observation: 0.297 [0.000, 64.000], loss: 78.285240, mean_absolute_error: 0.989428, mean_q: 0.834227, mean_eps: 0.100000\n",
      " 145655/175000: episode: 4075, duration: 0.649s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 173.583 [84.000, 224.000], mean observation: 0.357 [0.000, 72.000], loss: 2.150138, mean_absolute_error: 0.643112, mean_q: 0.722509, mean_eps: 0.100000\n",
      " 145701/175000: episode: 4076, duration: 0.884s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 147.674 [37.000, 212.000], mean observation: 0.415 [0.000, 92.000], loss: 6119.248976, mean_absolute_error: 28.086010, mean_q: 3.846588, mean_eps: 0.100000\n",
      " 145747/175000: episode: 4077, duration: 0.828s, episode steps: 46, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 155.565 [19.000, 212.000], mean observation: 0.315 [0.000, 92.000], loss: 0.243707, mean_absolute_error: 0.642443, mean_q: 0.983021, mean_eps: 0.100000\n",
      " 145798/175000: episode: 4078, duration: 0.916s, episode steps: 51, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.922 [25.000, 212.000], mean observation: 0.385 [0.000, 102.000], loss: 0.270993, mean_absolute_error: 0.633070, mean_q: 0.883826, mean_eps: 0.100000\n",
      " 145836/175000: episode: 4079, duration: 0.753s, episode steps: 38, steps per second: 50, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 171.474 [13.000, 212.000], mean observation: 0.318 [0.000, 76.000], loss: 0.282735, mean_absolute_error: 0.629305, mean_q: 0.930410, mean_eps: 0.100000\n",
      " 145871/175000: episode: 4080, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 175.486 [58.000, 212.000], mean observation: 0.257 [0.000, 70.000], loss: 0.291709, mean_absolute_error: 0.619364, mean_q: 1.009250, mean_eps: 0.100000\n",
      " 145912/175000: episode: 4081, duration: 0.859s, episode steps: 41, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 79.268 [3.000, 212.000], mean observation: 0.371 [0.000, 82.000], loss: 1.159675, mean_absolute_error: 0.755200, mean_q: 2.592840, mean_eps: 0.100000\n",
      " 145955/175000: episode: 4082, duration: 0.832s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 117.465 [42.000, 169.000], mean observation: 0.285 [0.000, 86.000], loss: 2053.373986, mean_absolute_error: 9.862684, mean_q: 2.654721, mean_eps: 0.100000\n",
      " 146000/175000: episode: 4083, duration: 0.901s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 52.378 [42.000, 196.000], mean observation: 0.240 [0.000, 90.000], loss: 0.260463, mean_absolute_error: 0.586556, mean_q: 0.960420, mean_eps: 0.100000\n",
      " 146049/175000: episode: 4084, duration: 0.953s, episode steps: 49, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 92.469 [10.000, 152.000], mean observation: 0.406 [0.000, 98.000], loss: 0.125118, mean_absolute_error: 0.616711, mean_q: 1.364103, mean_eps: 0.100000\n",
      " 146090/175000: episode: 4085, duration: 0.735s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 123.220 [3.000, 218.000], mean observation: 0.249 [0.000, 82.000], loss: 0.270769, mean_absolute_error: 0.599426, mean_q: 1.335335, mean_eps: 0.100000\n",
      " 146135/175000: episode: 4086, duration: 0.816s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 138.511 [18.000, 195.000], mean observation: 0.336 [0.000, 90.000], loss: 0.271884, mean_absolute_error: 0.607669, mean_q: 1.291801, mean_eps: 0.100000\n",
      " 146170/175000: episode: 4087, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.257 [125.000, 195.000], mean observation: 0.174 [0.000, 70.000], loss: 0.251129, mean_absolute_error: 0.593927, mean_q: 1.167755, mean_eps: 0.100000\n",
      " 146214/175000: episode: 4088, duration: 0.816s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 138.023 [125.000, 195.000], mean observation: 0.266 [0.000, 88.000], loss: 0.142455, mean_absolute_error: 0.615681, mean_q: 1.463954, mean_eps: 0.100000\n",
      " 146265/175000: episode: 4089, duration: 0.942s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 148.392 [15.000, 222.000], mean observation: 0.566 [0.000, 102.000], loss: 0.187644, mean_absolute_error: 0.603091, mean_q: 1.437669, mean_eps: 0.100000\n",
      " 146303/175000: episode: 4090, duration: 0.650s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 179.026 [41.000, 222.000], mean observation: 0.301 [0.000, 76.000], loss: 0.197457, mean_absolute_error: 0.596815, mean_q: 1.409307, mean_eps: 0.100000\n",
      " 146339/175000: episode: 4091, duration: 0.663s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 188.500 [15.000, 222.000], mean observation: 0.339 [0.000, 72.000], loss: 0.147646, mean_absolute_error: 0.561513, mean_q: 1.046958, mean_eps: 0.100000\n",
      " 146376/175000: episode: 4092, duration: 0.708s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 182.243 [68.000, 222.000], mean observation: 0.444 [0.000, 74.000], loss: 0.199246, mean_absolute_error: 0.584685, mean_q: 1.412385, mean_eps: 0.100000\n",
      " 146405/175000: episode: 4093, duration: 0.567s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 99.034 [17.000, 192.000], mean observation: 0.143 [0.000, 58.000], loss: 0.113763, mean_absolute_error: 0.568890, mean_q: 1.209364, mean_eps: 0.100000\n",
      " 146432/175000: episode: 4094, duration: 0.510s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.259 [34.000, 195.000], mean observation: 0.196 [0.000, 54.000], loss: 0.114047, mean_absolute_error: 0.580089, mean_q: 1.351993, mean_eps: 0.100000\n",
      " 146459/175000: episode: 4095, duration: 0.499s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 102.407 [5.000, 195.000], mean observation: 0.318 [0.000, 54.000], loss: 0.439772, mean_absolute_error: 0.573686, mean_q: 1.176823, mean_eps: 0.100000\n",
      " 146501/175000: episode: 4096, duration: 0.779s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 109.714 [34.000, 218.000], mean observation: 0.577 [0.000, 84.000], loss: 0.233672, mean_absolute_error: 0.577162, mean_q: 1.175892, mean_eps: 0.100000\n",
      " 146554/175000: episode: 4097, duration: 0.950s, episode steps: 53, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 69.057 [5.000, 195.000], mean observation: 0.830 [0.000, 106.000], loss: 0.118122, mean_absolute_error: 0.587015, mean_q: 1.209500, mean_eps: 0.100000\n",
      " 146582/175000: episode: 4098, duration: 0.474s, episode steps: 28, steps per second: 59, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 151.679 [7.000, 195.000], mean observation: 0.296 [0.000, 56.000], loss: 0.286787, mean_absolute_error: 0.580343, mean_q: 1.171526, mean_eps: 0.100000\n",
      " 146614/175000: episode: 4099, duration: 0.558s, episode steps: 32, steps per second: 57, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.594 [13.000, 187.000], mean observation: 0.218 [0.000, 64.000], loss: 0.507097, mean_absolute_error: 0.592934, mean_q: 1.473397, mean_eps: 0.100000\n",
      " 146640/175000: episode: 4100, duration: 0.532s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 91.038 [14.000, 145.000], mean observation: 0.193 [0.000, 52.000], loss: 1225.094952, mean_absolute_error: 6.304001, mean_q: 4.325211, mean_eps: 0.100000\n",
      " 146663/175000: episode: 4101, duration: 0.442s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 145.000 [60.000, 184.000], mean observation: 0.111 [0.000, 46.000], loss: 0.153251, mean_absolute_error: 0.612273, mean_q: 1.460182, mean_eps: 0.100000\n",
      " 146696/175000: episode: 4102, duration: 0.633s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 160.303 [57.000, 167.000], mean observation: 0.104 [0.000, 66.000], loss: 1.153347, mean_absolute_error: 0.621051, mean_q: 1.478138, mean_eps: 0.100000\n",
      " 146739/175000: episode: 4103, duration: 0.825s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 112.860 [9.000, 186.000], mean observation: 0.265 [0.000, 86.000], loss: 0.177086, mean_absolute_error: 0.595352, mean_q: 1.149393, mean_eps: 0.100000\n",
      " 146756/175000: episode: 4104, duration: 0.355s, episode steps: 17, steps per second: 48, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 117.588 [62.000, 177.000], mean observation: 0.068 [0.000, 34.000], loss: 0.143325, mean_absolute_error: 0.583610, mean_q: 1.206289, mean_eps: 0.100000\n",
      " 146799/175000: episode: 4105, duration: 0.813s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.372 [0.000, 135.000], mean observation: 0.223 [0.000, 86.000], loss: 7.188903, mean_absolute_error: 0.769248, mean_q: 2.881171, mean_eps: 0.100000\n",
      " 146836/175000: episode: 4106, duration: 0.729s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 148.351 [6.000, 208.000], mean observation: 0.228 [0.000, 74.000], loss: 1094.841522, mean_absolute_error: 5.631429, mean_q: 3.113339, mean_eps: 0.100000\n",
      " 146867/175000: episode: 4107, duration: 0.611s, episode steps: 31, steps per second: 51, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 164.129 [68.000, 208.000], mean observation: 0.189 [0.000, 62.000], loss: 2825.750249, mean_absolute_error: 13.337822, mean_q: 3.286199, mean_eps: 0.100000\n",
      " 146903/175000: episode: 4108, duration: 0.683s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 156.250 [24.000, 208.000], mean observation: 0.242 [0.000, 72.000], loss: 0.147426, mean_absolute_error: 0.591358, mean_q: 1.148884, mean_eps: 0.100000\n",
      " 146945/175000: episode: 4109, duration: 0.784s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.143 [24.000, 216.000], mean observation: 0.384 [0.000, 84.000], loss: 0.132188, mean_absolute_error: 0.601790, mean_q: 1.359067, mean_eps: 0.100000\n",
      " 146995/175000: episode: 4110, duration: 0.838s, episode steps: 50, steps per second: 60, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 140.400 [24.000, 224.000], mean observation: 0.626 [0.000, 100.000], loss: 0.527177, mean_absolute_error: 0.622623, mean_q: 1.559269, mean_eps: 0.100000\n",
      " 147025/175000: episode: 4111, duration: 0.573s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 152.600 [31.000, 208.000], mean observation: 0.165 [0.000, 60.000], loss: 0.130769, mean_absolute_error: 0.601458, mean_q: 1.119835, mean_eps: 0.100000\n",
      " 147064/175000: episode: 4112, duration: 0.731s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 143.205 [34.000, 207.000], mean observation: 0.268 [0.000, 78.000], loss: 0.335536, mean_absolute_error: 0.615751, mean_q: 1.343724, mean_eps: 0.100000\n",
      " 147095/175000: episode: 4113, duration: 0.580s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.484 [14.000, 207.000], mean observation: 0.231 [0.000, 62.000], loss: 0.238487, mean_absolute_error: 0.617844, mean_q: 1.407549, mean_eps: 0.100000\n",
      " 147125/175000: episode: 4114, duration: 0.603s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 150.633 [28.000, 215.000], mean observation: 0.324 [0.000, 60.000], loss: 0.274815, mean_absolute_error: 0.597933, mean_q: 1.198180, mean_eps: 0.100000\n",
      " 147170/175000: episode: 4115, duration: 0.813s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.644 [9.000, 205.000], mean observation: 0.479 [0.000, 90.000], loss: 45.710178, mean_absolute_error: 0.802256, mean_q: 1.277437, mean_eps: 0.100000\n",
      " 147223/175000: episode: 4116, duration: 0.930s, episode steps: 53, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 120.075 [24.000, 221.000], mean observation: 0.631 [0.000, 106.000], loss: 0.280279, mean_absolute_error: 0.606130, mean_q: 1.318205, mean_eps: 0.100000\n",
      " 147242/175000: episode: 4117, duration: 0.369s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 131.474 [64.000, 194.000], mean observation: 0.085 [0.000, 38.000], loss: 0.190300, mean_absolute_error: 0.617308, mean_q: 1.239704, mean_eps: 0.100000\n",
      " 147292/175000: episode: 4118, duration: 0.915s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 138.940 [10.000, 201.000], mean observation: 0.689 [0.000, 100.000], loss: 0.099064, mean_absolute_error: 0.616749, mean_q: 1.276469, mean_eps: 0.100000\n",
      " 147333/175000: episode: 4119, duration: 0.817s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 82.659 [14.000, 221.000], mean observation: 0.615 [0.000, 82.000], loss: 0.177466, mean_absolute_error: 0.613589, mean_q: 1.291800, mean_eps: 0.100000\n",
      " 147364/175000: episode: 4120, duration: 0.601s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 87.581 [27.000, 207.000], mean observation: 0.224 [0.000, 62.000], loss: 0.102084, mean_absolute_error: 0.616911, mean_q: 1.408865, mean_eps: 0.100000\n",
      " 147383/175000: episode: 4121, duration: 0.371s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 93.053 [27.000, 171.000], mean observation: 0.091 [0.000, 38.000], loss: 0.124602, mean_absolute_error: 0.613721, mean_q: 1.397188, mean_eps: 0.100000\n",
      " 147410/175000: episode: 4122, duration: 0.520s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 128.444 [9.000, 200.000], mean observation: 0.263 [0.000, 54.000], loss: 0.346755, mean_absolute_error: 0.606482, mean_q: 1.347789, mean_eps: 0.100000\n",
      " 147440/175000: episode: 4123, duration: 0.585s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.467 [27.000, 200.000], mean observation: 0.302 [0.000, 60.000], loss: 0.148529, mean_absolute_error: 0.585248, mean_q: 1.269019, mean_eps: 0.100000\n",
      " 147488/175000: episode: 4124, duration: 0.960s, episode steps: 48, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 150.854 [24.000, 215.000], mean observation: 0.665 [0.000, 96.000], loss: 0.129060, mean_absolute_error: 0.586272, mean_q: 1.489902, mean_eps: 0.100000\n",
      " 147539/175000: episode: 4125, duration: 0.934s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 124.294 [63.000, 215.000], mean observation: 0.594 [0.000, 102.000], loss: 0.354348, mean_absolute_error: 0.594648, mean_q: 1.601697, mean_eps: 0.100000\n",
      " 147557/175000: episode: 4126, duration: 0.372s, episode steps: 18, steps per second: 48, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 177.444 [50.000, 215.000], mean observation: 0.089 [0.000, 36.000], loss: 0.207285, mean_absolute_error: 0.590461, mean_q: 1.583157, mean_eps: 0.100000\n",
      " 147592/175000: episode: 4127, duration: 0.664s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 176.486 [0.000, 215.000], mean observation: 0.230 [0.000, 70.000], loss: 0.146607, mean_absolute_error: 0.598664, mean_q: 1.792722, mean_eps: 0.100000\n",
      " 147621/175000: episode: 4128, duration: 0.562s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 153.103 [19.000, 215.000], mean observation: 0.264 [0.000, 58.000], loss: 0.166980, mean_absolute_error: 0.600091, mean_q: 1.772414, mean_eps: 0.100000\n",
      " 147667/175000: episode: 4129, duration: 0.834s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 209.522 [62.000, 215.000], mean observation: 0.106 [0.000, 92.000], loss: 0.159621, mean_absolute_error: 0.605357, mean_q: 1.551799, mean_eps: 0.100000\n",
      " 147703/175000: episode: 4130, duration: 0.639s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 170.583 [42.000, 215.000], mean observation: 0.277 [0.000, 72.000], loss: 0.328454, mean_absolute_error: 0.619147, mean_q: 1.474900, mean_eps: 0.100000\n",
      " 147733/175000: episode: 4131, duration: 0.574s, episode steps: 30, steps per second: 52, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 215.000 [215.000, 215.000], mean observation: 0.070 [0.000, 60.000], loss: 0.155152, mean_absolute_error: 0.623226, mean_q: 1.573005, mean_eps: 0.100000\n",
      " 147769/175000: episode: 4132, duration: 0.648s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 203.250 [48.000, 215.000], mean observation: 0.177 [0.000, 72.000], loss: 0.255452, mean_absolute_error: 0.618310, mean_q: 1.544835, mean_eps: 0.100000\n",
      " 147807/175000: episode: 4133, duration: 0.659s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 162.553 [15.000, 218.000], mean observation: 0.205 [0.000, 76.000], loss: 462.543505, mean_absolute_error: 2.674963, mean_q: 1.605528, mean_eps: 0.100000\n",
      " 147842/175000: episode: 4134, duration: 0.696s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 136.543 [36.000, 215.000], mean observation: 0.165 [0.000, 70.000], loss: 0.280516, mean_absolute_error: 0.610759, mean_q: 1.566836, mean_eps: 0.100000\n",
      " 147897/175000: episode: 4135, duration: 0.983s, episode steps: 55, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 93.018 [8.000, 216.000], mean observation: 0.613 [0.000, 110.000], loss: 0.444738, mean_absolute_error: 0.608472, mean_q: 1.402804, mean_eps: 0.100000\n",
      " 147923/175000: episode: 4136, duration: 0.444s, episode steps: 26, steps per second: 59, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 137.500 [37.000, 209.000], mean observation: 0.210 [0.000, 52.000], loss: 0.324565, mean_absolute_error: 0.619682, mean_q: 1.425178, mean_eps: 0.100000\n",
      " 147959/175000: episode: 4137, duration: 0.633s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 127.528 [30.000, 209.000], mean observation: 0.191 [0.000, 72.000], loss: 0.483094, mean_absolute_error: 0.619543, mean_q: 1.370459, mean_eps: 0.100000\n",
      " 148021/175000: episode: 4138, duration: 1.199s, episode steps: 62, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 66.968 [5.000, 215.000], mean observation: 0.536 [0.000, 124.000], loss: 0.660705, mean_absolute_error: 0.631036, mean_q: 1.426376, mean_eps: 0.100000\n",
      " 148076/175000: episode: 4139, duration: 1.004s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 52.818 [4.000, 211.000], mean observation: 0.173 [0.000, 110.000], loss: 0.499060, mean_absolute_error: 0.632388, mean_q: 1.445378, mean_eps: 0.100000\n",
      " 148114/175000: episode: 4140, duration: 0.726s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 63.974 [4.000, 224.000], mean observation: 0.294 [0.000, 76.000], loss: 0.458274, mean_absolute_error: 0.618455, mean_q: 1.141865, mean_eps: 0.100000\n",
      " 148157/175000: episode: 4141, duration: 0.829s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 51.023 [4.000, 154.000], mean observation: 0.412 [0.000, 86.000], loss: 532.228182, mean_absolute_error: 3.130700, mean_q: 2.694750, mean_eps: 0.100000\n",
      " 148195/175000: episode: 4142, duration: 0.682s, episode steps: 38, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 47.368 [25.000, 202.000], mean observation: 0.094 [0.000, 76.000], loss: 0.165381, mean_absolute_error: 0.625172, mean_q: 1.152614, mean_eps: 0.100000\n",
      " 148242/175000: episode: 4143, duration: 0.849s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 45.745 [6.000, 175.000], mean observation: 0.192 [0.000, 94.000], loss: 0.223390, mean_absolute_error: 0.620261, mean_q: 1.044323, mean_eps: 0.100000\n",
      " 148298/175000: episode: 4144, duration: 1.042s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 53.768 [41.000, 212.000], mean observation: 0.176 [0.000, 112.000], loss: 2303.142497, mean_absolute_error: 11.072464, mean_q: 3.448914, mean_eps: 0.100000\n",
      " 148349/175000: episode: 4145, duration: 0.917s, episode steps: 51, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 47.667 [14.000, 192.000], mean observation: 0.556 [0.000, 102.000], loss: 1.067616, mean_absolute_error: 0.642324, mean_q: 1.296425, mean_eps: 0.100000\n",
      " 148369/175000: episode: 4146, duration: 0.352s, episode steps: 20, steps per second: 57, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 76.150 [14.000, 148.000], mean observation: 0.129 [0.000, 40.000], loss: 0.187633, mean_absolute_error: 0.638352, mean_q: 1.285352, mean_eps: 0.100000\n",
      " 148404/175000: episode: 4147, duration: 0.641s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 59.571 [16.000, 199.000], mean observation: 0.412 [0.000, 70.000], loss: 0.191236, mean_absolute_error: 0.626749, mean_q: 1.260687, mean_eps: 0.100000\n",
      " 148428/175000: episode: 4148, duration: 0.503s, episode steps: 24, steps per second: 48, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 106.792 [60.000, 137.000], mean observation: 0.124 [0.000, 48.000], loss: 0.144411, mean_absolute_error: 0.611982, mean_q: 1.055680, mean_eps: 0.100000\n",
      " 148473/175000: episode: 4149, duration: 0.851s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 107.956 [34.000, 221.000], mean observation: 0.420 [0.000, 90.000], loss: 0.126099, mean_absolute_error: 0.608434, mean_q: 1.166224, mean_eps: 0.100000\n",
      " 148521/175000: episode: 4150, duration: 0.885s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 148.812 [82.000, 180.000], mean observation: 0.408 [0.000, 96.000], loss: 721.778178, mean_absolute_error: 3.959201, mean_q: 2.711209, mean_eps: 0.100000\n",
      " 148560/175000: episode: 4151, duration: 0.754s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 124.436 [1.000, 195.000], mean observation: 0.356 [0.000, 78.000], loss: 0.137697, mean_absolute_error: 0.625216, mean_q: 1.349752, mean_eps: 0.100000\n",
      " 148606/175000: episode: 4152, duration: 0.833s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 132.565 [4.000, 195.000], mean observation: 0.533 [0.000, 92.000], loss: 0.219671, mean_absolute_error: 0.636004, mean_q: 1.561061, mean_eps: 0.100000\n",
      " 148635/175000: episode: 4153, duration: 0.503s, episode steps: 29, steps per second: 58, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 135.103 [83.000, 195.000], mean observation: 0.240 [0.000, 58.000], loss: 0.192957, mean_absolute_error: 0.625100, mean_q: 1.370937, mean_eps: 0.100000\n",
      " 148672/175000: episode: 4154, duration: 0.725s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 79.378 [1.000, 165.000], mean observation: 0.316 [0.000, 74.000], loss: 0.214702, mean_absolute_error: 0.625087, mean_q: 1.344722, mean_eps: 0.100000\n",
      " 148695/175000: episode: 4155, duration: 0.441s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 139.435 [96.000, 215.000], mean observation: 0.158 [0.000, 46.000], loss: 0.463579, mean_absolute_error: 0.619558, mean_q: 1.274923, mean_eps: 0.100000\n",
      " 148728/175000: episode: 4156, duration: 0.651s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 206.121 [149.000, 209.000], mean observation: 0.138 [0.000, 66.000], loss: 2.051758, mean_absolute_error: 0.626232, mean_q: 1.307208, mean_eps: 0.100000\n",
      " 148773/175000: episode: 4157, duration: 0.876s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 150.378 [16.000, 217.000], mean observation: 0.334 [0.000, 90.000], loss: 821.189790, mean_absolute_error: 4.395176, mean_q: 2.721210, mean_eps: 0.100000\n",
      " 148808/175000: episode: 4158, duration: 0.639s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 160.371 [61.000, 220.000], mean observation: 0.258 [0.000, 70.000], loss: 0.265187, mean_absolute_error: 0.633197, mean_q: 1.365691, mean_eps: 0.100000\n",
      " 148849/175000: episode: 4159, duration: 0.810s, episode steps: 41, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 105.683 [1.000, 217.000], mean observation: 0.309 [0.000, 82.000], loss: 0.223403, mean_absolute_error: 0.628302, mean_q: 1.391088, mean_eps: 0.100000\n",
      " 148884/175000: episode: 4160, duration: 0.640s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.743 [7.000, 221.000], mean observation: 0.255 [0.000, 70.000], loss: 0.364216, mean_absolute_error: 0.616148, mean_q: 1.451462, mean_eps: 0.100000\n",
      " 148928/175000: episode: 4161, duration: 0.884s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 167.000 [83.000, 221.000], mean observation: 0.317 [0.000, 88.000], loss: 0.198541, mean_absolute_error: 0.608137, mean_q: 1.360820, mean_eps: 0.100000\n",
      " 148963/175000: episode: 4162, duration: 0.729s, episode steps: 35, steps per second: 48, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 154.829 [23.000, 217.000], mean observation: 0.369 [0.000, 69.000], loss: 0.185701, mean_absolute_error: 0.629883, mean_q: 1.699022, mean_eps: 0.100000\n",
      " 148981/175000: episode: 4163, duration: 0.348s, episode steps: 18, steps per second: 52, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 142.778 [83.000, 221.000], mean observation: 0.083 [0.000, 36.000], loss: 99.954525, mean_absolute_error: 1.069960, mean_q: 1.717636, mean_eps: 0.100000\n",
      " 149038/175000: episode: 4164, duration: 1.011s, episode steps: 57, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 176.825 [24.000, 221.000], mean observation: 0.320 [0.000, 114.000], loss: 155.241896, mean_absolute_error: 1.298417, mean_q: 1.532675, mean_eps: 0.100000\n",
      " 149092/175000: episode: 4165, duration: 1.112s, episode steps: 54, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 117.056 [2.000, 215.000], mean observation: 0.290 [0.000, 108.000], loss: 171.265351, mean_absolute_error: 1.476059, mean_q: 2.667271, mean_eps: 0.100000\n",
      " 149128/175000: episode: 4166, duration: 0.741s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 84.444 [24.000, 205.000], mean observation: 0.203 [0.000, 72.000], loss: 0.332411, mean_absolute_error: 0.594728, mean_q: 1.139201, mean_eps: 0.100000\n",
      " 149171/175000: episode: 4167, duration: 0.836s, episode steps: 43, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.837 [24.000, 207.000], mean observation: 0.286 [0.000, 86.000], loss: 0.190175, mean_absolute_error: 0.601948, mean_q: 1.162104, mean_eps: 0.100000\n",
      " 149207/175000: episode: 4168, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 57.194 [12.000, 144.000], mean observation: 0.254 [0.000, 72.000], loss: 0.171596, mean_absolute_error: 0.602176, mean_q: 1.079029, mean_eps: 0.100000\n",
      " 149252/175000: episode: 4169, duration: 0.895s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 66.222 [16.000, 195.000], mean observation: 0.343 [0.000, 90.000], loss: 5.202627, mean_absolute_error: 0.623986, mean_q: 1.154453, mean_eps: 0.100000\n",
      " 149272/175000: episode: 4170, duration: 0.426s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 78.100 [15.000, 195.000], mean observation: 0.172 [0.000, 40.000], loss: 0.597802, mean_absolute_error: 0.597678, mean_q: 1.221012, mean_eps: 0.100000\n",
      " 149322/175000: episode: 4171, duration: 0.982s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.720 [15.000, 217.000], mean observation: 0.563 [0.000, 100.000], loss: 49.380482, mean_absolute_error: 0.801251, mean_q: 1.310944, mean_eps: 0.100000\n",
      " 149362/175000: episode: 4172, duration: 0.713s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 123.675 [10.000, 199.000], mean observation: 0.453 [0.000, 80.000], loss: 64.279992, mean_absolute_error: 0.865408, mean_q: 1.160220, mean_eps: 0.100000\n",
      " 149395/175000: episode: 4173, duration: 0.588s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 129.000 [1.000, 210.000], mean observation: 0.424 [0.000, 66.000], loss: 0.347099, mean_absolute_error: 0.607774, mean_q: 1.067350, mean_eps: 0.100000\n",
      " 149436/175000: episode: 4174, duration: 0.791s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 146.902 [42.000, 204.000], mean observation: 0.420 [0.000, 82.000], loss: 0.175205, mean_absolute_error: 0.585185, mean_q: 0.961223, mean_eps: 0.100000\n",
      " 149494/175000: episode: 4175, duration: 1.069s, episode steps: 58, steps per second: 54, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 104.241 [1.000, 214.000], mean observation: 0.637 [0.000, 116.000], loss: 0.120938, mean_absolute_error: 0.576489, mean_q: 0.885666, mean_eps: 0.100000\n",
      " 149546/175000: episode: 4176, duration: 0.944s, episode steps: 52, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 142.519 [40.000, 224.000], mean observation: 0.300 [0.000, 104.000], loss: 0.372586, mean_absolute_error: 0.582242, mean_q: 0.907746, mean_eps: 0.100000\n",
      " 149576/175000: episode: 4177, duration: 0.612s, episode steps: 30, steps per second: 49, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 76.000 [6.000, 207.000], mean observation: 0.333 [0.000, 60.000], loss: 0.190161, mean_absolute_error: 0.595266, mean_q: 1.197628, mean_eps: 0.100000\n",
      " 149619/175000: episode: 4178, duration: 0.790s, episode steps: 43, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.698 [6.000, 216.000], mean observation: 0.454 [0.000, 86.000], loss: 0.140347, mean_absolute_error: 0.596061, mean_q: 0.934447, mean_eps: 0.100000\n",
      " 149653/175000: episode: 4179, duration: 0.645s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.794 [7.000, 195.000], mean observation: 0.254 [0.000, 68.000], loss: 2161.980505, mean_absolute_error: 10.387258, mean_q: 2.788932, mean_eps: 0.100000\n",
      " 149701/175000: episode: 4180, duration: 0.870s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 166.625 [44.000, 195.000], mean observation: 0.514 [0.000, 96.000], loss: 0.171739, mean_absolute_error: 0.588827, mean_q: 0.752223, mean_eps: 0.100000\n",
      " 149736/175000: episode: 4181, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.800 [24.000, 199.000], mean observation: 0.385 [0.000, 70.000], loss: 0.184489, mean_absolute_error: 0.585889, mean_q: 0.707686, mean_eps: 0.100000\n",
      " 149781/175000: episode: 4182, duration: 0.871s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 159.600 [2.000, 215.000], mean observation: 0.648 [0.000, 90.000], loss: 1.082807, mean_absolute_error: 0.584039, mean_q: 0.663828, mean_eps: 0.100000\n",
      " 149809/175000: episode: 4183, duration: 0.547s, episode steps: 28, steps per second: 51, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 143.143 [14.000, 215.000], mean observation: 0.303 [0.000, 56.000], loss: 0.219265, mean_absolute_error: 0.586694, mean_q: 0.710942, mean_eps: 0.100000\n",
      " 149837/175000: episode: 4184, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 76.250 [1.000, 215.000], mean observation: 0.262 [0.000, 56.000], loss: 0.281440, mean_absolute_error: 0.583234, mean_q: 0.707536, mean_eps: 0.100000\n",
      " 149872/175000: episode: 4185, duration: 0.647s, episode steps: 35, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 83.229 [13.000, 215.000], mean observation: 0.427 [0.000, 70.000], loss: 2168.624249, mean_absolute_error: 10.420104, mean_q: 3.105643, mean_eps: 0.100000\n",
      " 149892/175000: episode: 4186, duration: 0.416s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 62.550 [58.000, 149.000], mean observation: 0.051 [0.000, 40.000], loss: 0.131852, mean_absolute_error: 0.571046, mean_q: 0.809624, mean_eps: 0.100000\n",
      " 149923/175000: episode: 4187, duration: 0.557s, episode steps: 31, steps per second: 56, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 84.290 [14.000, 188.000], mean observation: 0.339 [0.000, 62.000], loss: 0.356079, mean_absolute_error: 0.566959, mean_q: 0.882364, mean_eps: 0.100000\n",
      " 149957/175000: episode: 4188, duration: 0.643s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.824 [24.000, 222.000], mean observation: 0.545 [0.000, 68.000], loss: 0.191615, mean_absolute_error: 0.572470, mean_q: 0.806354, mean_eps: 0.100000\n",
      " 149997/175000: episode: 4189, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 107.025 [17.000, 187.000], mean observation: 0.533 [0.000, 80.000], loss: 0.168859, mean_absolute_error: 0.583726, mean_q: 0.903773, mean_eps: 0.100000\n",
      " 150037/175000: episode: 4190, duration: 0.804s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 119.275 [9.000, 224.000], mean observation: 0.448 [0.000, 80.000], loss: 1.649204, mean_absolute_error: 0.606333, mean_q: 1.029633, mean_eps: 0.100000\n",
      " 150071/175000: episode: 4191, duration: 0.593s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.088 [14.000, 187.000], mean observation: 0.382 [0.000, 68.000], loss: 0.357850, mean_absolute_error: 0.596867, mean_q: 0.927064, mean_eps: 0.100000\n",
      " 150127/175000: episode: 4192, duration: 1.021s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 98.768 [0.000, 187.000], mean observation: 0.603 [0.000, 112.000], loss: 0.197161, mean_absolute_error: 0.573491, mean_q: 0.970422, mean_eps: 0.100000\n",
      " 150162/175000: episode: 4193, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.171 [2.000, 224.000], mean observation: 0.294 [0.000, 70.000], loss: 0.177633, mean_absolute_error: 0.562548, mean_q: 1.077116, mean_eps: 0.100000\n",
      " 150210/175000: episode: 4194, duration: 0.859s, episode steps: 48, steps per second: 56, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.104 [30.000, 209.000], mean observation: 0.467 [0.000, 96.000], loss: 3.640364, mean_absolute_error: 0.570188, mean_q: 1.072328, mean_eps: 0.100000\n",
      " 150255/175000: episode: 4195, duration: 0.826s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.067 [27.000, 202.000], mean observation: 0.699 [0.000, 90.000], loss: 0.187775, mean_absolute_error: 0.546538, mean_q: 1.224842, mean_eps: 0.100000\n",
      " 150305/175000: episode: 4196, duration: 0.906s, episode steps: 50, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 133.200 [101.000, 195.000], mean observation: 0.472 [0.000, 100.000], loss: 0.114838, mean_absolute_error: 0.539881, mean_q: 1.080189, mean_eps: 0.100000\n",
      " 150341/175000: episode: 4197, duration: 0.643s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 101.528 [21.000, 218.000], mean observation: 0.490 [0.000, 72.000], loss: 0.158287, mean_absolute_error: 0.558351, mean_q: 1.062462, mean_eps: 0.100000\n",
      " 150397/175000: episode: 4198, duration: 1.034s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 77.161 [9.000, 217.000], mean observation: 0.637 [0.000, 112.000], loss: 0.167557, mean_absolute_error: 0.556125, mean_q: 1.058801, mean_eps: 0.100000\n",
      " 150427/175000: episode: 4199, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 80.567 [11.000, 174.000], mean observation: 0.171 [0.000, 60.000], loss: 0.233523, mean_absolute_error: 0.563475, mean_q: 1.080642, mean_eps: 0.100000\n",
      " 150449/175000: episode: 4200, duration: 0.442s, episode steps: 22, steps per second: 50, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 89.318 [11.000, 209.000], mean observation: 0.125 [0.000, 44.000], loss: 0.298009, mean_absolute_error: 0.573591, mean_q: 1.132542, mean_eps: 0.100000\n",
      " 150490/175000: episode: 4201, duration: 0.736s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 103.146 [5.000, 195.000], mean observation: 0.345 [0.000, 82.000], loss: 1008.395476, mean_absolute_error: 5.224938, mean_q: 2.733295, mean_eps: 0.100000\n",
      " 150506/175000: episode: 4202, duration: 0.289s, episode steps: 16, steps per second: 55, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 115.875 [93.000, 179.000], mean observation: 0.087 [0.000, 32.000], loss: 0.098027, mean_absolute_error: 0.593935, mean_q: 0.737472, mean_eps: 0.100000\n",
      " 150531/175000: episode: 4203, duration: 0.443s, episode steps: 25, steps per second: 56, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 79.640 [2.000, 190.000], mean observation: 0.198 [0.000, 50.000], loss: 0.182746, mean_absolute_error: 0.596139, mean_q: 0.717125, mean_eps: 0.100000\n",
      " 150554/175000: episode: 4204, duration: 0.443s, episode steps: 23, steps per second: 52, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 100.217 [14.000, 216.000], mean observation: 0.127 [0.000, 46.000], loss: 0.094425, mean_absolute_error: 0.585376, mean_q: 0.713451, mean_eps: 0.100000\n",
      " 150595/175000: episode: 4205, duration: 0.723s, episode steps: 41, steps per second: 57, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 74.537 [18.000, 208.000], mean observation: 0.521 [0.000, 82.000], loss: 1458.277414, mean_absolute_error: 7.228713, mean_q: 2.586161, mean_eps: 0.100000\n",
      " 150635/175000: episode: 4206, duration: 0.725s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 109.975 [13.000, 215.000], mean observation: 0.387 [0.000, 80.000], loss: 4.405986, mean_absolute_error: 0.591127, mean_q: 0.742325, mean_eps: 0.100000\n",
      " 150683/175000: episode: 4207, duration: 0.871s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 88.667 [8.000, 211.000], mean observation: 0.358 [0.000, 96.000], loss: 0.112207, mean_absolute_error: 0.565996, mean_q: 0.745149, mean_eps: 0.100000\n",
      " 150721/175000: episode: 4208, duration: 0.722s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 102.684 [24.000, 195.000], mean observation: 0.267 [0.000, 76.000], loss: 0.084263, mean_absolute_error: 0.570285, mean_q: 0.675228, mean_eps: 0.100000\n",
      " 150743/175000: episode: 4209, duration: 0.388s, episode steps: 22, steps per second: 57, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 90.909 [18.000, 203.000], mean observation: 0.152 [0.000, 44.000], loss: 0.152106, mean_absolute_error: 0.565564, mean_q: 0.737298, mean_eps: 0.100000\n",
      " 150770/175000: episode: 4210, duration: 0.535s, episode steps: 27, steps per second: 50, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 87.222 [24.000, 208.000], mean observation: 0.184 [0.000, 54.000], loss: 12.774336, mean_absolute_error: 0.613936, mean_q: 0.759450, mean_eps: 0.100000\n",
      " 150800/175000: episode: 4211, duration: 0.586s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 138.167 [7.000, 212.000], mean observation: 0.355 [0.000, 60.000], loss: 0.294618, mean_absolute_error: 0.547932, mean_q: 0.829800, mean_eps: 0.100000\n",
      " 150816/175000: episode: 4212, duration: 0.386s, episode steps: 16, steps per second: 41, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 83.500 [59.000, 208.000], mean observation: 0.041 [0.000, 32.000], loss: 0.336549, mean_absolute_error: 0.543753, mean_q: 0.923120, mean_eps: 0.100000\n",
      " 150860/175000: episode: 4213, duration: 0.889s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 165.295 [7.000, 208.000], mean observation: 0.403 [0.000, 88.000], loss: 7.138238, mean_absolute_error: 0.569568, mean_q: 0.896409, mean_eps: 0.100000\n",
      " 150911/175000: episode: 4214, duration: 0.941s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 164.824 [39.000, 208.000], mean observation: 0.485 [0.000, 102.000], loss: 0.155331, mean_absolute_error: 0.530037, mean_q: 0.920363, mean_eps: 0.100000\n",
      " 150957/175000: episode: 4215, duration: 0.858s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 161.065 [12.000, 208.000], mean observation: 0.353 [0.000, 92.000], loss: 1963.978252, mean_absolute_error: 9.387741, mean_q: 2.381502, mean_eps: 0.100000\n",
      " 151004/175000: episode: 4216, duration: 0.886s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 131.000 [12.000, 208.000], mean observation: 0.444 [0.000, 94.000], loss: 6.603464, mean_absolute_error: 0.558198, mean_q: 0.865212, mean_eps: 0.100000\n",
      " 151047/175000: episode: 4217, duration: 0.826s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 136.884 [12.000, 208.000], mean observation: 0.437 [0.000, 86.000], loss: 0.154603, mean_absolute_error: 0.528405, mean_q: 0.886618, mean_eps: 0.100000\n",
      " 151092/175000: episode: 4218, duration: 0.850s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 119.267 [12.000, 220.000], mean observation: 0.589 [0.000, 90.000], loss: 1.142736, mean_absolute_error: 0.537290, mean_q: 0.820429, mean_eps: 0.100000\n",
      " 151124/175000: episode: 4219, duration: 0.685s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 113.375 [1.000, 205.000], mean observation: 0.346 [0.000, 64.000], loss: 0.100400, mean_absolute_error: 0.534690, mean_q: 0.815732, mean_eps: 0.100000\n",
      " 151152/175000: episode: 4220, duration: 0.555s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 108.964 [27.000, 190.000], mean observation: 0.229 [0.000, 56.000], loss: 0.114605, mean_absolute_error: 0.543545, mean_q: 0.928226, mean_eps: 0.100000\n",
      " 151194/175000: episode: 4221, duration: 0.785s, episode steps: 42, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.167 [3.000, 208.000], mean observation: 0.531 [0.000, 84.000], loss: 0.118102, mean_absolute_error: 0.550381, mean_q: 0.961681, mean_eps: 0.100000\n",
      " 151223/175000: episode: 4222, duration: 0.560s, episode steps: 29, steps per second: 52, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 117.759 [27.000, 171.000], mean observation: 0.168 [0.000, 58.000], loss: 0.165946, mean_absolute_error: 0.556390, mean_q: 0.848529, mean_eps: 0.100000\n",
      " 151260/175000: episode: 4223, duration: 0.717s, episode steps: 37, steps per second: 52, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 98.459 [1.000, 210.000], mean observation: 0.326 [0.000, 74.000], loss: 0.102666, mean_absolute_error: 0.556131, mean_q: 0.785526, mean_eps: 0.100000\n",
      " 151282/175000: episode: 4224, duration: 0.461s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 148.091 [7.000, 219.000], mean observation: 0.087 [0.000, 44.000], loss: 0.091763, mean_absolute_error: 0.553776, mean_q: 0.751403, mean_eps: 0.100000\n",
      " 151311/175000: episode: 4225, duration: 0.531s, episode steps: 29, steps per second: 55, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 166.793 [165.000, 217.000], mean observation: 0.071 [0.000, 58.000], loss: 0.138032, mean_absolute_error: 0.551787, mean_q: 0.800995, mean_eps: 0.100000\n",
      " 151342/175000: episode: 4226, duration: 0.585s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 135.677 [8.000, 215.000], mean observation: 0.224 [0.000, 62.000], loss: 3266.528348, mean_absolute_error: 15.481023, mean_q: 5.490138, mean_eps: 0.100000\n",
      " 151382/175000: episode: 4227, duration: 0.724s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.000 [12.000, 215.000], mean observation: 0.382 [0.000, 80.000], loss: 0.093641, mean_absolute_error: 0.561390, mean_q: 0.740128, mean_eps: 0.100000\n",
      " 151416/175000: episode: 4228, duration: 0.661s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 169.824 [57.000, 215.000], mean observation: 0.179 [0.000, 68.000], loss: 2595.241186, mean_absolute_error: 12.301873, mean_q: 3.053256, mean_eps: 0.100000\n",
      " 151446/175000: episode: 4229, duration: 0.560s, episode steps: 30, steps per second: 54, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 169.067 [16.000, 215.000], mean observation: 0.168 [0.000, 60.000], loss: 0.426996, mean_absolute_error: 0.549884, mean_q: 0.778284, mean_eps: 0.100000\n",
      " 151492/175000: episode: 4230, duration: 0.882s, episode steps: 46, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.848 [38.000, 215.000], mean observation: 0.540 [0.000, 92.000], loss: 0.326814, mean_absolute_error: 0.554297, mean_q: 0.887431, mean_eps: 0.100000\n",
      " 151546/175000: episode: 4231, duration: 1.035s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 153.389 [23.000, 215.000], mean observation: 0.555 [0.000, 108.000], loss: 0.099772, mean_absolute_error: 0.557104, mean_q: 1.049825, mean_eps: 0.100000\n",
      " 151583/175000: episode: 4232, duration: 0.679s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 112.081 [26.000, 188.000], mean observation: 0.329 [0.000, 74.000], loss: 18.243728, mean_absolute_error: 0.618066, mean_q: 0.978470, mean_eps: 0.100000\n",
      " 151632/175000: episode: 4233, duration: 0.910s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 79.551 [1.000, 197.000], mean observation: 0.482 [0.000, 98.000], loss: 935.148257, mean_absolute_error: 4.833339, mean_q: 2.461447, mean_eps: 0.100000\n",
      " 151657/175000: episode: 4234, duration: 0.533s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 145.680 [38.000, 201.000], mean observation: 0.153 [0.000, 50.000], loss: 1159.040643, mean_absolute_error: 5.923987, mean_q: 3.569337, mean_eps: 0.100000\n",
      " 151702/175000: episode: 4235, duration: 0.845s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 87.267 [23.000, 195.000], mean observation: 0.308 [0.000, 90.000], loss: 5.243941, mean_absolute_error: 0.560797, mean_q: 0.957154, mean_eps: 0.100000\n",
      " 151729/175000: episode: 4236, duration: 0.521s, episode steps: 27, steps per second: 52, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 114.667 [38.000, 195.000], mean observation: 0.231 [0.000, 54.000], loss: 2562.573754, mean_absolute_error: 12.151658, mean_q: 3.574658, mean_eps: 0.100000\n",
      " 151773/175000: episode: 4237, duration: 0.811s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 98.364 [29.000, 207.000], mean observation: 0.219 [0.000, 88.000], loss: 12.518240, mean_absolute_error: 0.589378, mean_q: 1.070775, mean_eps: 0.100000\n",
      " 151801/175000: episode: 4238, duration: 0.536s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 94.250 [2.000, 211.000], mean observation: 0.201 [0.000, 56.000], loss: 1387.734512, mean_absolute_error: 6.933624, mean_q: 3.556767, mean_eps: 0.100000\n",
      " 151816/175000: episode: 4239, duration: 0.293s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 133.533 [54.000, 215.000], mean observation: 0.088 [0.000, 30.000], loss: 0.179186, mean_absolute_error: 0.534644, mean_q: 0.991612, mean_eps: 0.100000\n",
      " 151863/175000: episode: 4240, duration: 0.869s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 77.043 [38.000, 139.000], mean observation: 0.385 [0.000, 94.000], loss: 10.387693, mean_absolute_error: 0.577286, mean_q: 0.817775, mean_eps: 0.100000\n",
      " 151913/175000: episode: 4241, duration: 0.925s, episode steps: 50, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 118.280 [17.000, 172.000], mean observation: 0.365 [0.000, 100.000], loss: 0.405084, mean_absolute_error: 0.529323, mean_q: 0.793167, mean_eps: 0.100000\n",
      " 151939/175000: episode: 4242, duration: 0.451s, episode steps: 26, steps per second: 58, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 128.923 [36.000, 139.000], mean observation: 0.073 [0.000, 52.000], loss: 0.074148, mean_absolute_error: 0.524519, mean_q: 0.882659, mean_eps: 0.100000\n",
      " 151973/175000: episode: 4243, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 132.353 [99.000, 193.000], mean observation: 0.104 [0.000, 68.000], loss: 30.841001, mean_absolute_error: 0.668908, mean_q: 0.879364, mean_eps: 0.100000\n",
      " 151988/175000: episode: 4244, duration: 0.281s, episode steps: 15, steps per second: 53, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 117.067 [38.000, 208.000], mean observation: 0.090 [0.000, 30.000], loss: 0.104748, mean_absolute_error: 0.545864, mean_q: 0.863287, mean_eps: 0.100000\n",
      " 152019/175000: episode: 4245, duration: 0.598s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 123.194 [38.000, 208.000], mean observation: 0.192 [0.000, 62.000], loss: 0.100486, mean_absolute_error: 0.530543, mean_q: 0.855320, mean_eps: 0.100000\n",
      " 152047/175000: episode: 4246, duration: 0.493s, episode steps: 28, steps per second: 57, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 113.214 [38.000, 214.000], mean observation: 0.119 [0.000, 56.000], loss: 0.166313, mean_absolute_error: 0.537956, mean_q: 0.869922, mean_eps: 0.100000\n",
      " 152087/175000: episode: 4247, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 106.275 [8.000, 208.000], mean observation: 0.336 [0.000, 80.000], loss: 0.227987, mean_absolute_error: 0.537924, mean_q: 0.853042, mean_eps: 0.100000\n",
      " 152136/175000: episode: 4248, duration: 0.917s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 117.102 [27.000, 208.000], mean observation: 0.525 [0.000, 98.000], loss: 6.052278, mean_absolute_error: 0.551174, mean_q: 0.817225, mean_eps: 0.100000\n",
      " 152169/175000: episode: 4249, duration: 0.654s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 114.303 [50.000, 175.000], mean observation: 0.224 [0.000, 66.000], loss: 0.134819, mean_absolute_error: 0.526120, mean_q: 0.838528, mean_eps: 0.100000\n",
      " 152204/175000: episode: 4250, duration: 0.637s, episode steps: 35, steps per second: 55, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 88.143 [6.000, 224.000], mean observation: 0.264 [0.000, 70.000], loss: 1.573298, mean_absolute_error: 0.548684, mean_q: 0.918443, mean_eps: 0.100000\n",
      " 152242/175000: episode: 4251, duration: 0.716s, episode steps: 38, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 86.789 [38.000, 207.000], mean observation: 0.186 [0.000, 76.000], loss: 0.122234, mean_absolute_error: 0.530216, mean_q: 0.815672, mean_eps: 0.100000\n",
      " 152268/175000: episode: 4252, duration: 0.494s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 60.385 [37.000, 127.000], mean observation: 0.092 [0.000, 52.000], loss: 0.107350, mean_absolute_error: 0.549592, mean_q: 0.831861, mean_eps: 0.100000\n",
      " 152297/175000: episode: 4253, duration: 0.568s, episode steps: 29, steps per second: 51, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 77.103 [38.000, 161.000], mean observation: 0.122 [0.000, 58.000], loss: 0.357468, mean_absolute_error: 0.540606, mean_q: 0.812610, mean_eps: 0.100000\n",
      " 152330/175000: episode: 4254, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 71.030 [31.000, 184.000], mean observation: 0.234 [0.000, 66.000], loss: 10.471707, mean_absolute_error: 0.577556, mean_q: 0.844215, mean_eps: 0.100000\n",
      " 152353/175000: episode: 4255, duration: 0.450s, episode steps: 23, steps per second: 51, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 68.870 [16.000, 152.000], mean observation: 0.162 [0.000, 46.000], loss: 0.870949, mean_absolute_error: 0.535763, mean_q: 0.867337, mean_eps: 0.100000\n",
      " 152367/175000: episode: 4256, duration: 0.240s, episode steps: 14, steps per second: 58, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 104.643 [38.000, 152.000], mean observation: 0.074 [0.000, 28.000], loss: 0.084743, mean_absolute_error: 0.531794, mean_q: 0.753774, mean_eps: 0.100000\n",
      " 152419/175000: episode: 4257, duration: 0.920s, episode steps: 52, steps per second: 56, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 101.135 [9.000, 215.000], mean observation: 0.582 [0.000, 104.000], loss: 1.487185, mean_absolute_error: 0.533037, mean_q: 0.774998, mean_eps: 0.100000\n",
      " 152473/175000: episode: 4258, duration: 1.019s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 73.056 [1.000, 176.000], mean observation: 0.428 [0.000, 108.000], loss: 0.078168, mean_absolute_error: 0.526589, mean_q: 0.808656, mean_eps: 0.100000\n",
      " 152508/175000: episode: 4259, duration: 0.658s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 25.314 [12.000, 72.000], mean observation: 0.128 [0.000, 70.000], loss: 0.100043, mean_absolute_error: 0.528668, mean_q: 0.791082, mean_eps: 0.100000\n",
      " 152524/175000: episode: 4260, duration: 0.370s, episode steps: 16, steps per second: 43, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 82.062 [12.000, 187.000], mean observation: 0.113 [0.000, 32.000], loss: 0.093930, mean_absolute_error: 0.532404, mean_q: 0.796067, mean_eps: 0.100000\n",
      " 152566/175000: episode: 4261, duration: 0.762s, episode steps: 42, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 70.310 [8.000, 220.000], mean observation: 0.484 [0.000, 84.000], loss: 0.164295, mean_absolute_error: 0.530456, mean_q: 0.748120, mean_eps: 0.100000\n",
      " 152603/175000: episode: 4262, duration: 0.652s, episode steps: 37, steps per second: 57, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 113.946 [12.000, 222.000], mean observation: 0.344 [0.000, 74.000], loss: 0.131900, mean_absolute_error: 0.542790, mean_q: 0.709908, mean_eps: 0.100000\n",
      " 152627/175000: episode: 4263, duration: 0.443s, episode steps: 24, steps per second: 54, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 55.500 [12.000, 67.000], mean observation: 0.080 [0.000, 48.000], loss: 0.128319, mean_absolute_error: 0.558155, mean_q: 0.776090, mean_eps: 0.100000\n",
      " 152658/175000: episode: 4264, duration: 0.581s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 100.161 [12.000, 222.000], mean observation: 0.215 [0.000, 62.000], loss: 0.581557, mean_absolute_error: 0.568577, mean_q: 0.686311, mean_eps: 0.100000\n",
      " 152706/175000: episode: 4265, duration: 0.870s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 88.354 [12.000, 215.000], mean observation: 0.480 [0.000, 96.000], loss: 11.486358, mean_absolute_error: 0.631001, mean_q: 0.607424, mean_eps: 0.100000\n",
      " 152735/175000: episode: 4266, duration: 0.510s, episode steps: 29, steps per second: 57, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 152.793 [48.000, 224.000], mean observation: 0.317 [0.000, 58.000], loss: 11.483722, mean_absolute_error: 0.630848, mean_q: 0.641971, mean_eps: 0.100000\n",
      " 152772/175000: episode: 4267, duration: 0.724s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 86.243 [12.000, 169.000], mean observation: 0.295 [0.000, 74.000], loss: 3134.748970, mean_absolute_error: 14.680477, mean_q: 2.820527, mean_eps: 0.100000\n",
      " 152805/175000: episode: 4268, duration: 0.682s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 135.182 [8.000, 183.000], mean observation: 0.204 [0.000, 66.000], loss: 0.201945, mean_absolute_error: 0.558278, mean_q: 0.725856, mean_eps: 0.100000\n",
      " 152840/175000: episode: 4269, duration: 0.659s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.343 [30.000, 208.000], mean observation: 0.266 [0.000, 70.000], loss: 0.088284, mean_absolute_error: 0.558159, mean_q: 0.720266, mean_eps: 0.100000\n",
      " 152866/175000: episode: 4270, duration: 0.491s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 119.077 [42.000, 173.000], mean observation: 0.180 [0.000, 52.000], loss: 0.124477, mean_absolute_error: 0.560145, mean_q: 0.730616, mean_eps: 0.100000\n",
      " 152914/175000: episode: 4271, duration: 0.882s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.625 [18.000, 213.000], mean observation: 0.576 [0.000, 96.000], loss: 8.533800, mean_absolute_error: 0.597422, mean_q: 0.752497, mean_eps: 0.100000\n",
      " 152949/175000: episode: 4272, duration: 0.674s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 81.400 [3.000, 169.000], mean observation: 0.360 [0.000, 70.000], loss: 2116.330644, mean_absolute_error: 10.143373, mean_q: 2.991743, mean_eps: 0.100000\n",
      " 152986/175000: episode: 4273, duration: 0.667s, episode steps: 37, steps per second: 55, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 91.405 [14.000, 188.000], mean observation: 0.318 [0.000, 74.000], loss: 0.112007, mean_absolute_error: 0.553756, mean_q: 0.768089, mean_eps: 0.100000\n",
      " 153006/175000: episode: 4274, duration: 0.360s, episode steps: 20, steps per second: 56, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 100.700 [42.000, 169.000], mean observation: 0.142 [0.000, 40.000], loss: 0.133586, mean_absolute_error: 0.557298, mean_q: 0.731701, mean_eps: 0.100000\n",
      " 153053/175000: episode: 4275, duration: 0.880s, episode steps: 47, steps per second: 53, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 73.149 [18.000, 195.000], mean observation: 0.378 [0.000, 94.000], loss: 14.120002, mean_absolute_error: 0.620002, mean_q: 0.695239, mean_eps: 0.100000\n",
      " 153071/175000: episode: 4276, duration: 0.314s, episode steps: 18, steps per second: 57, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 119.222 [38.000, 195.000], mean observation: 0.155 [0.000, 36.000], loss: 0.115567, mean_absolute_error: 0.562146, mean_q: 0.872842, mean_eps: 0.100000\n",
      " 153103/175000: episode: 4277, duration: 0.606s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 126.719 [42.000, 207.000], mean observation: 0.341 [0.000, 64.000], loss: 0.191121, mean_absolute_error: 0.554047, mean_q: 0.791530, mean_eps: 0.100000\n",
      " 153148/175000: episode: 4278, duration: 0.842s, episode steps: 45, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 122.200 [24.000, 208.000], mean observation: 0.541 [0.000, 90.000], loss: 0.143878, mean_absolute_error: 0.559856, mean_q: 0.795731, mean_eps: 0.100000\n",
      " 153186/175000: episode: 4279, duration: 0.731s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 99.842 [17.000, 218.000], mean observation: 0.377 [0.000, 76.000], loss: 0.136735, mean_absolute_error: 0.580950, mean_q: 0.839929, mean_eps: 0.100000\n",
      " 153207/175000: episode: 4280, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 126.190 [25.000, 169.000], mean observation: 0.194 [0.000, 42.000], loss: 3.141812, mean_absolute_error: 0.610685, mean_q: 0.823194, mean_eps: 0.100000\n",
      " 153242/175000: episode: 4281, duration: 0.668s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 110.057 [61.000, 169.000], mean observation: 0.301 [0.000, 70.000], loss: 0.111625, mean_absolute_error: 0.592832, mean_q: 0.783162, mean_eps: 0.100000\n",
      " 153263/175000: episode: 4282, duration: 0.385s, episode steps: 21, steps per second: 55, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.333 [26.000, 199.000], mean observation: 0.163 [0.000, 42.000], loss: 3676.127850, mean_absolute_error: 17.251726, mean_q: 4.570558, mean_eps: 0.100000\n",
      " 153283/175000: episode: 4283, duration: 0.377s, episode steps: 20, steps per second: 53, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 128.400 [2.000, 173.000], mean observation: 0.138 [0.000, 40.000], loss: 0.208824, mean_absolute_error: 0.570331, mean_q: 0.582190, mean_eps: 0.100000\n",
      " 153330/175000: episode: 4284, duration: 0.898s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.957 [48.000, 216.000], mean observation: 0.489 [0.000, 94.000], loss: 0.107705, mean_absolute_error: 0.551268, mean_q: 0.568005, mean_eps: 0.100000\n",
      " 153353/175000: episode: 4285, duration: 0.438s, episode steps: 23, steps per second: 53, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 111.391 [21.000, 179.000], mean observation: 0.220 [0.000, 46.000], loss: 0.202926, mean_absolute_error: 0.548830, mean_q: 0.719773, mean_eps: 0.100000\n",
      " 153396/175000: episode: 4286, duration: 0.825s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.186 [26.000, 216.000], mean observation: 0.505 [0.000, 86.000], loss: 971.544998, mean_absolute_error: 5.049860, mean_q: 2.626567, mean_eps: 0.100000\n",
      " 153434/175000: episode: 4287, duration: 0.747s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.658 [10.000, 202.000], mean observation: 0.424 [0.000, 76.000], loss: 0.125977, mean_absolute_error: 0.588373, mean_q: 0.907341, mean_eps: 0.100000\n",
      " 153470/175000: episode: 4288, duration: 0.662s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.222 [50.000, 202.000], mean observation: 0.417 [0.000, 72.000], loss: 0.229029, mean_absolute_error: 0.592157, mean_q: 0.804263, mean_eps: 0.100000\n",
      " 153499/175000: episode: 4289, duration: 0.521s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 109.517 [45.000, 193.000], mean observation: 0.272 [0.000, 58.000], loss: 19.545294, mean_absolute_error: 0.692105, mean_q: 0.774138, mean_eps: 0.100000\n",
      " 153550/175000: episode: 4290, duration: 0.939s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 149.824 [55.000, 202.000], mean observation: 0.588 [0.000, 102.000], loss: 4.518972, mean_absolute_error: 0.627847, mean_q: 0.904958, mean_eps: 0.100000\n",
      " 153565/175000: episode: 4291, duration: 0.283s, episode steps: 15, steps per second: 53, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 102.133 [50.000, 148.000], mean observation: 0.115 [0.000, 30.000], loss: 2876.115394, mean_absolute_error: 13.797283, mean_q: 5.691931, mean_eps: 0.100000\n",
      " 153602/175000: episode: 4292, duration: 0.638s, episode steps: 37, steps per second: 58, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 138.108 [50.000, 210.000], mean observation: 0.432 [0.000, 74.000], loss: 0.090805, mean_absolute_error: 0.590310, mean_q: 0.815044, mean_eps: 0.100000\n",
      " 153622/175000: episode: 4293, duration: 0.388s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 93.400 [4.000, 148.000], mean observation: 0.169 [0.000, 40.000], loss: 0.321290, mean_absolute_error: 0.571336, mean_q: 0.795802, mean_eps: 0.100000\n",
      " 153674/175000: episode: 4294, duration: 0.920s, episode steps: 52, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 146.462 [50.000, 224.000], mean observation: 0.612 [0.000, 104.000], loss: 0.170160, mean_absolute_error: 0.570382, mean_q: 0.827424, mean_eps: 0.100000\n",
      " 153705/175000: episode: 4295, duration: 0.559s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 147.774 [21.000, 203.000], mean observation: 0.327 [0.000, 62.000], loss: 2094.166176, mean_absolute_error: 10.093600, mean_q: 3.260026, mean_eps: 0.100000\n",
      " 153727/175000: episode: 4296, duration: 0.397s, episode steps: 22, steps per second: 55, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 131.545 [37.000, 203.000], mean observation: 0.238 [0.000, 44.000], loss: 1204.132380, mean_absolute_error: 6.266129, mean_q: 4.763862, mean_eps: 0.100000\n",
      " 153792/175000: episode: 4297, duration: 1.212s, episode steps: 65, steps per second: 54, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 119.662 [15.000, 193.000], mean observation: 0.838 [0.000, 130.000], loss: 0.280232, mean_absolute_error: 0.573208, mean_q: 0.859702, mean_eps: 0.100000\n",
      " 153816/175000: episode: 4298, duration: 0.493s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 134.250 [44.000, 186.000], mean observation: 0.261 [0.000, 48.000], loss: 0.160987, mean_absolute_error: 0.562169, mean_q: 0.745035, mean_eps: 0.100000\n",
      " 153850/175000: episode: 4299, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 131.353 [44.000, 191.000], mean observation: 0.309 [0.000, 68.000], loss: 0.358501, mean_absolute_error: 0.568953, mean_q: 0.782855, mean_eps: 0.100000\n",
      " 153904/175000: episode: 4300, duration: 0.984s, episode steps: 54, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 102.852 [44.000, 191.000], mean observation: 0.696 [0.000, 108.000], loss: 13.378310, mean_absolute_error: 0.635624, mean_q: 0.786782, mean_eps: 0.100000\n",
      " 153927/175000: episode: 4301, duration: 0.469s, episode steps: 23, steps per second: 49, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 124.783 [42.000, 203.000], mean observation: 0.127 [0.000, 46.000], loss: 0.071826, mean_absolute_error: 0.561122, mean_q: 0.714645, mean_eps: 0.100000\n",
      " 153957/175000: episode: 4302, duration: 0.595s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 95.467 [42.000, 203.000], mean observation: 0.132 [0.000, 60.000], loss: 0.113989, mean_absolute_error: 0.566084, mean_q: 0.807969, mean_eps: 0.100000\n",
      " 153985/175000: episode: 4303, duration: 0.516s, episode steps: 28, steps per second: 54, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 86.679 [28.000, 197.000], mean observation: 0.165 [0.000, 56.000], loss: 0.106988, mean_absolute_error: 0.556515, mean_q: 0.750537, mean_eps: 0.100000\n",
      " 154016/175000: episode: 4304, duration: 0.590s, episode steps: 31, steps per second: 53, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 130.065 [16.000, 213.000], mean observation: 0.360 [0.000, 62.000], loss: 0.151456, mean_absolute_error: 0.553938, mean_q: 0.821128, mean_eps: 0.100000\n",
      " 154036/175000: episode: 4305, duration: 0.428s, episode steps: 20, steps per second: 47, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 112.900 [37.000, 191.000], mean observation: 0.153 [0.000, 40.000], loss: 44.712144, mean_absolute_error: 1.093898, mean_q: 4.760600, mean_eps: 0.100000\n",
      " 154064/175000: episode: 4306, duration: 0.587s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 131.714 [18.000, 201.000], mean observation: 0.241 [0.000, 56.000], loss: 0.359965, mean_absolute_error: 0.573941, mean_q: 0.791250, mean_eps: 0.100000\n",
      " 154085/175000: episode: 4307, duration: 0.419s, episode steps: 21, steps per second: 50, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 124.714 [37.000, 177.000], mean observation: 0.171 [0.000, 42.000], loss: 0.315349, mean_absolute_error: 0.608775, mean_q: 0.806348, mean_eps: 0.100000\n",
      " 154141/175000: episode: 4308, duration: 1.031s, episode steps: 56, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 154.304 [13.000, 187.000], mean observation: 0.743 [0.000, 112.000], loss: 0.730753, mean_absolute_error: 0.623175, mean_q: 1.028423, mean_eps: 0.100000\n",
      " 154160/175000: episode: 4309, duration: 0.359s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 121.158 [37.000, 202.000], mean observation: 0.096 [0.000, 38.000], loss: 0.093251, mean_absolute_error: 0.589802, mean_q: 0.823866, mean_eps: 0.100000\n",
      " 154196/175000: episode: 4310, duration: 0.754s, episode steps: 36, steps per second: 48, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 121.278 [37.000, 187.000], mean observation: 0.159 [0.000, 72.000], loss: 0.298252, mean_absolute_error: 0.581194, mean_q: 0.917941, mean_eps: 0.100000\n",
      " 154247/175000: episode: 4311, duration: 1.167s, episode steps: 51, steps per second: 44, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 143.157 [20.000, 192.000], mean observation: 0.346 [0.000, 102.000], loss: 0.193585, mean_absolute_error: 0.570342, mean_q: 0.855314, mean_eps: 0.100000\n",
      " 154284/175000: episode: 4312, duration: 0.826s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 117.919 [37.000, 193.000], mean observation: 0.252 [0.000, 74.000], loss: 901.568696, mean_absolute_error: 4.745199, mean_q: 3.095476, mean_eps: 0.100000\n",
      " 154327/175000: episode: 4313, duration: 0.898s, episode steps: 43, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 151.535 [14.000, 209.000], mean observation: 0.394 [0.000, 86.000], loss: 2347.166410, mean_absolute_error: 11.139500, mean_q: 2.730059, mean_eps: 0.100000\n",
      " 154370/175000: episode: 4314, duration: 0.994s, episode steps: 43, steps per second: 43, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 111.070 [25.000, 211.000], mean observation: 0.497 [0.000, 86.000], loss: 0.169676, mean_absolute_error: 0.575561, mean_q: 1.215563, mean_eps: 0.100000\n",
      " 154428/175000: episode: 4315, duration: 1.271s, episode steps: 58, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 110.759 [21.000, 204.000], mean observation: 0.707 [0.000, 116.000], loss: 0.130268, mean_absolute_error: 0.549558, mean_q: 1.116629, mean_eps: 0.100000\n",
      " 154444/175000: episode: 4316, duration: 0.417s, episode steps: 16, steps per second: 38, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 76.250 [38.000, 135.000], mean observation: 0.072 [0.000, 32.000], loss: 0.077963, mean_absolute_error: 0.551137, mean_q: 1.161177, mean_eps: 0.100000\n",
      " 154475/175000: episode: 4317, duration: 0.695s, episode steps: 31, steps per second: 45, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 88.387 [37.000, 158.000], mean observation: 0.172 [0.000, 62.000], loss: 42.585383, mean_absolute_error: 0.867087, mean_q: 2.771695, mean_eps: 0.100000\n",
      " 154497/175000: episode: 4318, duration: 0.527s, episode steps: 22, steps per second: 42, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 136.545 [87.000, 222.000], mean observation: 0.161 [0.000, 44.000], loss: 0.256367, mean_absolute_error: 0.557811, mean_q: 1.316478, mean_eps: 0.100000\n",
      " 154540/175000: episode: 4319, duration: 0.913s, episode steps: 43, steps per second: 47, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 91.116 [4.000, 200.000], mean observation: 0.276 [0.000, 86.000], loss: 781.267379, mean_absolute_error: 4.103008, mean_q: 2.274152, mean_eps: 0.100000\n",
      " 154573/175000: episode: 4320, duration: 0.768s, episode steps: 33, steps per second: 43, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 111.788 [14.000, 207.000], mean observation: 0.170 [0.000, 66.000], loss: 0.084181, mean_absolute_error: 0.566565, mean_q: 1.382863, mean_eps: 0.100000\n",
      " 154594/175000: episode: 4321, duration: 0.425s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 130.619 [87.000, 222.000], mean observation: 0.073 [0.000, 42.000], loss: 5.062766, mean_absolute_error: 0.591143, mean_q: 1.533766, mean_eps: 0.100000\n",
      " 154624/175000: episode: 4322, duration: 0.715s, episode steps: 30, steps per second: 42, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.400 [41.000, 210.000], mean observation: 0.174 [0.000, 60.000], loss: 2564.303901, mean_absolute_error: 12.192336, mean_q: 4.323019, mean_eps: 0.100000\n",
      " 154671/175000: episode: 4323, duration: 0.966s, episode steps: 47, steps per second: 49, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 93.426 [5.000, 213.000], mean observation: 0.295 [0.000, 94.000], loss: 17.704691, mean_absolute_error: 0.767799, mean_q: 2.799014, mean_eps: 0.100000\n",
      " 154699/175000: episode: 4324, duration: 0.565s, episode steps: 28, steps per second: 50, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 122.714 [5.000, 210.000], mean observation: 0.179 [0.000, 56.000], loss: 0.401633, mean_absolute_error: 0.563471, mean_q: 1.258281, mean_eps: 0.100000\n",
      " 154706/175000: episode: 4325, duration: 0.141s, episode steps: 7, steps per second: 50, episode reward: -1.000, mean reward: -0.143 [-1.000, 0.000], mean action: 158.000 [158.000, 158.000], mean observation: 0.019 [0.000, 14.000], loss: 0.100393, mean_absolute_error: 0.566529, mean_q: 1.407212, mean_eps: 0.100000\n",
      " 154741/175000: episode: 4326, duration: 0.676s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.200 [5.000, 210.000], mean observation: 0.238 [0.000, 70.000], loss: 0.126184, mean_absolute_error: 0.556939, mean_q: 1.273143, mean_eps: 0.100000\n",
      " 154793/175000: episode: 4327, duration: 0.957s, episode steps: 52, steps per second: 54, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 173.942 [42.000, 223.000], mean observation: 0.388 [0.000, 104.000], loss: 10.530185, mean_absolute_error: 0.627300, mean_q: 1.701543, mean_eps: 0.100000\n",
      " 154818/175000: episode: 4328, duration: 0.467s, episode steps: 25, steps per second: 54, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 136.240 [87.000, 186.000], mean observation: 0.116 [0.000, 50.000], loss: 0.178423, mean_absolute_error: 0.553057, mean_q: 1.449156, mean_eps: 0.100000\n",
      " 154852/175000: episode: 4329, duration: 0.631s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 134.912 [38.000, 202.000], mean observation: 0.184 [0.000, 68.000], loss: 0.153127, mean_absolute_error: 0.550806, mean_q: 1.388328, mean_eps: 0.100000\n",
      " 154872/175000: episode: 4330, duration: 0.453s, episode steps: 20, steps per second: 44, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 110.550 [16.000, 169.000], mean observation: 0.072 [0.000, 40.000], loss: 1.453971, mean_absolute_error: 0.605054, mean_q: 1.739645, mean_eps: 0.100000\n",
      " 154897/175000: episode: 4331, duration: 0.508s, episode steps: 25, steps per second: 49, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 78.120 [1.000, 219.000], mean observation: 0.149 [0.000, 50.000], loss: 13.305101, mean_absolute_error: 0.644583, mean_q: 1.330933, mean_eps: 0.100000\n",
      " 154926/175000: episode: 4332, duration: 0.521s, episode steps: 29, steps per second: 56, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 98.966 [1.000, 221.000], mean observation: 0.187 [0.000, 58.000], loss: 0.433180, mean_absolute_error: 0.579761, mean_q: 1.316273, mean_eps: 0.100000\n",
      " 154973/175000: episode: 4333, duration: 0.862s, episode steps: 47, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 60.851 [1.000, 193.000], mean observation: 0.557 [0.000, 94.000], loss: 26.781904, mean_absolute_error: 0.696323, mean_q: 1.326857, mean_eps: 0.100000\n",
      " 155000/175000: episode: 4334, duration: 0.498s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 111.889 [24.000, 200.000], mean observation: 0.235 [0.000, 54.000], loss: 0.256130, mean_absolute_error: 0.588332, mean_q: 1.606936, mean_eps: 0.100000\n",
      " 155027/175000: episode: 4335, duration: 0.507s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 112.519 [24.000, 196.000], mean observation: 0.160 [0.000, 54.000], loss: 0.557340, mean_absolute_error: 0.577017, mean_q: 1.640764, mean_eps: 0.100000\n",
      " 155048/175000: episode: 4336, duration: 0.434s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.095 [24.000, 139.000], mean observation: 0.053 [0.000, 42.000], loss: 0.697582, mean_absolute_error: 0.566545, mean_q: 1.141662, mean_eps: 0.100000\n",
      " 155094/175000: episode: 4337, duration: 0.849s, episode steps: 46, steps per second: 54, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 69.587 [1.000, 206.000], mean observation: 0.218 [0.000, 92.000], loss: 0.193128, mean_absolute_error: 0.586598, mean_q: 1.111370, mean_eps: 0.100000\n",
      " 155128/175000: episode: 4338, duration: 0.696s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.294 [1.000, 216.000], mean observation: 0.347 [0.000, 68.000], loss: 0.253511, mean_absolute_error: 0.616569, mean_q: 1.089128, mean_eps: 0.100000\n",
      " 155171/175000: episode: 4339, duration: 0.786s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.628 [1.000, 210.000], mean observation: 0.454 [0.000, 86.000], loss: 5.437944, mean_absolute_error: 0.633296, mean_q: 1.003558, mean_eps: 0.100000\n",
      " 155201/175000: episode: 4340, duration: 0.621s, episode steps: 30, steps per second: 48, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 82.267 [1.000, 222.000], mean observation: 0.272 [0.000, 60.000], loss: 9.275669, mean_absolute_error: 0.781144, mean_q: 2.775552, mean_eps: 0.100000\n",
      " 155257/175000: episode: 4341, duration: 1.025s, episode steps: 56, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 67.339 [1.000, 209.000], mean observation: 0.777 [0.000, 112.000], loss: 24.384772, mean_absolute_error: 0.697066, mean_q: 1.035435, mean_eps: 0.100000\n",
      " 155275/175000: episode: 4342, duration: 0.322s, episode steps: 18, steps per second: 56, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 78.000 [14.000, 223.000], mean observation: 0.114 [0.000, 36.000], loss: 99.385092, mean_absolute_error: 1.023586, mean_q: 0.921351, mean_eps: 0.100000\n",
      " 155315/175000: episode: 4343, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 88.750 [9.000, 223.000], mean observation: 0.508 [0.000, 80.000], loss: 0.453141, mean_absolute_error: 0.582607, mean_q: 1.055200, mean_eps: 0.100000\n",
      " 155334/175000: episode: 4344, duration: 0.360s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 170.842 [14.000, 223.000], mean observation: 0.112 [0.000, 38.000], loss: 0.238563, mean_absolute_error: 0.601021, mean_q: 1.159903, mean_eps: 0.100000\n",
      " 155349/175000: episode: 4345, duration: 0.310s, episode steps: 15, steps per second: 48, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 146.067 [1.000, 193.000], mean observation: 0.064 [0.000, 30.000], loss: 2840.568939, mean_absolute_error: 13.640264, mean_q: 5.993608, mean_eps: 0.100000\n",
      " 155381/175000: episode: 4346, duration: 0.579s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 93.594 [1.000, 193.000], mean observation: 0.204 [0.000, 64.000], loss: 0.161122, mean_absolute_error: 0.628486, mean_q: 1.231518, mean_eps: 0.100000\n",
      " 155430/175000: episode: 4347, duration: 0.875s, episode steps: 49, steps per second: 56, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 95.755 [9.000, 206.000], mean observation: 0.277 [0.000, 98.000], loss: 5.224817, mean_absolute_error: 0.629387, mean_q: 1.061990, mean_eps: 0.100000\n",
      " 155462/175000: episode: 4348, duration: 0.582s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.375 [7.000, 202.000], mean observation: 0.204 [0.000, 64.000], loss: 0.516280, mean_absolute_error: 0.624532, mean_q: 1.103113, mean_eps: 0.100000\n",
      " 155507/175000: episode: 4349, duration: 0.797s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 110.778 [37.000, 218.000], mean observation: 0.234 [0.000, 90.000], loss: 0.245815, mean_absolute_error: 0.614395, mean_q: 1.012808, mean_eps: 0.100000\n",
      " 155539/175000: episode: 4350, duration: 0.567s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 154.500 [62.000, 219.000], mean observation: 0.103 [0.000, 64.000], loss: 0.200225, mean_absolute_error: 0.810021, mean_q: 3.421810, mean_eps: 0.100000\n",
      " 155573/175000: episode: 4351, duration: 0.673s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 118.676 [1.000, 213.000], mean observation: 0.197 [0.000, 68.000], loss: 9.753408, mean_absolute_error: 0.820835, mean_q: 3.127733, mean_eps: 0.100000\n",
      " 155609/175000: episode: 4352, duration: 0.648s, episode steps: 36, steps per second: 56, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.583 [1.000, 154.000], mean observation: 0.235 [0.000, 72.000], loss: 1462.780643, mean_absolute_error: 7.263404, mean_q: 3.152992, mean_eps: 0.100000\n",
      " 155653/175000: episode: 4353, duration: 0.786s, episode steps: 44, steps per second: 56, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.886 [43.000, 208.000], mean observation: 0.236 [0.000, 88.000], loss: 1.086017, mean_absolute_error: 0.585851, mean_q: 1.004193, mean_eps: 0.100000\n",
      " 155678/175000: episode: 4354, duration: 0.476s, episode steps: 25, steps per second: 53, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 85.040 [6.000, 222.000], mean observation: 0.139 [0.000, 50.000], loss: 0.090076, mean_absolute_error: 0.590859, mean_q: 1.018053, mean_eps: 0.100000\n",
      " 155712/175000: episode: 4355, duration: 0.639s, episode steps: 34, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 113.206 [6.000, 161.000], mean observation: 0.189 [0.000, 68.000], loss: 0.104624, mean_absolute_error: 0.596534, mean_q: 1.070782, mean_eps: 0.100000\n",
      " 155727/175000: episode: 4356, duration: 0.295s, episode steps: 15, steps per second: 51, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 150.667 [6.000, 200.000], mean observation: 0.077 [0.000, 30.000], loss: 0.437115, mean_absolute_error: 0.592264, mean_q: 0.868425, mean_eps: 0.100000\n",
      " 155756/175000: episode: 4357, duration: 0.611s, episode steps: 29, steps per second: 47, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 185.241 [5.000, 195.000], mean observation: 0.081 [0.000, 58.000], loss: 0.093558, mean_absolute_error: 0.611368, mean_q: 0.878577, mean_eps: 0.100000\n",
      " 155790/175000: episode: 4358, duration: 0.634s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 140.559 [7.000, 207.000], mean observation: 0.303 [0.000, 68.000], loss: 1.055024, mean_absolute_error: 0.636000, mean_q: 0.890368, mean_eps: 0.100000\n",
      " 155823/175000: episode: 4359, duration: 0.575s, episode steps: 33, steps per second: 57, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.667 [11.000, 198.000], mean observation: 0.353 [0.000, 66.000], loss: 0.092469, mean_absolute_error: 0.632874, mean_q: 0.827534, mean_eps: 0.100000\n",
      " 155840/175000: episode: 4360, duration: 0.443s, episode steps: 17, steps per second: 38, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 75.294 [1.000, 188.000], mean observation: 0.079 [0.000, 34.000], loss: 0.164389, mean_absolute_error: 0.656697, mean_q: 0.918784, mean_eps: 0.100000\n",
      " 155870/175000: episode: 4361, duration: 0.592s, episode steps: 30, steps per second: 51, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.500 [29.000, 210.000], mean observation: 0.294 [0.000, 60.000], loss: 0.070727, mean_absolute_error: 0.616793, mean_q: 0.761317, mean_eps: 0.100000\n",
      " 155894/175000: episode: 4362, duration: 0.464s, episode steps: 24, steps per second: 52, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 145.042 [31.000, 222.000], mean observation: 0.146 [0.000, 48.000], loss: 0.345832, mean_absolute_error: 0.623169, mean_q: 0.806207, mean_eps: 0.100000\n",
      " 155908/175000: episode: 4363, duration: 0.296s, episode steps: 14, steps per second: 47, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 106.071 [67.000, 200.000], mean observation: 0.042 [0.000, 28.000], loss: 0.165526, mean_absolute_error: 0.613909, mean_q: 0.851158, mean_eps: 0.100000\n",
      " 155947/175000: episode: 4364, duration: 0.713s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 142.795 [27.000, 208.000], mean observation: 0.280 [0.000, 78.000], loss: 0.426373, mean_absolute_error: 0.629024, mean_q: 0.904260, mean_eps: 0.100000\n",
      " 155959/175000: episode: 4365, duration: 0.245s, episode steps: 12, steps per second: 49, episode reward: -1.000, mean reward: -0.083 [-1.000, 0.000], mean action: 141.500 [3.000, 223.000], mean observation: 0.059 [0.000, 24.000], loss: 0.492964, mean_absolute_error: 0.625854, mean_q: 0.912895, mean_eps: 0.100000\n",
      " 156010/175000: episode: 4366, duration: 0.938s, episode steps: 51, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 121.118 [6.000, 222.000], mean observation: 0.645 [0.000, 102.000], loss: 0.262592, mean_absolute_error: 0.645241, mean_q: 1.070142, mean_eps: 0.100000\n",
      " 156046/175000: episode: 4367, duration: 0.650s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.222 [44.000, 216.000], mean observation: 0.280 [0.000, 72.000], loss: 0.138832, mean_absolute_error: 0.656197, mean_q: 0.968292, mean_eps: 0.100000\n",
      " 156081/175000: episode: 4368, duration: 0.657s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 141.000 [46.000, 223.000], mean observation: 0.350 [0.000, 70.000], loss: 6.912218, mean_absolute_error: 0.675537, mean_q: 0.909697, mean_eps: 0.100000\n",
      " 156101/175000: episode: 4369, duration: 0.364s, episode steps: 20, steps per second: 55, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 80.650 [1.000, 222.000], mean observation: 0.098 [0.000, 40.000], loss: 6.908650, mean_absolute_error: 0.675727, mean_q: 0.842205, mean_eps: 0.100000\n",
      " 156149/175000: episode: 4370, duration: 0.834s, episode steps: 48, steps per second: 58, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 39.354 [1.000, 181.000], mean observation: 0.251 [0.000, 96.000], loss: 105.367829, mean_absolute_error: 1.258705, mean_q: 2.385348, mean_eps: 0.100000\n",
      " 156197/175000: episode: 4371, duration: 0.879s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 116.625 [1.000, 223.000], mean observation: 0.574 [0.000, 96.000], loss: 0.139789, mean_absolute_error: 0.645168, mean_q: 0.782831, mean_eps: 0.100000\n",
      " 156255/175000: episode: 4372, duration: 1.006s, episode steps: 58, steps per second: 58, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 121.931 [7.000, 223.000], mean observation: 0.801 [0.000, 116.000], loss: 0.429425, mean_absolute_error: 0.645998, mean_q: 0.686021, mean_eps: 0.100000\n",
      " 156312/175000: episode: 4373, duration: 1.056s, episode steps: 57, steps per second: 54, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 83.649 [5.000, 223.000], mean observation: 0.717 [0.000, 114.000], loss: 0.121595, mean_absolute_error: 0.626265, mean_q: 0.689094, mean_eps: 0.100000\n",
      " 156349/175000: episode: 4374, duration: 0.696s, episode steps: 37, steps per second: 53, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 132.135 [12.000, 213.000], mean observation: 0.216 [0.000, 74.000], loss: 0.269377, mean_absolute_error: 0.626892, mean_q: 0.995369, mean_eps: 0.100000\n",
      " 156380/175000: episode: 4375, duration: 0.561s, episode steps: 31, steps per second: 55, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 76.355 [27.000, 209.000], mean observation: 0.119 [0.000, 62.000], loss: 0.193931, mean_absolute_error: 0.601518, mean_q: 0.865022, mean_eps: 0.100000\n",
      " 156420/175000: episode: 4376, duration: 0.787s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 100.250 [27.000, 209.000], mean observation: 0.404 [0.000, 80.000], loss: 0.464213, mean_absolute_error: 0.599414, mean_q: 0.820106, mean_eps: 0.100000\n",
      " 156459/175000: episode: 4377, duration: 0.766s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 114.154 [1.000, 221.000], mean observation: 0.311 [0.000, 78.000], loss: 0.220471, mean_absolute_error: 0.623701, mean_q: 0.649107, mean_eps: 0.100000\n",
      " 156493/175000: episode: 4378, duration: 0.634s, episode steps: 34, steps per second: 54, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 96.559 [51.000, 200.000], mean observation: 0.321 [0.000, 68.000], loss: 0.163685, mean_absolute_error: 0.634927, mean_q: 0.782045, mean_eps: 0.100000\n",
      " 156524/175000: episode: 4379, duration: 0.573s, episode steps: 31, steps per second: 54, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 104.097 [61.000, 221.000], mean observation: 0.237 [0.000, 62.000], loss: 0.180254, mean_absolute_error: 0.656893, mean_q: 0.719090, mean_eps: 0.100000\n",
      " 156564/175000: episode: 4380, duration: 0.779s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 139.100 [27.000, 208.000], mean observation: 0.597 [0.000, 80.000], loss: 0.298004, mean_absolute_error: 0.655406, mean_q: 0.710746, mean_eps: 0.100000\n",
      " 156589/175000: episode: 4381, duration: 0.496s, episode steps: 25, steps per second: 50, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 106.880 [14.000, 189.000], mean observation: 0.179 [0.000, 50.000], loss: 0.164156, mean_absolute_error: 0.656345, mean_q: 0.602730, mean_eps: 0.100000\n",
      " 156629/175000: episode: 4382, duration: 0.730s, episode steps: 40, steps per second: 55, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 128.525 [25.000, 224.000], mean observation: 0.410 [0.000, 80.000], loss: 0.640119, mean_absolute_error: 0.662593, mean_q: 0.669173, mean_eps: 0.100000\n",
      " 156665/175000: episode: 4383, duration: 0.651s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 27.361 [25.000, 42.000], mean observation: 0.085 [0.000, 72.000], loss: 0.160019, mean_absolute_error: 0.663406, mean_q: 0.971753, mean_eps: 0.100000\n",
      " 156688/175000: episode: 4384, duration: 0.421s, episode steps: 23, steps per second: 55, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 36.087 [25.000, 42.000], mean observation: 0.086 [0.000, 46.000], loss: 2.558718, mean_absolute_error: 0.641398, mean_q: 0.654382, mean_eps: 0.100000\n",
      " 156741/175000: episode: 4385, duration: 1.010s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.887 [25.000, 200.000], mean observation: 0.571 [0.000, 106.000], loss: 0.780702, mean_absolute_error: 0.629485, mean_q: 0.750183, mean_eps: 0.100000\n",
      " 156769/175000: episode: 4386, duration: 0.500s, episode steps: 28, steps per second: 56, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 153.429 [12.000, 200.000], mean observation: 0.261 [0.000, 56.000], loss: 0.495307, mean_absolute_error: 0.628632, mean_q: 0.663777, mean_eps: 0.100000\n",
      " 156808/175000: episode: 4387, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.359 [29.000, 200.000], mean observation: 0.326 [0.000, 78.000], loss: 1002.654089, mean_absolute_error: 5.269924, mean_q: 2.856691, mean_eps: 0.100000\n",
      " 156845/175000: episode: 4388, duration: 0.720s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.324 [25.000, 200.000], mean observation: 0.314 [0.000, 74.000], loss: 0.541142, mean_absolute_error: 0.647981, mean_q: 0.826716, mean_eps: 0.100000\n",
      " 156881/175000: episode: 4389, duration: 0.655s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 132.333 [6.000, 214.000], mean observation: 0.311 [0.000, 72.000], loss: 0.142892, mean_absolute_error: 0.656738, mean_q: 0.948150, mean_eps: 0.100000\n",
      " 156923/175000: episode: 4390, duration: 0.746s, episode steps: 42, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 113.571 [18.000, 207.000], mean observation: 0.347 [0.000, 84.000], loss: 196.001879, mean_absolute_error: 1.551746, mean_q: 1.102271, mean_eps: 0.100000\n",
      " 156974/175000: episode: 4391, duration: 1.007s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 139.725 [29.000, 200.000], mean observation: 0.341 [0.000, 102.000], loss: 1.568842, mean_absolute_error: 0.671421, mean_q: 0.682392, mean_eps: 0.100000\n",
      " 157019/175000: episode: 4392, duration: 0.787s, episode steps: 45, steps per second: 57, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 116.556 [15.000, 213.000], mean observation: 0.499 [0.000, 90.000], loss: 0.468573, mean_absolute_error: 0.688533, mean_q: 0.637426, mean_eps: 0.100000\n",
      " 157065/175000: episode: 4393, duration: 0.871s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 86.348 [18.000, 198.000], mean observation: 0.521 [0.000, 92.000], loss: 0.621402, mean_absolute_error: 0.703338, mean_q: 0.542550, mean_eps: 0.100000\n",
      " 157098/175000: episode: 4394, duration: 0.601s, episode steps: 33, steps per second: 55, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.333 [29.000, 200.000], mean observation: 0.306 [0.000, 66.000], loss: 47.949467, mean_absolute_error: 0.921383, mean_q: 0.543523, mean_eps: 0.100000\n",
      " 157147/175000: episode: 4395, duration: 0.905s, episode steps: 49, steps per second: 54, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 110.082 [16.000, 200.000], mean observation: 0.510 [0.000, 98.000], loss: 0.319563, mean_absolute_error: 0.715672, mean_q: 0.490758, mean_eps: 0.100000\n",
      " 157179/175000: episode: 4396, duration: 0.579s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.469 [29.000, 200.000], mean observation: 0.295 [0.000, 64.000], loss: 0.733787, mean_absolute_error: 0.723286, mean_q: 0.587572, mean_eps: 0.100000\n",
      " 157205/175000: episode: 4397, duration: 0.528s, episode steps: 26, steps per second: 49, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 97.115 [1.000, 189.000], mean observation: 0.304 [0.000, 52.000], loss: 1.516451, mean_absolute_error: 0.741569, mean_q: 0.500514, mean_eps: 0.100000\n",
      " 157241/175000: episode: 4398, duration: 0.654s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.889 [7.000, 189.000], mean observation: 0.328 [0.000, 72.000], loss: 0.863463, mean_absolute_error: 0.748935, mean_q: 0.582735, mean_eps: 0.100000\n",
      " 157279/175000: episode: 4399, duration: 0.660s, episode steps: 38, steps per second: 58, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.842 [29.000, 202.000], mean observation: 0.344 [0.000, 76.000], loss: 22.504651, mean_absolute_error: 0.838298, mean_q: 0.653361, mean_eps: 0.100000\n",
      " 157330/175000: episode: 4400, duration: 0.929s, episode steps: 51, steps per second: 55, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 90.196 [20.000, 214.000], mean observation: 0.323 [0.000, 102.000], loss: 2.736011, mean_absolute_error: 0.760514, mean_q: 0.847110, mean_eps: 0.100000\n",
      " 157365/175000: episode: 4401, duration: 0.665s, episode steps: 35, steps per second: 53, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 68.343 [11.000, 172.000], mean observation: 0.147 [0.000, 70.000], loss: 1.135798, mean_absolute_error: 0.799182, mean_q: 0.762167, mean_eps: 0.100000\n",
      " 157406/175000: episode: 4402, duration: 0.738s, episode steps: 41, steps per second: 56, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 69.341 [0.000, 207.000], mean observation: 0.263 [0.000, 82.000], loss: 9.935911, mean_absolute_error: 0.861406, mean_q: 0.679979, mean_eps: 0.100000\n",
      " 157449/175000: episode: 4403, duration: 0.783s, episode steps: 43, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 69.047 [14.000, 213.000], mean observation: 0.476 [0.000, 86.000], loss: 0.806589, mean_absolute_error: 0.830994, mean_q: 0.649895, mean_eps: 0.100000\n",
      " 157482/175000: episode: 4404, duration: 0.617s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 106.909 [18.000, 195.000], mean observation: 0.232 [0.000, 66.000], loss: 0.349387, mean_absolute_error: 0.827279, mean_q: 0.507500, mean_eps: 0.100000\n",
      " 157530/175000: episode: 4405, duration: 0.869s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 129.917 [14.000, 218.000], mean observation: 0.665 [0.000, 96.000], loss: 4.151830, mean_absolute_error: 0.837302, mean_q: 0.516305, mean_eps: 0.100000\n",
      " 157574/175000: episode: 4406, duration: 0.808s, episode steps: 44, steps per second: 54, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 125.068 [5.000, 206.000], mean observation: 0.450 [0.000, 88.000], loss: 1.227119, mean_absolute_error: 0.809382, mean_q: 0.558104, mean_eps: 0.100000\n",
      " 157606/175000: episode: 4407, duration: 0.577s, episode steps: 32, steps per second: 55, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 95.719 [18.000, 206.000], mean observation: 0.342 [0.000, 64.000], loss: 22.006585, mean_absolute_error: 0.901729, mean_q: 0.642377, mean_eps: 0.100000\n",
      " 157639/175000: episode: 4408, duration: 0.586s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.606 [2.000, 206.000], mean observation: 0.312 [0.000, 66.000], loss: 1.979207, mean_absolute_error: 0.820907, mean_q: 0.588048, mean_eps: 0.100000\n",
      " 157671/175000: episode: 4409, duration: 0.574s, episode steps: 32, steps per second: 56, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 84.625 [63.000, 172.000], mean observation: 0.199 [0.000, 64.000], loss: 2.995156, mean_absolute_error: 0.860134, mean_q: 0.562258, mean_eps: 0.100000\n",
      " 157703/175000: episode: 4410, duration: 0.628s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 77.219 [7.000, 215.000], mean observation: 0.311 [0.000, 64.000], loss: 1.430994, mean_absolute_error: 0.888099, mean_q: 0.550396, mean_eps: 0.100000\n",
      " 157739/175000: episode: 4411, duration: 0.652s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 91.806 [7.000, 223.000], mean observation: 0.264 [0.000, 72.000], loss: 0.447837, mean_absolute_error: 0.914694, mean_q: 0.617817, mean_eps: 0.100000\n",
      " 157771/175000: episode: 4412, duration: 0.602s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 79.188 [23.000, 205.000], mean observation: 0.192 [0.000, 64.000], loss: 0.651506, mean_absolute_error: 1.112690, mean_q: 2.957036, mean_eps: 0.100000\n",
      " 157790/175000: episode: 4413, duration: 0.361s, episode steps: 19, steps per second: 53, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 84.316 [23.000, 221.000], mean observation: 0.145 [0.000, 38.000], loss: 34.875162, mean_absolute_error: 1.074995, mean_q: 0.760631, mean_eps: 0.100000\n",
      " 157811/175000: episode: 4414, duration: 0.374s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 108.857 [11.000, 205.000], mean observation: 0.106 [0.000, 42.000], loss: 45.046331, mean_absolute_error: 1.104734, mean_q: 0.670894, mean_eps: 0.100000\n",
      " 157847/175000: episode: 4415, duration: 0.675s, episode steps: 36, steps per second: 53, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 82.028 [5.000, 217.000], mean observation: 0.334 [0.000, 72.000], loss: 68.033697, mean_absolute_error: 1.204796, mean_q: 0.635753, mean_eps: 0.100000\n",
      " 157882/175000: episode: 4416, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 75.457 [7.000, 217.000], mean observation: 0.353 [0.000, 70.000], loss: 0.507001, mean_absolute_error: 0.906288, mean_q: 0.634173, mean_eps: 0.100000\n",
      " 157922/175000: episode: 4417, duration: 0.719s, episode steps: 40, steps per second: 56, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 117.900 [7.000, 223.000], mean observation: 0.347 [0.000, 80.000], loss: 0.889897, mean_absolute_error: 0.907651, mean_q: 0.615394, mean_eps: 0.100000\n",
      " 157968/175000: episode: 4418, duration: 0.874s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 115.196 [2.000, 178.000], mean observation: 0.484 [0.000, 92.000], loss: 1.408788, mean_absolute_error: 0.901400, mean_q: 0.648481, mean_eps: 0.100000\n",
      " 158022/175000: episode: 4419, duration: 1.038s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 98.074 [23.000, 187.000], mean observation: 0.553 [0.000, 108.000], loss: 2.544821, mean_absolute_error: 0.927722, mean_q: 0.596407, mean_eps: 0.100000\n",
      " 158077/175000: episode: 4420, duration: 1.037s, episode steps: 55, steps per second: 53, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 116.055 [23.000, 203.000], mean observation: 0.564 [0.000, 110.000], loss: 2.832008, mean_absolute_error: 0.975133, mean_q: 0.759692, mean_eps: 0.100000\n",
      " 158122/175000: episode: 4421, duration: 0.823s, episode steps: 45, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 70.511 [10.000, 205.000], mean observation: 0.212 [0.000, 90.000], loss: 2.079955, mean_absolute_error: 0.976084, mean_q: 0.751498, mean_eps: 0.100000\n",
      " 158169/175000: episode: 4422, duration: 0.867s, episode steps: 47, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 57.745 [10.000, 221.000], mean observation: 0.428 [0.000, 94.000], loss: 12.050139, mean_absolute_error: 1.044487, mean_q: 1.034868, mean_eps: 0.100000\n",
      " 158207/175000: episode: 4423, duration: 0.663s, episode steps: 38, steps per second: 57, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 102.947 [10.000, 215.000], mean observation: 0.312 [0.000, 76.000], loss: 2.113171, mean_absolute_error: 1.036151, mean_q: 0.921292, mean_eps: 0.100000\n",
      " 158226/175000: episode: 4424, duration: 0.364s, episode steps: 19, steps per second: 52, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 10.000 [10.000, 10.000], mean observation: 0.046 [0.000, 38.000], loss: 1.845258, mean_absolute_error: 1.059037, mean_q: 0.822325, mean_eps: 0.100000\n",
      " 158267/175000: episode: 4425, duration: 0.742s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.439 [10.000, 213.000], mean observation: 0.368 [0.000, 82.000], loss: 19510.846845, mean_absolute_error: 87.787757, mean_q: 0.883841, mean_eps: 0.100000\n",
      " 158306/175000: episode: 4426, duration: 0.752s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 112.436 [1.000, 197.000], mean observation: 0.332 [0.000, 78.000], loss: 1.037337, mean_absolute_error: 1.066388, mean_q: 0.689354, mean_eps: 0.100000\n",
      " 158333/175000: episode: 4427, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 90.370 [1.000, 205.000], mean observation: 0.235 [0.000, 54.000], loss: 0.758020, mean_absolute_error: 1.043465, mean_q: 0.641262, mean_eps: 0.100000\n",
      " 158369/175000: episode: 4428, duration: 0.699s, episode steps: 36, steps per second: 52, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 122.806 [1.000, 216.000], mean observation: 0.350 [0.000, 72.000], loss: 14.011765, mean_absolute_error: 1.082312, mean_q: 0.622679, mean_eps: 0.100000\n",
      " 158413/175000: episode: 4429, duration: 0.807s, episode steps: 44, steps per second: 55, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 120.023 [1.000, 218.000], mean observation: 0.451 [0.000, 88.000], loss: 0.986947, mean_absolute_error: 1.012795, mean_q: 0.500367, mean_eps: 0.100000\n",
      " 158456/175000: episode: 4430, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 154.000 [1.000, 215.000], mean observation: 0.501 [0.000, 86.000], loss: 0.321673, mean_absolute_error: 1.000821, mean_q: 0.448721, mean_eps: 0.100000\n",
      " 158509/175000: episode: 4431, duration: 0.993s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 151.245 [25.000, 215.000], mean observation: 0.535 [0.000, 106.000], loss: 7.293633, mean_absolute_error: 1.009927, mean_q: 0.488162, mean_eps: 0.100000\n",
      " 158540/175000: episode: 4432, duration: 0.615s, episode steps: 31, steps per second: 50, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 159.774 [25.000, 224.000], mean observation: 0.260 [0.000, 62.000], loss: 0.227709, mean_absolute_error: 0.968682, mean_q: 0.511454, mean_eps: 0.100000\n",
      " 158580/175000: episode: 4433, duration: 0.786s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 147.475 [25.000, 224.000], mean observation: 0.398 [0.000, 80.000], loss: 1813.969302, mean_absolute_error: 9.178086, mean_q: 2.363940, mean_eps: 0.100000\n",
      " 158601/175000: episode: 4434, duration: 0.438s, episode steps: 21, steps per second: 48, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 134.333 [27.000, 204.000], mean observation: 0.156 [0.000, 42.000], loss: 0.225739, mean_absolute_error: 0.952047, mean_q: 0.448854, mean_eps: 0.100000\n",
      " 158637/175000: episode: 4435, duration: 0.651s, episode steps: 36, steps per second: 55, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 68.222 [28.000, 156.000], mean observation: 0.207 [0.000, 72.000], loss: 0.282348, mean_absolute_error: 0.937586, mean_q: 0.500818, mean_eps: 0.100000\n",
      " 158688/175000: episode: 4436, duration: 0.983s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 67.824 [28.000, 202.000], mean observation: 0.261 [0.000, 102.000], loss: 0.431164, mean_absolute_error: 0.914273, mean_q: 0.531495, mean_eps: 0.100000\n",
      " 158723/175000: episode: 4437, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 39.371 [18.000, 135.000], mean observation: 0.125 [0.000, 70.000], loss: 20.856078, mean_absolute_error: 0.985547, mean_q: 0.796466, mean_eps: 0.100000\n",
      " 158755/175000: episode: 4438, duration: 0.597s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 65.438 [28.000, 156.000], mean observation: 0.133 [0.000, 64.000], loss: 0.238850, mean_absolute_error: 0.877421, mean_q: 0.790429, mean_eps: 0.100000\n",
      " 158781/175000: episode: 4439, duration: 0.551s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 46.346 [28.000, 171.000], mean observation: 0.070 [0.000, 52.000], loss: 0.278097, mean_absolute_error: 0.872905, mean_q: 0.800898, mean_eps: 0.100000\n",
      " 158798/175000: episode: 4440, duration: 0.294s, episode steps: 17, steps per second: 58, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 123.059 [28.000, 181.000], mean observation: 0.098 [0.000, 34.000], loss: 0.264641, mean_absolute_error: 0.860105, mean_q: 0.872072, mean_eps: 0.100000\n",
      " 158832/175000: episode: 4441, duration: 0.659s, episode steps: 34, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 156.147 [8.000, 222.000], mean observation: 0.261 [0.000, 68.000], loss: 2.607796, mean_absolute_error: 0.854890, mean_q: 0.750169, mean_eps: 0.100000\n",
      " 158861/175000: episode: 4442, duration: 0.631s, episode steps: 29, steps per second: 46, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 163.448 [72.000, 217.000], mean observation: 0.205 [0.000, 58.000], loss: 0.265230, mean_absolute_error: 0.832402, mean_q: 0.738560, mean_eps: 0.100000\n",
      " 158898/175000: episode: 4443, duration: 0.730s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 121.757 [0.000, 210.000], mean observation: 0.403 [0.000, 74.000], loss: 0.365699, mean_absolute_error: 0.825517, mean_q: 0.750492, mean_eps: 0.100000\n",
      " 158930/175000: episode: 4444, duration: 0.618s, episode steps: 32, steps per second: 52, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 137.625 [1.000, 222.000], mean observation: 0.307 [0.000, 64.000], loss: 0.549979, mean_absolute_error: 0.817252, mean_q: 0.798589, mean_eps: 0.100000\n",
      " 158963/175000: episode: 4445, duration: 0.666s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 112.909 [15.000, 198.000], mean observation: 0.201 [0.000, 66.000], loss: 0.809122, mean_absolute_error: 0.823161, mean_q: 0.914367, mean_eps: 0.100000\n",
      " 158977/175000: episode: 4446, duration: 0.302s, episode steps: 14, steps per second: 46, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 81.357 [18.000, 175.000], mean observation: 0.092 [0.000, 28.000], loss: 0.559364, mean_absolute_error: 0.824523, mean_q: 0.988542, mean_eps: 0.100000\n",
      " 159004/175000: episode: 4447, duration: 0.529s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.148 [47.000, 175.000], mean observation: 0.175 [0.000, 54.000], loss: 0.680695, mean_absolute_error: 0.835237, mean_q: 1.094034, mean_eps: 0.100000\n",
      " 159026/175000: episode: 4448, duration: 0.460s, episode steps: 22, steps per second: 48, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 162.455 [49.000, 220.000], mean observation: 0.146 [0.000, 44.000], loss: 0.248036, mean_absolute_error: 0.832962, mean_q: 0.745200, mean_eps: 0.100000\n",
      " 159048/175000: episode: 4449, duration: 0.429s, episode steps: 22, steps per second: 51, episode reward: 1.000, mean reward: 0.045 [0.000, 1.000], mean action: 139.227 [18.000, 221.000], mean observation: 0.199 [0.000, 43.000], loss: 0.173050, mean_absolute_error: 0.842640, mean_q: 0.636932, mean_eps: 0.100000\n",
      " 159085/175000: episode: 4450, duration: 0.783s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 132.270 [18.000, 220.000], mean observation: 0.294 [0.000, 74.000], loss: 0.289291, mean_absolute_error: 0.840861, mean_q: 0.575392, mean_eps: 0.100000\n",
      " 159120/175000: episode: 4451, duration: 0.736s, episode steps: 35, steps per second: 48, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.457 [8.000, 173.000], mean observation: 0.336 [0.000, 70.000], loss: 6.154920, mean_absolute_error: 0.851014, mean_q: 0.632351, mean_eps: 0.100000\n",
      " 159177/175000: episode: 4452, duration: 1.118s, episode steps: 57, steps per second: 51, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 125.842 [8.000, 224.000], mean observation: 0.590 [0.000, 114.000], loss: 2.200739, mean_absolute_error: 0.824098, mean_q: 0.675818, mean_eps: 0.100000\n",
      " 159195/175000: episode: 4453, duration: 0.309s, episode steps: 18, steps per second: 58, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 133.222 [18.000, 187.000], mean observation: 0.131 [0.000, 36.000], loss: 0.252139, mean_absolute_error: 0.811304, mean_q: 0.791193, mean_eps: 0.100000\n",
      " 159249/175000: episode: 4454, duration: 1.037s, episode steps: 54, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 115.815 [8.000, 173.000], mean observation: 0.471 [0.000, 108.000], loss: 0.617143, mean_absolute_error: 0.803406, mean_q: 0.729030, mean_eps: 0.100000\n",
      " 159282/175000: episode: 4455, duration: 0.592s, episode steps: 33, steps per second: 56, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 117.121 [8.000, 221.000], mean observation: 0.348 [0.000, 66.000], loss: 1.231675, mean_absolute_error: 0.799285, mean_q: 0.516191, mean_eps: 0.100000\n",
      " 159321/175000: episode: 4456, duration: 0.749s, episode steps: 39, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 117.513 [8.000, 202.000], mean observation: 0.408 [0.000, 78.000], loss: 0.725797, mean_absolute_error: 0.796183, mean_q: 0.541936, mean_eps: 0.100000\n",
      " 159360/175000: episode: 4457, duration: 0.695s, episode steps: 39, steps per second: 56, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.923 [3.000, 201.000], mean observation: 0.505 [0.000, 78.000], loss: 0.666199, mean_absolute_error: 0.796390, mean_q: 0.444429, mean_eps: 0.100000\n",
      " 159397/175000: episode: 4458, duration: 0.729s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 131.405 [8.000, 221.000], mean observation: 0.537 [0.000, 74.000], loss: 19.593638, mean_absolute_error: 0.882329, mean_q: 0.579000, mean_eps: 0.100000\n",
      " 159438/175000: episode: 4459, duration: 0.741s, episode steps: 41, steps per second: 55, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 83.366 [12.000, 193.000], mean observation: 0.474 [0.000, 82.000], loss: 0.597091, mean_absolute_error: 0.799160, mean_q: 0.587809, mean_eps: 0.100000\n",
      " 159485/175000: episode: 4460, duration: 0.904s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 71.915 [16.000, 193.000], mean observation: 0.529 [0.000, 94.000], loss: 0.523569, mean_absolute_error: 0.783199, mean_q: 0.552140, mean_eps: 0.100000\n",
      " 159515/175000: episode: 4461, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 93.467 [33.000, 194.000], mean observation: 0.284 [0.000, 60.000], loss: 1.482324, mean_absolute_error: 0.790877, mean_q: 0.743382, mean_eps: 0.100000\n",
      " 159539/175000: episode: 4462, duration: 0.472s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.750 [16.000, 223.000], mean observation: 0.133 [0.000, 48.000], loss: 1.147396, mean_absolute_error: 0.775711, mean_q: 0.684436, mean_eps: 0.100000\n",
      " 159575/175000: episode: 4463, duration: 0.661s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 109.139 [23.000, 198.000], mean observation: 0.223 [0.000, 72.000], loss: 4.597632, mean_absolute_error: 0.810063, mean_q: 0.789627, mean_eps: 0.100000\n",
      " 159607/175000: episode: 4464, duration: 0.649s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 122.031 [18.000, 221.000], mean observation: 0.306 [0.000, 64.000], loss: 0.458634, mean_absolute_error: 0.812792, mean_q: 0.796931, mean_eps: 0.100000\n",
      " 159649/175000: episode: 4465, duration: 0.786s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 175.476 [8.000, 221.000], mean observation: 0.509 [0.000, 84.000], loss: 1362.324781, mean_absolute_error: 7.019283, mean_q: 2.379839, mean_eps: 0.100000\n",
      " 159703/175000: episode: 4466, duration: 0.950s, episode steps: 54, steps per second: 57, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 78.278 [17.000, 221.000], mean observation: 0.717 [0.000, 108.000], loss: 0.267660, mean_absolute_error: 0.831983, mean_q: 0.657748, mean_eps: 0.100000\n",
      " 159738/175000: episode: 4467, duration: 0.669s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 98.086 [16.000, 202.000], mean observation: 0.315 [0.000, 70.000], loss: 0.191303, mean_absolute_error: 0.818176, mean_q: 0.572911, mean_eps: 0.100000\n",
      " 159781/175000: episode: 4468, duration: 0.812s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 66.140 [18.000, 188.000], mean observation: 0.438 [0.000, 86.000], loss: 0.204815, mean_absolute_error: 0.818495, mean_q: 0.629743, mean_eps: 0.100000\n",
      " 159811/175000: episode: 4469, duration: 0.513s, episode steps: 30, steps per second: 59, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 90.333 [18.000, 202.000], mean observation: 0.263 [0.000, 60.000], loss: 2723.421541, mean_absolute_error: 12.917370, mean_q: 0.632701, mean_eps: 0.100000\n",
      " 159847/175000: episode: 4470, duration: 0.666s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 94.167 [16.000, 222.000], mean observation: 0.354 [0.000, 72.000], loss: 0.185287, mean_absolute_error: 0.806788, mean_q: 0.584612, mean_eps: 0.100000\n",
      " 159895/175000: episode: 4471, duration: 0.888s, episode steps: 48, steps per second: 54, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 75.375 [16.000, 202.000], mean observation: 0.525 [0.000, 96.000], loss: 0.213234, mean_absolute_error: 0.793878, mean_q: 0.598230, mean_eps: 0.100000\n",
      " 159933/175000: episode: 4472, duration: 0.729s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 75.632 [16.000, 197.000], mean observation: 0.380 [0.000, 76.000], loss: 0.207123, mean_absolute_error: 0.783487, mean_q: 0.641227, mean_eps: 0.100000\n",
      " 159967/175000: episode: 4473, duration: 0.586s, episode steps: 34, steps per second: 58, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 100.618 [16.000, 207.000], mean observation: 0.230 [0.000, 68.000], loss: 0.195442, mean_absolute_error: 0.780811, mean_q: 0.592509, mean_eps: 0.100000\n",
      " 160011/175000: episode: 4474, duration: 0.872s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 76.114 [16.000, 204.000], mean observation: 0.459 [0.000, 88.000], loss: 0.305562, mean_absolute_error: 0.778540, mean_q: 0.566426, mean_eps: 0.100000\n",
      " 160040/175000: episode: 4475, duration: 0.576s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 95.862 [7.000, 185.000], mean observation: 0.222 [0.000, 58.000], loss: 0.941289, mean_absolute_error: 0.765279, mean_q: 0.610215, mean_eps: 0.100000\n",
      " 160079/175000: episode: 4476, duration: 0.731s, episode steps: 39, steps per second: 53, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 81.795 [7.000, 176.000], mean observation: 0.318 [0.000, 78.000], loss: 57.734358, mean_absolute_error: 1.000526, mean_q: 0.664251, mean_eps: 0.100000\n",
      " 160125/175000: episode: 4477, duration: 1.018s, episode steps: 46, steps per second: 45, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 90.391 [7.000, 176.000], mean observation: 0.304 [0.000, 92.000], loss: 24.312209, mean_absolute_error: 0.835884, mean_q: 0.820063, mean_eps: 0.100000\n",
      " 160158/175000: episode: 4478, duration: 0.750s, episode steps: 33, steps per second: 44, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 95.818 [7.000, 185.000], mean observation: 0.194 [0.000, 66.000], loss: 0.226063, mean_absolute_error: 0.717352, mean_q: 0.804376, mean_eps: 0.100000\n",
      " 160189/175000: episode: 4479, duration: 0.740s, episode steps: 31, steps per second: 42, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 95.710 [7.000, 207.000], mean observation: 0.129 [0.000, 62.000], loss: 0.199464, mean_absolute_error: 0.708751, mean_q: 0.784325, mean_eps: 0.100000\n",
      " 160228/175000: episode: 4480, duration: 0.834s, episode steps: 39, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 105.974 [27.000, 197.000], mean observation: 0.215 [0.000, 78.000], loss: 3310.084886, mean_absolute_error: 15.592733, mean_q: 2.946403, mean_eps: 0.100000\n",
      " 160250/175000: episode: 4481, duration: 0.530s, episode steps: 22, steps per second: 41, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 126.773 [35.000, 183.000], mean observation: 0.147 [0.000, 44.000], loss: 0.271138, mean_absolute_error: 0.704507, mean_q: 0.989054, mean_eps: 0.100000\n",
      " 160289/175000: episode: 4482, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.692 [17.000, 182.000], mean observation: 0.361 [0.000, 78.000], loss: 1.564236, mean_absolute_error: 0.709384, mean_q: 0.864165, mean_eps: 0.100000\n",
      " 160320/175000: episode: 4483, duration: 0.646s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 118.935 [12.000, 194.000], mean observation: 0.329 [0.000, 62.000], loss: 0.215369, mean_absolute_error: 0.707913, mean_q: 0.853614, mean_eps: 0.100000\n",
      " 160356/175000: episode: 4484, duration: 0.740s, episode steps: 36, steps per second: 49, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 79.944 [4.000, 207.000], mean observation: 0.291 [0.000, 72.000], loss: 3.874550, mean_absolute_error: 0.714588, mean_q: 0.763599, mean_eps: 0.100000\n",
      " 160403/175000: episode: 4485, duration: 0.948s, episode steps: 47, steps per second: 50, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 138.660 [112.000, 176.000], mean observation: 0.194 [0.000, 94.000], loss: 53.533041, mean_absolute_error: 0.950338, mean_q: 0.764013, mean_eps: 0.100000\n",
      " 160439/175000: episode: 4486, duration: 0.719s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 117.972 [35.000, 208.000], mean observation: 0.294 [0.000, 72.000], loss: 0.742795, mean_absolute_error: 0.692841, mean_q: 0.688364, mean_eps: 0.100000\n",
      " 160474/175000: episode: 4487, duration: 0.745s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 112.314 [1.000, 193.000], mean observation: 0.209 [0.000, 70.000], loss: 2.009996, mean_absolute_error: 0.688591, mean_q: 0.866036, mean_eps: 0.100000\n",
      " 160537/175000: episode: 4488, duration: 1.411s, episode steps: 63, steps per second: 45, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 147.810 [10.000, 204.000], mean observation: 0.696 [0.000, 126.000], loss: 72.430798, mean_absolute_error: 1.001232, mean_q: 0.774140, mean_eps: 0.100000\n",
      " 160572/175000: episode: 4489, duration: 0.678s, episode steps: 35, steps per second: 52, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 146.229 [72.000, 221.000], mean observation: 0.440 [0.000, 70.000], loss: 1.681873, mean_absolute_error: 0.687471, mean_q: 0.584937, mean_eps: 0.100000\n",
      " 160619/175000: episode: 4490, duration: 1.023s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.340 [55.000, 212.000], mean observation: 0.583 [0.000, 94.000], loss: 3.498872, mean_absolute_error: 0.697913, mean_q: 0.600061, mean_eps: 0.100000\n",
      " 160645/175000: episode: 4491, duration: 0.547s, episode steps: 26, steps per second: 48, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 176.846 [71.000, 211.000], mean observation: 0.135 [0.000, 52.000], loss: 10.525895, mean_absolute_error: 0.729803, mean_q: 0.669775, mean_eps: 0.100000\n",
      " 160678/175000: episode: 4492, duration: 0.626s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 153.727 [72.000, 211.000], mean observation: 0.268 [0.000, 66.000], loss: 0.350284, mean_absolute_error: 0.681711, mean_q: 0.637188, mean_eps: 0.100000\n",
      " 160716/175000: episode: 4493, duration: 0.772s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.605 [18.000, 222.000], mean observation: 0.386 [0.000, 76.000], loss: 2.964484, mean_absolute_error: 0.704555, mean_q: 0.772191, mean_eps: 0.100000\n",
      " 160740/175000: episode: 4494, duration: 0.518s, episode steps: 24, steps per second: 46, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 106.917 [16.000, 211.000], mean observation: 0.180 [0.000, 48.000], loss: 1.688892, mean_absolute_error: 0.701649, mean_q: 0.781506, mean_eps: 0.100000\n",
      " 160763/175000: episode: 4495, duration: 0.464s, episode steps: 23, steps per second: 50, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 180.957 [15.000, 211.000], mean observation: 0.141 [0.000, 46.000], loss: 9.085377, mean_absolute_error: 0.731773, mean_q: 0.751468, mean_eps: 0.100000\n",
      " 160793/175000: episode: 4496, duration: 0.676s, episode steps: 30, steps per second: 44, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 175.200 [72.000, 211.000], mean observation: 0.147 [0.000, 60.000], loss: 14.274119, mean_absolute_error: 0.755396, mean_q: 0.721502, mean_eps: 0.100000\n",
      " 160847/175000: episode: 4497, duration: 1.086s, episode steps: 54, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 148.537 [15.000, 211.000], mean observation: 0.670 [0.000, 108.000], loss: 0.574130, mean_absolute_error: 0.682387, mean_q: 0.966742, mean_eps: 0.100000\n",
      " 160900/175000: episode: 4498, duration: 1.009s, episode steps: 53, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 135.698 [46.000, 192.000], mean observation: 0.579 [0.000, 106.000], loss: 0.814683, mean_absolute_error: 0.694590, mean_q: 0.941383, mean_eps: 0.100000\n",
      " 160936/175000: episode: 4499, duration: 0.727s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 106.861 [31.000, 183.000], mean observation: 0.312 [0.000, 72.000], loss: 4.957342, mean_absolute_error: 0.731761, mean_q: 0.671126, mean_eps: 0.100000\n",
      " 160976/175000: episode: 4500, duration: 0.824s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 102.750 [16.000, 192.000], mean observation: 0.386 [0.000, 80.000], loss: 0.531388, mean_absolute_error: 0.712007, mean_q: 0.661978, mean_eps: 0.100000\n",
      " 161013/175000: episode: 4501, duration: 0.820s, episode steps: 37, steps per second: 45, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 83.730 [1.000, 183.000], mean observation: 0.349 [0.000, 74.000], loss: 3.192657, mean_absolute_error: 0.738922, mean_q: 0.585500, mean_eps: 0.100000\n",
      " 161060/175000: episode: 4502, duration: 1.021s, episode steps: 47, steps per second: 46, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 85.447 [1.000, 186.000], mean observation: 0.414 [0.000, 94.000], loss: 0.178245, mean_absolute_error: 0.744024, mean_q: 0.607278, mean_eps: 0.100000\n",
      " 161097/175000: episode: 4503, duration: 0.802s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 62.703 [22.000, 212.000], mean observation: 0.370 [0.000, 74.000], loss: 0.564416, mean_absolute_error: 0.746981, mean_q: 0.533592, mean_eps: 0.100000\n",
      " 161123/175000: episode: 4504, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 71.846 [5.000, 193.000], mean observation: 0.226 [0.000, 52.000], loss: 212.370876, mean_absolute_error: 1.927954, mean_q: 3.548816, mean_eps: 0.100000\n",
      " 161151/175000: episode: 4505, duration: 0.542s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 83.714 [22.000, 183.000], mean observation: 0.228 [0.000, 56.000], loss: 0.140227, mean_absolute_error: 0.742464, mean_q: 0.664459, mean_eps: 0.100000\n",
      " 161202/175000: episode: 4506, duration: 1.089s, episode steps: 51, steps per second: 47, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 66.098 [22.000, 183.000], mean observation: 0.587 [0.000, 102.000], loss: 0.784768, mean_absolute_error: 0.735966, mean_q: 0.835290, mean_eps: 0.100000\n",
      " 161240/175000: episode: 4507, duration: 0.777s, episode steps: 38, steps per second: 49, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 104.711 [16.000, 198.000], mean observation: 0.283 [0.000, 76.000], loss: 0.124316, mean_absolute_error: 0.724664, mean_q: 0.661413, mean_eps: 0.100000\n",
      " 161256/175000: episode: 4508, duration: 0.410s, episode steps: 16, steps per second: 39, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 90.750 [44.000, 182.000], mean observation: 0.100 [0.000, 32.000], loss: 7447.171977, mean_absolute_error: 34.217673, mean_q: 5.435029, mean_eps: 0.100000\n",
      " 161297/175000: episode: 4509, duration: 0.821s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 112.537 [16.000, 211.000], mean observation: 0.302 [0.000, 82.000], loss: 2221.802800, mean_absolute_error: 10.734113, mean_q: 2.356271, mean_eps: 0.100000\n",
      " 161359/175000: episode: 4510, duration: 1.200s, episode steps: 62, steps per second: 52, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 111.161 [2.000, 222.000], mean observation: 0.816 [0.000, 124.000], loss: 1.870825, mean_absolute_error: 0.721473, mean_q: 0.798911, mean_eps: 0.100000\n",
      " 161403/175000: episode: 4511, duration: 0.861s, episode steps: 44, steps per second: 51, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 109.977 [42.000, 222.000], mean observation: 0.452 [0.000, 88.000], loss: 8.520330, mean_absolute_error: 0.853900, mean_q: 2.279390, mean_eps: 0.100000\n",
      " 161436/175000: episode: 4512, duration: 0.659s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 132.818 [42.000, 222.000], mean observation: 0.283 [0.000, 66.000], loss: 0.624391, mean_absolute_error: 0.710772, mean_q: 0.914364, mean_eps: 0.100000\n",
      " 161463/175000: episode: 4513, duration: 0.565s, episode steps: 27, steps per second: 48, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 97.889 [1.000, 222.000], mean observation: 0.343 [0.000, 54.000], loss: 22.674808, mean_absolute_error: 0.811363, mean_q: 0.847032, mean_eps: 0.100000\n",
      " 161507/175000: episode: 4514, duration: 0.905s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 83.205 [1.000, 222.000], mean observation: 0.568 [0.000, 88.000], loss: 0.497175, mean_absolute_error: 0.718587, mean_q: 0.872304, mean_eps: 0.100000\n",
      " 161536/175000: episode: 4515, duration: 0.605s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 33.759 [1.000, 187.000], mean observation: 0.097 [0.000, 58.000], loss: 1.873512, mean_absolute_error: 0.729625, mean_q: 0.800202, mean_eps: 0.100000\n",
      " 161572/175000: episode: 4516, duration: 0.817s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 78.750 [1.000, 209.000], mean observation: 0.208 [0.000, 72.000], loss: 0.693714, mean_absolute_error: 0.730449, mean_q: 0.735184, mean_eps: 0.100000\n",
      " 161602/175000: episode: 4517, duration: 0.570s, episode steps: 30, steps per second: 53, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 115.467 [1.000, 215.000], mean observation: 0.223 [0.000, 60.000], loss: 0.258821, mean_absolute_error: 0.727869, mean_q: 0.737756, mean_eps: 0.100000\n",
      " 161635/175000: episode: 4518, duration: 0.635s, episode steps: 33, steps per second: 52, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 125.030 [45.000, 209.000], mean observation: 0.239 [0.000, 66.000], loss: 0.171085, mean_absolute_error: 0.715868, mean_q: 0.560123, mean_eps: 0.100000\n",
      " 161675/175000: episode: 4519, duration: 0.791s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 134.675 [24.000, 209.000], mean observation: 0.449 [0.000, 80.000], loss: 479.676388, mean_absolute_error: 2.862580, mean_q: 0.815560, mean_eps: 0.100000\n",
      " 161719/175000: episode: 4520, duration: 0.913s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 101.364 [10.000, 200.000], mean observation: 0.503 [0.000, 88.000], loss: 0.748494, mean_absolute_error: 0.715148, mean_q: 0.604034, mean_eps: 0.100000\n",
      " 161759/175000: episode: 4521, duration: 0.783s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 124.450 [11.000, 201.000], mean observation: 0.456 [0.000, 80.000], loss: 41.189001, mean_absolute_error: 0.892764, mean_q: 0.593899, mean_eps: 0.100000\n",
      " 161782/175000: episode: 4522, duration: 0.479s, episode steps: 23, steps per second: 48, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 109.261 [23.000, 223.000], mean observation: 0.164 [0.000, 46.000], loss: 0.175672, mean_absolute_error: 0.709238, mean_q: 0.644703, mean_eps: 0.100000\n",
      " 161814/175000: episode: 4523, duration: 0.609s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 101.781 [19.000, 218.000], mean observation: 0.279 [0.000, 64.000], loss: 1260.476745, mean_absolute_error: 6.501779, mean_q: 2.884694, mean_eps: 0.100000\n",
      " 161862/175000: episode: 4524, duration: 0.877s, episode steps: 48, steps per second: 55, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 106.083 [44.000, 194.000], mean observation: 0.257 [0.000, 96.000], loss: 0.875092, mean_absolute_error: 0.720504, mean_q: 0.650686, mean_eps: 0.100000\n",
      " 161907/175000: episode: 4525, duration: 0.892s, episode steps: 45, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 96.311 [20.000, 224.000], mean observation: 0.568 [0.000, 90.000], loss: 0.140285, mean_absolute_error: 0.704438, mean_q: 0.648415, mean_eps: 0.100000\n",
      " 161951/175000: episode: 4526, duration: 0.903s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 107.977 [58.000, 157.000], mean observation: 0.295 [0.000, 88.000], loss: 69.151225, mean_absolute_error: 1.013653, mean_q: 0.612680, mean_eps: 0.100000\n",
      " 161993/175000: episode: 4527, duration: 0.863s, episode steps: 42, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 116.024 [5.000, 144.000], mean observation: 0.275 [0.000, 84.000], loss: 4.694566, mean_absolute_error: 0.735813, mean_q: 0.740412, mean_eps: 0.100000\n",
      " 162042/175000: episode: 4528, duration: 0.927s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 107.020 [35.000, 205.000], mean observation: 0.337 [0.000, 98.000], loss: 12.205746, mean_absolute_error: 0.758073, mean_q: 0.698933, mean_eps: 0.100000\n",
      " 162067/175000: episode: 4529, duration: 0.522s, episode steps: 25, steps per second: 48, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 103.760 [12.000, 223.000], mean observation: 0.179 [0.000, 50.000], loss: 0.062974, mean_absolute_error: 0.714733, mean_q: 0.826717, mean_eps: 0.100000\n",
      " 162092/175000: episode: 4530, duration: 0.543s, episode steps: 25, steps per second: 46, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 84.600 [7.000, 186.000], mean observation: 0.125 [0.000, 50.000], loss: 6.056135, mean_absolute_error: 0.740395, mean_q: 0.772046, mean_eps: 0.100000\n",
      " 162134/175000: episode: 4531, duration: 0.823s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.595 [23.000, 200.000], mean observation: 0.358 [0.000, 84.000], loss: 0.210405, mean_absolute_error: 0.713867, mean_q: 0.877789, mean_eps: 0.100000\n",
      " 162175/175000: episode: 4532, duration: 0.793s, episode steps: 41, steps per second: 52, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 94.561 [31.000, 189.000], mean observation: 0.299 [0.000, 82.000], loss: 4.134443, mean_absolute_error: 0.722649, mean_q: 0.911547, mean_eps: 0.100000\n",
      " 162220/175000: episode: 4533, duration: 0.865s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 121.511 [0.000, 202.000], mean observation: 0.563 [0.000, 90.000], loss: 2083.475996, mean_absolute_error: 10.090522, mean_q: 2.541528, mean_eps: 0.100000\n",
      " 162247/175000: episode: 4534, duration: 0.501s, episode steps: 27, steps per second: 54, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 155.556 [59.000, 221.000], mean observation: 0.147 [0.000, 54.000], loss: 0.219129, mean_absolute_error: 0.676410, mean_q: 0.895127, mean_eps: 0.100000\n",
      " 162288/175000: episode: 4535, duration: 0.844s, episode steps: 41, steps per second: 49, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 56.537 [3.000, 163.000], mean observation: 0.234 [0.000, 82.000], loss: 1.112450, mean_absolute_error: 0.675003, mean_q: 0.870970, mean_eps: 0.100000\n",
      " 162339/175000: episode: 4536, duration: 0.995s, episode steps: 51, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 142.941 [45.000, 218.000], mean observation: 0.475 [0.000, 102.000], loss: 234.318013, mean_absolute_error: 1.756567, mean_q: 1.132124, mean_eps: 0.100000\n",
      " 162376/175000: episode: 4537, duration: 0.762s, episode steps: 37, steps per second: 49, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 61.649 [45.000, 219.000], mean observation: 0.144 [0.000, 74.000], loss: 7.288433, mean_absolute_error: 0.729742, mean_q: 0.959198, mean_eps: 0.100000\n",
      " 162429/175000: episode: 4538, duration: 1.097s, episode steps: 53, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.887 [45.000, 218.000], mean observation: 0.523 [0.000, 106.000], loss: 49.110196, mean_absolute_error: 0.921347, mean_q: 0.958288, mean_eps: 0.100000\n",
      " 162482/175000: episode: 4539, duration: 0.966s, episode steps: 53, steps per second: 55, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 36.113 [1.000, 174.000], mean observation: 0.573 [0.000, 106.000], loss: 775.802322, mean_absolute_error: 4.272430, mean_q: 2.420151, mean_eps: 0.100000\n",
      " 162541/175000: episode: 4540, duration: 1.189s, episode steps: 59, steps per second: 50, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 90.508 [10.000, 191.000], mean observation: 0.601 [0.000, 118.000], loss: 0.129224, mean_absolute_error: 0.707270, mean_q: 0.714865, mean_eps: 0.100000\n",
      " 162586/175000: episode: 4541, duration: 0.917s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 81.356 [1.000, 199.000], mean observation: 0.419 [0.000, 90.000], loss: 5.179356, mean_absolute_error: 0.861574, mean_q: 2.470367, mean_eps: 0.100000\n",
      " 162641/175000: episode: 4542, duration: 1.007s, episode steps: 55, steps per second: 55, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 101.327 [8.000, 222.000], mean observation: 0.518 [0.000, 110.000], loss: 0.284529, mean_absolute_error: 0.720459, mean_q: 0.861890, mean_eps: 0.100000\n",
      " 162678/175000: episode: 4543, duration: 0.745s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.108 [16.000, 120.000], mean observation: 0.248 [0.000, 74.000], loss: 6.097447, mean_absolute_error: 0.738716, mean_q: 0.775912, mean_eps: 0.100000\n",
      " 162712/175000: episode: 4544, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 111.647 [1.000, 203.000], mean observation: 0.256 [0.000, 68.000], loss: 4.327763, mean_absolute_error: 0.729762, mean_q: 0.762168, mean_eps: 0.100000\n",
      " 162756/175000: episode: 4545, duration: 1.152s, episode steps: 44, steps per second: 38, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 57.682 [1.000, 207.000], mean observation: 0.251 [0.000, 88.000], loss: 0.175067, mean_absolute_error: 0.708621, mean_q: 0.709183, mean_eps: 0.100000\n",
      " 162800/175000: episode: 4546, duration: 1.094s, episode steps: 44, steps per second: 40, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 35.182 [1.000, 206.000], mean observation: 0.224 [0.000, 88.000], loss: 0.121713, mean_absolute_error: 0.722270, mean_q: 0.664048, mean_eps: 0.100000\n",
      " 162832/175000: episode: 4547, duration: 0.783s, episode steps: 32, steps per second: 41, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 54.281 [1.000, 174.000], mean observation: 0.209 [0.000, 64.000], loss: 0.616922, mean_absolute_error: 0.736407, mean_q: 0.774341, mean_eps: 0.100000\n",
      " 162859/175000: episode: 4548, duration: 0.688s, episode steps: 27, steps per second: 39, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 8.370 [1.000, 195.000], mean observation: 0.087 [0.000, 54.000], loss: 4.612042, mean_absolute_error: 0.754927, mean_q: 0.927414, mean_eps: 0.100000\n",
      " 162891/175000: episode: 4549, duration: 0.684s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 8.875 [1.000, 193.000], mean observation: 0.119 [0.000, 64.000], loss: 2.041390, mean_absolute_error: 0.739243, mean_q: 0.639106, mean_eps: 0.100000\n",
      " 162946/175000: episode: 4550, duration: 1.094s, episode steps: 55, steps per second: 50, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 66.418 [1.000, 193.000], mean observation: 0.292 [0.000, 110.000], loss: 1894.799719, mean_absolute_error: 9.266745, mean_q: 2.163937, mean_eps: 0.100000\n",
      " 162988/175000: episode: 4551, duration: 0.829s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 7.000 [1.000, 155.000], mean observation: 0.126 [0.000, 84.000], loss: 1.583206, mean_absolute_error: 0.716261, mean_q: 0.745658, mean_eps: 0.100000\n",
      " 163032/175000: episode: 4552, duration: 0.955s, episode steps: 44, steps per second: 46, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 36.682 [1.000, 204.000], mean observation: 0.308 [0.000, 88.000], loss: 3.670871, mean_absolute_error: 0.742392, mean_q: 1.020449, mean_eps: 0.100000\n",
      " 163069/175000: episode: 4553, duration: 0.794s, episode steps: 37, steps per second: 47, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 102.270 [1.000, 204.000], mean observation: 0.322 [0.000, 74.000], loss: 0.473005, mean_absolute_error: 0.732236, mean_q: 0.911011, mean_eps: 0.100000\n",
      " 163092/175000: episode: 4554, duration: 0.485s, episode steps: 23, steps per second: 47, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 24.391 [1.000, 135.000], mean observation: 0.094 [0.000, 46.000], loss: 0.185952, mean_absolute_error: 0.741232, mean_q: 0.800780, mean_eps: 0.100000\n",
      " 163122/175000: episode: 4555, duration: 0.594s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 64.767 [1.000, 205.000], mean observation: 0.288 [0.000, 60.000], loss: 39.344238, mean_absolute_error: 0.923730, mean_q: 0.680450, mean_eps: 0.100000\n",
      " 163176/175000: episode: 4556, duration: 1.118s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 55.500 [1.000, 224.000], mean observation: 0.552 [0.000, 108.000], loss: 2291.569764, mean_absolute_error: 11.069019, mean_q: 2.323100, mean_eps: 0.100000\n",
      " 163203/175000: episode: 4557, duration: 0.549s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 127.815 [1.000, 186.000], mean observation: 0.243 [0.000, 54.000], loss: 2.024362, mean_absolute_error: 0.745155, mean_q: 0.865639, mean_eps: 0.100000\n",
      " 163245/175000: episode: 4558, duration: 0.879s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 63.286 [1.000, 221.000], mean observation: 0.217 [0.000, 84.000], loss: 0.700293, mean_absolute_error: 0.894581, mean_q: 2.502132, mean_eps: 0.100000\n",
      " 163283/175000: episode: 4559, duration: 0.730s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.711 [1.000, 212.000], mean observation: 0.236 [0.000, 76.000], loss: 5.125277, mean_absolute_error: 0.771256, mean_q: 0.651325, mean_eps: 0.100000\n",
      " 163311/175000: episode: 4560, duration: 0.593s, episode steps: 28, steps per second: 47, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 14.964 [1.000, 199.000], mean observation: 0.068 [0.000, 56.000], loss: 3.132455, mean_absolute_error: 0.772962, mean_q: 0.689140, mean_eps: 0.100000\n",
      " 163349/175000: episode: 4561, duration: 0.822s, episode steps: 38, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.526 [1.000, 180.000], mean observation: 0.194 [0.000, 76.000], loss: 3535.547487, mean_absolute_error: 16.629386, mean_q: 2.841755, mean_eps: 0.100000\n",
      " 163380/175000: episode: 4562, duration: 0.728s, episode steps: 31, steps per second: 43, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 48.548 [1.000, 218.000], mean observation: 0.181 [0.000, 62.000], loss: 0.135630, mean_absolute_error: 0.758619, mean_q: 1.031708, mean_eps: 0.100000\n",
      " 163413/175000: episode: 4563, duration: 0.784s, episode steps: 33, steps per second: 42, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 73.061 [1.000, 218.000], mean observation: 0.197 [0.000, 66.000], loss: 0.334461, mean_absolute_error: 0.756194, mean_q: 0.973222, mean_eps: 0.100000\n",
      " 163462/175000: episode: 4564, duration: 1.067s, episode steps: 49, steps per second: 46, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 101.163 [1.000, 218.000], mean observation: 0.532 [0.000, 98.000], loss: 0.502109, mean_absolute_error: 0.763518, mean_q: 1.034160, mean_eps: 0.100000\n",
      " 163499/175000: episode: 4565, duration: 0.800s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 34.432 [1.000, 152.000], mean observation: 0.133 [0.000, 74.000], loss: 45.415568, mean_absolute_error: 0.954370, mean_q: 0.918725, mean_eps: 0.100000\n",
      " 163551/175000: episode: 4566, duration: 1.064s, episode steps: 52, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 130.500 [1.000, 195.000], mean observation: 0.580 [0.000, 104.000], loss: 0.133216, mean_absolute_error: 0.738330, mean_q: 0.862893, mean_eps: 0.100000\n",
      " 163578/175000: episode: 4567, duration: 0.583s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 47.259 [1.000, 222.000], mean observation: 0.104 [0.000, 54.000], loss: 0.421629, mean_absolute_error: 0.748662, mean_q: 0.828954, mean_eps: 0.100000\n",
      " 163587/175000: episode: 4568, duration: 0.162s, episode steps: 9, steps per second: 55, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.024 [0.000, 18.000], loss: 0.083645, mean_absolute_error: 0.747145, mean_q: 0.828689, mean_eps: 0.100000\n",
      " 163617/175000: episode: 4569, duration: 0.599s, episode steps: 30, steps per second: 50, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 52.500 [1.000, 182.000], mean observation: 0.209 [0.000, 60.000], loss: 46.928030, mean_absolute_error: 1.154382, mean_q: 3.171126, mean_eps: 0.100000\n",
      " 163649/175000: episode: 4570, duration: 0.687s, episode steps: 32, steps per second: 47, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 65.281 [1.000, 175.000], mean observation: 0.323 [0.000, 64.000], loss: 0.291638, mean_absolute_error: 0.744891, mean_q: 0.882766, mean_eps: 0.100000\n",
      " 163695/175000: episode: 4571, duration: 0.922s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.022 [1.000, 216.000], mean observation: 0.478 [0.000, 92.000], loss: 4.307445, mean_absolute_error: 0.750846, mean_q: 0.719523, mean_eps: 0.100000\n",
      " 163732/175000: episode: 4572, duration: 0.809s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 51.514 [1.000, 208.000], mean observation: 0.416 [0.000, 74.000], loss: 13.028034, mean_absolute_error: 0.960264, mean_q: 2.822817, mean_eps: 0.100000\n",
      " 163761/175000: episode: 4573, duration: 0.609s, episode steps: 29, steps per second: 48, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 74.414 [1.000, 224.000], mean observation: 0.287 [0.000, 58.000], loss: 104.064462, mean_absolute_error: 1.202943, mean_q: 0.987113, mean_eps: 0.100000\n",
      " 163809/175000: episode: 4574, duration: 0.924s, episode steps: 48, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 28.167 [1.000, 150.000], mean observation: 0.229 [0.000, 96.000], loss: 12.243655, mean_absolute_error: 0.773660, mean_q: 0.784213, mean_eps: 0.100000\n",
      " 163849/175000: episode: 4575, duration: 0.823s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 59.250 [1.000, 212.000], mean observation: 0.314 [0.000, 80.000], loss: 11.611203, mean_absolute_error: 0.805666, mean_q: 1.032455, mean_eps: 0.100000\n",
      " 163882/175000: episode: 4576, duration: 0.657s, episode steps: 33, steps per second: 50, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 35.576 [1.000, 183.000], mean observation: 0.276 [0.000, 66.000], loss: 6.426734, mean_absolute_error: 0.783114, mean_q: 1.098162, mean_eps: 0.100000\n",
      " 163915/175000: episode: 4577, duration: 0.627s, episode steps: 33, steps per second: 53, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 110.576 [1.000, 219.000], mean observation: 0.253 [0.000, 66.000], loss: 18.488790, mean_absolute_error: 0.814184, mean_q: 0.976273, mean_eps: 0.100000\n",
      " 163969/175000: episode: 4578, duration: 1.123s, episode steps: 54, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 123.056 [1.000, 213.000], mean observation: 0.330 [0.000, 108.000], loss: 0.240136, mean_absolute_error: 0.740195, mean_q: 0.976940, mean_eps: 0.100000\n",
      " 164002/175000: episode: 4579, duration: 0.609s, episode steps: 33, steps per second: 54, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 128.788 [59.000, 188.000], mean observation: 0.202 [0.000, 66.000], loss: 2.205070, mean_absolute_error: 0.727621, mean_q: 0.768197, mean_eps: 0.100000\n",
      " 164031/175000: episode: 4580, duration: 0.545s, episode steps: 29, steps per second: 53, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 111.759 [59.000, 215.000], mean observation: 0.244 [0.000, 58.000], loss: 3.581305, mean_absolute_error: 0.771831, mean_q: 1.217014, mean_eps: 0.100000\n",
      " 164062/175000: episode: 4581, duration: 0.647s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 118.935 [5.000, 176.000], mean observation: 0.217 [0.000, 62.000], loss: 21.721376, mean_absolute_error: 0.826280, mean_q: 0.973588, mean_eps: 0.100000\n",
      " 164089/175000: episode: 4582, duration: 0.534s, episode steps: 27, steps per second: 51, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.185 [42.000, 176.000], mean observation: 0.184 [0.000, 54.000], loss: 16.224229, mean_absolute_error: 0.798885, mean_q: 0.975111, mean_eps: 0.100000\n",
      " 164100/175000: episode: 4583, duration: 0.294s, episode steps: 11, steps per second: 37, episode reward: -1.000, mean reward: -0.091 [-1.000, 0.000], mean action: 111.455 [32.000, 176.000], mean observation: 0.075 [0.000, 22.000], loss: 0.243795, mean_absolute_error: 0.712106, mean_q: 0.823007, mean_eps: 0.100000\n",
      " 164117/175000: episode: 4584, duration: 0.382s, episode steps: 17, steps per second: 45, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 24.824 [1.000, 126.000], mean observation: 0.048 [0.000, 34.000], loss: 0.152240, mean_absolute_error: 0.750894, mean_q: 1.327036, mean_eps: 0.100000\n",
      " 164153/175000: episode: 4585, duration: 0.709s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.250 [29.000, 185.000], mean observation: 0.453 [0.000, 72.000], loss: 0.469400, mean_absolute_error: 0.724236, mean_q: 0.789051, mean_eps: 0.100000\n",
      " 164204/175000: episode: 4586, duration: 0.983s, episode steps: 51, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 145.725 [32.000, 219.000], mean observation: 0.347 [0.000, 102.000], loss: 6.617007, mean_absolute_error: 0.770329, mean_q: 0.899133, mean_eps: 0.100000\n",
      " 164251/175000: episode: 4587, duration: 0.912s, episode steps: 47, steps per second: 52, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 130.106 [0.000, 223.000], mean observation: 0.313 [0.000, 94.000], loss: 35.579554, mean_absolute_error: 0.927466, mean_q: 1.192471, mean_eps: 0.100000\n",
      " 164283/175000: episode: 4588, duration: 0.598s, episode steps: 32, steps per second: 54, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.844 [33.000, 223.000], mean observation: 0.268 [0.000, 64.000], loss: 22.734195, mean_absolute_error: 0.882403, mean_q: 1.408286, mean_eps: 0.100000\n",
      " 164321/175000: episode: 4589, duration: 0.812s, episode steps: 38, steps per second: 47, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 57.947 [37.000, 208.000], mean observation: 0.135 [0.000, 76.000], loss: 7.338173, mean_absolute_error: 0.807782, mean_q: 1.285800, mean_eps: 0.100000\n",
      " 164345/175000: episode: 4590, duration: 0.471s, episode steps: 24, steps per second: 51, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 118.542 [17.000, 208.000], mean observation: 0.150 [0.000, 48.000], loss: 76.900938, mean_absolute_error: 1.132023, mean_q: 1.675063, mean_eps: 0.100000\n",
      " 164364/175000: episode: 4591, duration: 0.374s, episode steps: 19, steps per second: 51, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 174.053 [56.000, 208.000], mean observation: 0.096 [0.000, 38.000], loss: 31.046349, mean_absolute_error: 0.934506, mean_q: 1.517755, mean_eps: 0.100000\n",
      " 164398/175000: episode: 4592, duration: 0.691s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 167.971 [23.000, 208.000], mean observation: 0.147 [0.000, 68.000], loss: 0.724537, mean_absolute_error: 0.779455, mean_q: 1.457372, mean_eps: 0.100000\n",
      " 164436/175000: episode: 4593, duration: 0.746s, episode steps: 38, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 165.737 [4.000, 208.000], mean observation: 0.294 [0.000, 76.000], loss: 2.394389, mean_absolute_error: 0.779990, mean_q: 1.225460, mean_eps: 0.100000\n",
      " 164461/175000: episode: 4594, duration: 0.571s, episode steps: 25, steps per second: 44, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 116.240 [4.000, 208.000], mean observation: 0.214 [0.000, 50.000], loss: 0.085546, mean_absolute_error: 0.770346, mean_q: 0.918193, mean_eps: 0.100000\n",
      " 164483/175000: episode: 4595, duration: 0.391s, episode steps: 22, steps per second: 56, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 149.136 [4.000, 208.000], mean observation: 0.199 [0.000, 44.000], loss: 0.542970, mean_absolute_error: 0.774087, mean_q: 1.003067, mean_eps: 0.100000\n",
      " 164519/175000: episode: 4596, duration: 0.633s, episode steps: 36, steps per second: 57, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 149.194 [4.000, 208.000], mean observation: 0.314 [0.000, 72.000], loss: 0.191243, mean_absolute_error: 0.790204, mean_q: 1.080497, mean_eps: 0.100000\n",
      " 164556/175000: episode: 4597, duration: 0.737s, episode steps: 37, steps per second: 50, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.892 [4.000, 209.000], mean observation: 0.177 [0.000, 74.000], loss: 0.197253, mean_absolute_error: 0.799153, mean_q: 1.150889, mean_eps: 0.100000\n",
      " 164594/175000: episode: 4598, duration: 0.727s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 128.868 [4.000, 208.000], mean observation: 0.325 [0.000, 76.000], loss: 7.815550, mean_absolute_error: 0.824253, mean_q: 1.196669, mean_eps: 0.100000\n",
      " 164639/175000: episode: 4599, duration: 0.924s, episode steps: 45, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 133.689 [4.000, 208.000], mean observation: 0.241 [0.000, 90.000], loss: 16.572005, mean_absolute_error: 0.869039, mean_q: 1.093485, mean_eps: 0.100000\n",
      " 164667/175000: episode: 4600, duration: 0.538s, episode steps: 28, steps per second: 52, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 56.786 [32.000, 195.000], mean observation: 0.076 [0.000, 56.000], loss: 1.146444, mean_absolute_error: 0.802566, mean_q: 1.128655, mean_eps: 0.100000\n",
      " 164710/175000: episode: 4601, duration: 0.810s, episode steps: 43, steps per second: 53, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.302 [21.000, 191.000], mean observation: 0.211 [0.000, 86.000], loss: 3.957365, mean_absolute_error: 0.830745, mean_q: 1.191435, mean_eps: 0.100000\n",
      " 164744/175000: episode: 4602, duration: 0.684s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 93.618 [22.000, 189.000], mean observation: 0.140 [0.000, 68.000], loss: 10.972416, mean_absolute_error: 0.879036, mean_q: 1.257881, mean_eps: 0.100000\n",
      " 164778/175000: episode: 4603, duration: 0.681s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.118 [9.000, 189.000], mean observation: 0.190 [0.000, 68.000], loss: 36.065494, mean_absolute_error: 1.025753, mean_q: 1.436070, mean_eps: 0.100000\n",
      " 164819/175000: episode: 4604, duration: 0.756s, episode steps: 41, steps per second: 54, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 118.098 [3.000, 224.000], mean observation: 0.455 [0.000, 82.000], loss: 4.341290, mean_absolute_error: 0.895063, mean_q: 1.037702, mean_eps: 0.100000\n",
      " 164844/175000: episode: 4605, duration: 0.531s, episode steps: 25, steps per second: 47, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.920 [5.000, 189.000], mean observation: 0.273 [0.000, 50.000], loss: 0.616606, mean_absolute_error: 0.882534, mean_q: 1.172843, mean_eps: 0.100000\n",
      " 164876/175000: episode: 4606, duration: 0.665s, episode steps: 32, steps per second: 48, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 55.375 [2.000, 154.000], mean observation: 0.104 [0.000, 64.000], loss: 15.355731, mean_absolute_error: 0.964645, mean_q: 1.043174, mean_eps: 0.100000\n",
      " 164922/175000: episode: 4607, duration: 0.932s, episode steps: 46, steps per second: 49, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 118.391 [6.000, 207.000], mean observation: 0.286 [0.000, 92.000], loss: 1.418590, mean_absolute_error: 0.896336, mean_q: 0.862537, mean_eps: 0.100000\n",
      " 164962/175000: episode: 4608, duration: 0.790s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 119.125 [6.000, 210.000], mean observation: 0.310 [0.000, 80.000], loss: 4.687939, mean_absolute_error: 0.868893, mean_q: 0.598319, mean_eps: 0.100000\n",
      " 164994/175000: episode: 4609, duration: 0.600s, episode steps: 32, steps per second: 53, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 128.531 [27.000, 207.000], mean observation: 0.145 [0.000, 64.000], loss: 2.109453, mean_absolute_error: 0.843858, mean_q: 0.718316, mean_eps: 0.100000\n",
      " 165031/175000: episode: 4610, duration: 0.683s, episode steps: 37, steps per second: 54, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 84.892 [1.000, 223.000], mean observation: 0.371 [0.000, 74.000], loss: 1.371105, mean_absolute_error: 0.854379, mean_q: 0.878109, mean_eps: 0.100000\n",
      " 165069/175000: episode: 4611, duration: 0.728s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 60.605 [1.000, 187.000], mean observation: 0.363 [0.000, 76.000], loss: 0.479507, mean_absolute_error: 0.843773, mean_q: 0.770625, mean_eps: 0.100000\n",
      " 165115/175000: episode: 4612, duration: 0.832s, episode steps: 46, steps per second: 55, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 88.457 [3.000, 217.000], mean observation: 0.453 [0.000, 92.000], loss: 2.006275, mean_absolute_error: 0.848706, mean_q: 0.689997, mean_eps: 0.100000\n",
      " 165165/175000: episode: 4613, duration: 1.007s, episode steps: 50, steps per second: 50, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.380 [1.000, 212.000], mean observation: 0.796 [0.000, 100.000], loss: 2.136853, mean_absolute_error: 0.867064, mean_q: 0.738339, mean_eps: 0.100000\n",
      " 165199/175000: episode: 4614, duration: 0.596s, episode steps: 34, steps per second: 57, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 73.765 [1.000, 197.000], mean observation: 0.189 [0.000, 68.000], loss: 4.310176, mean_absolute_error: 0.912874, mean_q: 0.715039, mean_eps: 0.100000\n",
      " 165227/175000: episode: 4615, duration: 0.529s, episode steps: 28, steps per second: 53, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 77.786 [1.000, 210.000], mean observation: 0.147 [0.000, 56.000], loss: 12.625326, mean_absolute_error: 0.976972, mean_q: 0.794885, mean_eps: 0.100000\n",
      " 165254/175000: episode: 4616, duration: 0.513s, episode steps: 27, steps per second: 53, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 88.259 [40.000, 210.000], mean observation: 0.182 [0.000, 54.000], loss: 3.240299, mean_absolute_error: 0.948167, mean_q: 0.773388, mean_eps: 0.100000\n",
      " 165294/175000: episode: 4617, duration: 0.740s, episode steps: 40, steps per second: 54, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 63.650 [1.000, 210.000], mean observation: 0.439 [0.000, 80.000], loss: 12.652662, mean_absolute_error: 1.003827, mean_q: 0.730246, mean_eps: 0.100000\n",
      " 165336/175000: episode: 4618, duration: 0.818s, episode steps: 42, steps per second: 51, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 68.429 [1.000, 174.000], mean observation: 0.280 [0.000, 84.000], loss: 3.505297, mean_absolute_error: 0.979640, mean_q: 0.688345, mean_eps: 0.100000\n",
      " 165375/175000: episode: 4619, duration: 0.766s, episode steps: 39, steps per second: 51, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 55.385 [1.000, 174.000], mean observation: 0.351 [0.000, 78.000], loss: 2.719232, mean_absolute_error: 0.990089, mean_q: 0.617394, mean_eps: 0.100000\n",
      " 165409/175000: episode: 4620, duration: 0.676s, episode steps: 34, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 65.853 [1.000, 178.000], mean observation: 0.344 [0.000, 68.000], loss: 0.703296, mean_absolute_error: 0.975119, mean_q: 0.576949, mean_eps: 0.100000\n",
      " 165438/175000: episode: 4621, duration: 0.533s, episode steps: 29, steps per second: 54, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 90.034 [1.000, 198.000], mean observation: 0.196 [0.000, 58.000], loss: 0.703592, mean_absolute_error: 0.970955, mean_q: 0.529189, mean_eps: 0.100000\n",
      " 165458/175000: episode: 4622, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 131.400 [33.000, 206.000], mean observation: 0.099 [0.000, 40.000], loss: 0.602270, mean_absolute_error: 0.971942, mean_q: 0.493624, mean_eps: 0.100000\n",
      " 165478/175000: episode: 4623, duration: 0.396s, episode steps: 20, steps per second: 51, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 101.550 [3.000, 206.000], mean observation: 0.105 [0.000, 40.000], loss: 0.384990, mean_absolute_error: 0.971941, mean_q: 0.473366, mean_eps: 0.100000\n",
      " 165520/175000: episode: 4624, duration: 0.788s, episode steps: 42, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 119.143 [1.000, 206.000], mean observation: 0.430 [0.000, 84.000], loss: 4.195090, mean_absolute_error: 0.984660, mean_q: 0.495778, mean_eps: 0.100000\n",
      " 165564/175000: episode: 4625, duration: 0.879s, episode steps: 44, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 130.682 [45.000, 216.000], mean observation: 0.455 [0.000, 88.000], loss: 38.332237, mean_absolute_error: 1.142648, mean_q: 0.545374, mean_eps: 0.100000\n",
      " 165590/175000: episode: 4626, duration: 0.569s, episode steps: 26, steps per second: 46, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 149.808 [90.000, 216.000], mean observation: 0.176 [0.000, 52.000], loss: 0.304577, mean_absolute_error: 0.966754, mean_q: 0.475061, mean_eps: 0.100000\n",
      " 165618/175000: episode: 4627, duration: 0.513s, episode steps: 28, steps per second: 55, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 145.821 [66.000, 216.000], mean observation: 0.213 [0.000, 56.000], loss: 1.232529, mean_absolute_error: 0.966594, mean_q: 0.478812, mean_eps: 0.100000\n",
      " 165663/175000: episode: 4628, duration: 0.800s, episode steps: 45, steps per second: 56, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 163.822 [5.000, 216.000], mean observation: 0.400 [0.000, 90.000], loss: 43.903990, mean_absolute_error: 1.151097, mean_q: 0.524619, mean_eps: 0.100000\n",
      " 165684/175000: episode: 4629, duration: 0.491s, episode steps: 21, steps per second: 43, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 120.810 [91.000, 203.000], mean observation: 0.085 [0.000, 42.000], loss: 27.101199, mean_absolute_error: 1.074810, mean_q: 0.593405, mean_eps: 0.100000\n",
      " 165712/175000: episode: 4630, duration: 0.643s, episode steps: 28, steps per second: 44, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 98.607 [5.000, 211.000], mean observation: 0.270 [0.000, 56.000], loss: 199.257524, mean_absolute_error: 1.849329, mean_q: 0.639272, mean_eps: 0.100000\n",
      " 165742/175000: episode: 4631, duration: 0.655s, episode steps: 30, steps per second: 46, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 72.500 [16.000, 206.000], mean observation: 0.283 [0.000, 60.000], loss: 1.838853, mean_absolute_error: 0.982107, mean_q: 0.421209, mean_eps: 0.100000\n",
      " 165763/175000: episode: 4632, duration: 0.395s, episode steps: 21, steps per second: 53, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 132.762 [77.000, 210.000], mean observation: 0.105 [0.000, 42.000], loss: 7.234879, mean_absolute_error: 1.035426, mean_q: 0.472362, mean_eps: 0.100000\n",
      " 165795/175000: episode: 4633, duration: 0.635s, episode steps: 32, steps per second: 50, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 119.594 [1.000, 206.000], mean observation: 0.161 [0.000, 64.000], loss: 2.315915, mean_absolute_error: 1.032919, mean_q: 0.618701, mean_eps: 0.100000\n",
      " 165841/175000: episode: 4634, duration: 0.869s, episode steps: 46, steps per second: 53, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 93.500 [1.000, 203.000], mean observation: 0.507 [0.000, 92.000], loss: 2.379533, mean_absolute_error: 1.040805, mean_q: 0.414245, mean_eps: 0.100000\n",
      " 165888/175000: episode: 4635, duration: 1.101s, episode steps: 47, steps per second: 43, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 71.149 [10.000, 191.000], mean observation: 0.585 [0.000, 94.000], loss: 74.290713, mean_absolute_error: 1.373187, mean_q: 0.472631, mean_eps: 0.100000\n",
      " 165921/175000: episode: 4636, duration: 0.675s, episode steps: 33, steps per second: 49, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 74.818 [1.000, 215.000], mean observation: 0.217 [0.000, 66.000], loss: 2.940843, mean_absolute_error: 1.083850, mean_q: 0.364444, mean_eps: 0.100000\n",
      " 165962/175000: episode: 4637, duration: 0.776s, episode steps: 41, steps per second: 53, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 65.220 [1.000, 215.000], mean observation: 0.271 [0.000, 82.000], loss: 39.899846, mean_absolute_error: 1.287472, mean_q: 0.422282, mean_eps: 0.100000\n",
      " 166005/175000: episode: 4638, duration: 0.862s, episode steps: 43, steps per second: 50, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 105.977 [1.000, 202.000], mean observation: 0.323 [0.000, 86.000], loss: 1.508577, mean_absolute_error: 1.143249, mean_q: 0.322457, mean_eps: 0.100000\n",
      " 166046/175000: episode: 4639, duration: 0.813s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 138.366 [0.000, 214.000], mean observation: 0.409 [0.000, 82.000], loss: 67.944348, mean_absolute_error: 1.456973, mean_q: 0.274386, mean_eps: 0.100000\n",
      " 166080/175000: episode: 4640, duration: 0.717s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 122.794 [77.000, 202.000], mean observation: 0.106 [0.000, 68.000], loss: 2.211805, mean_absolute_error: 1.170514, mean_q: 0.256182, mean_eps: 0.100000\n",
      " 166122/175000: episode: 4641, duration: 0.869s, episode steps: 42, steps per second: 48, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.333 [3.000, 219.000], mean observation: 0.553 [0.000, 84.000], loss: 28730.102638, mean_absolute_error: 128.977882, mean_q: 1.974341, mean_eps: 0.100000\n",
      " 166159/175000: episode: 4642, duration: 0.721s, episode steps: 37, steps per second: 51, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 131.676 [1.000, 203.000], mean observation: 0.197 [0.000, 74.000], loss: 1.297155, mean_absolute_error: 1.139159, mean_q: 0.370366, mean_eps: 0.100000\n",
      " 166187/175000: episode: 4643, duration: 0.603s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 106.857 [1.000, 202.000], mean observation: 0.111 [0.000, 56.000], loss: 2.068538, mean_absolute_error: 1.125790, mean_q: 0.465597, mean_eps: 0.100000\n",
      " 166207/175000: episode: 4644, duration: 0.419s, episode steps: 20, steps per second: 48, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 84.750 [1.000, 208.000], mean observation: 0.126 [0.000, 40.000], loss: 60.426548, mean_absolute_error: 1.373813, mean_q: 0.504058, mean_eps: 0.100000\n",
      " 166234/175000: episode: 4645, duration: 0.552s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 84.741 [1.000, 176.000], mean observation: 0.123 [0.000, 54.000], loss: 6.707946, mean_absolute_error: 1.119916, mean_q: 0.448424, mean_eps: 0.100000\n",
      " 166276/175000: episode: 4646, duration: 0.952s, episode steps: 42, steps per second: 44, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 126.714 [1.000, 211.000], mean observation: 0.364 [0.000, 84.000], loss: 0.349422, mean_absolute_error: 1.073141, mean_q: 0.470644, mean_eps: 0.100000\n",
      " 166296/175000: episode: 4647, duration: 0.431s, episode steps: 20, steps per second: 46, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 98.950 [1.000, 218.000], mean observation: 0.157 [0.000, 40.000], loss: 97.810613, mean_absolute_error: 1.491759, mean_q: 0.494596, mean_eps: 0.100000\n",
      " 166324/175000: episode: 4648, duration: 0.629s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.107 [1.000, 202.000], mean observation: 0.239 [0.000, 56.000], loss: 38.916686, mean_absolute_error: 1.209533, mean_q: 0.423512, mean_eps: 0.100000\n",
      " 166364/175000: episode: 4649, duration: 0.821s, episode steps: 40, steps per second: 49, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 78.325 [1.000, 214.000], mean observation: 0.591 [0.000, 80.000], loss: 0.769172, mean_absolute_error: 1.027465, mean_q: 0.330564, mean_eps: 0.100000\n",
      " 166414/175000: episode: 4650, duration: 1.029s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 146.380 [1.000, 211.000], mean observation: 0.653 [0.000, 100.000], loss: 1.456896, mean_absolute_error: 1.012017, mean_q: 0.428410, mean_eps: 0.100000\n",
      " 166448/175000: episode: 4651, duration: 0.664s, episode steps: 34, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 108.382 [1.000, 223.000], mean observation: 0.301 [0.000, 68.000], loss: 89.805239, mean_absolute_error: 1.383565, mean_q: 0.492776, mean_eps: 0.100000\n",
      " 166501/175000: episode: 4652, duration: 1.072s, episode steps: 53, steps per second: 49, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 76.226 [1.000, 148.000], mean observation: 0.437 [0.000, 106.000], loss: 1828.292970, mean_absolute_error: 9.193675, mean_q: 1.830412, mean_eps: 0.100000\n",
      " 166545/175000: episode: 4653, duration: 0.917s, episode steps: 44, steps per second: 48, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 103.386 [1.000, 212.000], mean observation: 0.362 [0.000, 88.000], loss: 5.199815, mean_absolute_error: 0.972794, mean_q: 0.526857, mean_eps: 0.100000\n",
      " 166555/175000: episode: 4654, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 100.400 [1.000, 156.000], mean observation: 0.052 [0.000, 20.000], loss: 0.211236, mean_absolute_error: 0.946952, mean_q: 0.392058, mean_eps: 0.100000\n",
      " 166611/175000: episode: 4655, duration: 1.006s, episode steps: 56, steps per second: 56, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 90.286 [1.000, 174.000], mean observation: 0.760 [0.000, 112.000], loss: 4.704614, mean_absolute_error: 0.960328, mean_q: 0.505260, mean_eps: 0.100000\n",
      " 166637/175000: episode: 4656, duration: 0.558s, episode steps: 26, steps per second: 47, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 127.000 [25.000, 177.000], mean observation: 0.173 [0.000, 52.000], loss: 6.187030, mean_absolute_error: 0.949712, mean_q: 0.442380, mean_eps: 0.100000\n",
      " 166680/175000: episode: 4657, duration: 0.881s, episode steps: 43, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 127.488 [35.000, 167.000], mean observation: 0.324 [0.000, 86.000], loss: 5.904755, mean_absolute_error: 0.938740, mean_q: 0.440851, mean_eps: 0.100000\n",
      " 166725/175000: episode: 4658, duration: 0.887s, episode steps: 45, steps per second: 51, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 141.178 [70.000, 173.000], mean observation: 0.364 [0.000, 90.000], loss: 1.996813, mean_absolute_error: 0.906176, mean_q: 0.409410, mean_eps: 0.100000\n",
      " 166777/175000: episode: 4659, duration: 1.007s, episode steps: 52, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 114.154 [6.000, 212.000], mean observation: 0.463 [0.000, 104.000], loss: 2436.986847, mean_absolute_error: 11.830051, mean_q: 1.804141, mean_eps: 0.100000\n",
      " 166812/175000: episode: 4660, duration: 0.740s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 121.657 [22.000, 188.000], mean observation: 0.283 [0.000, 70.000], loss: 0.836253, mean_absolute_error: 0.872393, mean_q: 0.409708, mean_eps: 0.100000\n",
      " 166845/175000: episode: 4661, duration: 0.711s, episode steps: 33, steps per second: 46, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 118.697 [1.000, 196.000], mean observation: 0.334 [0.000, 66.000], loss: 0.479758, mean_absolute_error: 0.863224, mean_q: 0.452285, mean_eps: 0.100000\n",
      " 166881/175000: episode: 4662, duration: 0.717s, episode steps: 36, steps per second: 50, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 135.000 [18.000, 188.000], mean observation: 0.279 [0.000, 72.000], loss: 3.764192, mean_absolute_error: 0.864556, mean_q: 0.445273, mean_eps: 0.100000\n",
      " 166924/175000: episode: 4663, duration: 0.823s, episode steps: 43, steps per second: 52, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 120.535 [5.000, 222.000], mean observation: 0.312 [0.000, 86.000], loss: 2.019753, mean_absolute_error: 0.841498, mean_q: 0.508865, mean_eps: 0.100000\n",
      " 166977/175000: episode: 4664, duration: 1.024s, episode steps: 53, steps per second: 52, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.547 [3.000, 188.000], mean observation: 0.480 [0.000, 106.000], loss: 0.292927, mean_absolute_error: 0.825944, mean_q: 0.411475, mean_eps: 0.100000\n",
      " 167015/175000: episode: 4665, duration: 0.733s, episode steps: 38, steps per second: 52, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 65.342 [1.000, 148.000], mean observation: 0.251 [0.000, 76.000], loss: 1.880998, mean_absolute_error: 0.828102, mean_q: 0.541071, mean_eps: 0.100000\n",
      " 167055/175000: episode: 4666, duration: 0.790s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 93.850 [18.000, 203.000], mean observation: 0.445 [0.000, 80.000], loss: 85.391898, mean_absolute_error: 1.191511, mean_q: 0.483902, mean_eps: 0.100000\n",
      " 167107/175000: episode: 4667, duration: 1.039s, episode steps: 52, steps per second: 50, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 137.596 [18.000, 201.000], mean observation: 0.750 [0.000, 104.000], loss: 0.540820, mean_absolute_error: 0.808820, mean_q: 0.453147, mean_eps: 0.100000\n",
      " 167157/175000: episode: 4668, duration: 0.986s, episode steps: 50, steps per second: 51, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 154.780 [3.000, 212.000], mean observation: 0.546 [0.000, 100.000], loss: 16.844973, mean_absolute_error: 0.880186, mean_q: 0.384059, mean_eps: 0.100000\n",
      " 167183/175000: episode: 4669, duration: 0.491s, episode steps: 26, steps per second: 53, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 115.077 [39.000, 188.000], mean observation: 0.202 [0.000, 52.000], loss: 56.940318, mean_absolute_error: 1.316008, mean_q: 3.369770, mean_eps: 0.100000\n",
      " 167219/175000: episode: 4670, duration: 0.669s, episode steps: 36, steps per second: 54, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 138.639 [20.000, 190.000], mean observation: 0.361 [0.000, 72.000], loss: 3.622281, mean_absolute_error: 0.818942, mean_q: 0.302636, mean_eps: 0.100000\n",
      " 167273/175000: episode: 4671, duration: 1.028s, episode steps: 54, steps per second: 53, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 141.481 [20.000, 188.000], mean observation: 0.593 [0.000, 108.000], loss: 1.135626, mean_absolute_error: 0.811720, mean_q: 0.374074, mean_eps: 0.100000\n",
      " 167315/175000: episode: 4672, duration: 0.845s, episode steps: 42, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 114.214 [0.000, 178.000], mean observation: 0.508 [0.000, 84.000], loss: 45.016581, mean_absolute_error: 1.002868, mean_q: 0.359356, mean_eps: 0.100000\n",
      " 167343/175000: episode: 4673, duration: 0.584s, episode steps: 28, steps per second: 48, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 124.643 [1.000, 219.000], mean observation: 0.231 [0.000, 56.000], loss: 3.917212, mean_absolute_error: 0.816430, mean_q: 0.330209, mean_eps: 0.100000\n",
      " 167383/175000: episode: 4674, duration: 0.793s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 140.425 [52.000, 209.000], mean observation: 0.181 [0.000, 80.000], loss: 68.326347, mean_absolute_error: 1.107547, mean_q: 0.396357, mean_eps: 0.100000\n",
      " 167399/175000: episode: 4675, duration: 0.347s, episode steps: 16, steps per second: 46, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 127.750 [52.000, 153.000], mean observation: 0.041 [0.000, 32.000], loss: 3.812967, mean_absolute_error: 0.814548, mean_q: 0.374624, mean_eps: 0.100000\n",
      " 167423/175000: episode: 4676, duration: 0.478s, episode steps: 24, steps per second: 50, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 135.750 [15.000, 192.000], mean observation: 0.174 [0.000, 48.000], loss: 23.306253, mean_absolute_error: 0.897828, mean_q: 0.429297, mean_eps: 0.100000\n",
      " 167463/175000: episode: 4677, duration: 0.788s, episode steps: 40, steps per second: 51, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 122.925 [52.000, 195.000], mean observation: 0.384 [0.000, 80.000], loss: 0.108517, mean_absolute_error: 0.794526, mean_q: 0.484734, mean_eps: 0.100000\n",
      " 167499/175000: episode: 4678, duration: 0.781s, episode steps: 36, steps per second: 46, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 144.889 [32.000, 222.000], mean observation: 0.386 [0.000, 72.000], loss: 3339.237036, mean_absolute_error: 15.800526, mean_q: 2.531916, mean_eps: 0.100000\n",
      " 167558/175000: episode: 4679, duration: 1.275s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 132.593 [1.000, 192.000], mean observation: 0.692 [0.000, 118.000], loss: 1.726909, mean_absolute_error: 0.799553, mean_q: 0.414231, mean_eps: 0.100000\n",
      " 167592/175000: episode: 4680, duration: 0.759s, episode steps: 34, steps per second: 45, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 114.794 [1.000, 219.000], mean observation: 0.350 [0.000, 68.000], loss: 0.109582, mean_absolute_error: 0.794902, mean_q: 0.382519, mean_eps: 0.100000\n",
      " 167610/175000: episode: 4681, duration: 0.430s, episode steps: 18, steps per second: 42, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 132.222 [15.000, 219.000], mean observation: 0.123 [0.000, 36.000], loss: 0.108360, mean_absolute_error: 0.800075, mean_q: 0.442799, mean_eps: 0.100000\n",
      " 167644/175000: episode: 4682, duration: 0.723s, episode steps: 34, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 119.059 [1.000, 219.000], mean observation: 0.375 [0.000, 68.000], loss: 4.582470, mean_absolute_error: 0.816037, mean_q: 0.415626, mean_eps: 0.100000\n",
      " 167675/175000: episode: 4683, duration: 0.631s, episode steps: 31, steps per second: 49, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 121.161 [64.000, 201.000], mean observation: 0.282 [0.000, 62.000], loss: 0.119390, mean_absolute_error: 0.784846, mean_q: 0.370189, mean_eps: 0.100000\n",
      " 167708/175000: episode: 4684, duration: 0.691s, episode steps: 33, steps per second: 48, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 106.697 [64.000, 158.000], mean observation: 0.278 [0.000, 66.000], loss: 3.780850, mean_absolute_error: 0.798199, mean_q: 0.399785, mean_eps: 0.100000\n",
      " 167729/175000: episode: 4685, duration: 0.426s, episode steps: 21, steps per second: 49, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 121.095 [64.000, 148.000], mean observation: 0.153 [0.000, 42.000], loss: 0.226189, mean_absolute_error: 0.776673, mean_q: 0.367832, mean_eps: 0.100000\n",
      " 167753/175000: episode: 4686, duration: 0.490s, episode steps: 24, steps per second: 49, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 117.708 [18.000, 212.000], mean observation: 0.203 [0.000, 48.000], loss: 0.288732, mean_absolute_error: 0.778039, mean_q: 0.389627, mean_eps: 0.100000\n",
      " 167785/175000: episode: 4687, duration: 0.622s, episode steps: 32, steps per second: 51, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 108.125 [0.000, 156.000], mean observation: 0.337 [0.000, 64.000], loss: 0.878970, mean_absolute_error: 0.784183, mean_q: 0.311327, mean_eps: 0.100000\n",
      " 167799/175000: episode: 4688, duration: 0.283s, episode steps: 14, steps per second: 50, episode reward: -1.000, mean reward: -0.071 [-1.000, 0.000], mean action: 84.714 [70.000, 144.000], mean observation: 0.036 [0.000, 28.000], loss: 0.116837, mean_absolute_error: 0.787018, mean_q: 0.413145, mean_eps: 0.100000\n",
      " 167827/175000: episode: 4689, duration: 0.610s, episode steps: 28, steps per second: 46, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 106.036 [7.000, 147.000], mean observation: 0.137 [0.000, 56.000], loss: 0.730559, mean_absolute_error: 0.791177, mean_q: 0.326150, mean_eps: 0.100000\n",
      " 167840/175000: episode: 4690, duration: 0.281s, episode steps: 13, steps per second: 46, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 88.000 [16.000, 145.000], mean observation: 0.063 [0.000, 26.000], loss: 0.180094, mean_absolute_error: 0.795175, mean_q: 0.217066, mean_eps: 0.100000\n",
      " 167857/175000: episode: 4691, duration: 0.411s, episode steps: 17, steps per second: 41, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 80.765 [16.000, 212.000], mean observation: 0.142 [0.000, 34.000], loss: 0.384149, mean_absolute_error: 0.807436, mean_q: 0.373525, mean_eps: 0.100000\n",
      " 167873/175000: episode: 4692, duration: 0.326s, episode steps: 16, steps per second: 49, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 82.188 [16.000, 190.000], mean observation: 0.084 [0.000, 32.000], loss: 2.047388, mean_absolute_error: 0.817062, mean_q: 0.252724, mean_eps: 0.100000\n",
      " 167933/175000: episode: 4693, duration: 1.237s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 93.833 [0.000, 192.000], mean observation: 0.622 [0.000, 120.000], loss: 641.281681, mean_absolute_error: 3.756592, mean_q: 1.425491, mean_eps: 0.100000\n",
      " 167962/175000: episode: 4694, duration: 0.580s, episode steps: 29, steps per second: 50, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 96.552 [1.000, 189.000], mean observation: 0.230 [0.000, 58.000], loss: 0.162193, mean_absolute_error: 0.805858, mean_q: 0.229830, mean_eps: 0.100000\n",
      " 168014/175000: episode: 4695, duration: 1.097s, episode steps: 52, steps per second: 47, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 89.192 [16.000, 191.000], mean observation: 0.567 [0.000, 104.000], loss: 83.321189, mean_absolute_error: 1.172685, mean_q: 0.185138, mean_eps: 0.100000\n",
      " 168055/175000: episode: 4696, duration: 0.825s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 127.195 [43.000, 213.000], mean observation: 0.281 [0.000, 82.000], loss: 59.890776, mean_absolute_error: 1.213805, mean_q: 2.009785, mean_eps: 0.100000\n",
      " 168082/175000: episode: 4697, duration: 0.582s, episode steps: 27, steps per second: 46, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 99.444 [1.000, 199.000], mean observation: 0.136 [0.000, 54.000], loss: 0.528998, mean_absolute_error: 0.790441, mean_q: 0.074229, mean_eps: 0.100000\n",
      " 168122/175000: episode: 4698, duration: 0.799s, episode steps: 40, steps per second: 50, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 64.050 [1.000, 158.000], mean observation: 0.386 [0.000, 80.000], loss: 2.609060, mean_absolute_error: 0.804834, mean_q: 0.178861, mean_eps: 0.100000\n",
      " 168158/175000: episode: 4699, duration: 0.708s, episode steps: 36, steps per second: 51, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 118.917 [17.000, 218.000], mean observation: 0.405 [0.000, 72.000], loss: 3.285428, mean_absolute_error: 0.802371, mean_q: 0.135595, mean_eps: 0.100000\n",
      " 168208/175000: episode: 4700, duration: 1.025s, episode steps: 50, steps per second: 49, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 78.060 [44.000, 221.000], mean observation: 0.579 [0.000, 100.000], loss: 0.594667, mean_absolute_error: 0.780075, mean_q: 0.175178, mean_eps: 0.100000\n",
      " 168244/175000: episode: 4701, duration: 0.821s, episode steps: 36, steps per second: 44, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 128.111 [6.000, 221.000], mean observation: 0.422 [0.000, 72.000], loss: 9.761795, mean_absolute_error: 0.815984, mean_q: 0.261057, mean_eps: 0.100000\n",
      " 168294/175000: episode: 4702, duration: 0.955s, episode steps: 50, steps per second: 52, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 103.880 [4.000, 186.000], mean observation: 0.504 [0.000, 100.000], loss: 23.252765, mean_absolute_error: 0.885880, mean_q: 0.239891, mean_eps: 0.100000\n",
      " 168354/175000: episode: 4703, duration: 1.245s, episode steps: 60, steps per second: 48, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 145.767 [40.000, 223.000], mean observation: 1.039 [0.000, 120.000], loss: 1.029297, mean_absolute_error: 0.787219, mean_q: 0.168931, mean_eps: 0.100000\n",
      " 168385/175000: episode: 4704, duration: 0.650s, episode steps: 31, steps per second: 48, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 157.387 [66.000, 207.000], mean observation: 0.281 [0.000, 62.000], loss: 0.263284, mean_absolute_error: 0.785000, mean_q: 0.126663, mean_eps: 0.100000\n",
      " 168415/175000: episode: 4705, duration: 0.674s, episode steps: 30, steps per second: 45, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 141.400 [33.000, 207.000], mean observation: 0.279 [0.000, 60.000], loss: 0.103006, mean_absolute_error: 0.788677, mean_q: 0.191697, mean_eps: 0.100000\n",
      " 168454/175000: episode: 4706, duration: 0.855s, episode steps: 39, steps per second: 46, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 97.641 [18.000, 202.000], mean observation: 0.437 [0.000, 78.000], loss: 3.665994, mean_absolute_error: 0.797418, mean_q: 0.272832, mean_eps: 0.100000\n",
      " 168488/175000: episode: 4707, duration: 0.688s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 41.147 [18.000, 207.000], mean observation: 0.110 [0.000, 68.000], loss: 10.558116, mean_absolute_error: 0.834368, mean_q: 0.282397, mean_eps: 0.100000\n",
      " 168524/175000: episode: 4708, duration: 0.773s, episode steps: 36, steps per second: 47, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 124.861 [14.000, 223.000], mean observation: 0.470 [0.000, 72.000], loss: 0.441574, mean_absolute_error: 0.792998, mean_q: 0.100496, mean_eps: 0.100000\n",
      " 168550/175000: episode: 4709, duration: 0.580s, episode steps: 26, steps per second: 45, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 108.769 [62.000, 207.000], mean observation: 0.248 [0.000, 52.000], loss: 0.097875, mean_absolute_error: 0.792972, mean_q: 0.162135, mean_eps: 0.100000\n",
      " 168591/175000: episode: 4710, duration: 0.817s, episode steps: 41, steps per second: 50, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.756 [20.000, 202.000], mean observation: 0.469 [0.000, 82.000], loss: 6.502237, mean_absolute_error: 0.815191, mean_q: 0.181732, mean_eps: 0.100000\n",
      " 168644/175000: episode: 4711, duration: 1.170s, episode steps: 53, steps per second: 45, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 87.396 [6.000, 215.000], mean observation: 0.689 [0.000, 106.000], loss: 0.526380, mean_absolute_error: 0.794725, mean_q: 0.335032, mean_eps: 0.100000\n",
      " 168665/175000: episode: 4712, duration: 0.465s, episode steps: 21, steps per second: 45, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 69.095 [6.000, 163.000], mean observation: 0.115 [0.000, 42.000], loss: 0.283359, mean_absolute_error: 0.781668, mean_q: 0.174997, mean_eps: 0.100000\n",
      " 168700/175000: episode: 4713, duration: 0.706s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.314 [6.000, 212.000], mean observation: 0.244 [0.000, 70.000], loss: 1.707822, mean_absolute_error: 0.791559, mean_q: 0.223077, mean_eps: 0.100000\n",
      " 168768/175000: episode: 4714, duration: 1.358s, episode steps: 68, steps per second: 50, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 139.353 [9.000, 224.000], mean observation: 0.668 [0.000, 136.000], loss: 2.327025, mean_absolute_error: 0.793403, mean_q: 0.198433, mean_eps: 0.100000\n",
      " 168799/175000: episode: 4715, duration: 0.596s, episode steps: 31, steps per second: 52, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.387 [45.000, 224.000], mean observation: 0.309 [0.000, 62.000], loss: 3.771527, mean_absolute_error: 0.800812, mean_q: 0.184680, mean_eps: 0.100000\n",
      " 168834/175000: episode: 4716, duration: 0.699s, episode steps: 35, steps per second: 50, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 175.229 [45.000, 224.000], mean observation: 0.435 [0.000, 70.000], loss: 678.982045, mean_absolute_error: 3.812993, mean_q: 0.367642, mean_eps: 0.100000\n",
      " 168866/175000: episode: 4717, duration: 0.656s, episode steps: 32, steps per second: 49, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 35.250 [1.000, 203.000], mean observation: 0.080 [0.000, 64.000], loss: 0.756607, mean_absolute_error: 0.786858, mean_q: 0.202668, mean_eps: 0.100000\n",
      " 168915/175000: episode: 4718, duration: 0.921s, episode steps: 49, steps per second: 53, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 176.449 [1.000, 224.000], mean observation: 0.661 [0.000, 98.000], loss: 3.078496, mean_absolute_error: 0.791141, mean_q: 0.315708, mean_eps: 0.100000\n",
      " 168950/175000: episode: 4719, duration: 0.691s, episode steps: 35, steps per second: 51, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 123.000 [25.000, 224.000], mean observation: 0.462 [0.000, 70.000], loss: 4.364840, mean_absolute_error: 0.815025, mean_q: 0.210786, mean_eps: 0.100000\n",
      " 168978/175000: episode: 4720, duration: 0.567s, episode steps: 28, steps per second: 49, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 167.429 [25.000, 224.000], mean observation: 0.197 [0.000, 56.000], loss: 0.237419, mean_absolute_error: 0.801273, mean_q: 0.146156, mean_eps: 0.100000\n",
      " 169006/175000: episode: 4721, duration: 0.618s, episode steps: 28, steps per second: 45, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 143.964 [25.000, 224.000], mean observation: 0.265 [0.000, 56.000], loss: 0.594280, mean_absolute_error: 0.793284, mean_q: 0.120004, mean_eps: 0.100000\n",
      " 169041/175000: episode: 4722, duration: 0.738s, episode steps: 35, steps per second: 47, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 78.829 [25.000, 214.000], mean observation: 0.380 [0.000, 70.000], loss: 0.351070, mean_absolute_error: 0.790562, mean_q: 0.252332, mean_eps: 0.100000\n",
      " 169100/175000: episode: 4723, duration: 1.286s, episode steps: 59, steps per second: 46, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 101.712 [0.000, 214.000], mean observation: 0.714 [0.000, 118.000], loss: 17.625794, mean_absolute_error: 0.881159, mean_q: 0.228688, mean_eps: 0.100000\n",
      " 169158/175000: episode: 4724, duration: 1.230s, episode steps: 58, steps per second: 47, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 117.586 [1.000, 207.000], mean observation: 0.902 [0.000, 116.000], loss: 1.684203, mean_absolute_error: 0.834080, mean_q: 0.242647, mean_eps: 0.100000\n",
      " 169209/175000: episode: 4725, duration: 1.061s, episode steps: 51, steps per second: 48, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 145.902 [32.000, 217.000], mean observation: 0.655 [0.000, 102.000], loss: 5.872917, mean_absolute_error: 0.849809, mean_q: 0.169347, mean_eps: 0.100000\n",
      " 169253/175000: episode: 4726, duration: 0.895s, episode steps: 44, steps per second: 49, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 65.023 [25.000, 223.000], mean observation: 0.597 [0.000, 88.000], loss: 0.757717, mean_absolute_error: 0.827899, mean_q: 0.305286, mean_eps: 0.100000\n",
      " 169284/175000: episode: 4727, duration: 0.654s, episode steps: 31, steps per second: 47, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 83.968 [25.000, 207.000], mean observation: 0.316 [0.000, 62.000], loss: 5.579128, mean_absolute_error: 0.871438, mean_q: 0.329250, mean_eps: 0.100000\n",
      " 169311/175000: episode: 4728, duration: 0.554s, episode steps: 27, steps per second: 49, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 106.852 [25.000, 214.000], mean observation: 0.196 [0.000, 54.000], loss: 1.133588, mean_absolute_error: 0.856760, mean_q: 0.368268, mean_eps: 0.100000\n",
      " 169359/175000: episode: 4729, duration: 0.934s, episode steps: 48, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 109.000 [25.000, 212.000], mean observation: 0.492 [0.000, 96.000], loss: 9.968524, mean_absolute_error: 0.899280, mean_q: 0.449335, mean_eps: 0.100000\n",
      " 169401/175000: episode: 4730, duration: 1.044s, episode steps: 42, steps per second: 40, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 92.476 [10.000, 211.000], mean observation: 0.540 [0.000, 84.000], loss: 3.295643, mean_absolute_error: 0.860966, mean_q: 0.460876, mean_eps: 0.100000\n",
      " 169435/175000: episode: 4731, duration: 0.821s, episode steps: 34, steps per second: 41, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 125.500 [10.000, 201.000], mean observation: 0.286 [0.000, 68.000], loss: 553.710890, mean_absolute_error: 3.316449, mean_q: 0.228829, mean_eps: 0.100000\n",
      " 169482/175000: episode: 4732, duration: 1.129s, episode steps: 47, steps per second: 42, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 119.319 [11.000, 223.000], mean observation: 0.526 [0.000, 94.000], loss: 3.037851, mean_absolute_error: 0.879486, mean_q: 0.162004, mean_eps: 0.100000\n",
      " 169519/175000: episode: 4733, duration: 0.808s, episode steps: 37, steps per second: 46, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 141.649 [16.000, 223.000], mean observation: 0.375 [0.000, 74.000], loss: 3617.362249, mean_absolute_error: 17.122542, mean_q: 2.181973, mean_eps: 0.100000\n",
      " 169564/175000: episode: 4734, duration: 0.860s, episode steps: 45, steps per second: 52, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 106.822 [4.000, 223.000], mean observation: 0.468 [0.000, 90.000], loss: 1.775872, mean_absolute_error: 0.893996, mean_q: 0.329302, mean_eps: 0.100000\n",
      " 169597/175000: episode: 4735, duration: 0.647s, episode steps: 33, steps per second: 51, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 115.667 [34.000, 221.000], mean observation: 0.298 [0.000, 66.000], loss: 0.158827, mean_absolute_error: 0.879160, mean_q: 0.231904, mean_eps: 0.100000\n",
      " 169644/175000: episode: 4736, duration: 0.925s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 86.830 [17.000, 201.000], mean observation: 0.397 [0.000, 94.000], loss: 0.217256, mean_absolute_error: 0.874025, mean_q: 0.220244, mean_eps: 0.100000\n",
      " 169669/175000: episode: 4737, duration: 0.628s, episode steps: 25, steps per second: 40, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 83.920 [45.000, 202.000], mean observation: 0.167 [0.000, 50.000], loss: 0.148792, mean_absolute_error: 0.861437, mean_q: 0.222005, mean_eps: 0.100000\n",
      " 169687/175000: episode: 4738, duration: 0.339s, episode steps: 18, steps per second: 53, episode reward: -1.000, mean reward: -0.056 [-1.000, 0.000], mean action: 103.667 [1.000, 202.000], mean observation: 0.094 [0.000, 36.000], loss: 0.668120, mean_absolute_error: 0.859254, mean_q: 0.221169, mean_eps: 0.100000\n",
      " 169721/175000: episode: 4739, duration: 0.694s, episode steps: 34, steps per second: 49, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 80.882 [1.000, 210.000], mean observation: 0.288 [0.000, 68.000], loss: 0.398479, mean_absolute_error: 0.850589, mean_q: 0.277640, mean_eps: 0.100000\n",
      " 169768/175000: episode: 4740, duration: 0.919s, episode steps: 47, steps per second: 51, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 124.191 [3.000, 210.000], mean observation: 0.356 [0.000, 94.000], loss: 0.139859, mean_absolute_error: 0.842187, mean_q: 0.297049, mean_eps: 0.100000\n",
      " 169820/175000: episode: 4741, duration: 1.072s, episode steps: 52, steps per second: 48, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 71.385 [3.000, 222.000], mean observation: 0.607 [0.000, 104.000], loss: 0.939580, mean_absolute_error: 0.829955, mean_q: 0.221499, mean_eps: 0.100000\n",
      " 169866/175000: episode: 4742, duration: 0.912s, episode steps: 46, steps per second: 50, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 99.739 [3.000, 210.000], mean observation: 0.569 [0.000, 92.000], loss: 2.403009, mean_absolute_error: 0.828578, mean_q: 0.178030, mean_eps: 0.100000\n",
      " 169918/175000: episode: 4743, duration: 1.238s, episode steps: 52, steps per second: 42, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 85.731 [4.000, 204.000], mean observation: 0.382 [0.000, 104.000], loss: 2.085800, mean_absolute_error: 0.827618, mean_q: 0.292123, mean_eps: 0.100000\n",
      " 169957/175000: episode: 4744, duration: 1.051s, episode steps: 39, steps per second: 37, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.487 [16.000, 204.000], mean observation: 0.267 [0.000, 78.000], loss: 3.767671, mean_absolute_error: 0.838511, mean_q: 0.377053, mean_eps: 0.100000\n",
      " 170005/175000: episode: 4745, duration: 1.158s, episode steps: 48, steps per second: 41, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 120.938 [16.000, 204.000], mean observation: 0.417 [0.000, 96.000], loss: 0.589524, mean_absolute_error: 0.832611, mean_q: 0.250473, mean_eps: 0.100000\n",
      " 170041/175000: episode: 4746, duration: 1.309s, episode steps: 36, steps per second: 28, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 125.361 [4.000, 204.000], mean observation: 0.335 [0.000, 72.000], loss: 0.502966, mean_absolute_error: 0.831289, mean_q: 0.206919, mean_eps: 0.100000\n",
      " 170071/175000: episode: 4747, duration: 1.182s, episode steps: 30, steps per second: 25, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 84.633 [26.000, 194.000], mean observation: 0.241 [0.000, 60.000], loss: 25.556195, mean_absolute_error: 0.928546, mean_q: 0.284765, mean_eps: 0.100000\n",
      " 170086/175000: episode: 4748, duration: 0.727s, episode steps: 15, steps per second: 21, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 99.733 [11.000, 212.000], mean observation: 0.107 [0.000, 30.000], loss: 0.552783, mean_absolute_error: 0.812058, mean_q: 0.427192, mean_eps: 0.100000\n",
      " 170144/175000: episode: 4749, duration: 2.333s, episode steps: 58, steps per second: 25, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 105.190 [56.000, 212.000], mean observation: 0.818 [0.000, 116.000], loss: 0.567788, mean_absolute_error: 0.800851, mean_q: 0.405567, mean_eps: 0.100000\n",
      " 170179/175000: episode: 4750, duration: 1.359s, episode steps: 35, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 82.257 [2.000, 216.000], mean observation: 0.568 [0.000, 70.000], loss: 1.831680, mean_absolute_error: 0.792037, mean_q: 0.437788, mean_eps: 0.100000\n",
      " 170214/175000: episode: 4751, duration: 1.375s, episode steps: 35, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 77.543 [48.000, 217.000], mean observation: 0.352 [0.000, 70.000], loss: 0.965638, mean_absolute_error: 0.788520, mean_q: 0.392458, mean_eps: 0.100000\n",
      " 170267/175000: episode: 4752, duration: 2.043s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 94.000 [29.000, 221.000], mean observation: 0.759 [0.000, 106.000], loss: 5.779631, mean_absolute_error: 0.802542, mean_q: 0.416114, mean_eps: 0.100000\n",
      " 170311/175000: episode: 4753, duration: 1.802s, episode steps: 44, steps per second: 24, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 90.795 [8.000, 202.000], mean observation: 0.688 [0.000, 88.000], loss: 2272.761287, mean_absolute_error: 11.020992, mean_q: 2.193906, mean_eps: 0.100000\n",
      " 170338/175000: episode: 4754, duration: 1.024s, episode steps: 27, steps per second: 26, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 126.333 [5.000, 191.000], mean observation: 0.270 [0.000, 54.000], loss: 0.867956, mean_absolute_error: 0.782608, mean_q: 0.593769, mean_eps: 0.100000\n",
      " 170360/175000: episode: 4755, duration: 0.920s, episode steps: 22, steps per second: 24, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 123.909 [62.000, 223.000], mean observation: 0.200 [0.000, 44.000], loss: 3.465965, mean_absolute_error: 0.786870, mean_q: 0.620195, mean_eps: 0.100000\n",
      " 170373/175000: episode: 4756, duration: 0.589s, episode steps: 13, steps per second: 22, episode reward: -1.000, mean reward: -0.077 [-1.000, 0.000], mean action: 136.231 [62.000, 204.000], mean observation: 0.109 [0.000, 26.000], loss: 0.638434, mean_absolute_error: 0.770543, mean_q: 0.793651, mean_eps: 0.100000\n",
      " 170428/175000: episode: 4757, duration: 2.087s, episode steps: 55, steps per second: 26, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 129.945 [10.000, 216.000], mean observation: 0.622 [0.000, 110.000], loss: 0.721808, mean_absolute_error: 0.761523, mean_q: 0.708368, mean_eps: 0.100000\n",
      " 170472/175000: episode: 4758, duration: 1.857s, episode steps: 44, steps per second: 24, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 144.045 [34.000, 204.000], mean observation: 0.571 [0.000, 88.000], loss: 13.516958, mean_absolute_error: 0.816029, mean_q: 0.666567, mean_eps: 0.100000\n",
      " 170499/175000: episode: 4759, duration: 1.069s, episode steps: 27, steps per second: 25, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 146.370 [5.000, 213.000], mean observation: 0.280 [0.000, 54.000], loss: 5.200204, mean_absolute_error: 0.777066, mean_q: 0.535137, mean_eps: 0.100000\n",
      " 170545/175000: episode: 4760, duration: 1.924s, episode steps: 46, steps per second: 24, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 92.891 [0.000, 209.000], mean observation: 0.636 [0.000, 92.000], loss: 5.173532, mean_absolute_error: 0.779711, mean_q: 0.486135, mean_eps: 0.100000\n",
      " 170572/175000: episode: 4761, duration: 1.074s, episode steps: 27, steps per second: 25, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 128.407 [0.000, 204.000], mean observation: 0.200 [0.000, 54.000], loss: 0.489992, mean_absolute_error: 0.763531, mean_q: 0.526517, mean_eps: 0.100000\n",
      " 170610/175000: episode: 4762, duration: 1.581s, episode steps: 38, steps per second: 24, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 68.605 [0.000, 223.000], mean observation: 0.391 [0.000, 76.000], loss: 0.686459, mean_absolute_error: 0.761243, mean_q: 0.507968, mean_eps: 0.100000\n",
      " 170657/175000: episode: 4763, duration: 1.871s, episode steps: 47, steps per second: 25, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 79.851 [0.000, 210.000], mean observation: 0.592 [0.000, 94.000], loss: 7.461236, mean_absolute_error: 0.781346, mean_q: 0.573177, mean_eps: 0.100000\n",
      " 170698/175000: episode: 4764, duration: 1.675s, episode steps: 41, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 150.732 [13.000, 210.000], mean observation: 0.485 [0.000, 82.000], loss: 9.851178, mean_absolute_error: 0.790217, mean_q: 0.738908, mean_eps: 0.100000\n",
      " 170740/175000: episode: 4765, duration: 1.669s, episode steps: 42, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 104.262 [1.000, 210.000], mean observation: 0.434 [0.000, 84.000], loss: 3.889342, mean_absolute_error: 0.772989, mean_q: 0.783358, mean_eps: 0.100000\n",
      " 170763/175000: episode: 4766, duration: 0.910s, episode steps: 23, steps per second: 25, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 131.087 [10.000, 210.000], mean observation: 0.120 [0.000, 46.000], loss: 0.294128, mean_absolute_error: 0.768682, mean_q: 0.693766, mean_eps: 0.100000\n",
      " 170796/175000: episode: 4767, duration: 1.382s, episode steps: 33, steps per second: 24, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 143.182 [7.000, 210.000], mean observation: 0.284 [0.000, 66.000], loss: 2.922376, mean_absolute_error: 0.790501, mean_q: 0.681860, mean_eps: 0.100000\n",
      " 170841/175000: episode: 4768, duration: 1.813s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 158.378 [38.000, 204.000], mean observation: 0.512 [0.000, 90.000], loss: 400.214503, mean_absolute_error: 2.692209, mean_q: 1.952002, mean_eps: 0.100000\n",
      " 170872/175000: episode: 4769, duration: 1.286s, episode steps: 31, steps per second: 24, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 138.548 [45.000, 212.000], mean observation: 0.311 [0.000, 62.000], loss: 7.062956, mean_absolute_error: 0.837932, mean_q: 0.406722, mean_eps: 0.100000\n",
      " 170902/175000: episode: 4770, duration: 1.250s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 140.067 [1.000, 212.000], mean observation: 0.319 [0.000, 60.000], loss: 0.642916, mean_absolute_error: 0.811524, mean_q: 0.415955, mean_eps: 0.100000\n",
      " 170943/175000: episode: 4771, duration: 1.591s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.902 [1.000, 212.000], mean observation: 0.529 [0.000, 82.000], loss: 18.380707, mean_absolute_error: 0.888519, mean_q: 0.396819, mean_eps: 0.100000\n",
      " 170972/175000: episode: 4772, duration: 1.204s, episode steps: 29, steps per second: 24, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 116.517 [1.000, 184.000], mean observation: 0.295 [0.000, 58.000], loss: 22.721098, mean_absolute_error: 0.915771, mean_q: 0.407458, mean_eps: 0.100000\n",
      " 171008/175000: episode: 4773, duration: 1.516s, episode steps: 36, steps per second: 24, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 139.917 [0.000, 173.000], mean observation: 0.312 [0.000, 72.000], loss: 5.013429, mean_absolute_error: 0.844079, mean_q: 0.529024, mean_eps: 0.100000\n",
      " 171042/175000: episode: 4774, duration: 1.391s, episode steps: 34, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 124.735 [1.000, 210.000], mean observation: 0.496 [0.000, 68.000], loss: 0.459832, mean_absolute_error: 0.806945, mean_q: 0.433879, mean_eps: 0.100000\n",
      " 171077/175000: episode: 4775, duration: 1.394s, episode steps: 35, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 130.571 [1.000, 184.000], mean observation: 0.337 [0.000, 70.000], loss: 0.099549, mean_absolute_error: 0.807297, mean_q: 0.377584, mean_eps: 0.100000\n",
      " 171105/175000: episode: 4776, duration: 1.075s, episode steps: 28, steps per second: 26, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 141.679 [4.000, 219.000], mean observation: 0.264 [0.000, 56.000], loss: 0.824411, mean_absolute_error: 0.808016, mean_q: 0.354831, mean_eps: 0.100000\n",
      " 171137/175000: episode: 4777, duration: 1.321s, episode steps: 32, steps per second: 24, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 136.656 [57.000, 173.000], mean observation: 0.279 [0.000, 64.000], loss: 10.648420, mean_absolute_error: 0.857465, mean_q: 0.314591, mean_eps: 0.100000\n",
      " 171191/175000: episode: 4778, duration: 2.062s, episode steps: 54, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 129.278 [19.000, 195.000], mean observation: 0.659 [0.000, 108.000], loss: 0.375876, mean_absolute_error: 0.799082, mean_q: 0.348384, mean_eps: 0.100000\n",
      " 171235/175000: episode: 4779, duration: 1.765s, episode steps: 44, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 135.909 [24.000, 214.000], mean observation: 0.499 [0.000, 88.000], loss: 1.020821, mean_absolute_error: 0.793574, mean_q: 0.346270, mean_eps: 0.100000\n",
      " 171273/175000: episode: 4780, duration: 1.533s, episode steps: 38, steps per second: 25, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 120.289 [26.000, 176.000], mean observation: 0.340 [0.000, 76.000], loss: 1.366284, mean_absolute_error: 0.793882, mean_q: 0.412890, mean_eps: 0.100000\n",
      " 171310/175000: episode: 4781, duration: 1.384s, episode steps: 37, steps per second: 27, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 139.514 [10.000, 216.000], mean observation: 0.399 [0.000, 74.000], loss: 0.147571, mean_absolute_error: 0.781256, mean_q: 0.389678, mean_eps: 0.100000\n",
      " 171349/175000: episode: 4782, duration: 1.529s, episode steps: 39, steps per second: 26, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 140.385 [2.000, 218.000], mean observation: 0.296 [0.000, 78.000], loss: 61.137783, mean_absolute_error: 1.050374, mean_q: 0.265506, mean_eps: 0.100000\n",
      " 171388/175000: episode: 4783, duration: 1.774s, episode steps: 39, steps per second: 22, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 132.000 [5.000, 214.000], mean observation: 0.313 [0.000, 78.000], loss: 0.164172, mean_absolute_error: 0.778949, mean_q: 0.267895, mean_eps: 0.100000\n",
      " 171405/175000: episode: 4784, duration: 0.812s, episode steps: 17, steps per second: 21, episode reward: -1.000, mean reward: -0.059 [-1.000, 0.000], mean action: 133.294 [71.000, 204.000], mean observation: 0.084 [0.000, 34.000], loss: 0.115155, mean_absolute_error: 0.790883, mean_q: 0.375626, mean_eps: 0.100000\n",
      " 171436/175000: episode: 4785, duration: 1.378s, episode steps: 31, steps per second: 23, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 146.484 [59.000, 204.000], mean observation: 0.291 [0.000, 62.000], loss: 1.059881, mean_absolute_error: 0.789286, mean_q: 0.311826, mean_eps: 0.100000\n",
      " 171461/175000: episode: 4786, duration: 1.118s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 166.960 [11.000, 217.000], mean observation: 0.211 [0.000, 50.000], loss: 0.335496, mean_absolute_error: 0.786732, mean_q: 0.369186, mean_eps: 0.100000\n",
      " 171506/175000: episode: 4787, duration: 1.806s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 137.133 [5.000, 218.000], mean observation: 0.547 [0.000, 90.000], loss: 66.913189, mean_absolute_error: 1.084651, mean_q: 0.407003, mean_eps: 0.100000\n",
      " 171527/175000: episode: 4788, duration: 0.848s, episode steps: 21, steps per second: 25, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 101.571 [10.000, 211.000], mean observation: 0.183 [0.000, 42.000], loss: 66.900895, mean_absolute_error: 1.087214, mean_q: 0.390790, mean_eps: 0.100000\n",
      " 171577/175000: episode: 4789, duration: 1.902s, episode steps: 50, steps per second: 26, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 117.660 [10.000, 196.000], mean observation: 0.267 [0.000, 100.000], loss: 2.699298, mean_absolute_error: 0.795954, mean_q: 0.495133, mean_eps: 0.100000\n",
      " 171610/175000: episode: 4790, duration: 1.237s, episode steps: 33, steps per second: 27, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 152.182 [29.000, 207.000], mean observation: 0.389 [0.000, 66.000], loss: 51097.783776, mean_absolute_error: 228.074267, mean_q: 2.626136, mean_eps: 0.100000\n",
      " 171634/175000: episode: 4791, duration: 0.969s, episode steps: 24, steps per second: 25, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 130.792 [119.000, 198.000], mean observation: 0.078 [0.000, 48.000], loss: 9.501136, mean_absolute_error: 0.833447, mean_q: 0.351961, mean_eps: 0.100000\n",
      " 171653/175000: episode: 4792, duration: 0.771s, episode steps: 19, steps per second: 25, episode reward: -1.000, mean reward: -0.053 [-1.000, 0.000], mean action: 142.632 [119.000, 218.000], mean observation: 0.097 [0.000, 38.000], loss: 18.341618, mean_absolute_error: 0.874020, mean_q: 0.441382, mean_eps: 0.100000\n",
      " 171698/175000: episode: 4793, duration: 1.781s, episode steps: 45, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 125.133 [27.000, 223.000], mean observation: 0.290 [0.000, 90.000], loss: 2.034029, mean_absolute_error: 0.807437, mean_q: 0.429678, mean_eps: 0.100000\n",
      " 171727/175000: episode: 4794, duration: 1.097s, episode steps: 29, steps per second: 26, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 122.862 [7.000, 204.000], mean observation: 0.218 [0.000, 58.000], loss: 0.155083, mean_absolute_error: 0.794009, mean_q: 0.455289, mean_eps: 0.100000\n",
      " 171749/175000: episode: 4795, duration: 0.907s, episode steps: 22, steps per second: 24, episode reward: -1.000, mean reward: -0.045 [-1.000, 0.000], mean action: 140.273 [27.000, 191.000], mean observation: 0.124 [0.000, 44.000], loss: 18.414282, mean_absolute_error: 0.874781, mean_q: 0.470245, mean_eps: 0.100000\n",
      " 171785/175000: episode: 4796, duration: 1.404s, episode steps: 36, steps per second: 26, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 143.944 [68.000, 192.000], mean observation: 0.188 [0.000, 72.000], loss: 1.431179, mean_absolute_error: 0.794278, mean_q: 0.464200, mean_eps: 0.100000\n",
      " 171826/175000: episode: 4797, duration: 1.577s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 139.659 [13.000, 191.000], mean observation: 0.226 [0.000, 82.000], loss: 80.233409, mean_absolute_error: 1.146329, mean_q: 0.409573, mean_eps: 0.100000\n",
      " 171847/175000: episode: 4798, duration: 0.829s, episode steps: 21, steps per second: 25, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 98.429 [38.000, 191.000], mean observation: 0.167 [0.000, 42.000], loss: 0.166318, mean_absolute_error: 0.785947, mean_q: 0.247024, mean_eps: 0.100000\n",
      " 171898/175000: episode: 4799, duration: 1.996s, episode steps: 51, steps per second: 26, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 99.529 [22.000, 191.000], mean observation: 0.423 [0.000, 102.000], loss: 24528.666386, mean_absolute_error: 109.934151, mean_q: 1.607160, mean_eps: 0.100000\n",
      " 171934/175000: episode: 4800, duration: 1.405s, episode steps: 36, steps per second: 26, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 130.333 [68.000, 191.000], mean observation: 0.312 [0.000, 72.000], loss: 0.702934, mean_absolute_error: 0.807620, mean_q: 0.231815, mean_eps: 0.100000\n",
      " 171977/175000: episode: 4801, duration: 1.635s, episode steps: 43, steps per second: 26, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 99.744 [62.000, 204.000], mean observation: 0.563 [0.000, 86.000], loss: 9.067812, mean_absolute_error: 0.851483, mean_q: 0.189851, mean_eps: 0.100000\n",
      " 172016/175000: episode: 4802, duration: 1.595s, episode steps: 39, steps per second: 24, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 93.077 [52.000, 191.000], mean observation: 0.284 [0.000, 78.000], loss: 2.058925, mean_absolute_error: 0.819644, mean_q: 0.207615, mean_eps: 0.100000\n",
      " 172040/175000: episode: 4803, duration: 1.026s, episode steps: 24, steps per second: 23, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 101.667 [52.000, 191.000], mean observation: 0.184 [0.000, 48.000], loss: 0.153787, mean_absolute_error: 0.805299, mean_q: 0.248377, mean_eps: 0.100000\n",
      " 172086/175000: episode: 4804, duration: 1.833s, episode steps: 46, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 152.065 [52.000, 195.000], mean observation: 0.369 [0.000, 92.000], loss: 0.077067, mean_absolute_error: 0.801067, mean_q: 0.245538, mean_eps: 0.100000\n",
      " 172117/175000: episode: 4805, duration: 1.221s, episode steps: 31, steps per second: 25, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 182.194 [12.000, 202.000], mean observation: 0.093 [0.000, 62.000], loss: 0.101600, mean_absolute_error: 0.802784, mean_q: 0.448405, mean_eps: 0.100000\n",
      " 172158/175000: episode: 4806, duration: 1.544s, episode steps: 41, steps per second: 27, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 130.146 [58.000, 202.000], mean observation: 0.196 [0.000, 82.000], loss: 0.093213, mean_absolute_error: 0.786046, mean_q: 0.373804, mean_eps: 0.100000\n",
      " 172181/175000: episode: 4807, duration: 0.901s, episode steps: 23, steps per second: 26, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 128.130 [7.000, 207.000], mean observation: 0.109 [0.000, 46.000], loss: 0.095870, mean_absolute_error: 0.783937, mean_q: 0.453480, mean_eps: 0.100000\n",
      " 172215/175000: episode: 4808, duration: 1.290s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 156.471 [64.000, 204.000], mean observation: 0.263 [0.000, 68.000], loss: 67.311270, mean_absolute_error: 1.077533, mean_q: 0.340417, mean_eps: 0.100000\n",
      " 172239/175000: episode: 4809, duration: 0.967s, episode steps: 24, steps per second: 25, episode reward: -1.000, mean reward: -0.042 [-1.000, 0.000], mean action: 110.583 [38.000, 191.000], mean observation: 0.143 [0.000, 48.000], loss: 17.950079, mean_absolute_error: 0.853970, mean_q: 0.401788, mean_eps: 0.100000\n",
      " 172266/175000: episode: 4810, duration: 1.346s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 110.074 [16.000, 202.000], mean observation: 0.096 [0.000, 54.000], loss: 1.137724, mean_absolute_error: 0.773950, mean_q: 0.367637, mean_eps: 0.100000\n",
      " 172291/175000: episode: 4811, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 80.480 [7.000, 202.000], mean observation: 0.112 [0.000, 50.000], loss: 120.941390, mean_absolute_error: 1.313425, mean_q: 0.421225, mean_eps: 0.100000\n",
      " 172322/175000: episode: 4812, duration: 1.431s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 68.903 [7.000, 176.000], mean observation: 0.109 [0.000, 62.000], loss: 0.216355, mean_absolute_error: 0.771770, mean_q: 0.524278, mean_eps: 0.100000\n",
      " 172350/175000: episode: 4813, duration: 1.195s, episode steps: 28, steps per second: 23, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 89.321 [7.000, 191.000], mean observation: 0.190 [0.000, 56.000], loss: 2.534148, mean_absolute_error: 0.787915, mean_q: 0.503151, mean_eps: 0.100000\n",
      " 172402/175000: episode: 4814, duration: 2.145s, episode steps: 52, steps per second: 24, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 107.058 [7.000, 197.000], mean observation: 0.450 [0.000, 104.000], loss: 1.234781, mean_absolute_error: 0.776801, mean_q: 0.438228, mean_eps: 0.100000\n",
      " 172445/175000: episode: 4815, duration: 2.013s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 118.465 [7.000, 192.000], mean observation: 0.347 [0.000, 86.000], loss: 0.121793, mean_absolute_error: 0.770761, mean_q: 0.319447, mean_eps: 0.100000\n",
      " 172496/175000: episode: 4816, duration: 2.193s, episode steps: 51, steps per second: 23, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 166.078 [1.000, 211.000], mean observation: 0.543 [0.000, 102.000], loss: 7.987081, mean_absolute_error: 0.797805, mean_q: 0.338336, mean_eps: 0.100000\n",
      " 172527/175000: episode: 4817, duration: 1.301s, episode steps: 31, steps per second: 24, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 142.645 [27.000, 211.000], mean observation: 0.209 [0.000, 62.000], loss: 24.550784, mean_absolute_error: 0.875651, mean_q: 0.373577, mean_eps: 0.100000\n",
      " 172561/175000: episode: 4818, duration: 1.436s, episode steps: 34, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 117.382 [27.000, 211.000], mean observation: 0.287 [0.000, 68.000], loss: 0.091857, mean_absolute_error: 0.769036, mean_q: 0.466855, mean_eps: 0.100000\n",
      " 172596/175000: episode: 4819, duration: 1.474s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 139.800 [12.000, 212.000], mean observation: 0.331 [0.000, 70.000], loss: 0.116252, mean_absolute_error: 0.778766, mean_q: 0.330866, mean_eps: 0.100000\n",
      " 172619/175000: episode: 4820, duration: 0.975s, episode steps: 23, steps per second: 24, episode reward: -1.000, mean reward: -0.043 [-1.000, 0.000], mean action: 168.957 [0.000, 212.000], mean observation: 0.209 [0.000, 46.000], loss: 2.914585, mean_absolute_error: 0.795827, mean_q: 0.353932, mean_eps: 0.100000\n",
      " 172651/175000: episode: 4821, duration: 1.424s, episode steps: 32, steps per second: 22, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 145.000 [0.000, 212.000], mean observation: 0.327 [0.000, 64.000], loss: 0.088267, mean_absolute_error: 0.785297, mean_q: 0.332784, mean_eps: 0.100000\n",
      " 172677/175000: episode: 4822, duration: 1.187s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 152.808 [44.000, 212.000], mean observation: 0.279 [0.000, 52.000], loss: 6.807835, mean_absolute_error: 0.821561, mean_q: 0.331432, mean_eps: 0.100000\n",
      " 172708/175000: episode: 4823, duration: 1.255s, episode steps: 31, steps per second: 25, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 137.645 [43.000, 212.000], mean observation: 0.332 [0.000, 62.000], loss: 1.900135, mean_absolute_error: 0.792701, mean_q: 0.315865, mean_eps: 0.100000\n",
      " 172744/175000: episode: 4824, duration: 1.577s, episode steps: 36, steps per second: 23, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 119.833 [7.000, 212.000], mean observation: 0.377 [0.000, 72.000], loss: 0.353966, mean_absolute_error: 0.791345, mean_q: 0.370961, mean_eps: 0.100000\n",
      " 172776/175000: episode: 4825, duration: 1.380s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 132.438 [36.000, 195.000], mean observation: 0.318 [0.000, 64.000], loss: 21.101715, mean_absolute_error: 0.882650, mean_q: 0.318922, mean_eps: 0.100000\n",
      " 172806/175000: episode: 4826, duration: 1.267s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 145.567 [7.000, 213.000], mean observation: 0.341 [0.000, 60.000], loss: 0.448015, mean_absolute_error: 0.798124, mean_q: 0.442012, mean_eps: 0.100000\n",
      " 172838/175000: episode: 4827, duration: 1.318s, episode steps: 32, steps per second: 24, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 91.812 [10.000, 209.000], mean observation: 0.321 [0.000, 64.000], loss: 0.164526, mean_absolute_error: 0.802869, mean_q: 0.395943, mean_eps: 0.100000\n",
      " 172863/175000: episode: 4828, duration: 1.006s, episode steps: 25, steps per second: 25, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 103.720 [11.000, 213.000], mean observation: 0.243 [0.000, 50.000], loss: 33.409360, mean_absolute_error: 0.945231, mean_q: 0.344603, mean_eps: 0.100000\n",
      " 172905/175000: episode: 4829, duration: 1.782s, episode steps: 42, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 95.738 [2.000, 213.000], mean observation: 0.470 [0.000, 84.000], loss: 0.138453, mean_absolute_error: 0.807750, mean_q: 0.320474, mean_eps: 0.100000\n",
      " 172925/175000: episode: 4830, duration: 0.896s, episode steps: 20, steps per second: 22, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 108.900 [66.000, 222.000], mean observation: 0.216 [0.000, 40.000], loss: 213.877720, mean_absolute_error: 1.756543, mean_q: 0.390386, mean_eps: 0.100000\n",
      " 172951/175000: episode: 4831, duration: 1.122s, episode steps: 26, steps per second: 23, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 144.231 [26.000, 222.000], mean observation: 0.272 [0.000, 52.000], loss: 0.111595, mean_absolute_error: 0.801989, mean_q: 0.354920, mean_eps: 0.100000\n",
      " 172992/175000: episode: 4832, duration: 1.685s, episode steps: 41, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 131.195 [19.000, 221.000], mean observation: 0.535 [0.000, 82.000], loss: 1.716206, mean_absolute_error: 0.810360, mean_q: 0.316344, mean_eps: 0.100000\n",
      " 173022/175000: episode: 4833, duration: 1.248s, episode steps: 30, steps per second: 24, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 158.300 [111.000, 191.000], mean observation: 0.221 [0.000, 60.000], loss: 2.808070, mean_absolute_error: 0.819851, mean_q: 0.354676, mean_eps: 0.100000\n",
      " 173057/175000: episode: 4834, duration: 1.434s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 161.429 [111.000, 203.000], mean observation: 0.304 [0.000, 70.000], loss: 0.101595, mean_absolute_error: 0.789339, mean_q: 0.314778, mean_eps: 0.100000\n",
      " 173084/175000: episode: 4835, duration: 1.186s, episode steps: 27, steps per second: 23, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 149.630 [53.000, 183.000], mean observation: 0.239 [0.000, 54.000], loss: 28.234457, mean_absolute_error: 0.921230, mean_q: 0.287249, mean_eps: 0.100000\n",
      " 173124/175000: episode: 4836, duration: 1.649s, episode steps: 40, steps per second: 24, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 151.775 [29.000, 183.000], mean observation: 0.394 [0.000, 80.000], loss: 3.853963, mean_absolute_error: 0.818361, mean_q: 0.323253, mean_eps: 0.100000\n",
      " 173168/175000: episode: 4837, duration: 1.978s, episode steps: 44, steps per second: 22, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 164.773 [43.000, 221.000], mean observation: 0.475 [0.000, 88.000], loss: 0.181204, mean_absolute_error: 0.797311, mean_q: 0.373247, mean_eps: 0.100000\n",
      " 173206/175000: episode: 4838, duration: 1.539s, episode steps: 38, steps per second: 25, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 137.658 [39.000, 221.000], mean observation: 0.419 [0.000, 76.000], loss: 27.069514, mean_absolute_error: 0.926199, mean_q: 0.384402, mean_eps: 0.100000\n",
      " 173241/175000: episode: 4839, duration: 1.434s, episode steps: 35, steps per second: 24, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 151.514 [23.000, 221.000], mean observation: 0.336 [0.000, 70.000], loss: 0.631977, mean_absolute_error: 0.825782, mean_q: 0.398987, mean_eps: 0.100000\n",
      " 173301/175000: episode: 4840, duration: 2.432s, episode steps: 60, steps per second: 25, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 102.350 [10.000, 223.000], mean observation: 0.741 [0.000, 120.000], loss: 3.826705, mean_absolute_error: 0.867639, mean_q: 0.640171, mean_eps: 0.100000\n",
      " 173333/175000: episode: 4841, duration: 1.378s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 67.938 [10.000, 199.000], mean observation: 0.185 [0.000, 64.000], loss: 1.400576, mean_absolute_error: 0.885291, mean_q: 0.741365, mean_eps: 0.100000\n",
      " 173360/175000: episode: 4842, duration: 1.135s, episode steps: 27, steps per second: 24, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 89.296 [10.000, 199.000], mean observation: 0.183 [0.000, 54.000], loss: 1.202340, mean_absolute_error: 0.909851, mean_q: 0.862157, mean_eps: 0.100000\n",
      " 173401/175000: episode: 4843, duration: 1.656s, episode steps: 41, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 73.512 [28.000, 199.000], mean observation: 0.238 [0.000, 82.000], loss: 5.284052, mean_absolute_error: 0.966694, mean_q: 0.759488, mean_eps: 0.100000\n",
      " 173447/175000: episode: 4844, duration: 1.793s, episode steps: 46, steps per second: 26, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 64.587 [27.000, 207.000], mean observation: 0.621 [0.000, 92.000], loss: 15.788327, mean_absolute_error: 1.041267, mean_q: 0.643672, mean_eps: 0.100000\n",
      " 173497/175000: episode: 4845, duration: 2.039s, episode steps: 50, steps per second: 25, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 87.900 [12.000, 219.000], mean observation: 0.587 [0.000, 100.000], loss: 1.404954, mean_absolute_error: 0.984169, mean_q: 0.698051, mean_eps: 0.100000\n",
      " 173538/175000: episode: 4846, duration: 1.583s, episode steps: 41, steps per second: 26, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 111.634 [12.000, 202.000], mean observation: 0.319 [0.000, 82.000], loss: 0.816984, mean_absolute_error: 1.005261, mean_q: 0.734096, mean_eps: 0.100000\n",
      " 173571/175000: episode: 4847, duration: 1.312s, episode steps: 33, steps per second: 25, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 62.758 [12.000, 212.000], mean observation: 0.304 [0.000, 66.000], loss: 0.353538, mean_absolute_error: 1.018125, mean_q: 0.595370, mean_eps: 0.100000\n",
      " 173617/175000: episode: 4848, duration: 1.867s, episode steps: 46, steps per second: 25, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 59.370 [12.000, 172.000], mean observation: 0.515 [0.000, 92.000], loss: 3296.073355, mean_absolute_error: 15.782140, mean_q: 2.045535, mean_eps: 0.100000\n",
      " 173660/175000: episode: 4849, duration: 1.742s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 58.488 [12.000, 201.000], mean observation: 0.494 [0.000, 86.000], loss: 0.196204, mean_absolute_error: 0.998946, mean_q: 0.589172, mean_eps: 0.100000\n",
      " 173705/175000: episode: 4850, duration: 1.933s, episode steps: 45, steps per second: 23, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 65.822 [12.000, 223.000], mean observation: 0.518 [0.000, 90.000], loss: 0.163925, mean_absolute_error: 0.981616, mean_q: 0.445390, mean_eps: 0.100000\n",
      " 173771/175000: episode: 4851, duration: 2.601s, episode steps: 66, steps per second: 25, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 70.924 [10.000, 214.000], mean observation: 0.975 [0.000, 132.000], loss: 1.114500, mean_absolute_error: 0.964969, mean_q: 0.295869, mean_eps: 0.100000\n",
      " 173813/175000: episode: 4852, duration: 1.735s, episode steps: 42, steps per second: 24, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 106.738 [12.000, 222.000], mean observation: 0.252 [0.000, 84.000], loss: 0.167379, mean_absolute_error: 0.936364, mean_q: 0.308988, mean_eps: 0.100000\n",
      " 173856/175000: episode: 4853, duration: 1.708s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 62.349 [12.000, 220.000], mean observation: 0.397 [0.000, 86.000], loss: 0.140854, mean_absolute_error: 0.927036, mean_q: 0.301269, mean_eps: 0.100000\n",
      " 173882/175000: episode: 4854, duration: 1.060s, episode steps: 26, steps per second: 25, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 73.615 [21.000, 220.000], mean observation: 0.256 [0.000, 52.000], loss: 432.343676, mean_absolute_error: 3.273870, mean_q: 5.489378, mean_eps: 0.100000\n",
      " 173914/175000: episode: 4855, duration: 1.249s, episode steps: 32, steps per second: 26, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 163.500 [12.000, 218.000], mean observation: 0.124 [0.000, 64.000], loss: 35.756470, mean_absolute_error: 1.075611, mean_q: 0.539653, mean_eps: 0.100000\n",
      " 173942/175000: episode: 4856, duration: 1.103s, episode steps: 28, steps per second: 25, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 82.714 [6.000, 223.000], mean observation: 0.313 [0.000, 56.000], loss: 7.295796, mean_absolute_error: 0.950207, mean_q: 0.581462, mean_eps: 0.100000\n",
      " 173957/175000: episode: 4857, duration: 0.632s, episode steps: 15, steps per second: 24, episode reward: -1.000, mean reward: -0.067 [-1.000, 0.000], mean action: 160.333 [96.000, 223.000], mean observation: 0.098 [0.000, 30.000], loss: 0.090477, mean_absolute_error: 0.909846, mean_q: 0.590049, mean_eps: 0.100000\n",
      " 174000/175000: episode: 4858, duration: 1.694s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 143.953 [11.000, 223.000], mean observation: 0.412 [0.000, 86.000], loss: 2.257458, mean_absolute_error: 0.906952, mean_q: 0.497584, mean_eps: 0.100000\n",
      " 174045/175000: episode: 4859, duration: 1.842s, episode steps: 45, steps per second: 24, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 140.111 [21.000, 223.000], mean observation: 0.441 [0.000, 90.000], loss: 25.603784, mean_absolute_error: 0.998857, mean_q: 0.561578, mean_eps: 0.100000\n",
      " 174088/175000: episode: 4860, duration: 1.622s, episode steps: 43, steps per second: 27, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 140.512 [16.000, 223.000], mean observation: 0.495 [0.000, 86.000], loss: 32.546565, mean_absolute_error: 1.027187, mean_q: 0.518644, mean_eps: 0.100000\n",
      " 174130/175000: episode: 4861, duration: 1.689s, episode steps: 42, steps per second: 25, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 99.762 [21.000, 223.000], mean observation: 0.382 [0.000, 84.000], loss: 0.148499, mean_absolute_error: 0.887947, mean_q: 0.416445, mean_eps: 0.100000\n",
      " 174166/175000: episode: 4862, duration: 1.450s, episode steps: 36, steps per second: 25, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 137.167 [21.000, 223.000], mean observation: 0.321 [0.000, 72.000], loss: 0.105481, mean_absolute_error: 0.886527, mean_q: 0.313162, mean_eps: 0.100000\n",
      " 174219/175000: episode: 4863, duration: 2.032s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 128.981 [20.000, 215.000], mean observation: 0.621 [0.000, 106.000], loss: 0.133662, mean_absolute_error: 0.898131, mean_q: 0.359103, mean_eps: 0.100000\n",
      " 174268/175000: episode: 4864, duration: 2.048s, episode steps: 49, steps per second: 24, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 77.327 [21.000, 215.000], mean observation: 0.470 [0.000, 98.000], loss: 0.567729, mean_absolute_error: 0.898289, mean_q: 0.286421, mean_eps: 0.100000\n",
      " 174297/175000: episode: 4865, duration: 1.180s, episode steps: 29, steps per second: 25, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 64.655 [21.000, 154.000], mean observation: 0.128 [0.000, 58.000], loss: 0.516515, mean_absolute_error: 0.890948, mean_q: 0.244901, mean_eps: 0.100000\n",
      " 174331/175000: episode: 4866, duration: 1.314s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 120.000 [21.000, 215.000], mean observation: 0.278 [0.000, 68.000], loss: 0.075693, mean_absolute_error: 0.885914, mean_q: 0.168258, mean_eps: 0.100000\n",
      " 174385/175000: episode: 4867, duration: 2.235s, episode steps: 54, steps per second: 24, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 160.611 [3.000, 221.000], mean observation: 0.598 [0.000, 108.000], loss: 13.329325, mean_absolute_error: 0.947654, mean_q: 0.102058, mean_eps: 0.100000\n",
      " 174433/175000: episode: 4868, duration: 1.856s, episode steps: 48, steps per second: 26, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 165.542 [3.000, 221.000], mean observation: 0.491 [0.000, 96.000], loss: 4.131916, mean_absolute_error: 0.902447, mean_q: 0.089602, mean_eps: 0.100000\n",
      " 174486/175000: episode: 4869, duration: 2.022s, episode steps: 53, steps per second: 26, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 64.415 [21.000, 191.000], mean observation: 0.543 [0.000, 106.000], loss: 3.695922, mean_absolute_error: 0.889822, mean_q: 0.078346, mean_eps: 0.100000\n",
      " 174520/175000: episode: 4870, duration: 1.332s, episode steps: 34, steps per second: 26, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 76.412 [25.000, 200.000], mean observation: 0.262 [0.000, 68.000], loss: 0.386380, mean_absolute_error: 0.869039, mean_q: 0.135576, mean_eps: 0.100000\n",
      " 174569/175000: episode: 4871, duration: 2.021s, episode steps: 49, steps per second: 24, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 63.245 [18.000, 189.000], mean observation: 0.465 [0.000, 98.000], loss: 6.800269, mean_absolute_error: 0.901599, mean_q: 0.255886, mean_eps: 0.100000\n",
      " 174601/175000: episode: 4872, duration: 1.292s, episode steps: 32, steps per second: 25, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 94.344 [25.000, 189.000], mean observation: 0.293 [0.000, 64.000], loss: 0.137794, mean_absolute_error: 0.881650, mean_q: 0.322293, mean_eps: 0.100000\n",
      " 174637/175000: episode: 4873, duration: 1.420s, episode steps: 36, steps per second: 25, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 110.694 [33.000, 208.000], mean observation: 0.345 [0.000, 72.000], loss: 2.040972, mean_absolute_error: 0.889029, mean_q: 0.465575, mean_eps: 0.100000\n",
      " 174670/175000: episode: 4874, duration: 1.229s, episode steps: 33, steps per second: 27, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 83.879 [33.000, 202.000], mean observation: 0.270 [0.000, 66.000], loss: 4.456309, mean_absolute_error: 0.892913, mean_q: 0.451376, mean_eps: 0.100000\n",
      " 174721/175000: episode: 4875, duration: 2.063s, episode steps: 51, steps per second: 25, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 114.941 [33.000, 189.000], mean observation: 0.462 [0.000, 102.000], loss: 0.369816, mean_absolute_error: 0.863120, mean_q: 0.341565, mean_eps: 0.100000\n",
      " 174759/175000: episode: 4876, duration: 1.463s, episode steps: 38, steps per second: 26, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 160.500 [33.000, 211.000], mean observation: 0.266 [0.000, 76.000], loss: 0.085133, mean_absolute_error: 0.866423, mean_q: 0.458974, mean_eps: 0.100000\n",
      " 174788/175000: episode: 4877, duration: 1.206s, episode steps: 29, steps per second: 24, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 136.621 [46.000, 189.000], mean observation: 0.183 [0.000, 58.000], loss: 0.115012, mean_absolute_error: 0.847566, mean_q: 0.380788, mean_eps: 0.100000\n",
      " 174813/175000: episode: 4878, duration: 1.041s, episode steps: 25, steps per second: 24, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 120.320 [13.000, 213.000], mean observation: 0.236 [0.000, 50.000], loss: 0.192087, mean_absolute_error: 0.838429, mean_q: 0.357407, mean_eps: 0.100000\n",
      " 174842/175000: episode: 4879, duration: 1.139s, episode steps: 29, steps per second: 25, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 120.724 [33.000, 218.000], mean observation: 0.309 [0.000, 58.000], loss: 6.427044, mean_absolute_error: 0.861638, mean_q: 0.347207, mean_eps: 0.100000\n",
      " 174885/175000: episode: 4880, duration: 1.731s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 87.814 [11.000, 195.000], mean observation: 0.499 [0.000, 86.000], loss: 1.791559, mean_absolute_error: 0.861799, mean_q: 0.205183, mean_eps: 0.100000\n",
      " 174894/175000: episode: 4881, duration: 0.318s, episode steps: 9, steps per second: 28, episode reward: -1.000, mean reward: -0.111 [-1.000, 0.000], mean action: 126.889 [29.000, 197.000], mean observation: 0.042 [0.000, 18.000], loss: 0.500019, mean_absolute_error: 0.853906, mean_q: 0.131513, mean_eps: 0.100000\n",
      " 174928/175000: episode: 4882, duration: 1.380s, episode steps: 34, steps per second: 25, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 144.676 [29.000, 221.000], mean observation: 0.315 [0.000, 68.000], loss: 0.243343, mean_absolute_error: 0.871459, mean_q: 0.187529, mean_eps: 0.100000\n",
      " 174971/175000: episode: 4883, duration: 1.704s, episode steps: 43, steps per second: 25, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 152.419 [3.000, 211.000], mean observation: 0.347 [0.000, 86.000], loss: 302.914660, mean_absolute_error: 2.372906, mean_q: 1.691216, mean_eps: 0.100000\n",
      "done, took 3574.997 seconds\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ _ _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ X _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X _ _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O _ _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ O _ O _ _ _ X X _ \n",
      "_ _ _ _ _ O O _ O _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ X _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ O O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ O _ _ O _ _ \n",
      "_ O _ _ _ _ O X _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ X _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ O _ _ _ \n",
      "_ _ _ _ O _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ X \n",
      "_ _ _ _ _ _ _ O _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ X O _ O O _ X _ O _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 30\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 45\n",
      "Episode 2: reward: -1.000, steps: 34\n",
      "Episode 3: reward: -1.000, steps: 34\n",
      "Episode 4: reward: -1.000, steps: 32\n",
      "Episode 5: reward: -1.000, steps: 47\n",
      "Episode 6: reward: -1.000, steps: 38\n",
      "Episode 7: reward: -1.000, steps: 16\n",
      "Episode 8: reward: -1.000, steps: 21\n",
      "Episode 9: reward: -1.000, steps: 31\n",
      "Episode 10: reward: -1.000, steps: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ccba2ca58>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=175000, log_interval=10000, verbose = 2 )\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights('latest_dqn', overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 10 episodes.\n",
    "dqn.test(env, nb_episodes=1, visualize=True)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarsa = SarsaAgent(model=model_policy, nb_actions=225)\n",
    "sarsa.compile(Adam(), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarsa.load_weights(\"sarsa_Renju9_weights.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = RenjuTEST(1, 'kn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "_ O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O _ _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ O _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ O _ O O _ _ _ O O _ _ _ _ \n",
      "O O O _ _ O _ O _ _ _ _ O _ _ \n",
      "_ O _ _ _ O _ _ _ O _ _ _ _ _ \n",
      "_ _ O _ O O _ _ O _ _ _ _ _ _ \n",
      "_ O O O O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ O O _ _ \n",
      "_ _ _ _ _ O _ O _ _ _ _ _ _ O \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ O \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ O O _ _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ _ _ \n",
      "_ O _ _ O _ _ _ O _ _ _ _ X _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 48\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 45\n",
      "Episode 2: reward: -1.000, steps: 44\n",
      "Episode 3: reward: -1.000, steps: 25\n",
      "Episode 4: reward: -1.000, steps: 41\n",
      "Episode 5: reward: -1.000, steps: 22\n",
      "Episode 6: reward: -1.000, steps: 20\n",
      "Episode 7: reward: -1.000, steps: 33\n",
      "Episode 8: reward: -1.000, steps: 19\n",
      "Episode 9: reward: -1.000, steps: 45\n",
      "Episode 10: reward: -1.000, steps: 56\n",
      "Opponent: kn\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "_ _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ _ O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O _ _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ _ _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ _ _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ _ O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ _ _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ O _ _ _ _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ _ _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O _ O _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ O _ O _ X _ _ _ O _ _ _ O _ \n",
      "_ O _ _ _ _ _ _ X _ _ _ _ _ _ \n",
      "_ _ O _ _ O _ _ _ _ _ O X _ _ \n",
      "_ _ O _ O O _ _ _ _ _ _ _ O O \n",
      "_ O O _ O _ _ _ X O _ _ _ O _ _ \n",
      "_ _ O _ _ _ _ O _ O _ O _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X O _ _ O _ _ _ _ O _ _ _ \n",
      "O _ _ _ _ O O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O O _ _ _ _ _ _ _ X _ \n",
      "O _ O O _ _ _ O O _ _ O _ _ _ \n",
      "_ _ _ O O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ O _ _ _ _ _ _ \n",
      "O X O O O _ _ O _ X _ _ O _ _ \n",
      "_ O O _ O O _ _ _ _ O _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "Episode 1: reward: -1.000, steps: 53\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -1.000, steps: 34\n",
      "Episode 2: reward: -1.000, steps: 36\n",
      "Episode 3: reward: -1.000, steps: 26\n",
      "Episode 4: reward: -1.000, steps: 36\n",
      "Episode 5: reward: -1.000, steps: 39\n",
      "Episode 6: reward: -1.000, steps: 34\n",
      "Episode 7: reward: -1.000, steps: 39\n",
      "Episode 8: reward: -1.000, steps: 41\n",
      "Episode 9: reward: -1.000, steps: 34\n",
      "Episode 10: reward: -1.000, steps: 57\n",
      "Opponent: kn\n",
      "Testing for 1 episodes ...\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ O _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ _ _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ _ _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ _ X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n",
      "_ _ _ _ _ X O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ O _ O X _ _ \n",
      "_ _ _ _ _ _ _ O O _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ O _ _ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ O _ _ _ _ _ _ _ O _ _ \n",
      "O _ X _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "O _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ O _ _ _ _ _ O _ _ _ _ _ X _ \n",
      "_ _ _ _ _ _ _ O _ _ _ O _ _ _ \n",
      "_ _ _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ O _ _ _ _ _ _ _ _ \n",
      "_ X _ _ O _ _ _ _ _ O _ _ _ _ \n",
      "_ O _ _ _ O _ _ _ _ _ _ _ _ _ \n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d27f3e4b2383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRenjuTEST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kn'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'neural'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3e186e96ea69>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, mode)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 320\u001b[0;31m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/axcel/.local/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    dqn.fit(env, nb_steps=17500, log_interval=10000, verbose = 0 )\n",
    "\n",
    "    # After training is done, we save the final weights.\n",
    "    dqn.save_weights('dqn', overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    dqn.test(env, nb_episodes=1, visualize=True)\n",
    "    env = RenjuTEST(1, 'kn' if random.randint(0,1) == 0 else 'neural')\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "    print('Opponent:', env.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
